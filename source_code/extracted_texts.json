{"Introduction to Information Retrieval": "Online edition (c)\n2009 Cambridge UPAn\nIntroduction\nto\nInformation\nRetrieval\nDraft of April 1, 2009\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPAn\nIntroduction\nto\nInformation\nRetrieval\nChristopher D. Manning\nPrabhakar Raghavan\nHinrich Sch\u00fctze\nCambridge University Press\nCambridge, England\nOnline edition (c)\n2009 Cambridge UPDRAFT!\nDONOT DISTRIBUTE WITHOUT PRIORPERMISSION\n\u00a92009 Cambridge University Press\nByChristopher D. Manning, Prabhakar Raghavan &Hinrich Sch \u00fctze\nPrinted onApril 1,2009\nWebsite: http://www.informationretrieval.org/\nComments, corrections, andother feedback most welcome at:\ninformationretrieval@yahoogroups.com\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . v\nBrief Contents\n1 Boolean retrieval 1\n2 The term vocabulary and postings lists 19\n3 Dictionaries and tolerant retrieval 49\n4 Index construction 67\n5 Index compression 85\n6 Scoring, term weighting and the vector space model 109\n7 Computing scores in a complete search system 135\n8 Evaluation in information retrieval 151\n9 Relevance feedback and query expansion 177\n10 XML retrieval 195\n11 Probabilistic information retrieval 219\n12 Language models for information retrieval 237\n13 Text classi\ufb01cation and Naive Bayes 253\n14 Vector space classi\ufb01cation 289\n15 Support vector machines and machine learning on document s 319\n16 Flat clustering 349\n17 Hierarchical clustering 377\n18 Matrix decompositions and latent semantic indexing 403\n19 Web search basics 421\n20 Web crawling and indexes 443\n21 Link analysis 461\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . vii\nContents\nList of Tables xv\nList of Figures xix\nTable of Notation xxvii\nPreface xxxi\n1Boolean retrieval 1\n1.1 An example information retrieval problem 3\n1.2 A \ufb01rst take at building an inverted index 6\n1.3 Processing Boolean queries 10\n1.4 The extended Boolean model versus ranked retrieval 14\n1.5 References and further reading 17\n2The term vocabulary and postings lists 19\n2.1 Document delineation and character sequence decoding 19\n2.1.1 Obtaining the character sequence in a document 19\n2.1.2 Choosing a document unit 20\n2.2 Determining the vocabulary of terms 22\n2.2.1 Tokenization 22\n2.2.2 Dropping common terms: stop words 27\n2.2.3 Normalization (equivalence classing of terms) 28\n2.2.4 Stemming and lemmatization 32\n2.3 Faster postings list intersection via skip pointers 36\n2.4 Positional postings and phrase queries 39\n2.4.1 Biword indexes 39\n2.4.2 Positional indexes 41\n2.4.3 Combination schemes 43\n2.5 References and further reading 45\nOnline edition (c)\n2009 Cambridge UPviii Contents\n3Dictionaries and tolerant retrieval 49\n3.1 Search structures for dictionaries 49\n3.2 Wildcard queries 51\n3.2.1 General wildcard queries 53\n3.2.2 k-gram indexes for wildcard queries 54\n3.3 Spelling correction 56\n3.3.1 Implementing spelling correction 57\n3.3.2 Forms of spelling correction 57\n3.3.3 Edit distance 58\n3.3.4 k-gram indexes for spelling correction 60\n3.3.5 Context sensitive spelling correction 62\n3.4 Phonetic correction 63\n3.5 References and further reading 65\n4Index construction 67\n4.1 Hardware basics 68\n4.2 Blocked sort-based indexing 69\n4.3 Single-pass in-memory indexing 73\n4.4 Distributed indexing 74\n4.5 Dynamic indexing 78\n4.6 Other types of indexes 80\n4.7 References and further reading 83\n5Index compression 85\n5.1 Statistical properties of terms in information retriev al 86\n5.1.1 Heaps\u2019 law: Estimating the number of terms 88\n5.1.2 Zipf\u2019s law: Modeling the distribution of terms 89\n5.2 Dictionary compression 90\n5.2.1 Dictionary as a string 91\n5.2.2 Blocked storage 92\n5.3 Postings \ufb01le compression 95\n5.3.1 Variable byte codes 96\n5.3.2 \u03b3codes 98\n5.4 References and further reading 105\n6Scoring, term weighting and the vector space model 109\n6.1 Parametric and zone indexes 110\n6.1.1 Weighted zone scoring 112\n6.1.2 Learning weights 113\n6.1.3 The optimal weight g 115\n6.2 Term frequency and weighting 117\n6.2.1 Inverse document frequency 117\n6.2.2 Tf-idf weighting 118\nOnline edition (c)\n2009 Cambridge UPContents ix\n6.3 The vector space model for scoring 120\n6.3.1 Dot products 120\n6.3.2 Queries as vectors 123\n6.3.3 Computing vector scores 124\n6.4 Variant tf-idf functions 126\n6.4.1 Sublinear tf scaling 126\n6.4.2 Maximum tf normalization 127\n6.4.3 Document and query weighting schemes 128\n6.4.4 Pivoted normalized document length 129\n6.5 References and further reading 133\n7Computing scores in a complete search system 135\n7.1 Ef\ufb01cient scoring and ranking 135\n7.1.1 Inexact top Kdocument retrieval 137\n7.1.2 Index elimination 137\n7.1.3 Champion lists 138\n7.1.4 Static quality scores and ordering 138\n7.1.5 Impact ordering 140\n7.1.6 Cluster pruning 141\n7.2 Components of an information retrieval system 143\n7.2.1 Tiered indexes 143\n7.2.2 Query-term proximity 144\n7.2.3 Designing parsing and scoring functions 145\n7.2.4 Putting it all together 146\n7.3 Vector space scoring and query operator interaction 147\n7.4 References and further reading 149\n8Evaluation in information retrieval 151\n8.1 Information retrieval system evaluation 152\n8.2 Standard test collections 153\n8.3 Evaluation of unranked retrieval sets 154\n8.4 Evaluation of ranked retrieval results 158\n8.5 Assessing relevance 164\n8.5.1 Critiques and justi\ufb01cations of the concept of\nrelevance 166\n8.6 A broader perspective: System quality and user utility 168\n8.6.1 System issues 168\n8.6.2 User utility 169\n8.6.3 Re\ufb01ning a deployed system 170\n8.7 Results snippets 170\n8.8 References and further reading 173\n9Relevance feedback and query expansion 177\nOnline edition (c)\n2009 Cambridge UPx Contents\n9.1 Relevance feedback and pseudo relevance feedback 178\n9.1.1 The Rocchio algorithm for relevance feedback 178\n9.1.2 Probabilistic relevance feedback 183\n9.1.3 When does relevance feedback work? 183\n9.1.4 Relevance feedback on the web 185\n9.1.5 Evaluation of relevance feedback strategies 186\n9.1.6 Pseudo relevance feedback 187\n9.1.7 Indirect relevance feedback 187\n9.1.8 Summary 188\n9.2 Global methods for query reformulation 189\n9.2.1 Vocabulary tools for query reformulation 189\n9.2.2 Query expansion 189\n9.2.3 Automatic thesaurus generation 192\n9.3 References and further reading 193\n10XML retrieval 195\n10.1 Basic XML concepts 197\n10.2 Challenges in XML retrieval 201\n10.3 A vector space model for XML retrieval 206\n10.4 Evaluation of XML retrieval 210\n10.5 Text-centric vs. data-centric XML retrieval 214\n10.6 References and further reading 216\n10.7 Exercises 217\n11Probabilistic information retrieval 219\n11.1 Review of basic probability theory 220\n11.2 The Probability Ranking Principle 221\n11.2.1 The 1/0 loss case 221\n11.2.2 The PRP with retrieval costs 222\n11.3 The Binary Independence Model 222\n11.3.1 Deriving a ranking function for query terms 224\n11.3.2 Probability estimates in theory 226\n11.3.3 Probability estimates in practice 227\n11.3.4 Probabilistic approaches to relevance feedback 228\n11.4 An appraisal and some extensions 230\n11.4.1 An appraisal of probabilistic models 230\n11.4.2 Tree-structured dependencies between terms 231\n11.4.3 Okapi BM25: a non-binary model 232\n11.4.4 Bayesian network approaches to IR 234\n11.5 References and further reading 235\n12Language models for information retrieval 237\n12.1 Language models 237\nOnline edition (c)\n2009 Cambridge UPContents xi\n12.1.1 Finite automata and language models 237\n12.1.2 Types of language models 240\n12.1.3 Multinomial distributions over words 241\n12.2 The query likelihood model 242\n12.2.1 Using query likelihood language models in IR 242\n12.2.2 Estimating the query generation probability 243\n12.2.3 Ponte and Croft\u2019s Experiments 246\n12.3 Language modeling versus other approaches in IR 248\n12.4 Extended language modeling approaches 250\n12.5 References and further reading 252\n13Text classi\ufb01cation and Naive Bayes 253\n13.1 The text classi\ufb01cation problem 256\n13.2 Naive Bayes text classi\ufb01cation 258\n13.2.1 Relation to multinomial unigram language model 262\n13.3 The Bernoulli model 263\n13.4 Properties of Naive Bayes 265\n13.4.1 A variant of the multinomial model 270\n13.5 Feature selection 271\n13.5.1 Mutual information 272\n13.5.2 \u03c72Feature selection 275\n13.5.3 Frequency-based feature selection 277\n13.5.4 Feature selection for multiple classi\ufb01ers 278\n13.5.5 Comparison of feature selection methods 278\n13.6 Evaluation of text classi\ufb01cation 279\n13.7 References and further reading 286\n14Vector space classi\ufb01cation 289\n14.1 Document representations and measures of relatedness in\nvector spaces 291\n14.2 Rocchio classi\ufb01cation 292\n14.3 knearest neighbor 297\n14.3.1 Time complexity and optimality of kNN 299\n14.4 Linear versus nonlinear classi\ufb01ers 301\n14.5 Classi\ufb01cation with more than two classes 306\n14.6 The bias-variance tradeoff 308\n14.7 References and further reading 314\n14.8 Exercises 315\n15Support vector machines and machine learning on documents 319\n15.1 Support vector machines: The linearly separable case 320\n15.2 Extensions to the SVM model 327\n15.2.1 Soft margin classi\ufb01cation 327\nOnline edition (c)\n2009 Cambridge UPxii Contents\n15.2.2 Multiclass SVMs 330\n15.2.3 Nonlinear SVMs 330\n15.2.4 Experimental results 333\n15.3 Issues in the classi\ufb01cation of text documents 334\n15.3.1 Choosing what kind of classi\ufb01er to use 335\n15.3.2 Improving classi\ufb01er performance 337\n15.4 Machine learning methods in ad hoc information retriev al 341\n15.4.1 A simple example of machine-learned scoring 341\n15.4.2 Result ranking by machine learning 344\n15.5 References and further reading 346\n16Flat clustering 349\n16.1 Clustering in information retrieval 350\n16.2 Problem statement 354\n16.2.1 Cardinality \u2013 the number of clusters 355\n16.3 Evaluation of clustering 356\n16.4 K-means 360\n16.4.1 Cluster cardinality in K-means 365\n16.5 Model-based clustering 368\n16.6 References and further reading 372\n16.7 Exercises 374\n17Hierarchical clustering 377\n17.1 Hierarchical agglomerative clustering 378\n17.2 Single-link and complete-link clustering 382\n17.2.1 Time complexity of HAC 385\n17.3 Group-average agglomerative clustering 388\n17.4 Centroid clustering 391\n17.5 Optimality of HAC 393\n17.6 Divisive clustering 395\n17.7 Cluster labeling 396\n17.8 Implementation notes 398\n17.9 References and further reading 399\n17.10 Exercises 401\n18Matrix decompositions and latent semantic indexing 403\n18.1 Linear algebra review 403\n18.1.1 Matrix decompositions 406\n18.2 Term-document matrices and singular value\ndecompositions 407\n18.3 Low-rank approximations 410\n18.4 Latent semantic indexing 412\n18.5 References and further reading 417\nOnline edition (c)\n2009 Cambridge UPContents xiii\n19Web search basics 421\n19.1 Background and history 421\n19.2 Web characteristics 423\n19.2.1 The web graph 425\n19.2.2 Spam 427\n19.3 Advertising as the economic model 429\n19.4 The search user experience 432\n19.4.1 User query needs 432\n19.5 Index size and estimation 433\n19.6 Near-duplicates and shingling 437\n19.7 References and further reading 441\n20Web crawling and indexes 443\n20.1 Overview 443\n20.1.1 Features a crawler must provide 443\n20.1.2 Features a crawler should provide 444\n20.2 Crawling 444\n20.2.1 Crawler architecture 445\n20.2.2 DNS resolution 449\n20.2.3 The URL frontier 451\n20.3 Distributing indexes 454\n20.4 Connectivity servers 455\n20.5 References and further reading 458\n21Link analysis 461\n21.1 The Web as a graph 462\n21.1.1 Anchor text and the web graph 462\n21.2 PageRank 464\n21.2.1 Markov chains 465\n21.2.2 The PageRank computation 468\n21.2.3 Topic-speci\ufb01c PageRank 471\n21.3 Hubs and Authorities 474\n21.3.1 Choosing the subset of the Web 477\n21.4 References and further reading 480\nBibliography 483\nAuthor Index 519\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . xv\nList of Tables\n4.1 Typical system parameters in 2007. The seek time is the ti me\nneeded to position the disk head in a new position. The\ntransfer time per byte is the rate of transfer from disk to\nmemory when the head is in the right position. 68\n4.2 Collection statistics for Reuters-RCV1. Values are rou nded for\nthe computations in this book. The unrounded values are:\n806,791 documents, 222 tokens per document, 391,523\n(distinct) terms, 6.04 bytes per token with spaces and\npunctuation, 4.5 bytes per token without spaces and\npunctuation, 7.5 bytes per term, and 96,969,056 tokens. The\nnumbers in this table correspond to the third line (\u201ccase\nfolding\u201d) in Table 5.1(page 87). 70\n4.3 The \ufb01ve steps in constructing an index for Reuters-RCV1 i n\nblocked sort-based indexing. Line numbers refer to Figure 4.2. 82\n4.4 Collection statistics for a large collection. 82\n5.1 The effect of preprocessing on the number of terms,\nnonpositional postings, and tokens for Reuters-RCV1. \u201c \u2206%\u201d\nindicates the reduction in size from the previous line, exce pt\nthat \u201c30 stop words\u201d and \u201c150 stop words\u201d both use \u201ccase\nfolding\u201d as their reference line. \u201cT%\u201d is the cumulative\n(\u201ctotal\u201d) reduction from un\ufb01ltered. We performed stemming\nwith the Porter stemmer (Chapter 2, page 33). 87\n5.2 Dictionary compression for Reuters-RCV1. 95\n5.3 Encoding gaps instead of document IDs. For example, we\nstore gaps 107, 5, 43, . . . , instead of docIDs 283154, 283159,\n283202, . . . for computer . The \ufb01rst docID is left unchanged\n(only shown for arachnocentric ). 96\n5.4 VB encoding. 97\nOnline edition (c)\n2009 Cambridge UPxvi List of Tables\n5.5 Some examples of unary and \u03b3codes. Unary codes are only\nshown for the smaller numbers. Commas in \u03b3codes are for\nreadability only and are not part of the actual codes. 98\n5.6 Index and dictionary compression for Reuters-RCV1. The\ncompression ratio depends on the proportion of actual text i n\nthe collection. Reuters-RCV1 contains a large amount of XML\nmarkup. Using the two best compression schemes, \u03b3\nencoding and blocking with front coding, the ratio\ncompressed index to collection size is therefore especiall y\nsmall for Reuters-RCV1: (101+5.9)/3600\u22480.03. 103\n5.7 Two gap sequences to be merged in blocked sort-based\nindexing 105\n6.1 Cosine computation for Exercise 6.19. 132\n8.1 Calculation of 11-point Interpolated Average Precisio n. 159\n8.2 Calculating the kappa statistic. 165\n10.1 RDB (relational database) search, unstructured infor mation\nretrieval and structured information retrieval. 196\n10.2 INEX 2002 collection statistics. 211\n10.3 INEX 2002 results of the vector space model in Section 10.3 for\ncontent-and-structure (CAS) queries and the quantization\nfunction Q. 213\n10.4 A comparison of content-only and full-structure searc h in\nINEX 2003/2004. 214\n13.1 Data for parameter estimation examples. 261\n13.2 Training and test times for NB. 261\n13.3 Multinomial versus Bernoulli model. 268\n13.4 Correct estimation implies accurate prediction, but a ccurate\nprediction does not imply correct estimation. 269\n13.5 A set of documents for which the NB independence\nassumptions are problematic. 270\n13.6 Critical values of the \u03c72distribution with one degree of\nfreedom. For example, if the two events are independent,\nthen P(X2>6.63)<0.01. So for X2>6.63 the assumption of\nindependence can be rejected with 99% con\ufb01dence. 277\n13.7 The ten largest classes in the Reuters-21578 collectio n with\nnumber of documents in training and test sets. 280\nOnline edition (c)\n2009 Cambridge UPList of Tables xvii\n13.8 Macro- and microaveraging. \u201cTruth\u201d is the true class an d\n\u201ccall\u201d the decision of the classi\ufb01er. In this example,\nmacroaveraged precision is\n[10/(10+10) +90/(10+90)]/2= (0.5+0.9)/2=0.7.\nMicroaveraged precision is 100/ (100+20)\u22480.83. 282\n13.9 Text classi\ufb01cation effectiveness numbers on Reuters- 21578 for\nF1(in percent). Results from Li and Yang (2003 ) (a), Joachims\n(1998 ) (b: kNN) and Dumais et al. (1998 ) (b: NB, Rocchio,\ntrees, SVM). 282\n13.10 Data for parameter estimation exercise. 284\n14.1 Vectors and class centroids for the data in Table 13.1. 294\n14.2 Training and test times for Rocchio classi\ufb01cation. 296\n14.3 Training and test times for kNN classi\ufb01cation. 299\n14.4 A linear classi\ufb01er. 303\n14.5 A confusion matrix for Reuters-21578. 308\n15.1 Training and testing complexity of various classi\ufb01ers\nincluding SVMs. 329\n15.2 SVM classi\ufb01er break-even F 1from ( Joachims 2002a , p. 114). 334\n15.3 Training examples for machine-learned scoring. 342\n16.1 Some applications of clustering in information retrie val. 351\n16.2 The four external evaluation measures applied to the\nclustering in Figure 16.4. 357\n16.3 The EM clustering algorithm. 371\n17.1 Comparison of HAC algorithms. 395\n17.2 Automatically computed cluster labels. 397\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . xix\nList of Figures\n1.1 A term-document incidence matrix. 4\n1.2 Results from Shakespeare for the query Brutus ANDCaesar\nAND NOT Calpurnia . 5\n1.3 The two parts of an inverted index. 7\n1.4 Building an index by sorting and grouping. 8\n1.5 Intersecting the postings lists for Brutus andCalpurnia from\nFigure 1.3. 10\n1.6 Algorithm for the intersection of two postings lists p1and p2. 11\n1.7 Algorithm for conjunctive queries that returns the set o f\ndocuments containing each term in the input list of terms. 12\n2.1 An example of a vocalized Modern Standard Arabic word. 21\n2.2 The conceptual linear order of characters is not necessa rily the\norder that you see on the page. 21\n2.3 The standard unsegmented form of Chinese text using the\nsimpli\ufb01ed characters of mainland China. 26\n2.4 Ambiguities in Chinese word segmentation. 26\n2.5 A stop list of 25 semantically non-selective words which are\ncommon in Reuters-RCV1. 26\n2.6 An example of how asymmetric expansion of query terms can\nusefully model users\u2019 expectations. 28\n2.7 Japanese makes use of multiple intermingled writing sys tems\nand, like Chinese, does not segment words. 31\n2.8 A comparison of three stemming algorithms on a sample tex t. 34\n2.9 Postings lists with skip pointers. 36\n2.10 Postings lists intersection with skip pointers. 37\n2.11 Positional index example. 41\n2.12 An algorithm for proximity intersection of postings li stsp1\nand p2. 42\nOnline edition (c)\n2009 Cambridge UPxx List of Figures\n3.1 A binary search tree. 51\n3.2 A B-tree. 52\n3.3 A portion of a permuterm index. 54\n3.4 Example of a postings list in a 3-gram index. 55\n3.5 Dynamic programming algorithm for computing the edit\ndistance between strings s1and s2. 59\n3.6 Example Levenshtein distance computation. 59\n3.7 Matching at least two of the three 2-grams in the query bord. 61\n4.1 Document from the Reuters newswire. 70\n4.2 Blocked sort-based indexing. 71\n4.3 Merging in blocked sort-based indexing. 72\n4.4 Inversion of a block in single-pass in-memory indexing 73\n4.5 An example of distributed indexing with MapReduce.\nAdapted from Dean and Ghemawat (2004 ). 76\n4.6 Map and reduce functions in MapReduce. 77\n4.7 Logarithmic merging. Each token (termID,docID) is init ially\nadded to in-memory index Z0by LM ERGE ADDTOKEN .\nLOGARITHMIC MERGE initializes Z0and indexes . 79\n4.8 A user-document matrix for access control lists. Elemen t(i,j)\nis 1 if user ihas access to document jand 0 otherwise. During\nquery processing, a user\u2019s access postings list is intersec ted\nwith the results list returned by the text part of the index. 81\n5.1 Heaps\u2019 law. 88\n5.2 Zipf\u2019s law for Reuters-RCV1. 90\n5.3 Storing the dictionary as an array of \ufb01xed-width entries . 91\n5.4 Dictionary-as-a-string storage. 92\n5.5 Blocked storage with four terms per block. 93\n5.6 Search of the uncompressed dictionary (a) and a dictiona ry\ncompressed by blocking with k=4 (b). 94\n5.7 Front coding. 94\n5.8 VB encoding and decoding. 97\n5.9 Entropy H(P)as a function of P(x1)for a sample space with\ntwo outcomes x1and x2. 100\n5.10 Strati\ufb01cation of terms for estimating the size of a \u03b3encoded\ninverted index. 102\n6.1 Parametric search. 111\n6.2 Basic zone index 111\n6.3 Zone index in which the zone is encoded in the postings\nrather than the dictionary. 111\nOnline edition (c)\n2009 Cambridge UPList of Figures xxi\n6.4 Algorithm for computing the weighted zone score from two\npostings lists. 113\n6.5 An illustration of training examples. 115\n6.6 The four possible combinations of sTand sB. 115\n6.7 Collection frequency (cf) and document frequency (df) b ehave\ndifferently, as in this example from the Reuters collection . 118\n6.8 Example of idf values. 119\n6.9 Table of tf values for Exercise 6.10. 120\n6.10 Cosine similarity illustrated. 121\n6.11 Euclidean normalized tf values for documents in Figure 6.9. 122\n6.12 Term frequencies in three novels. 122\n6.13 Term vectors for the three novels of Figure 6.12. 123\n6.14 The basic algorithm for computing vector space scores. 125\n6.15 SMART notation for tf-idf variants. 128\n6.16 Pivoted document length normalization. 130\n6.17 Implementing pivoted document length normalization b y\nlinear scaling. 131\n7.1 A faster algorithm for vector space scores. 136\n7.2 A static quality-ordered index. 139\n7.3 Cluster pruning. 142\n7.4 Tiered indexes. 144\n7.5 A complete search system. 147\n8.1 Graph comparing the harmonic mean to other means. 157\n8.2 Precision/recall graph. 158\n8.3 Averaged 11-point precision/recall graph across 50 que ries\nfor a representative TREC system. 160\n8.4 The ROC curve corresponding to the precision-recall cur ve in\nFigure 8.2. 162\n8.5 An example of selecting text for a dynamic snippet. 172\n9.1 Relevance feedback searching over images. 179\n9.2 Example of relevance feedback on a text collection. 180\n9.3 The Rocchio optimal query for separating relevant and\nnonrelevant documents. 181\n9.4 An application of Rocchio\u2019s algorithm. 182\n9.5 Results showing pseudo relevance feedback greatly\nimproving performance. 187\n9.6 An example of query expansion in the interface of the Yaho o!\nweb search engine in 2006. 190\n9.7 Examples of query expansion via the PubMed thesaurus. 191\n9.8 An example of an automatically generated thesaurus. 192\nOnline edition (c)\n2009 Cambridge UPxxii List of Figures\n10.1 An XML document. 198\n10.2 The XML document in Figure 10.1 as a simpli\ufb01ed DOM object. 198\n10.3 An XML query in NEXI format and its partial representati on\nas a tree. 199\n10.4 Tree representation of XML documents and queries. 200\n10.5 Partitioning an XML document into non-overlapping\nindexing units. 202\n10.6 Schema heterogeneity: intervening nodes and mismatch ed\nnames. 204\n10.7 A structural mismatch between two queries and a documen t. 206\n10.8 A mapping of an XML document (left) to a set of lexicalize d\nsubtrees (right). 207\n10.9 The algorithm for scoring documents with S IMNOMERGE . 209\n10.10 Scoring of a query with one structural term in S IMNOMERGE . 209\n10.11 Simpli\ufb01ed schema of the documents in the INEX collecti on. 211\n11.1 A tree of dependencies between terms. 232\n12.1 A simple \ufb01nite automaton and some of the strings in the\nlanguage it generates. 238\n12.2 A one-state \ufb01nite automaton that acts as a unigram langu age\nmodel. 238\n12.3 Partial speci\ufb01cation of two unigram language models. 239\n12.4 Results of a comparison of tf-idf with language modelin g\n(LM) term weighting by Ponte and Croft (1998 ). 247\n12.5 Three ways of developing the language modeling approac h:\n(a) query likelihood, (b) document likelihood, and (c) mode l\ncomparison. 250\n13.1 Classes, training set, and test set in text classi\ufb01cati on . 257\n13.2 Naive Bayes algorithm (multinomial model): Training a nd\ntesting. 260\n13.3 NB algorithm (Bernoulli model): Training and testing. 263\n13.4 The multinomial NB model. 266\n13.5 The Bernoulli NB model. 267\n13.6 Basic feature selection algorithm for selecting the kbest features. 271\n13.7 Features with high mutual information scores for six\nReuters-RCV1 classes. 274\n13.8 Effect of feature set size on accuracy for multinomial a nd\nBernoulli models. 275\n13.9 A sample document from the Reuters-21578 collection. 281\n14.1 Vector space classi\ufb01cation into three classes. 290\nOnline edition (c)\n2009 Cambridge UPList of Figures xxiii\n14.2 Projections of small areas of the unit sphere preserve d istances. 291\n14.3 Rocchio classi\ufb01cation. 293\n14.4 Rocchio classi\ufb01cation: Training and testing. 295\n14.5 The multimodal class \u201ca\u201d consists of two different clus ters\n(small upper circles centered on X\u2019s). 295\n14.6 Voronoi tessellation and decision boundaries (double lines) in\n1NN classi\ufb01cation. 297\n14.7 kNN training (with preprocessing) and testing. 298\n14.8 There are an in\ufb01nite number of hyperplanes that separat e two\nlinearly separable classes. 301\n14.9 Linear classi\ufb01cation algorithm. 302\n14.10 A linear problem with noise. 304\n14.11 A nonlinear problem. 305\n14.12 Jhyperplanes do not divide space into Jdisjoint regions. 307\n14.13 Arithmetic transformations for the bias-variance de composition. 310\n14.14 Example for differences between Euclidean distance, dot\nproduct similarity and cosine similarity. 316\n14.15 A simple non-separable set of points. 317\n15.1 The support vectors are the 5 points right up against the\nmargin of the classi\ufb01er. 320\n15.2 An intuition for large-margin classi\ufb01cation. 321\n15.3 The geometric margin of a point ( r) and a decision boundary ( \u03c1).323\n15.4 A tiny 3 data point training set for an SVM. 325\n15.5 Large margin classi\ufb01cation with slack variables. 327\n15.6 Projecting data that is not linearly separable into a hi gher\ndimensional space can make it linearly separable. 331\n15.7 A collection of training examples. 343\n16.1 An example of a data set with a clear cluster structure. 349\n16.2 Clustering of search results to improve recall. 352\n16.3 An example of a user session in Scatter-Gather. 353\n16.4 Purity as an external evaluation criterion for cluster quality. 357\n16.5 The K-means algorithm. 361\n16.6 A K-means example for K=2 inR2. 362\n16.7 The outcome of clustering in K-means depends on the initial\nseeds. 364\n16.8 Estimated minimal residual sum of squares as a function of\nthe number of clusters in K-means. 366\n17.1 A dendrogram of a single-link clustering of 30 document s\nfrom Reuters-RCV1. 379\n17.2 A simple, but inef\ufb01cient HAC algorithm. 381\nOnline edition (c)\n2009 Cambridge UPxxiv List of Figures\n17.3 The different notions of cluster similarity used by the four\nHAC algorithms. 381\n17.4 A single-link (left) and complete-link (right) cluste ring of\neight documents. 382\n17.5 A dendrogram of a complete-link clustering. 383\n17.6 Chaining in single-link clustering. 384\n17.7 Outliers in complete-link clustering. 385\n17.8 The priority-queue algorithm for HAC. 386\n17.9 Single-link clustering algorithm using an NBM array. 387\n17.10 Complete-link clustering is not best-merge persiste nt. 388\n17.11 Three iterations of centroid clustering. 391\n17.12 Centroid clustering is not monotonic. 392\n18.1 Illustration of the singular-value decomposition. 409\n18.2 Illustration of low rank approximation using the\nsingular-value decomposition. 411\n18.3 The documents of Example 18.4 reduced to two dimensions\nin(V\u2032)T. 416\n18.4 Documents for Exercise 18.11 . 418\n18.5 Glossary for Exercise 18.11 . 418\n19.1 A dynamically generated web page. 425\n19.2 Two nodes of the web graph joined by a link. 425\n19.3 A sample small web graph. 426\n19.4 The bowtie structure of the Web. 427\n19.5 Cloaking as used by spammers. 428\n19.6 Search advertising triggered by query keywords. 431\n19.7 The various components of a web search engine. 434\n19.8 Illustration of shingle sketches. 439\n19.9 Two sets Sj1and Sj2; their Jaccard coef\ufb01cient is 2/5. 440\n20.1 The basic crawler architecture. 446\n20.2 Distributing the basic crawl architecture. 449\n20.3 The URL frontier. 452\n20.4 Example of an auxiliary hosts-to-back queues table. 453\n20.5 A lexicographically ordered set of URLs. 456\n20.6 A four-row segment of the table of links. 457\n21.1 The random surfer at node A proceeds with probability 1/ 3 to\neach of B, C and D. 464\n21.2 A simple Markov chain with three states; the numbers on t he\nlinks indicate the transition probabilities. 466\n21.3 The sequence of probability vectors. 469\nOnline edition (c)\n2009 Cambridge UPList of Figures xxv\n21.4 A small web graph. 470\n21.5 Topic-speci\ufb01c PageRank. 472\n21.6 A sample run of HITS on the query japan elementaryschools . 479\n21.7 Web graph for Exercise 21.22 . 480\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . xxvii\nTable of Notation\nSymbol Page Meaning\n\u03b3 p.98 \u03b3code\n\u03b3 p.256 Classi\ufb01cation or clustering function: \u03b3(d)isd\u2019s class\nor cluster\n\u0393 p.256 Supervised learning method in Chapters 13and 14:\n\u0393(D)is the classi\ufb01cation function \u03b3learned from\ntraining set D\n\u03bb p.404 Eigenvalue\n/vector\u00b5(.) p.292 Centroid of a class (in Rocchio classi\ufb01cation) or a\ncluster (in K-means and centroid clustering)\n\u03a6 p.114 Training example\n\u03c3 p.408 Singular value\n\u0398(\u00b7) p.11 A tight bound on the complexity of an algorithm\n\u03c9,\u03c9k p.357 Cluster in clustering\n\u2126 p.357 Clustering or set of clusters {\u03c91, . . . , \u03c9K}\narg maxxf(x)p.181 The value of xfor which freaches its maximum\narg minxf(x)p.181 The value of xfor which freaches its minimum\nc,cj p.256 Class or category in classi\ufb01cation\ncft p.89 The collection frequency of term t(the total number\nof times the term appears in the document collec-\ntion)\nC p.256 Set{c1, . . . , cJ}of all classes\nC p.268 A random variable that takes as values members of\nC\nOnline edition (c)\n2009 Cambridge UPxxviii Table of Notation\nC p.403 Term-document matrix\nd p.4 Index of the dthdocument in the collection D\nd p.71 A document\n/vectord,/vectorq p.181 Document vector, query vector\nD p.354 Set{d1, . . . , dN}of all documents\nDc p.292 Set of documents that is in class c\nD p.256 Set{/an}\u230ara\u230bketle{td1,c1/an}\u230ara\u230bketri}ht, . . . ,/an}\u230ara\u230bketle{tdN,cN/an}\u230ara\u230bketri}ht}of all labeled documents\nin Chapters 13\u201315\ndft p.118 The document frequency of term t(the total number\nof documents in the collection the term appears in)\nH p.99 Entropy\nHM p.101 Mth harmonic number\nI(X;Y) p.272 Mutual information of random variables Xand Y\nidft p.118 Inverse document frequency of term t\nJ p.256 Number of classes\nk p.290 Top kitems from a set, e.g., knearest neighbors in\nkNN, top kretrieved documents, top kselected fea-\ntures from the vocabulary V\nk p.54 Sequence of kcharacters\nK p.354 Number of clusters\nLd p.233 Length of document d(in tokens)\nLa p.262 Length of the test document (or application docu-\nment) in tokens\nLave p.70 Average length of a document (in tokens)\nM p.5 Size of the vocabulary ( |V|)\nMa p.262 Size of the vocabulary of the test document (or ap-\nplication document)\nMave p.78 Average size of the vocabulary in a document in the\ncollection\nMd p.237 Language model for document d\nN p.4 Number of documents in the retrieval or training\ncollection\nNc p.259 Number of documents in class c\nN(\u03c9) p.298 Number of times the event \u03c9occurred\nOnline edition (c)\n2009 Cambridge UPTable of Notation xxix\nO(\u00b7) p.11 A bound on the complexity of an algorithm\nO(\u00b7) p.221 The odds of an event\nP p.155 Precision\nP(\u00b7) p.220 Probability\nP p.465 Transition probability matrix\nq p.59 A query\nR p.155 Recall\nsi p.58 A string\nsi p.112 Boolean values for zone scoring\nsim(d1,d2) p.121 Similarity score for documents d1,d2\nT p.43 Total number of tokens in the document collection\nTct p.259 Number of occurrences of word tin documents of\nclass c\nt p.4 Index of the tthterm in the vocabulary V\nt p.61 A term in the vocabulary\ntft,d p.117 The term frequency of term tin document d(the to-\ntal number of occurrences of tind)\nUt p.266 Random variable taking values 0 (term tis present)\nand 1 ( tis not present)\nV p.208 Vocabulary of terms {t1, . . . , tM}in a collection (a.k.a.\nthe lexicon)\n/vectorv(d) p.122 Length-normalized document vector\n/vectorV(d) p.120 Vector of document d, not length-normalized\nwft,d p.125 Weight of term tin document d\nw p.112 A weight, for example for zones or terms\n/vectorwT/vectorx=b p.293 Hyperplane; /vectorwis the normal vector of the hyper-\nplane and wicomponent iof/vectorw\n/vectorx p.222 Term incidence vector /vectorx= (x1, . . . , xM); more gen-\nerally: document feature representation\nX p.266 Random variable taking values in V, the vocabulary\n(e.g., at a given position kin a document)\nX p.256 Document space in text classi\ufb01cation\n|A| p.61 Set cardinality: the number of members of set A\n|S| p.404 Determinant of the square matrix S\nOnline edition (c)\n2009 Cambridge UPxxx Table of Notation\n|si| p.58 Length in characters of string si\n|/vectorx| p.139 Length of vector /vectorx\n|/vectorx\u2212/vectory| p.131 Euclidean distance of /vectorxand/vectory(which is the length of\n(/vectorx\u2212/vectory))\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . xxxi\nPreface\nAs recently as the 1990s, studies showed that most people pre ferred getting\ninformation from other people rather than from information retrieval sys-\ntems. Of course, in that time period, most people also used hu man travel\nagents to book their travel. However, during the last decade , relentless opti-\nmization of information retrieval effectiveness has drive n web search engines\nto new quality levels where most people are satis\ufb01ed most of t he time, and\nweb search has become a standard and often preferred source o f information\n\ufb01nding. For example, the 2004 Pew Internet Survey ( Fallows 2004 ) found\nthat \u201c92% of Internet users say the Internet is a good place to go for getting\neveryday information.\u201d To the surprise of many, the \ufb01eld of i nformation re-\ntrieval has moved from being a primarily academic disciplin e to being the\nbasis underlying most people\u2019s preferred means of informat ion access. This\nbook presents the scienti\ufb01c underpinnings of this \ufb01eld, at a level accessible\nto graduate students as well as advanced undergraduates.\nInformation retrieval did not begin with the Web. In respons e to various\nchallenges of providing information access, the \ufb01eld of inf ormation retrieval\nevolved to give principled approaches to searching various forms of con-\ntent. The \ufb01eld began with scienti\ufb01c publications and librar y records, but\nsoon spread to other forms of content, particularly those of information pro-\nfessionals, such as journalists, lawyers, and doctors. Muc h of the scienti\ufb01c\nresearch on information retrieval has occurred in these con texts, and much of\nthe continued practice of information retrieval deals with providing access to\nunstructured information in various corporate and governm ental domains,\nand this work forms much of the foundation of our book.\nNevertheless, in recent years, a principal driver of innova tion has been the\nWorld Wide Web, unleashing publication at the scale of tens o f millions of\ncontent creators. This explosion of published information would be moot\nif the information could not be found, annotated and analyze d so that each\nuser can quickly \ufb01nd information that is both relevant and co mprehensive\nfor their needs. By the late 1990s, many people felt that cont inuing to index\nOnline edition (c)\n2009 Cambridge UPxxxii Preface\nthe whole Web would rapidly become impossible, due to the Web \u2019s expo-\nnential growth in size. But major scienti\ufb01c innovations, su perb engineering,\nthe rapidly declining price of computer hardware, and the ri se of a commer-\ncial underpinning for web search have all conspired to power today\u2019s major\nsearch engines, which are able to provide high-quality resu lts within subsec-\nond response times for hundreds of millions of searches a day over billions\nof web pages.\nBook organization and course development\nThis book is the result of a series of courses we have taught at Stanford Uni-\nversity and at the University of Stuttgart, in a range of dura tions including\na single quarter, one semester and two quarters. These cours es were aimed\nat early-stage graduate students in computer science, but w e have also had\nenrollment from upper-class computer science undergradua tes, as well as\nstudents from law, medical informatics, statistics, lingu istics and various en-\ngineering disciplines. The key design principle for this bo ok, therefore, was\nto cover what we believe to be important in a one-term graduat e course on\ninformation retrieval. An additional principle is to build each chapter around\nmaterial that we believe can be covered in a single lecture of 75 to 90 minutes.\nThe \ufb01rst eight chapters of the book are devoted to the basics o f informa-\ntion retrieval, and in particular the heart of search engine s; we consider this\nmaterial to be core to any course on information retrieval. C hapter 1in-\ntroduces inverted indexes, and shows how simple Boolean que ries can be\nprocessed using such indexes. Chapter 2builds on this introduction by de-\ntailing the manner in which documents are preprocessed befo re indexing\nand by discussing how inverted indexes are augmented in vari ous ways for\nfunctionality and speed. Chapter 3discusses search structures for dictionar-\nies and how to process queries that have spelling errors and o ther imprecise\nmatches to the vocabulary in the document collection being s earched. Chap-\nter4describes a number of algorithms for constructing the inver ted index\nfrom a text collection with particular attention to highly s calable and dis-\ntributed algorithms that can be applied to very large collec tions. Chapter 5\ncovers techniques for compressing dictionaries and invert ed indexes. These\ntechniques are critical for achieving subsecond response t imes to user queries\nin large search engines. The indexes and queries considered in Chapters 1\u20135\nonly deal with Boolean retrieval , in which a document either matches a query,\nor does not. A desire to measure the extent to which a document matches a\nquery, or the score of a document for a query, motivates the de velopment of\nterm weighting and the computation of scores in Chapters 6and 7, leading\nto the idea of a list of documents that are rank-ordered for a q uery. Chapter 8\nfocuses on the evaluation of an information retrieval syste m based on the\nOnline edition (c)\n2009 Cambridge UPPreface xxxiii\nrelevance of the documents it retrieves, allowing us to comp are the relative\nperformances of different systems on benchmark document co llections and\nqueries.\nChapters 9\u201321build on the foundation of the \ufb01rst eight chapters to cover\na variety of more advanced topics. Chapter 9discusses methods by which\nretrieval can be enhanced through the use of techniques like relevance feed-\nback and query expansion, which aim at increasing the likeli hood of retriev-\ning relevant documents. Chapter 10considers information retrieval from\ndocuments that are structured with markup languages like XM L and HTML.\nWe treat structured retrieval by reducing it to the vector sp ace scoring meth-\nods developed in Chapter 6. Chapters 11and 12invoke probability theory to\ncompute scores for documents on queries. Chapter 11develops traditional\nprobabilistic information retrieval, which provides a fra mework for comput-\ning the probability of relevance of a document, given a set of query terms.\nThis probability may then be used as a score in ranking. Chapt er12illus-\ntrates an alternative, wherein for each document in a collec tion, we build a\nlanguage model from which one can estimate a probability tha t the language\nmodel generates a given query. This probability is another q uantity with\nwhich we can rank-order documents.\nChapters 13\u201317give a treatment of various forms of machine learning and\nnumerical methods in information retrieval. Chapters 13\u201315treat the prob-\nlem of classifying documents into a set of known categories, given a set of\ndocuments along with the classes they belong to. Chapter 13motivates sta-\ntistical classi\ufb01cation as one of the key technologies neede d for a successful\nsearch engine, introduces Naive Bayes, a conceptually simp le and ef\ufb01cient\ntext classi\ufb01cation method, and outlines the standard metho dology for evalu-\nating text classi\ufb01ers. Chapter 14employs the vector space model from Chap-\nter6and introduces two classi\ufb01cation methods, Rocchio and kNN, that op-\nerate on document vectors. It also presents the bias-varian ce tradeoff as an\nimportant characterization of learning problems that prov ides criteria for se-\nlecting an appropriate method for a text classi\ufb01cation prob lem. Chapter 15\nintroduces support vector machines, which many researcher s currently view\nas the most effective text classi\ufb01cation method. We also dev elop connections\nin this chapter between the problem of classi\ufb01cation and see mingly disparate\ntopics such as the induction of scoring functions from a set o f training exam-\nples.\nChapters 16\u201318consider the problem of inducing clusters of related doc-\numents from a collection. In Chapter 16, we \ufb01rst give an overview of a\nnumber of important applications of clustering in informat ion retrieval. We\nthen describe two \ufb02at clustering algorithms: the K-means algorithm, an ef-\n\ufb01cient and widely used document clustering method; and the E xpectation-\nMaximization algorithm, which is computationally more exp ensive, but also\nmore \ufb02exible. Chapter 17motivates the need for hierarchically structured\nOnline edition (c)\n2009 Cambridge UPxxxiv Preface\nclusterings (instead of \ufb02at clusterings) in many applicati ons in information\nretrieval and introduces a number of clustering algorithms that produce a\nhierarchy of clusters. The chapter also addresses the dif\ufb01c ult problem of\nautomatically computing labels for clusters. Chapter 18develops methods\nfrom linear algebra that constitute an extension of cluster ing, and also offer\nintriguing prospects for algebraic methods in information retrieval, which\nhave been pursued in the approach of latent semantic indexin g.\nChapters 19\u201321treat the problem of web search. We give in Chapter 19a\nsummary of the basic challenges in web search, together with a set of tech-\nniques that are pervasive in web information retrieval. Nex t, Chapter 20\ndescribes the architecture and requirements of a basic web c rawler. Finally,\nChapter 21considers the power of link analysis in web search, using in t he\nprocess several methods from linear algebra and advanced pr obability the-\nory.\nThis book is not comprehensive in covering all topics relate d to informa-\ntion retrieval. We have put aside a number of topics, which we deemed\noutside the scope of what we wished to cover in an introductio n to infor-\nmation retrieval class. Nevertheless, for people interest ed in these topics, we\nprovide a few pointers to mainly textbook coverage here.\nCross-language IR (Grossman and Frieder 2004 , ch. 4) and ( Oard and Dorr\n1996 ).\nImage and Multimedia IR (Grossman and Frieder 2004 , ch. 4), ( Baeza-Yates\nand Ribeiro-Neto 1999 , ch. 6), ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 11),\n(Baeza-Yates and Ribeiro-Neto 1999 , ch. 12), ( del Bimbo 1999 ), (Lew 2001 ),\nand ( Smeulders et al. 2000 ).\nSpeech retrieval (Coden et al. 2002 ).\nMusic Retrieval (Downie 2006 ) andhttp://www.ismir.net/ .\nUser interfaces for IR (Baeza-Yates and Ribeiro-Neto 1999 , ch. 10).\nParallel and Peer-to-Peer IR (Grossman and Frieder 2004 , ch. 7), ( Baeza-Yates\nand Ribeiro-Neto 1999 , ch. 9), and ( Aberer 2001 ).\nDigital libraries (Baeza-Yates and Ribeiro-Neto 1999 , ch. 15) and ( Lesk 2004 ).\nInformation science perspective (Korfhage 1997 ), (Meadow et al. 1999 ), and\n(Ingwersen and J\u00e4rvelin 2005 ).\nLogic-based approaches to IR (van Rijsbergen 1989 ).\nNatural Language Processing techniques (Manning and Sch\u00fctze 1999 ), (Ju-\nrafsky and Martin 2008 ), and ( Lewis and Jones 1996 ).\nOnline edition (c)\n2009 Cambridge UPPreface xxxv\nPrerequisites\nIntroductory courses in data structures and algorithms, in linear algebra and\nin probability theory suf\ufb01ce as prerequisites for all 21 cha pters. We now give\nmore detail for the bene\ufb01t of readers and instructors who wis h to tailor their\nreading to some of the chapters.\nChapters 1\u20135assume as prerequisite a basic course in algorithms and data\nstructures. Chapters 6and 7require, in addition, a knowledge of basic lin-\near algebra including vectors and dot products. No addition al prerequisites\nare assumed until Chapter 11, where a basic course in probability theory is\nrequired; Section 11.1 gives a quick review of the concepts necessary in Chap-\nters 11\u201313. Chapter 15assumes that the reader is familiar with the notion of\nnonlinear optimization, although the chapter may be read wi thout detailed\nknowledge of algorithms for nonlinear optimization. Chapt er18demands a\n\ufb01rst course in linear algebra including familiarity with th e notions of matrix\nrank and eigenvectors; a brief review is given in Section 18.1. The knowledge\nof eigenvalues and eigenvectors is also necessary in Chapte r21.\nBook layout\n\u270eWorked examples in the text appear with a pencil sign next to t hem in the left\nmargin. Advanced or dif\ufb01cult material appears in sections o r subsections\nindicated with scissors in the margin. Exercises are marked in the margin\u2704with a question mark. The level of dif\ufb01culty of exercises is i ndicated as easy\n(\u22c6), medium ( \u22c6\u22c6), or dif\ufb01cult ( \u22c6 \u22c6 \u22c6 ).?\nAcknowledgments\nWe would like to thank Cambridge University Press for allowi ng us to make\nthe draft book available online, which facilitated much of t he feedback we\nhave received while writing the book. We also thank Lauren Co wles, who\nhas been an outstanding editor, providing several rounds of comments on\neach chapter, on matters of style, organization, and covera ge, as well as de-\ntailed comments on the subject matter of the book. To the exte nt that we\nhave achieved our goals in writing this book, she deserves an important part\nof the credit.\nWe are very grateful to the many people who have given us comme nts,\nsuggestions, and corrections based on draft versions of thi s book. We thank\nfor providing various corrections and comments: Cheryl Aas heim, Josh At-\ntenberg, Daniel Beck, Luc B\u00e9langer, Georg Buscher, Tom Breu el, Daniel Bur-\nckhardt, Fazli Can, Dinquan Chen, Stephen Clark, Ernest Dav is, Pedro Domin-\ngos, Rodrigo Panchiniak Fernandes, Paolo Ferragina, Alex F raser, Norbert\nOnline edition (c)\n2009 Cambridge UPxxxvi Preface\nFuhr, Vignesh Ganapathy, Elmer Garduno, Xiubo Geng, David G ondek, Ser-\ngio Govoni, Corinna Habets, Ben Handy, Donna Harman, Benjam in Haskell,\nThomas H\u00fchn, Deepak Jain, Ralf Jankowitsch, Dinakar Jayara jan, Vinay Kakade,\nMei Kobayashi, Wessel Kraaij, Rick La\ufb02eur, Florian Laws, Ha ng Li, David\nLosada, David Mann, Ennio Masi, Sven Meyer zu Eissen, Alexan der Murzaku,\nGonzalo Navarro, Frank McCown, Paul McNamee, Christoph M\u00fcl ler, Scott\nOlsson, Tao Qin, Megha Raghavan, Michal Rosen-Zvi, Klaus Ro thenh\u00e4usler,\nKenyu L. Runner, Alexander Salamanca, Grigory Sapunov, Evg eny Shad-\nchnev, Tobias Scheffer, Nico Schlaefer, Ian Soboroff, Benn o Stein, Marcin\nSydow, Andrew Turner, Jason Utt, Huey Vo, Travis Wade, Mike W alsh, Changliang\nWang, Renjing Wang, and Thomas Zeume.\nMany people gave us detailed feedback on individual chapter s, either at\nour request or through their own initiative. For this, we\u2019re particularly grate-\nful to: James Allan, Omar Alonso, Ismail Sengor Altingovde, Vo Ngoc Anh,\nRoi Blanco, Eric Breck, Eric Brown, Mark Carman, Carlos Cast illo, Junghoo\nCho, Aron Culotta, Doug Cutting, Meghana Deodhar, Susan Dum ais, Jo-\nhannes F\u00fcrnkranz, Andreas He\u00df, Djoerd Hiemstra, David Hull , Thorsten\nJoachims, Siddharth Jonathan J. B., Jaap Kamps, Mounia Lalm as, Amy Langville,\nNicholas Lester, Dave Lewis, Daniel Lowd, Yosi Mass, Jeff Mi chels, Alessan-\ndro Moschitti, Amir Najmi, Marc Najork, Giorgio Maria Di Nun zio, Paul\nOgilvie, Priyank Patel, Jan Pedersen, Kathryn Pedings, Vas silis Plachouras,\nDaniel Ramage, Ghulam Raza, Stefan Riezler, Michael Schieh len, Helmut\nSchmid, Falk Nicolas Scholer, Sabine Schulte im Walde, Fabr izio Sebastiani,\nSarabjeet Singh, Valentin Spitkovsky, Alexander Strehl, J ohn Tait, Shivaku-\nmar Vaithyanathan, Ellen Voorhees, Gerhard Weikum, Dawid W eiss, Yiming\nYang, Yisong Yue, Jian Zhang, and Justin Zobel.\nAnd \ufb01nally there were a few reviewers who absolutely stood ou t in terms\nof the quality and quantity of comments that they provided. W e thank them\nfor their signi\ufb01cant impact on the content and structure of t he book. We\nexpress our gratitude to Pavel Berkhin, Stefan B\u00fcttcher, Ja mie Callan, Byron\nDom, Torsten Suel, and Andrew Trotman.\nParts of the initial drafts of Chapters 13\u201315were based on slides that were\ngenerously provided by Ray Mooney. While the material has go ne through\nextensive revisions, we gratefully acknowledge Ray\u2019s cont ribution to the\nthree chapters in general and to the description of the time c omplexities of\ntext classi\ufb01cation algorithms in particular.\nThe above is unfortunately an incomplete list: we are still i n the process of\nincorporating feedback we have received. And, like all opin ionated authors,\nwe did not always heed the advice that was so freely given. The published\nversions of the chapters remain solely the responsibility o f the authors.\nThe authors thank Stanford University and the University of Stuttgart for\nproviding a stimulating academic environment for discussi ng ideas and the\nopportunity to teach courses from which this book arose and i n which its\nOnline edition (c)\n2009 Cambridge UPPreface xxxvii\ncontents were re\ufb01ned. CM thanks his family for the many hours they\u2019ve let\nhim spend working on this book, and hopes he\u2019ll have a bit more free time on\nweekends next year. PR thanks his family for their patient su pport through\nthe writing of this book and is also grateful to Yahoo! Inc. fo r providing a\nfertile environment in which to work on this book. HS would li ke to thank\nhis parents, family, and friends for their support while wri ting this book.\nWeb and contact information\nThis book has a companion website at http://informationretrieval.org . As well as\nlinks to some more general resources, it is our intent to main tain on this web-\nsite a set of slides for each chapter which may be used for the c orresponding\nlecture. We gladly welcome further feedback, corrections, and suggestions\non the book, which may be sent to all the authors at informationretrieval(at)yahoogroups (dot)com .\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 1\n1 Boolean retrieval\nThe meaning of the term information retrieval can be very broad. Just getting\na credit card out of your wallet so that you can type in the card number\nis a form of information retrieval. However, as an academic \ufb01 eld of study,\ninformation retrieval might be de\ufb01ned thus: INFORMATION\nRETRIEVAL\nInformation retrieval (IR) is \ufb01nding material (usually doc uments) of\nan unstructured nature (usually text) that satis\ufb01es an info rmation need\nfrom within large collections (usually stored on computers ).\nAs de\ufb01ned in this way, information retrieval used to be an act ivity that only\na few people engaged in: reference librarians, paralegals, and similar pro-\nfessional searchers. Now the world has changed, and hundred s of millions\nof people engage in information retrieval every day when the y use a web\nsearch engine or search their email.1Information retrieval is fast becoming\nthe dominant form of information access, overtaking tradit ional database-\nstyle searching (the sort that is going on when a clerk says to you: \u201cI\u2019m sorry,\nI can only look up your order if you can give me your Order ID\u201d).\nIR can also cover other kinds of data and information problem s beyond\nthat speci\ufb01ed in the core de\ufb01nition above. The term \u201cunstruc tured data\u201d\nrefers to data which does not have clear, semantically overt , easy-for-a-computer\nstructure. It is the opposite of structured data, the canoni cal example of\nwhich is a relational database, of the sort companies usuall y use to main-\ntain product inventories and personnel records. In reality , almost no data\nare truly \u201cunstructured\u201d. This is de\ufb01nitely true of all text data if you count\nthe latent linguistic structure of human languages. But eve n accepting that\nthe intended notion of structure is overt structure, most te xt has structure,\nsuch as headings and paragraphs and footnotes, which is comm only repre-\nsented in documents by explicit markup (such as the coding un derlying web\n1. In modern parlance, the word \u201csearch\u201d has tended to replac e \u201c(information) retrieval\u201d; the\nterm \u201csearch\u201d is quite ambiguous, but in context we use the tw o synonymously.\nOnline edition (c)\n2009 Cambridge UP2 1 Boolean retrieval\npages). IR is also used to facilitate \u201csemistructured\u201d sear ch such as \ufb01nding a\ndocument where the title contains Java and the body contains threading .\nThe \ufb01eld of information retrieval also covers supporting us ers in browsing\nor \ufb01ltering document collections or further processing a se t of retrieved doc-\numents. Given a set of documents, clustering is the task of co ming up with a\ngood grouping of the documents based on their contents. It is similar to ar-\nranging books on a bookshelf according to their topic. Given a set of topics,\nstanding information needs, or other categories (such as su itability of texts\nfor different age groups), classi\ufb01cation is the task of deci ding which class(es),\nif any, each of a set of documents belongs to. It is often appro ached by \ufb01rst\nmanually classifying some documents and then hoping to be ab le to classify\nnew documents automatically.\nInformation retrieval systems can also be distinguished by the scale at\nwhich they operate, and it is useful to distinguish three pro minent scales.\nInweb search , the system has to provide search over billions of documents\nstored on millions of computers. Distinctive issues are nee ding to gather\ndocuments for indexing, being able to build systems that wor k ef\ufb01ciently\nat this enormous scale, and handling particular aspects of t he web, such as\nthe exploitation of hypertext and not being fooled by site pr oviders manip-\nulating page content in an attempt to boost their search engi ne rankings,\ngiven the commercial importance of the web. We focus on all th ese issues\nin Chapters 19\u201321. At the other extreme is personal information retrieval . In\nthe last few years, consumer operating systems have integra ted information\nretrieval (such as Apple\u2019s Mac OS X Spotlight or Windows Vist a\u2019s Instant\nSearch). Email programs usually not only provide search but also text clas-\nsi\ufb01cation: they at least provide a spam (junk mail) \ufb01lter, an d commonly also\nprovide either manual or automatic means for classifying ma il so that it can\nbe placed directly into particular folders. Distinctive is sues here include han-\ndling the broad range of document types on a typical personal computer,\nand making the search system maintenance free and suf\ufb01cient ly lightweight\nin terms of startup, processing, and disk space usage that it can run on one\nmachine without annoying its owner. In between is the space o fenterprise,\ninstitutional, and domain-speci\ufb01c search , where retrieval might be provided for\ncollections such as a corporation\u2019s internal documents, a d atabase of patents,\nor research articles on biochemistry. In this case, the docu ments will typi-\ncally be stored on centralized \ufb01le systems and one or a handfu l of dedicated\nmachines will provide search over the collection. This book contains tech-\nniques of value over this whole spectrum, but our coverage of some aspects\nof parallel and distributed search in web-scale search syst ems is compara-\ntively light owing to the relatively small published litera ture on the details\nof such systems. However, outside of a handful of web search c ompanies, a\nsoftware developer is most likely to encounter the personal search and en-\nterprise scenarios.\nOnline edition (c)\n2009 Cambridge UP1.1 An example information retrieval problem 3\nIn this chapter we begin with a very simple example of an infor mation\nretrieval problem, and introduce the idea of a term-documen t matrix (Sec-\ntion 1.1) and the central inverted index data structure (Section 1.2). We will\nthen examine the Boolean retrieval model and how Boolean que ries are pro-\ncessed (Sections 1.3and 1.4).\n1.1 An example information retrieval problem\nA fat book which many people own is Shakespeare\u2019s Collected W orks. Sup-\npose you wanted to determine which plays of Shakespeare cont ain the words\nBrutus ANDCaesar AND NOT Calpurnia . One way to do that is to start at the\nbeginning and to read through all the text, noting for each pl ay whether\nit contains Brutus andCaesar and excluding it from consideration if it con-\ntainsCalpurnia . The simplest form of document retrieval is for a computer\nto do this sort of linear scan through documents. This proces s is commonly\nreferred to as grepping through text, after the Unix command grep , which GREP\nperforms this process. Grepping through text can be a very ef fective process,\nespecially given the speed of modern computers, and often al lows useful\npossibilities for wildcard pattern matching through the us e of regular expres-\nsions. With modern computers, for simple querying of modest collections\n(the size of Shakespeare\u2019s Collected Works is a bit under one million words\nof text in total), you really need nothing more.\nBut for many purposes, you do need more:\n1. To process large document collections quickly. The amoun t of online data\nhas grown at least as quickly as the speed of computers, and we would\nnow like to be able to search collections that total in the ord er of billions\nto trillions of words.\n2. To allow more \ufb02exible matching operations. For example, i t is impractical\nto perform the query Romans NEARcountrymen withgrep , where NEAR\nmight be de\ufb01ned as \u201cwithin 5 words\u201d or \u201cwithin the same senten ce\u201d.\n3. To allow ranked retrieval: in many cases you want the best a nswer to an\ninformation need among many documents that contain certain words.\nThe way to avoid linearly scanning the texts for each query is toindex the INDEX\ndocuments in advance. Let us stick with Shakespeare\u2019s Colle cted Works,\nand use it to introduce the basics of the Boolean retrieval mo del. Suppose\nwe record for each document \u2013 here a play of Shakespeare\u2019s \u2013 wh ether it\ncontains each word out of all the words Shakespeare used (Sha kespeare used\nabout 32,000 different words). The result is a binary term-d ocument incidence INCIDENCE MATRIX\nmatrix , as in Figure 1.1.Terms are the indexed units (further discussed in TERM\nSection 2.2); they are usually words, and for the moment you can think of\nOnline edition (c)\n2009 Cambridge UP4 1 Boolean retrieval\nAntony Julius The Hamlet Othello Macbeth . . .\nand Caesar Tempest\nCleopatra\nAntony 1 1 0 0 0 1\nBrutus 1 1 0 1 0 0\nCaesar 1 1 0 1 1 1\nCalpurnia 0 1 0 0 0 0\nCleopatra 1 0 0 0 0 0\nmercy 1 0 1 1 1 1\nworser 1 0 1 1 1 0\n. . .\n\u25eeFigure 1.1 A term-document incidence matrix. Matrix element (t,d)is 1 if the\nplay in column dcontains the word in row t, and is 0 otherwise.\nthem as words, but the information retrieval literature nor mally speaks of\nterms because some of them, such as perhaps I-9orHongKong are not usually\nthought of as words. Now, depending on whether we look at the m atrix rows\nor columns, we can have a vector for each term, which shows the documents\nit appears in, or a vector for each document, showing the term s that occur in\nit.2\nTo answer the query Brutus ANDCaesar AND NOT Calpurnia , we take the\nvectors for Brutus ,Caesar andCalpurnia , complement the last, and then do a\nbitwise AND :\n110100 AND 110111 AND 101111 = 100100\nThe answers for this query are thus Antony and Cleopatra and Hamlet (Fig-\nure1.2).\nThe Boolean retrieval model is a model for information retrieval in which we BOOLEAN RETRIEVAL\nMODEL can pose any query which is in the form of a Boolean expression of terms,\nthat is, in which terms are combined with the operators AND ,OR, and NOT .\nThe model views each document as just a set of words.\nLet us now consider a more realistic scenario, simultaneous ly using the\nopportunity to introduce some terminology and notation. Su ppose we have\nN=1 million documents. By documents we mean whatever units we have DOCUMENT\ndecided to build a retrieval system over. They might be indiv idual memos\nor chapters of a book (see Section 2.1.2 (page 20) for further discussion). We\nwill refer to the group of documents over which we perform ret rieval as the\n(document) collection . It is sometimes also referred to as a corpus (abody of COLLECTION\nCORPUS texts). Suppose each document is about 1000 words long (2\u20133 b ook pages). If\n2. Formally, we take the transpose of the matrix to be able to g et the terms as column vectors.\nOnline edition (c)\n2009 Cambridge UP1.1 An example information retrieval problem 5\nAntony and Cleopatra, Act III, Scene ii\nAgrippa [Aside to Domitius Enobarbus]: Why, Enobarbus,\nWhen Antony found Julius Caesar dead,\nHe cried almost to roaring; and he wept\nWhen at Philippi he found Brutus slain.\nHamlet, Act III, Scene ii\nLord Polonius: I did enact Julius Caesar: I was killed i\u2019 the\nCapitol; Brutus killed me.\n\u25eeFigure 1.2 Results from Shakespeare for the query Brutus ANDCaesar AND NOT\nCalpurnia .\nwe assume an average of 6 bytes per word including spaces and p unctuation,\nthen this is a document collection about 6 GB in size. Typical ly, there might\nbe about M=500,000 distinct terms in these documents. There is nothing\nspecial about the numbers we have chosen, and they might vary by an order\nof magnitude or more, but they give us some idea of the dimensi ons of the\nkinds of problems we need to handle. We will discuss and model these size\nassumptions in Section 5.1(page 86).\nOur goal is to develop a system to address the ad hoc retrieval task. This is AD HOC RETRIEVAL\nthe most standard IR task. In it, a system aims to provide docu ments from\nwithin the collection that are relevant to an arbitrary user information need,\ncommunicated to the system by means of a one-off, user-initi ated query. An\ninformation need is the topic about which the user desires to know more, and INFORMATION NEED\nis differentiated from a query , which is what the user conveys to the com- QUERY\nputer in an attempt to communicate the information need. A do cument is\nrelevant if it is one that the user perceives as containing informatio n of value RELEVANCE\nwith respect to their personal information need. Our exampl e above was\nrather arti\ufb01cial in that the information need was de\ufb01ned in t erms of par-\nticular words, whereas usually a user is interested in a topi c like \u201cpipeline\nleaks\u201d and would like to \ufb01nd relevant documents regardless o f whether they\nprecisely use those words or express the concept with other w ords such as\npipeline rupture . To assess the effectiveness of an IR system (i.e., the quality of EFFECTIVENESS\nits search results), a user will usually want to know two key s tatistics about\nthe system\u2019s returned results for a query:\nPrecision : What fraction of the returned results are relevant to the in forma- PRECISION\ntion need?\nRecall : What fraction of the relevant documents in the collection w ere re- RECALL\nturned by the system?\nOnline edition (c)\n2009 Cambridge UP6 1 Boolean retrieval\nDetailed discussion of relevance and evaluation measures i ncluding preci-\nsion and recall is found in Chapter 8.\nWe now cannot build a term-document matrix in a naive way. A 50 0K\u00d7\n1M matrix has half-a-trillion 0\u2019s and 1\u2019s \u2013 too many to \ufb01t in a c omputer\u2019s\nmemory. But the crucial observation is that the matrix is ext remely sparse,\nthat is, it has few non-zero entries. Because each document i s 1000 words\nlong, the matrix has no more than one billion 1\u2019s, so a minimum of 99.8% of\nthe cells are zero. A much better representation is to record only the things\nthat do occur, that is, the 1 positions.\nThis idea is central to the \ufb01rst major concept in information retrieval, the\ninverted index . The name is actually redundant: an index always maps back INVERTED INDEX\nfrom terms to the parts of a document where they occur. Nevert heless, in-\nverted index , or sometimes inverted \ufb01le , has become the standard term in infor-\nmation retrieval.3The basic idea of an inverted index is shown in Figure 1.3.\nWe keep a dictionary of terms (sometimes also referred to as a vocabulary or DICTIONARY\nVOCABULARY lexicon ; in this book, we use dictionary for the data structure and vocabulary\nLEXICONfor the set of terms). Then for each term, we have a list that re cords which\ndocuments the term occurs in. Each item in the list \u2013 which rec ords that a\nterm appeared in a document (and, later, often, the position s in the docu-\nment) \u2013 is conventionally called a posting .4The list is then called a postings POSTING\nPOSTINGS LIST list(or inverted list), and all the postings lists taken togethe r are referred to as\nthepostings . The dictionary in Figure 1.3has been sorted alphabetically and POSTINGS\neach postings list is sorted by document ID. We will see why th is is useful in\nSection 1.3, below, but later we will also consider alternatives to doin g this\n(Section 7.1.5 ).\n1.2 A \ufb01rst take at building an inverted index\nTo gain the speed bene\ufb01ts of indexing at retrieval time, we ha ve to build the\nindex in advance. The major steps in this are:\n1. Collect the documents to be indexed:\nFriends, Romans, countrymen. So let it be with Caesar . . .\n2. Tokenize the text, turning each document into a list of tok ens:\nFriends Romans countrymen So. . .\n3. Some information retrieval researchers prefer the term inverted \ufb01le , but expressions like in-\ndex construction andindex compression are much more common than inverted \ufb01le construction and\ninverted\ufb01lecompression . For consistency, we use (inverted)index throughout this book.\n4. In a (non-positional) inverted index, a posting is just a d ocument ID, but it is inherently\nassociated with a term, via the postings list it is placed on; sometimes we will also talk of a\n(term, docID) pair as a posting.\nOnline edition (c)\n2009 Cambridge UP1.2 A \ufb01rst take at building an inverted index 7\nBrutus\u2212\u2192 1 2 4 11 31 45 173 174\nCaesar\u2212\u2192 1 2 4 5 616 57 132 . . .\nCalpurnia\u2212\u2192 231 54 101\n...\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright /bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\nDictionary Postings\n\u25eeFigure 1.3 The two parts of an inverted index. The dictionary is commonl y kept\nin memory, with pointers to each postings list, which is stor ed on disk.\n3. Do linguistic preprocessing, producing a list of normali zed tokens, which\nare the indexing terms: friend roman countryman so. . .\n4. Index the documents that each term occurs in by creating an inverted in-\ndex, consisting of a dictionary and postings.\nWe will de\ufb01ne and discuss the earlier stages of processing, t hat is, steps 1\u20133,\nin Section 2.2(page 22). Until then you can think of tokens and normalized\ntokens as also loosely equivalent to words . Here, we assume that the \ufb01rst\n3 steps have already been done, and we examine building a basi c inverted\nindex by sort-based indexing.\nWithin a document collection, we assume that each document h as a unique\nserial number, known as the document identi\ufb01er ( docID ). During index con- DOC ID\nstruction, we can simply assign successive integers to each new document\nwhen it is \ufb01rst encountered. The input to indexing is a list of normalized\ntokens for each document, which we can equally think of as a li st of pairs of\nterm and docID, as in Figure 1.4. The core indexing step is sorting this list SORTING\nso that the terms are alphabetical, giving us the representa tion in the middle\ncolumn of Figure 1.4. Multiple occurrences of the same term from the same\ndocument are then merged.5Instances of the same term are then grouped,\nand the result is split into a dictionary and postings , as shown in the right\ncolumn of Figure 1.4. Since a term generally occurs in a number of docu-\nments, this data organization already reduces the storage r equirements of\nthe index. The dictionary also records some statistics, suc h as the number of\ndocuments which contain each term (the document frequency , which is here DOCUMENT\nFREQUENCY also the length of each postings list). This information is n ot vital for a ba-\nsic Boolean search engine, but it allows us to improve the ef\ufb01 ciency of the\n5. Unix users can note that these steps are similar to use of th esort and then uniq commands.\nOnline edition (c)\n2009 Cambridge UP8 1 Boolean retrieval\nDoc 1 Doc 2\nI did enact Julius Caesar: I was killed\ni\u2019 the Capitol; Brutus killed me.So let it be with Caesar. The noble Brutus\nhath told you Caesar was ambitious:\nterm docID\nI 1\ndid 1\nenact 1\njulius 1\ncaesar 1\nI 1\nwas 1\nkilled 1\ni\u2019 1\nthe 1\ncapitol 1\nbrutus 1\nkilled 1\nme 1\nso 2\nlet 2\nit 2\nbe 2\nwith 2\ncaesar 2\nthe 2\nnoble 2\nbrutus 2\nhath 2\ntold 2\nyou 2\ncaesar 2\nwas 2\nambitious 2=\u21d2term docID\nambitious 2\nbe 2\nbrutus 1\nbrutus 2\ncapitol 1\ncaesar 1\ncaesar 2\ncaesar 2\ndid 1\nenact 1\nhath 1\nI 1\nI 1\ni\u2019 1\nit 2\njulius 1\nkilled 1\nkilled 1\nlet 2\nme 1\nnoble 2\nso 2\nthe 1\nthe 2\ntold 2\nyou 2\nwas 1\nwas 2\nwith 2=\u21d2term doc. freq. \u2192 postings lists\nambitious 1\u2192 2\nbe 1\u2192 2\nbrutus 2\u2192 1\u21922\ncapitol 1\u2192 1\ncaesar 2\u2192 1\u21922\ndid 1\u2192 1\nenact 1\u2192 1\nhath 1\u2192 2\nI1\u2192 1\ni\u20191\u2192 1\nit1\u2192 2\njulius 1\u2192 1\nkilled 1\u2192 1\nlet 1\u2192 2\nme 1\u2192 1\nnoble 1\u2192 2\nso 1\u2192 2\nthe 2\u2192 1\u21922\ntold 1\u2192 2\nyou 1\u2192 2\nwas 2\u2192 1\u21922\nwith 1\u2192 2\n\u25eeFigure 1.4 Building an index by sorting and grouping. The sequence of te rms\nin each document, tagged by their documentID (left) is sorte d alphabetically (mid-\ndle). Instances of the same term are then grouped by word and t hen by documentID.\nThe terms and documentIDs are then separated out (right). Th e dictionary stores\nthe terms, and has a pointer to the postings list for each term . It commonly also\nstores other summary information such as, here, the documen t frequency of each\nterm. We use this information for improving query time ef\ufb01ci ency and, later, for\nweighting in ranked retrieval models. Each postings list st ores the list of documents\nin which a term occurs, and may store other information such a s the term frequency\n(the frequency of each term in each document) or the position (s) of the term in each\ndocument.\nOnline edition (c)\n2009 Cambridge UP1.2 A \ufb01rst take at building an inverted index 9\nsearch engine at query time, and it is a statistic later used i n many ranked re-\ntrieval models. The postings are secondarily sorted by docI D. This provides\nthe basis for ef\ufb01cient query processing. This inverted inde x structure is es-\nsentially without rivals as the most ef\ufb01cient structure for supporting ad hoc\ntext search.\nIn the resulting index, we pay for storage of both the diction ary and the\npostings lists. The latter are much larger, but the dictiona ry is commonly\nkept in memory, while postings lists are normally kept on dis k, so the size\nof each is important, and in Chapter 5we will examine how each can be\noptimized for storage and access ef\ufb01ciency. What data struc ture should be\nused for a postings list? A \ufb01xed length array would be wastefu l as some\nwords occur in many documents, and others in very few. For an i n-memory\npostings list, two good alternatives are singly linked list s or variable length\narrays. Singly linked lists allow cheap insertion of docume nts into postings\nlists (following updates, such as when recrawling the web fo r updated doc-\numents), and naturally extend to more advanced indexing str ategies such as\nskip lists (Section 2.3), which require additional pointers. Variable length ar-\nrays win in space requirements by avoiding the overhead for p ointers and in\ntime requirements because their use of contiguous memory in creases speed\non modern processors with memory caches. Extra pointers can in practice be\nencoded into the lists as offsets. If updates are relatively infrequent, variable\nlength arrays will be more compact and faster to traverse. We can also use a\nhybrid scheme with a linked list of \ufb01xed length arrays for eac h term. When\npostings lists are stored on disk, they are stored (perhaps c ompressed) as a\ncontiguous run of postings without explicit pointers (as in Figure 1.3), so as\nto minimize the size of the postings list and the number of dis k seeks to read\na postings list into memory.\n?Exercise 1.1 [\u22c6]\nDraw the inverted index that would be built for the following document collection.\n(See Figure 1.3for an example.)\nDoc 1 new home sales top forecasts\nDoc 2 home sales rise in july\nDoc 3 increase in home sales in july\nDoc 4 july new home sales rise\nExercise 1.2 [\u22c6]\nConsider these documents:\nDoc 1 breakthrough drug for schizophrenia\nDoc 2 new schizophrenia drug\nDoc 3 new approach for treatment of schizophrenia\nDoc 4 new hopes for schizophrenia patients\na. Draw the term-document incidence matrix for this documen t collection.\nOnline edition (c)\n2009 Cambridge UP10 1 Boolean retrieval\nBrutus\u2212\u2192 1\u21922\u21924\u219211\u219231\u219245\u2192173\u2192174\nCalpurnia\u2212\u2192 2\u219231\u219254\u2192101\nIntersection =\u21d2 2\u219231\n\u25eeFigure 1.5 Intersecting the postings lists for Brutus andCalpurnia from Figure 1.3.\nb. Draw the inverted index representation for this collecti on, as in Figure 1.3(page 7).\nExercise 1.3 [\u22c6]\nFor the document collection shown in Exercise 1.2, what are the returned results for\nthese queries:\na.schizophrenia ANDdrug\nb.forAND NOT(drugORapproach)\n1.3 Processing Boolean queries\nHow do we process a query using an inverted index and the basic Boolean\nretrieval model? Consider processing the simple conjunctive query : SIMPLE CONJUNCTIVE\nQUERIES\n(1.1)Brutus ANDCalpurnia\nover the inverted index partially shown in Figure 1.3(page 7). We:\n1.LocateBrutus in the Dictionary\n2.Retrieve its postings\n3.LocateCalpurnia in the Dictionary\n4.Retrieve its postings\n5.Intersect the two postings lists, as shown in Figure 1.5.\nThe intersection operation is the crucial one: we need to ef\ufb01ciently intersec t POSTINGS LIST\nINTERSECTION postings lists so as to be able to quickly \ufb01nd documents that c ontain both\nterms. (This operation is sometimes referred to as merging postings lists: POSTINGS MERGE\nthis slightly counterintuitive name re\ufb02ects using the term merge algorithm for\na general family of algorithms that combine multiple sorted lists by inter-\nleaved advancing of pointers through each; here we are mergi ng the lists\nwith a logical AND operation.)\nThere is a simple and effective method of intersecting posti ngs lists using\nthe merge algorithm (see Figure 1.6): we maintain pointers into both lists\nOnline edition (c)\n2009 Cambridge UP1.3 Processing Boolean queries 11\nINTERSECT (p1,p2)\n1answer\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2while p1/ne}ationslash=NILand p2/ne}ationslash=NIL\n3do if docID (p1) =docID (p2)\n4 then ADD(answer ,docID (p1))\n5 p1\u2190next(p1)\n6 p2\u2190next(p2)\n7 else if docID (p1)<docID (p2)\n8 then p1\u2190next(p1)\n9 else p2\u2190next(p2)\n10 return answer\n\u25eeFigure 1.6 Algorithm for the intersection of two postings lists p1and p2.\nand walk through the two postings lists simultaneously, in t ime linear in\nthe total number of postings entries. At each step, we compar e the docID\npointed to by both pointers. If they are the same, we put that d ocID in the\nresults list, and advance both pointers. Otherwise we advan ce the pointer\npointing to the smaller docID. If the lengths of the postings lists are xand\ny, the intersection takes O(x+y)operations. Formally, the complexity of\nquerying is \u0398(N), where Nis the number of documents in the collection.6\nOur indexing methods gain us just a constant, not a differenc e in \u0398time\ncomplexity compared to a linear scan, but in practice the con stant is huge.\nTo use this algorithm, it is crucial that postings be sorted b y a single global\nordering. Using a numeric sort by docID is one simple way to ac hieve this.\nWe can extend the intersection operation to process more com plicated queries\nlike:\n(1.2) (Brutus ORCaesar )AND NOT Calpurnia\nQuery optimization is the process of selecting how to organize the work of an- QUERY OPTIMIZATION\nswering a query so that the least total amount of work needs to be done by\nthe system. A major element of this for Boolean queries is the order in which\npostings lists are accessed. What is the best order for query processing? Con-\nsider a query that is an AND oftterms, for instance:\n(1.3)Brutus ANDCaesar ANDCalpurnia\nFor each of the tterms, we need to get its postings, then AND them together.\nThe standard heuristic is to process terms in order of increa sing document\n6. The notation \u0398(\u00b7)is used to express an asymptotically tight bound on the compl exity of\nan algorithm. Informally, this is often written as O(\u00b7), but this notation really expresses an\nasymptotic upper bound, which need not be tight ( Cormen et al. 1990 ).\nOnline edition (c)\n2009 Cambridge UP12 1 Boolean retrieval\nINTERSECT (/an}\u230ara\u230bketle{tt1, . . . , tn/an}\u230ara\u230bketri}ht)\n1terms\u2190SORTBYINCREASING FREQUENCY (/an}\u230ara\u230bketle{tt1, . . . , tn/an}\u230ara\u230bketri}ht)\n2result\u2190postings (f irst(terms))\n3terms\u2190rest(terms)\n4while terms/ne}ationslash=NILand result/ne}ationslash=NIL\n5doresult\u2190INTERSECT (result ,postings (f irst(terms)))\n6 terms\u2190rest(terms)\n7return result\n\u25eeFigure 1.7 Algorithm for conjunctive queries that returns the set of do cuments\ncontaining each term in the input list of terms.\nfrequency: if we start by intersecting the two smallest post ings lists, then all\nintermediate results must be no bigger than the smallest pos tings list, and we\nare therefore likely to do the least amount of total work. So, for the postings\nlists in Figure 1.3(page 7), we execute the above query as:\n(1.4) (Calpurnia ANDBrutus )ANDCaesar\nThis is a \ufb01rst justi\ufb01cation for keeping the frequency of term s in the dictionary:\nit allows us to make this ordering decision based on in-memor y data before\naccessing any postings list.\nConsider now the optimization of more general queries, such as:\n(1.5) (madding ORcrowd )AND (ignoble ORstrife )AND (killed ORslain)\nAs before, we will get the frequencies for all terms, and we ca n then (con-\nservatively) estimate the size of each ORby the sum of the frequencies of its\ndisjuncts. We can then process the query in increasing order of the size of\neach disjunctive term.\nFor arbitrary Boolean queries, we have to evaluate and tempo rarily store\nthe answers for intermediate expressions in a complex expre ssion. However,\nin many circumstances, either because of the nature of the qu ery language,\nor just because this is the most common type of query that user s submit, a\nquery is purely conjunctive. In this case, rather than viewi ng merging post-\nings lists as a function with two inputs and a distinct output , it is more ef-\n\ufb01cient to intersect each retrieved postings list with the cu rrent intermediate\nresult in memory, where we initialize the intermediate resu lt by loading the\npostings list of the least frequent term. This algorithm is s hown in Figure 1.7.\nThe intersection operation is then asymmetric: the interme diate results list\nis in memory while the list it is being intersected with is bei ng read from\ndisk. Moreover the intermediate results list is always at le ast as short as the\nother list, and in many cases it is orders of magnitude shorte r. The postings\nOnline edition (c)\n2009 Cambridge UP1.3 Processing Boolean queries 13\nintersection can still be done by the algorithm in Figure 1.6, but when the\ndifference between the list lengths is very large, opportun ities to use alter-\nnative techniques open up. The intersection can be calculat ed in place by\ndestructively modifying or marking invalid items in the int ermediate results\nlist. Or the intersection can be done as a sequence of binary s earches in the\nlong postings lists for each posting in the intermediate res ults list. Another\npossibility is to store the long postings list as a hashtable , so that membership\nof an intermediate result item can be calculated in constant rather than linear\nor log time. However, such alternative techniques are dif\ufb01c ult to combine\nwith postings list compression of the sort discussed in Chap ter5. Moreover,\nstandard postings list intersection operations remain nec essary when both\nterms of a query are very common.\n?Exercise 1.4 [\u22c6]\nFor the queries below, can we still run through the intersect ion in time O(x+y),\nwhere xand yare the lengths of the postings lists for Brutus andCaesar ? If not, what\ncan we achieve?\na.Brutus AND NOT Caesar\nb.Brutus OR NOTCaesar\nExercise 1.5 [\u22c6]\nExtend the postings merge algorithm to arbitrary Boolean qu ery formulas. What is\nits time complexity? For instance, consider:\nc.(Brutus ORCaesar )AND NOT (Antony ORCleopatra )\nCan we always merge in linear time? Linear in what? Can we do be tter than this?\nExercise 1.6 [\u22c6\u22c6]\nWe can use distributive laws for AND and ORto rewrite queries.\na.Show how to rewrite the query in Exercise 1.5into disjunctive normal form using\nthe distributive laws.\nb.Would the resulting query be more or less ef\ufb01ciently evaluat ed than the original\nform of this query?\nc.Is this result true in general or does it depend on the words an d the contents of\nthe document collection?\nExercise 1.7 [\u22c6]\nRecommend a query processing order for\nd.(tangerine ORtrees )AND (marmalade ORskies )AND (kaleidoscope OReyes)\ngiven the following postings list sizes:\nOnline edition (c)\n2009 Cambridge UP14 1 Boolean retrieval\nTerm Postings size\neyes 213312\nkaleidoscope 87009\nmarmalade 107913\nskies 271658\ntangerine 46653\ntrees 316812\nExercise 1.8 [\u22c6]\nIf the query is:\ne.friends ANDromans AND (NOTcountrymen )\nhow could we use the frequency of countrymen in evaluating the best query evaluation\norder? In particular, propose a way of handling negation in d etermining the order of\nquery processing.\nExercise 1.9 [\u22c6\u22c6]\nFor a conjunctive query, is processing postings lists in ord er of size guaranteed to be\noptimal? Explain why it is, or give an example where it isn\u2019t.\nExercise 1.10 [\u22c6\u22c6]\nWrite out a postings merge algorithm, in the style of Figure 1.6(page 11), for an xORy\nquery.\nExercise 1.11 [\u22c6\u22c6]\nHow should the Boolean query xAND NOT ybe handled? Why is naive evaluation\nof this query normally very expensive? Write out a postings m erge algorithm that\nevaluates this query ef\ufb01ciently.\n1.4 The extended Boolean model versus ranked retrieval\nThe Boolean retrieval model contrasts with ranked retrieval models such as the RANKED RETRIEVAL\nMODEL vector space model (Section 6.3), in which users largely use free text queries ,\nFREE TEXT QUERIESthat is, just typing one or more words rather than using a prec ise language\nwith operators for building up query expressions, and the sy stem decides\nwhich documents best satisfy the query. Despite decades of a cademic re-\nsearch on the advantages of ranked retrieval, systems imple menting the Boo-\nlean retrieval model were the main or only search option prov ided by large\ncommercial information providers for three decades until t he early 1990s (ap-\nproximately the date of arrival of the World Wide Web). Howev er, these\nsystems did not have just the basic Boolean operations ( AND ,OR, and NOT )\nwhich we have presented so far. A strict Boolean expression o ver terms with\nan unordered results set is too limited for many of the inform ation needs\nthat people have, and these systems implemented extended Bo olean retrieval\nmodels by incorporating additional operators such as term p roximity oper-\nators. A proximity operator is a way of specifying that two terms in a query PROXIMITY OPERATOR\nOnline edition (c)\n2009 Cambridge UP1.4 The extended Boolean model versus ranked retrieval 15\nmust occur close to each other in a document, where closeness may be mea-\nsured by limiting the allowed number of intervening words or by reference\nto a structural unit such as a sentence or paragraph.\n\u270eExample 1.1: Commercial Boolean searching: Westlaw. Westlaw ( http://www.westlaw.com/ )\nis the largest commercial legal search service (in terms of t he number of paying sub-\nscribers), with over half a million subscribers performing millions of searches a day\nover tens of terabytes of text data. The service was started i n 1975. In 2005, Boolean\nsearch (called \u201cTerms and Connectors\u201d by Westlaw) was still the default, and used\nby a large percentage of users, although ranked free text que rying (called \u201cNatural\nLanguage\u201d by Westlaw) was added in 1992. Here are some exampl e Boolean queries\non Westlaw:\nInformation need: Information on the legal theories involved in preventing th e\ndisclosure of trade secrets by employees formerly employed by a competing\ncompany. Query: \"trade secret\" /s disclos! /s prevent /s employe!\nInformation need: Requirements for disabled people to be able to access a work-\nplace.\nQuery: disab! /p access! /s work-site work-place (employment /3 pl ace)\nInformation need: Cases about a host\u2019s responsibility for drunk guests.\nQuery: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\nNote the long, precise queries and the use of proximity opera tors, both uncommon\nin web search. Submitted queries average about ten words in l ength. Unlike web\nsearch conventions, a space between words represents disju nction (the tightest bind-\ning operator), & is AND and /s, /p, and / kask for matches in the same sentence,\nsame paragraph or within kwords respectively. Double quotes give a phrase search\n(consecutive words); see Section 2.4(page 39). The exclamation mark (!) gives a trail-\ning wildcard query (see Section 3.2, page 51); thusliab! matches all words starting\nwithliab. Additionally work-site matches any of worksite ,work-site orwork site ; see\nSection 2.2.1 (page 22). Typical expert queries are usually carefully de\ufb01ned and i ncre-\nmentally developed until they obtain what look to be good res ults to the user.\nMany users, particularly professionals, prefer Boolean qu ery models. Boolean\nqueries are precise: a document either matches the query or i t does not. This of-\nfers the user greater control and transparency over what is r etrieved. And some do-\nmains, such as legal materials, allow an effective means of d ocument ranking within a\nBoolean model: Westlaw returns documents in reverse chrono logical order, which is\nin practice quite effective. In 2007, the majority of law lib rarians still seem to rec-\nommend terms and connectors for high recall searches, and th e majority of legal\nusers think they are getting greater control by using them. H owever, this does not\nmean that Boolean queries are more effective for profession al searchers. Indeed, ex-\nperimenting on a Westlaw subcollection, Turtle (1994 ) found that free text queries\nproduced better results than Boolean queries prepared by We stlaw\u2019s own reference\nlibrarians for the majority of the information needs in his e xperiments. A general\nproblem with Boolean search is that using AND operators tends to produce high pre-\ncision but low recall searches, while using ORoperators gives low precision but high\nrecall searches, and it is dif\ufb01cult or impossible to \ufb01nd a sat isfactory middle ground.\nIn this chapter, we have looked at the structure and construc tion of a basic\nOnline edition (c)\n2009 Cambridge UP16 1 Boolean retrieval\ninverted index, comprising a dictionary and postings lists . We introduced\nthe Boolean retrieval model, and examined how to do ef\ufb01cient retrieval via\nlinear time merges and simple query optimization. In Chapte rs2\u20137we will\nconsider in detail richer query models and the sort of augmen ted index struc-\ntures that are needed to handle them ef\ufb01ciently. Here we just mention a few\nof the main additional things we would like to be able to do:\n1.We would like to better determine the set of terms in the dicti onary and\nto provide retrieval that is tolerant to spelling mistakes a nd inconsistent\nchoice of words.\n2.It is often useful to search for compounds or phrases that den ote a concept\nsuch as\u201coperating system\u201d . As the Westlaw examples show, we might also\nwish to do proximity queries such as Gates NEARMicrosoft . To answer\nsuch queries, the index has to be augmented to capture the pro ximities of\nterms in documents.\n3.A Boolean model only records term presence or absence, but of ten we\nwould like to accumulate evidence, giving more weight to doc uments that\nhave a term several times as opposed to ones that contain it on ly once. To\nbe able to do this we need term frequency information (the number of times TERM FREQUENCY\na term occurs in a document) in postings lists.\n4.Boolean queries just retrieve a set of matching documents, b ut commonly\nwe wish to have an effective method to order (or \u201crank\u201d) the re turned\nresults. This requires having a mechanism for determining a document\nscore which encapsulates how good a match a document is for a q uery.\nWith these additional ideas, we will have seen most of the bas ic technol-\nogy that supports ad hoc searching over unstructured inform ation. Ad hoc\nsearching over documents has recently conquered the world, powering not\nonly web search engines but the kind of unstructured search t hat lies behind\nthe large eCommerce websites. Although the main web search e ngines differ\nby emphasizing free text querying, most of the basic issues a nd technologies\nof indexing and querying remain the same, as we will see in lat er chapters.\nMoreover, over time, web search engines have added at least p artial imple-\nmentations of some of the most popular operators from extend ed Boolean\nmodels: phrase search is especially popular and most have a v ery partial\nimplementation of Boolean operators. Nevertheless, while these options are\nliked by expert searchers, they are little used by most peopl e and are not the\nmain focus in work on trying to improve web search engine perf ormance.\n?Exercise 1.12 [\u22c6]\nWrite a query using Westlaw syntax which would \ufb01nd any of the w ordsprofessor ,\nteacher , orlecturer in the same sentence as a form of the verb explain .\nOnline edition (c)\n2009 Cambridge UP1.5 References and further reading 17\nExercise 1.13 [\u22c6]\nTry using the Boolean search features on a couple of major web search engines. For\ninstance, choose a word, such as burglar , and submit the queries (i) burglar , (ii)burglar\nANDburglar , and (iii) burglar ORburglar . Look at the estimated number of results and\ntop hits. Do they make sense in terms of Boolean logic? Often t hey haven\u2019t for major\nsearch engines. Can you make sense of what is going on? What ab out if you try\ndifferent words? For example, query for (i) knight , (ii)conquer , and then (iii) knight OR\nconquer . What bound should the number of results from the \ufb01rst two que ries place\non the third query? Is this bound observed?\n1.5 References and further reading\nThe practical pursuit of computerized information retriev al began in the late\n1940s ( Cleverdon 1991 ,Liddy 2005 ). A great increase in the production of\nscienti\ufb01c literature, much in the form of less formal techni cal reports rather\nthan traditional journal articles, coupled with the availa bility of computers,\nled to interest in automatic document retrieval. However, i n those days, doc-\nument retrieval was always based on author, title, and keywo rds; full-text\nsearch came much later.\nThe article of Bush (1945 ) provided lasting inspiration for the new \ufb01eld:\n\u201cConsider a future device for individual use, which is a sort of mech-\nanized private \ufb01le and library. It needs a name, and, to coin o ne at\nrandom, \u2018memex\u2019 will do. A memex is a device in which an indivi dual\nstores all his books, records, and communications, and whic h is mech-\nanized so that it may be consulted with exceeding speed and \ufb02e xibility.\nIt is an enlarged intimate supplement to his memory.\u201d\nThe term Information Retrieval was coined by Calvin Mooers in 1948/1950\n(Mooers 1950 ).\nIn 1958, much newspaper attention was paid to demonstration s at a con-\nference (see Taube and Wooster 1958 ) of IBM \u201cauto-indexing\u201d machines, based\nprimarily on the work of H. P . Luhn. Commercial interest quic kly gravitated\ntowards Boolean retrieval systems, but the early years saw a heady debate\nover various disparate technologies for retrieval systems . For example Moo-\ners(1961 ) dissented:\n\u201cIt is a common fallacy, underwritten at this date by the inve stment of\nseveral million dollars in a variety of retrieval hardware, that the al-\ngebra of George Boole (1847) is the appropriate formalism fo r retrieval\nsystem design. This view is as widely and uncritically accep ted as it is\nwrong.\u201d\nThe observation of AND vs. ORgiving you opposite extremes in a precision/\nrecall tradeoff, but not the middle ground comes from ( Lee and Fox 1988 ).\nOnline edition (c)\n2009 Cambridge UP18 1 Boolean retrieval\nThe book ( Witten et al. 1999 ) is the standard reference for an in-depth com-\nparison of the space and time ef\ufb01ciency of the inverted index versus other\npossible data structures; a more succinct and up-to-date pr esentation ap-\npears in Zobel and Moffat (2006 ). We further discuss several approaches in\nChapter 5.\nFriedl (2006 ) covers the practical usage of regular expressions for searching. REGULAR EXPRESSIONS\nThe underlying computer science appears in ( Hopcroft et al. 2000 ).\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 19\n2The term vocabulary and postings\nlists\nRecall the major steps in inverted index construction:\n1.Collect the documents to be indexed.\n2.Tokenize the text.\n3.Do linguistic preprocessing of tokens.\n4.Index the documents that each term occurs in.\nIn this chapter we \ufb01rst brie\ufb02y mention how the basic unit of a d ocument can\nbe de\ufb01ned and how the character sequence that it comprises is determined\n(Section 2.1). We then examine in detail some of the substantive linguis-\ntic issues of tokenization and linguistic preprocessing, w hich determine the\nvocabulary of terms which a system uses (Section 2.2). Tokenization is the\nprocess of chopping character streams into tokens, while li nguistic prepro-\ncessing then deals with building equivalence classes of tok ens which are the\nset of terms that are indexed. Indexing itself is covered in C hapters 1and 4.\nThen we return to the implementation of postings lists. In Se ction 2.3, we\nexamine an extended postings list data structure that suppo rts faster query-\ning, while Section 2.4covers building postings data structures suitable for\nhandling phrase and proximity queries, of the sort that comm only appear in\nboth extended Boolean models and on the web.\n2.1 Document delineation and character sequence decoding\n2.1.1 Obtaining the character sequence in a document\nDigital documents that are the input to an indexing process a re typically\nbytes in a \ufb01le or on a web server. The \ufb01rst step of processing is to convert this\nbyte sequence into a linear sequence of characters. For the c ase of plain En-\nglish text in ASCII encoding, this is trivial. But often thin gs get much more\nOnline edition (c)\n2009 Cambridge UP20 2 The term vocabulary and postings lists\ncomplex. The sequence of characters may be encoded by one of v arious sin-\ngle byte or multibyte encoding schemes, such as Unicode UTF- 8, or various\nnational or vendor-speci\ufb01c standards. We need to determine the correct en-\ncoding. This can be regarded as a machine learning classi\ufb01ca tion problem,\nas discussed in Chapter 13,1but is often handled by heuristic methods, user\nselection, or by using provided document metadata. Once the encoding is\ndetermined, we decode the byte sequence to a character seque nce. We might\nsave the choice of encoding because it gives some evidence ab out what lan-\nguage the document is written in.\nThe characters may have to be decoded out of some binary repre sentation\nlike Microsoft Word DOC \ufb01les and/or a compressed format such as zip \ufb01les.\nAgain, we must determine the document format, and then an app ropriate\ndecoder has to be used. Even for plain text documents, additi onal decoding\nmay need to be done. In XML documents (Section 10.1, page 197), charac-\nter entities, such as &amp; , need to be decoded to give the correct character,\nnamely &for&amp; . Finally, the textual part of the document may need to\nbe extracted out of other material that will not be processed . This might be\nthe desired handling for XML \ufb01les, if the markup is going to be ignored; we\nwould almost certainly want to do this with postscript or PDF \ufb01les. We will\nnot deal further with these issues in this book, and will assu me henceforth\nthat our documents are a list of characters. Commercial prod ucts usually\nneed to support a broad range of document types and encodings , since users\nwant things to just work with their data as is. Often, they jus t think of docu-\nments as text inside applications and are not even aware of ho w it is encoded\non disk. This problem is usually solved by licensing a softwa re library that\nhandles decoding document formats and character encodings .\nThe idea that text is a linear sequence of characters is also c alled into ques-\ntion by some writing systems, such as Arabic, where text take s on some\ntwo dimensional and mixed order characteristics, as shown i n Figures 2.1\nand 2.2. But, despite some complicated writing system conventions , there\nis an underlying sequence of sounds being represented and he nce an essen-\ntially linear structure remains, and this is what is represe nted in the digital\nrepresentation of Arabic, as shown in Figure 2.1.\n2.1.2 Choosing a document unit\nThe next phase is to determine what the document unit for indexing is. Thus DOCUMENT UNIT\nfar we have assumed that documents are \ufb01xed units for the purp oses of in-\ndexing. For example, we take each \ufb01le in a folder as a document . But there\n1. A classi\ufb01er is a function that takes objects of some sort an d assigns them to one of a number\nof distinct classes (see Chapter 13). Usually classi\ufb01cation is done by machine learning method s\nsuch as probabilistic models, but it can also be done by hand- written rules.\nOnline edition (c)\n2009 Cambridge UP2.1 Document delineation and character sequence decoding 21\n/\u0628\u064c afii62760/\u064e afii62766\u0622\u0650    \u21d0     \u064c    \u0628   \u0627  \u062a  \u0650   \u0643     \n             un b \u0101  t  i k  \n/kit\u0101bun/ \u2018a book\u2019  \n \n\u25eeFigure 2.1 An example of a vocalized Modern Standard Arabic word. The wr iting\nis from right to left and letters undergo complex mutations a s they are combined. The\nrepresentation of short vowels (here, /i/ and /u/) and the \ufb01n al /n/ (nunation) de-\nparts from strict linearity by being represented as diacrit ics above and below letters.\nNevertheless, the represented text is still clearly a linea r ordering of characters repre-\nsenting sounds. Full vocalization, as here, normally appea rs only in the Koran and\nchildren\u2019s books. Day-to-day text is unvocalized (short vo wels are not represented\nbut the letter for \u00afa would still appear) or partially vocalized, with short vow els in-\nserted in places where the writer perceives ambiguities. Th ese choices add further\ncomplexities to indexing.\n /afii62764/afii62824/afii62784 /afii62833/afii62808 /afii62782/afii62757/\u0627afii62783/afii62772/afii62817/ \u0627afii62767/afii62818/afii62812/afii62766/afii62784\u06271962  /afii62780/afii62803/afii62761 132 /afii62833/afii62785/afii62823/afii62782/afii62809/afii62817/\u0644 \u0627afii62841/afii62766/afii62774/afii62840/ \u0627afii62825/afii62820 /afii62760/afii62820/afii62760/afii62802 .   \n                               \u2190 \u2192   \u2190 \u2192                   \u2190 START  \n\u2018Algeria achieved its independence in 1962 after 132 years of French occupation.\u2019 \n \n\u25eeFigure 2.2 The conceptual linear order of characters is not necessaril y the order\nthat you see on the page. In languages that are written right- to-left, such as Hebrew\nand Arabic, it is quite common to also have left-to-right tex t interspersed, such as\nnumbers and dollar amounts. With modern Unicode representa tion concepts, the\norder of characters in \ufb01les matches the conceptual order, an d the reversal of displayed\ncharacters is handled by the rendering system, but this may n ot be true for documents\nin older encodings.\nare many cases in which you might want to do something differe nt. A tra-\nditional Unix (mbox-format) email \ufb01le stores a sequence of e mail messages\n(an email folder) in one \ufb01le, but you might wish to regard each email mes-\nsage as a separate document. Many email messages now contain attached\ndocuments, and you might then want to regard the email messag e and each\ncontained attachment as separate documents. If an email mes sage has an\nattached zip \ufb01le, you might want to decode the zip \ufb01le and rega rd each \ufb01le\nit contains as a separate document. Going in the opposite dir ection, various\npieces of web software (such as latex2html ) take things that you might regard\nas a single document (e.g., a Powerpoint \ufb01le or a L ATEX document) and split\nthem into separate HTML pages for each slide or subsection, s tored as sep-\narate \ufb01les. In these cases, you might want to combine multipl e \ufb01les into a\nsingle document.\nMore generally, for very long documents, the issue of indexi nggranularity INDEXING\nGRANULARITY arises. For a collection of books, it would usually be a bad id ea to index an\nOnline edition (c)\n2009 Cambridge UP22 2 The term vocabulary and postings lists\nentire book as a document. A search for Chinese toys might bring up a book\nthat mentions China in the \ufb01rst chapter and toys in the last chapter, but this\ndoes not make it relevant to the query. Instead, we may well wi sh to index\neach chapter or paragraph as a mini-document. Matches are th en more likely\nto be relevant, and since the documents are smaller it will be much easier for\nthe user to \ufb01nd the relevant passages in the document. But why stop there?\nWe could treat individual sentences as mini-documents. It b ecomes clear\nthat there is a precision/recall tradeoff here. If the units get too small, we\nare likely to miss important passages because terms were dis tributed over\nseveral mini-documents, while if units are too large we tend to get spurious\nmatches and the relevant information is hard for the user to \ufb01 nd.\nThe problems with large document units can be alleviated by u se of ex-\nplicit or implicit proximity search (Sections 2.4.2 and 7.2.2 ), and the trade-\noffs in resulting system performance that we are hinting at a re discussed\nin Chapter 8. The issue of index granularity, and in particular a need to\nsimultaneously index documents at multiple levels of granu larity, appears\nprominently in XML retrieval, and is taken up again in Chapte r10. An IR\nsystem should be designed to offer choices of granularity. F or this choice to\nbe made well, the person who is deploying the system must have a good\nunderstanding of the document collection, the users, and th eir likely infor-\nmation needs and usage patterns. For now, we will henceforth assume that\na suitable size document unit has been chosen, together with an appropriate\nway of dividing or aggregating \ufb01les, if needed.\n2.2 Determining the vocabulary of terms\n2.2.1 Tokenization\nGiven a character sequence and a de\ufb01ned document unit, token ization is the\ntask of chopping it up into pieces, called tokens , perhaps at the same time\nthrowing away certain characters, such as punctuation. Her e is an example\nof tokenization:\nInput: Friends, Romans, Countrymen, lend me your ears;\nOutput: Friends Romans Countrymen lend me your ears\nThese tokens are often loosely referred to as terms or words, but it is some-\ntimes important to make a type/token distinction. A token is an instance TOKEN\nof a sequence of characters in some particular document that are grouped\ntogether as a useful semantic unit for processing. A type is the class of all TYPE\ntokens containing the same character sequence. A term is a (perhaps nor- TERM\nmalized) type that is included in the IR system\u2019s dictionary . The set of index\nterms could be entirely distinct from the tokens, for instan ce, they could be\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 23\nsemantic identi\ufb01ers in a taxonomy, but in practice in modern IR systems they\nare strongly related to the tokens in the document. However, rather than be-\ning exactly the tokens that appear in the document, they are u sually derived\nfrom them by various normalization processes which are disc ussed in Sec-\ntion 2.2.3 .2For example, if the document to be indexed is to sleep perchance to\ndream , then there are 5 tokens, but only 4 types (since there are 2 in stances of\nto). However, if tois omitted from the index (as a stop word, see Section 2.2.2\n(page 27)), then there will be only 3 terms: sleep ,perchance , and dream .\nThe major question of the tokenization phase is what are the c orrect tokens\nto use? In this example, it looks fairly trivial: you chop on w hitespace and\nthrow away punctuation characters. This is a starting point , but even for\nEnglish there are a number of tricky cases. For example, what do you do\nabout the various uses of the apostrophe for possession and c ontractions?\nMr. O\u2019Neill thinks that the boys\u2019 stories about Chile\u2019s capi tal aren\u2019t\namusing.\nForO\u2019Neill , which of the following is the desired tokenization?\nneill\noneill\no\u2019neill\no\u2019neill\noneill ?\nAnd for aren\u2019t , is it:\naren\u2019t\narent\nare n\u2019t\naren t?\nA simple strategy is to just split on all non-alphanumeric ch aracters, but\nwhile oneill looks okay, aren tlooks intuitively bad. For all of them,\nthe choices determine which Boolean queries will match. A qu ery ofneill\nANDcapital will match in three cases but not the other two. In how many\ncases would a query of o\u2019neill ANDcapital match? If no preprocessing of a\nquery is done, then it would match in only one of the \ufb01ve cases. For either\n2. That is, as de\ufb01ned here, tokens that are not indexed (stop w ords) are not terms, and if mul-\ntiple tokens are collapsed together via normalization, the y are indexed as one term, under the\nnormalized form. However, we later relax this de\ufb01nition whe n discussing classi\ufb01cation and\nclustering in Chapters 13\u201318, where there is no index. In these chapters, we drop the requi re-\nment of inclusion in the dictionary. A term means a normalized word.\nOnline edition (c)\n2009 Cambridge UP24 2 The term vocabulary and postings lists\nBoolean or free text queries, you always want to do the exact s ame tokeniza-\ntion of document and query words, generally by processing qu eries with the\nsame tokenizer. This guarantees that a sequence of characte rs in a text will\nalways match the same sequence typed in a query.3\nThese issues of tokenization are language-speci\ufb01c. It thus requires the lan-\nguage of the document to be known. Language identi\ufb01cation based on clas- LANGUAGE\nIDENTIFICATION si\ufb01ers that use short character subsequences as features is highly effective;\nmost languages have distinctive signature patterns (see pa ge46for refer-\nences).\nFor most languages and particular domains within them there are unusual\nspeci\ufb01c tokens that we wish to recognize as terms, such as the programming\nlanguages C++ andC#, aircraft names like B-52 , or a T.V . show name such\nasM*A*S*H \u2013 which is suf\ufb01ciently integrated into popular culture that you\n\ufb01nd usages such as M*A*S*H-style hospitals . Computer technology has in-\ntroduced new types of character sequences that a tokenizer s hould probably\ntokenize as a single token, including email addresses ( jblack@mail.yahoo.com ),\nweb URLs ( http://stuff.big.com/new/specials.html ),numeric IP addresses ( 142.32.48.231 ),\npackage tracking numbers ( 1Z9999W99845399981 ), and more. One possible\nsolution is to omit from indexing tokens such as monetary amo unts, num-\nbers, and URLs, since their presence greatly expands the siz e of the vocab-\nulary. However, this comes at a large cost in restricting wha t people can\nsearch for. For instance, people might want to search in a bug database for\nthe line number where an error occurs. Items such as the date o f an email,\nwhich have a clear semantic type, are often indexed separate ly as document\nmetadata (see Section 6.1, page 110).\nIn English, hyphenation is used for various purposes ranging from split- HYPHENS\nting up vowels in words ( co-education ) to joining nouns as names ( Hewlett-\nPackard ) to a copyediting device to show word grouping ( the hold-him-back-\nand-drag-him-away maneuver ). It is easy to feel that the \ufb01rst example should be\nregarded as one token (and is indeed more commonly written as just coedu-\ncation ), the last should be separated into words, and that the middl e case is\nunclear. Handling hyphens automatically can thus be comple x: it can either\nbe done as a classi\ufb01cation problem, or more commonly by some h euristic\nrules, such as allowing short hyphenated pre\ufb01xes on words, b ut not longer\nhyphenated forms.\nConceptually, splitting on white space can also split what s hould be re-\ngarded as a single token. This occurs most commonly with name s (San Fran-\ncisco, Los Angeles ) but also with borrowed foreign phrases ( au fait ) and com-\n3. For the free text case, this is straightforward. The Boole an case is more complex: this tok-\nenization may produce multiple terms from one query word. Th is can be handled by combining\nthe terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system\nto handle the opposite case where the user entered as two term s something that was tokenized\ntogether in the document processing.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 25\npounds that are sometimes written as a single word and someti mes space\nseparated (such as white space vs.whitespace ). Other cases with internal spaces\nthat we might wish to regard as a single token include phone nu mbers ((800)234-\n2333 ) and dates ( Mar 11, 1983 ). Splitting tokens on spaces can cause bad\nretrieval results, for example, if a search for York University mainly returns\ndocuments containing New York University . The problems of hyphens and\nnon-separating whitespace can even interact. Advertiseme nts for air fares\nfrequently contain items like San Francisco-Los Angeles , where simply doing\nwhitespace splitting would give unfortunate results. In su ch cases, issues of\ntokenization interact with handling phrase queries (which we discuss in Sec-\ntion 2.4(page 39)), particularly if we would like queries for all of lowercase ,\nlower-case and lower case to return the same results. The last two can be han-\ndled by splitting on hyphens and using a phrase index. Gettin g the \ufb01rst case\nright would depend on knowing that it is sometimes written as two words\nand also indexing it in this way. One effective strategy in pr actice, which\nis used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis\n(Example 1.1), is to encourage users to enter hyphens wherever they may be\npossible, and whenever there is a hyphenated form, the syste m will general-\nize the query to cover all three of the one word, hyphenated, a nd two word\nforms, so that a query for over-eager will search for over-eager OR\u201cover eager\u201d\nORovereager . However, this strategy depends on user training, since if y ou\nquery using either of the other two forms, you get no generali zation.\nEach new language presents some new issues. For instance, Fr ench has a\nvariant use of the apostrophe for a reduced de\ufb01nite article \u2018 the\u2019 before a word\nbeginning with a vowel (e.g., l\u2019ensemble ) and has some uses of the hyphen\nwith postposed clitic pronouns in imperatives and question s (e.g., donne-\nmoi\u2018give me\u2019). Getting the \ufb01rst case correct will affect the cor rect indexing\nof a fair percentage of nouns and adjectives: you would want d ocuments\nmentioning both l\u2019ensemble and un ensemble to be indexed under ensemble .\nOther languages make the problem harder in new ways. German w rites\ncompound nouns without spaces (e.g., Computerlinguistik \u2018computational lin- COMPOUNDS\nguistics\u2019; Lebensversicherungsgesellschaftsangestellter \u2018life insurance company\nemployee\u2019). Retrieval systems for German greatly bene\ufb01t fr om the use of a\ncompound-splitter module, which is usually implemented by seeing if a word COMPOUND -SPLITTER\ncan be subdivided into multiple words that appear in a vocabu lary. This phe-\nnomenon reaches its limit case with major East Asian Languag es (e.g., Chi-\nnese, Japanese, Korean, and Thai), where text is written wit hout any spaces\nbetween words. An example is shown in Figure 2.3. One approach here is to\nperform word segmentation as prior linguistic processing. Methods of word WORD SEGMENTATION\nsegmentation vary from having a large vocabulary and taking the longest\nvocabulary match with some heuristics for unknown words to t he use of\nmachine learning sequence models, such as hidden Markov mod els or condi-\ntional random \ufb01elds, trained over hand-segmented words (se e the references\nOnline edition (c)\n2009 Cambridge UP26 2 The term vocabulary and postings lists\u0000 \u0001 \u0002 \u0003 \u0004 \u0005 \u0006 \u0007 \u0005 \b \t \n \u000b \f \r \u000e \u000f \u0010 \u0011 \u0012 \u0013 \u0014 \u0015 \u0016\u0017 \u0018 \u0019\n\u0000 \u0001 \u0002 \u0003 \u0005 \b \t \u001a \u001b \u001c \u001d \u001e \u001f  ! \" # $ % & '\u0018\n\u0012 '\u0018 ( ) * \u0019\n\u0000 \u0001 \u0002 \u0003 + , # - \b \r . / \u0012\n\u25eeFigure 2.3 The standard unsegmented form of Chinese text using the simp li\ufb01ed\ncharacters of mainland China. There is no whitespace betwee n words, not even be-\ntween sentences \u2013 the apparent space after the Chinese perio d (\u25e6) is just a typograph-\nical illusion caused by placing the character on the left sid e of its square box. The\n\ufb01rst sentence is just words in Chinese characters with no spa ces between them. The\nsecond and third sentences include Arabic numerals and punc tuation breaking up\nthe Chinese characters.\n\u25eeFigure 2.4 Ambiguities in Chinese word segmentation. The two characte rs can\nbe treated as one word meaning \u2018monk\u2019 or as a sequence of two wo rds meaning \u2018and\u2019\nand \u2018still\u2019.\na an and are as at be by for from\nhas he in is it its of on that the\nto was were will with\n\u25eeFigure 2.5 A stop list of 25 semantically non-selective words which are common\nin Reuters-RCV1.\nin Section 2.5). Since there are multiple possible segmentations of chara cter\nsequences (see Figure 2.4), all such methods make mistakes sometimes, and\nso you are never guaranteed a consistent unique tokenizatio n. The other ap-\nproach is to abandon word-based indexing and to do all indexi ng via just\nshort subsequences of characters (character k-grams), regardless of whether\nparticular sequences cross word boundaries or not. Three re asons why this\napproach is appealing are that an individual Chinese charac ter is more like a\nsyllable than a letter and usually has some semantic content , that most words\nare short (the commonest length is 2 characters), and that, g iven the lack of\nstandardization of word breaking in the writing system, it i s not always clear\nwhere word boundaries should be placed anyway. Even in Engli sh, some\ncases of where to put word boundaries are just orthographic c onventions \u2013\nthink of notwithstanding vs.not to mention orinto vs.on to \u2013 but people are\neducated to write the words with consistent use of spaces.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 27\n2.2.2 Dropping common terms: stop words\nSometimes, some extremely common words which would appear t o be of\nlittle value in helping select documents matching a user nee d are excluded\nfrom the vocabulary entirely. These words are called stop words . The general STOP WORDS\nstrategy for determining a stop list is to sort the terms by collection frequency COLLECTION\nFREQUENCY (the total number of times each term appears in the document c ollection),\nand then to take the most frequent terms, often hand-\ufb01ltered for their se-\nmantic content relative to the domain of the documents being indexed, as\nastop list , the members of which are then discarded during indexing. An STOP LIST\nexample of a stop list is shown in Figure 2.5. Using a stop list signi\ufb01cantly\nreduces the number of postings that a system has to store; we w ill present\nsome statistics on this in Chapter 5(see Table 5.1, page 87). And a lot of\nthe time not indexing stop words does little harm: keyword se arches with\nterms like theandbydon\u2019t seem very useful. However, this is not true for\nphrase searches. The phrase query \u201cPresident of the United States\u201d , which con-\ntains two stop words, is more precise than President AND \u201cUnited States\u201d . The\nmeaning of \ufb02ightstoLondon is likely to be lost if the word tois stopped out. A\nsearch for Vannevar Bush\u2019s article As we may think will be dif\ufb01cult if the \ufb01rst\nthree words are stopped out, and the system searches simply f or documents\ncontaining the word think . Some special query types are disproportionately\naffected. Some song titles and well known pieces of verse con sist entirely of\nwords that are commonly on stop lists ( To be or not to be ,Let It Be ,I don\u2019t want\nto be, . . . ).\nThe general trend in IR systems over time has been from standa rd use of\nquite large stop lists (200\u2013300 terms) to very small stop lis ts (7\u201312 terms)\nto no stop list whatsoever. Web search engines generally do n ot use stop\nlists. Some of the design of modern IR systems has focused pre cisely on\nhow we can exploit the statistics of language so as to be able t o cope with\ncommon words in better ways. We will show in Section 5.3(page 95) how\ngood compression techniques greatly reduce the cost of stor ing the postings\nfor common words. Section 6.2.1 (page 117) then discusses how standard\nterm weighting leads to very common words having little impa ct on doc-\nument rankings. Finally, Section 7.1.5 (page 140) shows how an IR system\nwith impact-sorted indexes can terminate scanning a postin gs list early when\nweights get small, and hence common words do not cause a large additional\nprocessing cost for the average query, even though postings lists for stop\nwords are very long. So for most modern IR systems, the additi onal cost of\nincluding stop words is not that big \u2013 neither in terms of inde x size nor in\nterms of query processing time.\nOnline edition (c)\n2009 Cambridge UP28 2 The term vocabulary and postings lists\nQuery term Terms in documents that should be matched\nWindows Windows\nwindows Windows, windows, window\nwindow window, windows\n\u25eeFigure 2.6 An example of how asymmetric expansion of query terms can use fully\nmodel users\u2019 expectations.\n2.2.3 Normalization (equivalence classing of terms)\nHaving broken up our documents (and also our query) into toke ns, the easy\ncase is if tokens in the query just match tokens in the token li st of the doc-\nument. However, there are many cases when two character sequ ences are\nnot quite the same but you would like a match to occur. For inst ance, if you\nsearch for USA , you might hope to also match documents containing U.S.A .\nToken normalization is the process of canonicalizing tokens so that matches TOKEN\nNORMALIZATION occur despite super\ufb01cial differences in the character sequ ences of the to-\nkens.4The most standard way to normalize is to implicitly create equivalence EQUIVALENCE CLASSES\nclasses , which are normally named after one member of the set. For ins tance,\nif the tokens anti-discriminatory and antidiscriminatory are both mapped onto\nthe term antidiscriminatory , in both the document text and queries, then searches\nfor one term will retrieve documents that contain either.\nThe advantage of just using mapping rules that remove charac ters like hy-\nphens is that the equivalence classing to be done is implicit , rather than being\nfully calculated in advance: the terms that happen to become identical as the\nresult of these rules are the equivalence classes. It is only easy to write rules\nof this sort that remove characters. Since the equivalence c lasses are implicit,\nit is not obvious when you might want to add characters. For in stance, it\nwould be hard to know to turn antidiscriminatory into anti-discriminatory .\nAn alternative to creating equivalence classes is to mainta in relations be-\ntween unnormalized tokens. This method can be extended to ha nd-constructed\nlists of synonyms such as carand automobile , a topic we discuss further in\nChapter 9. These term relationships can be achieved in two ways. The us ual\nway is to index unnormalized tokens and to maintain a query ex pansion list\nof multiple vocabulary entries to consider for a certain que ry term. A query\nterm is then effectively a disjunction of several postings l ists. The alterna-\ntive is to perform the expansion during index construction. When the doc-\nument contains automobile , we index it under caras well (and, usually, also\nvice-versa). Use of either of these methods is considerably less ef\ufb01cient than\nequivalence classing, as there are more postings to store an d merge. The \ufb01rst\n4. It is also often referred to as term normalization , but we prefer to reserve the name term for the\noutput of the normalization process.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 29\nmethod adds a query expansion dictionary and requires more p rocessing at\nquery time, while the second method requires more space for s toring post-\nings. Traditionally, expanding the space required for the p ostings lists was\nseen as more disadvantageous, but with modern storage costs , the increased\n\ufb02exibility that comes from distinct postings lists is appea ling.\nThese approaches are more \ufb02exible than equivalence classes because the\nexpansion lists can overlap while not being identical. This means there can\nbe an asymmetry in expansion. An example of how such an asymme try can\nbe exploited is shown in Figure 2.6: if the user enters windows , we wish to\nallow matches with the capitalized Windows operating system, but this is not\nplausible if the user enters window , even though it is plausible for this query\nto also match lowercase windows .\nThe best amount of equivalence classing or query expansion t o do is a\nfairly open question. Doing some de\ufb01nitely seems a good idea . But doing a\nlot can easily have unexpected consequences of broadening q ueries in unin-\ntended ways. For instance, equivalence-classing U.S.A. and USA to the latter\nby deleting periods from tokens might at \ufb01rst seem very reaso nable, given\nthe prevalent pattern of optional use of periods in acronyms . However, if I\nput in as my query term C.A.T. , I might be rather upset if it matches every\nappearance of the word catin documents.5\nBelow we present some of the forms of normalization that are c ommonly\nemployed and how they are implemented. In many cases they see m helpful,\nbut they can also do harm. In fact, you can worry about many det ails of\nequivalence classing, but it often turns out that providing processing is done\nconsistently to the query and to documents, the \ufb01ne details m ay not have\nmuch aggregate effect on performance.\nAccents and diacritics. Diacritics on characters in English have a fairly\nmarginal status, and we might well want clich\u00e9 and cliche to match, or naive\nand na\u00efve . This can be done by normalizing tokens to remove diacritics . In\nmany other languages, diacritics are a regular part of the wr iting system and\ndistinguish different sounds. Occasionally words are dist inguished only by\ntheir accents. For instance, in Spanish, pe\u00f1a is \u2018a cliff\u2019, while pena is \u2018sorrow\u2019.\nNevertheless, the important question is usually not prescr iptive or linguistic\nbut is a question of how users are likely to write queries for t hese words. In\nmany cases, users will enter queries for words without diacr itics, whether\nfor reasons of speed, laziness, limited software, or habits born of the days\nwhen it was hard to use non-ASCII text on many computer system s. In these\ncases, it might be best to equate all words to a form without di acritics.\n5. At the time we wrote this chapter (Aug. 2005), this was actu ally the case on Google: the top\nresult for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/ .\nOnline edition (c)\n2009 Cambridge UP30 2 The term vocabulary and postings lists\nCapitalization/case-folding. A common strategy is to do case-folding by re- CASE -FOLDING\nducing all letters to lower case. Often this is a good idea: it will allow in-\nstances of Automobile at the beginning of a sentence to match with a query of\nautomobile . It will also help on a web search engine when most of your user s\ntype in ferrari when they are interested in a Ferrari car. On the other hand,\nsuch case folding can equate words that might better be kept a part. Many\nproper nouns are derived from common nouns and so are disting uished only\nby case, including companies ( General Motors ,The Associated Press ), govern-\nment organizations ( the Fed vs.fed) and person names ( Bush ,Black ). We al-\nready mentioned an example of unintended query expansion wi th acronyms,\nwhich involved not only acronym normalization ( C.A.T.\u2192CAT ) but also\ncase-folding ( CAT\u2192cat).\nFor English, an alternative to making every token lowercase is to just make\nsome tokens lowercase. The simplest heuristic is to convert to lowercase\nwords at the beginning of a sentence and all words occurring i n a title that is\nall uppercase or in which most or all words are capitalized. T hese words are\nusually ordinary words that have been capitalized. Mid-sen tence capitalized\nwords are left as capitalized (which is usually correct). Th is will mostly avoid\ncase-folding in cases where distinctions should be kept apa rt. The same task\ncan be done more accurately by a machine learning sequence mo del which\nuses more features to make the decision of when to case-fold. This is known\nastruecasing . However, trying to get capitalization right in this way pro bably TRUECASING\ndoesn\u2019t help if your users usually use lowercase regardless of the correct case\nof words. Thus, lowercasing everything often remains the mo st practical\nsolution.\nOther issues in English. Other possible normalizations are quite idiosyn-\ncratic and particular to English. For instance, you might wi sh to equate\nne\u2019er and never or the British spelling colour and the American spelling color .\nDates, times and similar items come in multiple formats, pre senting addi-\ntional challenges. You might wish to collapse together 3/12/91 and Mar. 12,\n1991 . However, correct processing here is complicated by the fac t that in the\nU.S., 3/12/91 isMar. 12, 1991 , whereas in Europe it is 3 Dec 1991 .\nOther languages. English has maintained a dominant position on the WWW;\napproximately 60% of web pages are in English ( Gerrand 2007 ). But that still\nleaves 40% of the web, and the non-English portion might be ex pected to\ngrow over time, since less than one third of Internet users an d less than 10%\nof the world\u2019s population primarily speak English. And ther e are signs of\nchange: Sifry (2007 ) reports that only about one third of blog posts are in\nEnglish.\nOther languages again present distinctive issues in equiva lence classing.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 31\u0000 \u0001 \u0002 \u0003 \u0004 \u0005 \u0006 \u0007 \b \u0006 \t \n \u000b \f \r \u000e \u000f \u0010 \u0001 \u0011 \u0012 \u0013 \u0014 \u0015 \u0016 \u0017 \u0018 \u0019 \u0007 \u001a \u001b\u001c \u001d \u001e \u001f \u001f  ! \"  ! # $\f % \u0001 \f & ' ( ) \t * + , - . / 0 ) \u0010 \r1\f 2 3 4 5 6 7 & + 8 9 \n : ; : < \u0007 = > \t ? @ A B C \u0015 - D E6 8 9 \n : ; : < ) F G * H I \t * :\n\u001c J) K + L M N ? O P\n\u001c Q RS\u0001 T \u0007 U V V W X Y & Z [ N ? ) \u001b + \\ ] ; ^ _ + \u0012 ` 4 a + b; c \u0007 d e * f V g h V - ? i N j k l m n \u0013 : A o \u0006 \b \u0006 p N 5 +q\nV r s t u & v w x )\nQ y z {h | & } \u0006 \u0015 ~ \u007f M ? @ A\n\u25eeFigure 2.7 Japanese makes use of multiple intermingled writing system s and,\nlike Chinese, does not segment words. The text is mainly Chin ese characters with\nthe hiragana syllabary for in\ufb02ectional endings and functio n words. The part in latin\nletters is actually a Japanese expression, but has been take n up as the name of an\nenvironmental campaign by 2004 Nobel Peace Prize winner Wan gari Maathai. His\nname is written using the katakana syllabary in the middle of the \ufb01rst line. The \ufb01rst\nfour characters of the \ufb01nal line express a monetary amount th at we would want to\nmatch with \u00a5500,000 (500,000 Japanese yen).\nThe French word for thehas distinctive forms based not only on the gender\n(masculine or feminine) and number of the following noun, bu t also depend-\ning on whether the following word begins with a vowel: le,la,l\u2019,les. We may\nwell wish to equivalence class these various forms of the. German has a con-\nvention whereby vowels with an umlaut can be rendered instea d as a two\nvowel digraph. We would want to treat Sch\u00fctze and Schuetze as equivalent.\nJapanese is a well-known dif\ufb01cult writing system, as illust rated in Fig-\nure2.7. Modern Japanese is standardly an intermingling of multipl e alpha-\nbets, principally Chinese characters, two syllabaries (hi ragana and katakana)\nand western characters (Latin letters, Arabic numerals, an d various sym-\nbols). While there are strong conventions and standardizat ion through the\neducation system over the choice of writing system, in many c ases the same\nword can be written with multiple writing systems. For examp le, a word\nmay be written in katakana for emphasis (somewhat like itali cs). Or a word\nmay sometimes be written in hiragana and sometimes in Chines e charac-\nters. Successful retrieval thus requires complex equivale nce classing across\nthe writing systems. In particular, an end user might common ly present a\nquery entirely in hiragana, because it is easier to type, jus t as Western end\nusers commonly use all lowercase.\nDocument collections being indexed can include documents f rom many\ndifferent languages. Or a single document can easily contai n text from mul-\ntiple languages. For instance, a French email might quote cl auses from a\ncontract document written in English. Most commonly, the la nguage is de-\ntected and language-particular tokenization and normaliz ation rules are ap-\nplied at a predetermined granularity, such as whole documen ts or individual\nparagraphs, but this still will not correctly deal with case s where language\nchanges occur for brief quotations. When document collecti ons contain mul-\nOnline edition (c)\n2009 Cambridge UP32 2 The term vocabulary and postings lists\ntiple languages, a single index may have to contain terms of s everal lan-\nguages. One option is to run a language identi\ufb01cation classi \ufb01er on docu-\nments and then to tag terms in the vocabulary for their langua ge. Or this\ntagging can simply be omitted, since it is relatively rare fo r the exact same\ncharacter sequence to be a word in different languages.\nWhen dealing with foreign or complex words, particularly fo reign names,\nthe spelling may be unclear or there may be variant translite ration standards\ngiving different spellings (for example, Chebyshev and Tchebycheff orBeijing\nand Peking ). One way of dealing with this is to use heuristics to equiva-\nlence class or expand terms with phonetic equivalents. The t raditional and\nbest known such algorithm is the Soundex algorithm, which we cover in\nSection 3.4(page 63).\n2.2.4 Stemming and lemmatization\nFor grammatical reasons, documents are going to use differe nt forms of a\nword, such as organize ,organizes , and organizing . Additionally, there are fami-\nlies of derivationally related words with similar meanings , such as democracy ,\ndemocratic , and democratization . In many situations, it seems as if it would be\nuseful for a search for one of these words to return documents that contain\nanother word in the set.\nThe goal of both stemming and lemmatization is to reduce in\ufb02e ctional\nforms and sometimes derivationally related forms of a word t o a common\nbase form. For instance:\nam, are, is\u21d2be\ncar, cars, car\u2019s, cars\u2019 \u21d2car\nThe result of this mapping of text will be something like:\nthe boy\u2019s cars are different colors \u21d2\nthe boy car be differ color\nHowever, the two words differ in their \ufb02avor. Stemming usually refers to STEMMING\na crude heuristic process that chops off the ends of words in t he hope of\nachieving this goal correctly most of the time, and often inc ludes the re-\nmoval of derivational af\ufb01xes. Lemmatization usually refers to doing things LEMMATIZATION\nproperly with the use of a vocabulary and morphological anal ysis of words,\nnormally aiming to remove in\ufb02ectional endings only and to re turn the base\nor dictionary form of a word, which is known as the lemma . If confronted LEMMA\nwith the token saw, stemming might return just s, whereas lemmatization\nwould attempt to return either seeorsawdepending on whether the use of\nthe token was as a verb or a noun. The two may also differ in that stemming\nmost commonly collapses derivationally related words, whe reas lemmatiza-\ntion commonly only collapses the different in\ufb02ectional for ms of a lemma.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 33\nLinguistic processing for stemming or lemmatization is oft en done by an ad-\nditional plug-in component to the indexing process, and a nu mber of such\ncomponents exist, both commercial and open-source.\nThe most common algorithm for stemming English, and one that has re-\npeatedly been shown to be empirically very effective, is Porter\u2019s algorithm PORTER STEMMER\n(Porter 1980 ). The entire algorithm is too long and intricate to present h ere,\nbut we will indicate its general nature. Porter\u2019s algorithm consists of 5 phases\nof word reductions, applied sequentially. Within each phas e there are var-\nious conventions to select rules, such as selecting the rule from each rule\ngroup that applies to the longest suf\ufb01x. In the \ufb01rst phase, th is convention is\nused with the following rule group:\n(2.1) Rule Example\nSSES\u2192 SS caresses \u2192 caress\nIES\u2192 I ponies \u2192 poni\nSS\u2192 SS caress \u2192 caress\nS\u2192 cats\u2192 cat\nMany of the later rules use a concept of the measure of a word, which loosely\nchecks the number of syllables to see whether a word is long en ough that it\nis reasonable to regard the matching portion of a rule as a suf \ufb01x rather than\nas part of the stem of a word. For example, the rule:\n(m>1) EMENT\u2192\nwould map replacement toreplac , but not cement toc. The of\ufb01cial site for the\nPorter Stemmer is:\nhttp://www.tartarus.org/ \u02dcmartin/PorterStemmer/\nOther stemmers exist, including the older, one-pass Lovins stemmer ( Lovins\n1968 ), and newer entrants like the Paice/Husk stemmer ( Paice 1990 ); see:\nhttp://www.cs.waikato.ac.nz/ \u02dceibe/stemmers/\nhttp://www.comp.lancs.ac.uk/computing/research/stem ming/\nFigure 2.8presents an informal comparison of the different behaviors of these\nstemmers. Stemmers use language-speci\ufb01c rules, but they re quire less know-\nledge than a lemmatizer, which needs a complete vocabulary a nd morpho-\nlogical analysis to correctly lemmatize words. Particular domains may also\nrequire special stemming rules. However, the exact stemmed form does not\nmatter, only the equivalence classes it forms.\nRather than using a stemmer, you can use a lemmatizer , a tool from Nat- LEMMATIZER\nural Language Processing which does full morphological ana lysis to accu-\nrately identify the lemma for each word. Doing full morpholo gical analysis\nproduces at most very modest bene\ufb01ts for retrieval. It is har d to say more,\nOnline edition (c)\n2009 Cambridge UP34 2 The term vocabulary and postings lists\nSample text: Such an analysis can reveal features that are not easily visi ble\nfrom the variations in the individual genes and can lead to a p icture of\nexpression that is more biologically transparent and acces sible to\ninterpretation\nLovins stemmer: such an analys can reve featur that ar not eas vis from th\nvari in th individu gen and can lead to a pictur of expres that i s mor\nbiolog transpar and acces to interpres\nPorter stemmer: such an analysi can reveal featur that ar not easili visibl\nfrom the variat in the individu gene and can lead to a pictur of express\nthat is more biolog transpar and access to interpret\nPaice stemmer: such an analys can rev feat that are not easy vis from the\nvary in the individ gen and can lead to a pict of express that is mor\nbiolog transp and access to interpret\n\u25eeFigure 2.8 A comparison of three stemming algorithms on a sample text.\nbecause either form of normalization tends not to improve En glish informa-\ntion retrieval performance in aggregate \u2013 at least not by ver y much. While\nit helps a lot for some queries, it equally hurts performance a lot for others.\nStemming increases recall while harming precision. As an ex ample of what\ncan go wrong, note that the Porter stemmer stems all of the fol lowing words:\noperate operating operates operation operative operative s operational\ntooper. However, since operate in its various forms is a common verb, we\nwould expect to lose considerable precision on queries such as the following\nwith Porter stemming:\noperational ANDresearch\noperating ANDsystem\noperative ANDdentistry\nFor a case like this, moving to using a lemmatizer would not co mpletely \ufb01x\nthe problem because particular in\ufb02ectional forms are used i n particular col-\nlocations: a sentence with the words operate and system is not a good match\nfor the query operating ANDsystem . Getting better value from term normaliza-\ntion depends more on pragmatic issues of word use than on form al issues of\nlinguistic morphology.\nThe situation is different for languages with much more morp hology (such\nas Spanish, German, and Finnish). Results in the European CL EF evaluations\nhave repeatedly shown quite large gains from the use of stemm ers (and com-\npound splitting for languages like German); see the referen ces in Section 2.5.\nOnline edition (c)\n2009 Cambridge UP2.2 Determining the vocabulary of terms 35\n?Exercise 2.1 [\u22c6]\nAre the following statements true or false?\na.In a Boolean retrieval system, stemming never lowers precis ion.\nb.In a Boolean retrieval system, stemming never lowers recall .\nc.Stemming increases the size of the vocabulary.\nd.Stemming should be invoked at indexing time but not while pro cessing a query.\nExercise 2.2 [\u22c6]\nSuggest what normalized form should be used for these words ( including the word\nitself as a possibility):\na.\u2019Cos\nb.Shi\u2019ite\nc.cont\u2019d\nd.Hawai\u2019i\ne.O\u2019Rourke\nExercise 2.3 [\u22c6]\nThe following pairs of words are stemmed to the same form by th e Porter stemmer.\nWhich pairs would you argue shouldn\u2019t be con\ufb02ated. Give your reasoning.\na.abandon/abandonment\nb.absorbency/absorbent\nc.marketing/markets\nd.university/universe\ne.volume/volumes\nExercise 2.4 [\u22c6]\nFor the Porter stemmer rule group shown in ( 2.1):\na.What is the purpose of including an identity rule such as SS \u2192SS?\nb.Applying just this rule group, what will the following words be stemmed to?\ncircus canaries boss\nc.What rule should be added to correctly stem pony ?\nd.The stemming for ponies and pony might seem strange. Does it have a deleterious\neffect on retrieval? Why or why not?\nOnline edition (c)\n2009 Cambridge UP36 2 The term vocabulary and postings lists\n\u25eeFigure 2.9 Postings lists with skip pointers. The postings intersecti on can use a\nskip pointer when the end point is still less than the item on t he other list.\n2.3 Faster postings list intersection via skip pointers\nIn the remainder of this chapter, we will discuss extensions to postings list\ndata structures and ways to increase the ef\ufb01ciency of using p ostings lists. Re-\ncall the basic postings list intersection operation from Se ction 1.3(page 10):\nwe walk through the two postings lists simultaneously, in ti me linear in the\ntotal number of postings entries. If the list lengths are mand n, the intersec-\ntion takes O(m+n)operations. Can we do better than this? That is, empiri-\ncally, can we usually process postings list intersection in sublinear time? We\ncan, if the index isn\u2019t changing too fast.\nOne way to do this is to use a skip list by augmenting postings lists with SKIP LIST\nskip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are\neffectively shortcuts that allow us to avoid processing par ts of the postings\nlist that will not \ufb01gure in the search results. The two questi ons are then where\nto place skip pointers and how to do ef\ufb01cient merging using sk ip pointers.\nConsider \ufb01rst ef\ufb01cient merging, with Figure 2.9as an example. Suppose\nwe\u2019ve stepped through the lists in the \ufb01gure until we have mat ched 8on\neach list and moved it to the results list. We advance both poi nters, giving us\n16on the upper list and 41on the lower list. The smallest item is then the\nelement 16on the top list. Rather than simply advancing the upper point er,\nwe \ufb01rst check the skip list pointer and note that 28 is also les s than 41. Hence\nwe can follow the skip list pointer, and then we advance the up per pointer\nto28. We thus avoid stepping to 19and 23on the upper list. A number\nof variant versions of postings list intersection with skip pointers is possible\ndepending on when exactly you check the skip pointer. One ver sion is shown\nOnline edition (c)\n2009 Cambridge UP2.3 Faster postings list intersection via skip pointers 37\nINTERSECT WITHSKIPS(p1,p2)\n1answer\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2while p1/ne}ationslash=NILand p2/ne}ationslash=NIL\n3do if docID (p1) =docID (p2)\n4 then ADD(answer ,docID (p1))\n5 p1\u2190next(p1)\n6 p2\u2190next(p2)\n7 else if docID (p1)<docID (p2)\n8 then if hasSkip (p1)and(docID (skip(p1))\u2264docID (p2))\n9 then while hasSkip (p1)and(docID (skip(p1))\u2264docID (p2))\n10 dop1\u2190skip(p1)\n11 else p1\u2190next(p1)\n12 else if hasSkip (p2)and(docID (skip(p2))\u2264docID (p1))\n13 then while hasSkip (p2)and(docID (skip(p2))\u2264docID (p1))\n14 dop2\u2190skip(p2)\n15 else p2\u2190next(p2)\n16 return answer\n\u25eeFigure 2.10 Postings lists intersection with skip pointers.\nin Figure 2.10. Skip pointers will only be available for the original posti ngs\nlists. For an intermediate result in a complex query, the cal lhasSkip (p)will\nalways return false. Finally, note that the presence of skip pointers only helps\nfor AND queries, not for ORqueries.\nWhere do we place skips? There is a tradeoff. More skips means shorter\nskip spans, and that we are more likely to skip. But it also mea ns lots of\ncomparisons to skip pointers, and lots of space storing skip pointers. Fewer\nskips means few pointer comparisons, but then long skip span s which means\nthat there will be fewer opportunities to skip. A simple heur istic for placing\nskips, which has been found to work well in practice, is that f or a postings\nlist of length P, use\u221a\nPevenly-spaced skip pointers. This heuristic can be\nimproved upon; it ignores any details of the distribution of query terms.\nBuilding effective skip pointers is easy if an index is relat ively static; it\nis harder if a postings list keeps changing because of update s. A malicious\ndeletion strategy can render skip lists ineffective.\nChoosing the optimal encoding for an inverted index is an eve r-changing\ngame for the system builder, because it is strongly dependen t on underly-\ning computer technologies and their relative speeds and siz es. Traditionally,\nCPUs were slow, and so highly compressed techniques were not optimal.\nNow CPUs are fast and disk is slow, so reducing disk postings l ist size dom-\ninates. However, if you\u2019re running a search engine with ever ything in mem-\nOnline edition (c)\n2009 Cambridge UP38 2 The term vocabulary and postings lists\nory then the equation changes again. We discuss the impact of hardware\nparameters on index construction time in Section 4.1(page 68) and the im-\npact of index size on system speed in Chapter 5.\n?Exercise 2.5 [\u22c6]\nWhy are skip pointers not useful for queries of the form xORy?\nExercise 2.6 [\u22c6]\nWe have a two-word query. For one term the postings list consi sts of the following 16\nentries:\n[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\nand for the other it is the one entry postings list:\n[47].\nWork out how many comparisons would be done to intersect the t wo postings lists\nwith the following two strategies. Brie\ufb02y justify your answ ers:\na.Using standard postings lists\nb.Using postings lists stored with skip pointers, with a skip l ength of\u221a\nP, as sug-\ngested in Section 2.3.\nExercise 2.7 [\u22c6]\nConsider a postings intersection between this postings lis t, with skip pointers:\n3 5 9 15 24 39 60 68 75 81 84 89 92 96 97 100 115\nand the following intermediate result postings list (which hence has no skip pointers):\n3 5 89 95 97 99 100 101\nTrace through the postings intersection algorithm in Figur e2.10 (page 37).\na.How often is a skip pointer followed (i.e., p1is advanced to skip(p1))?\nb.How many postings comparisons will be made by this algorithm while intersect-\ning the two lists?\nc.How many postings comparisons would be made if the postings l ists are inter-\nsected without the use of skip pointers?\nOnline edition (c)\n2009 Cambridge UP2.4 Positional postings and phrase queries 39\n2.4 Positional postings and phrase queries\nMany complex or technical concepts and many organization an d product\nnames are multiword compounds or phrases. We would like to be able to\npose a query such as Stanford University by treating it as a phrase so that a\nsentence in a document like The inventor Stanford Ovshinsky never went to uni-\nversity. is not a match. Most recent search engines support a double qu otes\nsyntax (\u201cstanford university\u201d ) for phrase queries , which has proven to be very PHRASE QUERIES\neasily understood and successfully used by users. As many as 10% of web\nqueries are phrase queries, and many more are implicit phras e queries (such\nas person names), entered without use of double quotes. To be able to sup-\nport such queries, it is no longer suf\ufb01cient for postings lis ts to be simply lists\nof documents that contain individual terms. In this section we consider two\napproaches to supporting phrase queries and their combinat ion. A search\nengine should not only support phrase queries, but implemen t them ef\ufb01-\nciently. A related but distinct concept is term proximity we ighting, where a\ndocument is preferred to the extent that the query terms appe ar close to each\nother in the text. This technique is covered in Section 7.2.2 (page 144) in the\ncontext of ranked retrieval.\n2.4.1 Biword indexes\nOne approach to handling phrases is to consider every pair of consecutive\nterms in a document as a phrase. For example, the text Friends, Romans,\nCountrymen would generate the biwords : BIWORD INDEX\nfriendsromans\nromanscountrymen\nIn this model, we treat each of these biwords as a vocabulary t erm. Being\nable to process two-word phrase queries is immediate. Longe r phrases can\nbe processed by breaking them down. The query stanford university palo alto\ncan be broken into the Boolean query on biwords:\n\u201cstanforduniversity\u201d AND\u201cuniversitypalo\u201d AND\u201cpaloalto\u201d\nThis query could be expected to work fairly well in practice, but there can\nand will be occasional false positives. Without examining t he documents,\nwe cannot verify that the documents matching the above Boole an query do\nactually contain the original 4 word phrase.\nAmong possible queries, nouns and noun phrases have a specia l status in\ndescribing the concepts people are interested in searching for. But related\nnouns can often be divided from each other by various functio n words, in\nphrases such as the abolition of slavery orrenegotiation of the constitution . These\nneeds can be incorporated into the biword indexing model in t he following\nOnline edition (c)\n2009 Cambridge UP40 2 The term vocabulary and postings lists\nway. First, we tokenize the text and perform part-of-speech -tagging.6We\ncan then group terms into nouns, including proper nouns, (N) and function\nwords, including articles and prepositions, (X), among oth er classes. Now\ndeem any string of terms of the form NX*N to be an extended biwo rd. Each\nsuch extended biword is made a term in the vocabulary. For exa mple:\nrenegotiation of the constitution\nN X X N\nTo process a query using such an extended biword index, we nee d to also\nparse it into N\u2019s and X\u2019s, and then segment the query into exte nded biwords,\nwhich can be looked up in the index.\nThis algorithm does not always work in an intuitively optima l manner\nwhen parsing longer queries into Boolean queries. Using the above algo-\nrithm, the query\ncostoverruns ona power plant\nis parsed into\n\u201ccostoverruns\u201d AND\u201coverrunspower\u201d AND\u201cpowerplant\u201d\nwhereas it might seem a better query to omit the middle biword . Better\nresults can be obtained by using more precise part-of-speec h patterns that\nde\ufb01ne which extended biwords should be indexed.\nThe concept of a biword index can be extended to longer sequen ces of\nwords, and if the index includes variable length word sequen ces, it is gen-\nerally referred to as a phrase index . Indeed, searches for a single term are PHRASE INDEX\nnot naturally handled in a biword index (you would need to sca n the dic-\ntionary for all biwords containing the term), and so we also n eed to have an\nindex of single-word terms. While there is always a chance of false positive\nmatches, the chance of a false positive match on indexed phra ses of length 3\nor more becomes very small indeed. But on the other hand, stor ing longer\nphrases has the potential to greatly expand the vocabulary s ize. Maintain-\ning exhaustive phrase indexes for phrases of length greater than two is a\ndaunting prospect, and even use of an exhaustive biword dict ionary greatly\nexpands the size of the vocabulary. However, towards the end of this sec-\ntion we discuss the utility of the strategy of using a partial phrase index in a\ncompound indexing scheme.\n6. Part of speech taggers classify words as nouns, verbs, etc . \u2013 or, in practice, often as \ufb01ner-\ngrained classes like \u201cplural proper noun\u201d. Many fairly accu rate (c. 96% per-tag accuracy) part-\nof-speech taggers now exist, usually trained by machine lea rning methods on hand-tagged text.\nSee, for instance, Manning and Sch\u00fctze (1999 , ch. 10).\nOnline edition (c)\n2009 Cambridge UP2.4 Positional postings and phrase queries 41\nto, 993427:\n/an}\u230ara\u230bketle{t1, 6:/an}\u230ara\u230bketle{t7, 18, 33, 72, 86, 231 /an}\u230ara\u230bketri}ht;\n2, 5:/an}\u230ara\u230bketle{t1, 17, 74, 222, 255/an}\u230ara\u230bketri}ht;\n4, 5:/an}\u230ara\u230bketle{t8, 16, 190, 429, 433 /an}\u230ara\u230bketri}ht;\n5, 2:/an}\u230ara\u230bketle{t363, 367/an}\u230ara\u230bketri}ht;\n7, 3:/an}\u230ara\u230bketle{t13, 23, 191/an}\u230ara\u230bketri}ht; . . ./an}\u230ara\u230bketri}ht\nbe, 178239:\n/an}\u230ara\u230bketle{t1, 2:/an}\u230ara\u230bketle{t17, 25/an}\u230ara\u230bketri}ht;\n4, 5:/an}\u230ara\u230bketle{t17, 191, 291, 430, 434 /an}\u230ara\u230bketri}ht;\n5, 3:/an}\u230ara\u230bketle{t14, 19, 101/an}\u230ara\u230bketri}ht; . . ./an}\u230ara\u230bketri}ht\n\u25eeFigure 2.11 Positional index example. The word tohas a document frequency\n993,477, and occurs 6 times in document 1 at positions 7, 18, 3 3, etc.\n2.4.2 Positional indexes\nFor the reasons given, a biword index is not the standard solu tion. Rather,\napositional index is most commonly employed. Here, for each term in the POSITIONAL INDEX\nvocabulary, we store postings of the form docID: /an}\u230ara\u230bketle{tposition1, position2, . . . /an}\u230ara\u230bketri}ht,\nas shown in Figure 2.11, where each position is a token index in the docu-\nment. Each posting will also usually record the term frequen cy, for reasons\ndiscussed in Chapter 6.\nTo process a phrase query, you still need to access the invert ed index en-\ntries for each distinct term. As before, you would start with the least frequent\nterm and then work to further restrict the list of possible ca ndidates. In the\nmerge operation, the same general technique is used as befor e, but rather\nthan simply checking that both terms are in a document, you al so need to\ncheck that their positions of appearance in the document are compatible with\nthe phrase query being evaluated. This requires working out offsets between\nthe words.\n\u270eExample 2.1: Satisfying phrase queries. Suppose the postings lists for toand\nbeare as in Figure 2.11, and the query is \u201ctobeornotto be\u201d . The postings lists to access\nare:to,be,or,not. We will examine intersecting the postings lists for toandbe. We\n\ufb01rst look for documents that contain both terms. Then, we loo k for places in the lists\nwhere there is an occurrence of bewith a token index one higher than a position of to,\nand then we look for another occurrence of each word with toke n index 4 higher than\nthe \ufb01rst occurrence. In the above lists, the pattern of occur rences that is a possible\nmatch is:\nto:/an}\u230ara\u230bketle{t. . . ; 4:/an}\u230ara\u230bketle{t. . . ,429,433/an}\u230ara\u230bketri}ht; . . ./an}\u230ara\u230bketri}ht\nbe:/an}\u230ara\u230bketle{t. . . ; 4:/an}\u230ara\u230bketle{t. . . ,430,434/an}\u230ara\u230bketri}ht; . . ./an}\u230ara\u230bketri}ht\nOnline edition (c)\n2009 Cambridge UP42 2 The term vocabulary and postings lists\nPOSITIONAL INTERSECT (p1,p2,k)\n1answer\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2while p1/ne}ationslash=NILand p2/ne}ationslash=NIL\n3do if docID (p1) =docID (p2)\n4 then l\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n5 pp1\u2190positions (p1)\n6 pp2\u2190positions (p2)\n7 while pp1/ne}ationslash=NIL\n8 do while pp2/ne}ationslash=NIL\n9 do if|pos(pp1)\u2212pos(pp2)|\u2264k\n10 then ADD(l,pos(pp2))\n11 else if pos(pp2)>pos(pp1)\n12 then break\n13 pp2\u2190next(pp2)\n14 while l/ne}ationslash=/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}htand|l[0]\u2212pos(pp1)|>k\n15 doDELETE (l[0])\n16 for each ps\u2208l\n17 doADD(answer ,/an}\u230ara\u230bketle{tdocID (p1),pos(pp1),ps/an}\u230ara\u230bketri}ht)\n18 pp1\u2190next(pp1)\n19 p1\u2190next(p1)\n20 p2\u2190next(p2)\n21 else if docID (p1)<docID (p2)\n22 then p1\u2190next(p1)\n23 else p2\u2190next(p2)\n24 return answer\n\u25eeFigure 2.12 An algorithm for proximity intersection of postings lists p1and p2.\nThe algorithm \ufb01nds places where the two terms appear within kwords of each other\nand returns a list of triples giving docID and the term positi on in p1and p2.\nThe same general method is applied for within kword proximity searches,\nof the sort we saw in Example 1.1(page 15):\nemployment/3place\nHere, / kmeans \u201cwithin kwords of (on either side)\u201d. Clearly, positional in-\ndexes can be used for such queries; biword indexes cannot. We show in\nFigure 2.12 an algorithm for satisfying within kword proximity searches; it\nis further discussed in Exercise 2.12.\nPositional index size. Adopting a positional index expands required post-\nings storage signi\ufb01cantly, even if we compress position val ues/offsets as we\nOnline edition (c)\n2009 Cambridge UP2.4 Positional postings and phrase queries 43\nwill discuss in Section 5.3(page 95). Indeed, moving to a positional index\nalso changes the asymptotic complexity of a postings inters ection operation,\nbecause the number of items to check is now bounded not by the n umber of\ndocuments but by the total number of tokens in the document co llection T.\nThat is, the complexity of a Boolean query is \u0398(T)rather than \u0398(N). How-\never, most applications have little choice but to accept thi s, since most users\nnow expect to have the functionality of phrase and proximity searches.\nLet\u2019s examine the space implications of having a positional index. A post-\ning now needs an entry for each occurrence of a term. The index size thus\ndepends on the average document size. The average web page ha s less than\n1000 terms, but documents like SEC stock \ufb01lings, books, and e ven some epic\npoems easily reach 100,000 terms. Consider a term with frequ ency 1 in 1000\nterms on average. The result is that large documents cause an increase of two\norders of magnitude in the space required to store the postin gs list:\nExpected Expected entries\nDocument size postings in positional posting\n1000 1 1\n100,000 1 100\nWhile the exact numbers depend on the type of documents and th e language\nbeing indexed, some rough rules of thumb are to expect a posit ional index to\nbe 2 to 4 times as large as a non-positional index, and to expec t a compressed\npositional index to be about one third to one half the size of t he raw text\n(after removal of markup, etc.) of the original uncompresse d documents.\nSpeci\ufb01c numbers for an example collection are given in Table 5.1(page 87)\nand Table 5.6(page 103).\n2.4.3 Combination schemes\nThe strategies of biword indexes and positional indexes can be fruitfully\ncombined. If users commonly query on particular phrases, su ch asMichael\nJackson , it is quite inef\ufb01cient to keep merging positional postings lists. A\ncombination strategy uses a phrase index, or just a biword in dex, for certain\nqueries and uses a positional index for other phrase queries . Good queries\nto include in the phrase index are ones known to be common base d on re-\ncent querying behavior. But this is not the only criterion: t he most expensive\nphrase queries to evaluate are ones where the individual wor ds are com-\nmon but the desired phrase is comparatively rare. Adding Britney Spears as\na phrase index entry may only give a speedup factor to that que ry of about\n3, since most documents that mention either word are valid re sults, whereas\nadding The Who as a phrase index entry may speed up that query by a factor\nof 1000. Hence, having the latter is more desirable, even if i t is a relatively\nless common query.\nOnline edition (c)\n2009 Cambridge UP44 2 The term vocabulary and postings lists\nWilliams et al. (2004 ) evaluate an even more sophisticated scheme which\nemploys indexes of both these sorts and additionally a parti al next word\nindex as a halfway house between the \ufb01rst two strategies. For each term, a\nnext word index records terms that follow it in a document. They conclude NEXT WORD INDEX\nthat such a strategy allows a typical mixture of web phrase qu eries to be\ncompleted in one quarter of the time taken by use of a position al index alone,\nwhile taking up 26% more space than use of a positional index a lone.\n?Exercise 2.8 [\u22c6]\nAssume a biword index. Give an example of a document which wil l be returned\nfor a query of New York University but is actually a false positive which should not be\nreturned.\nExercise 2.9 [\u22c6]\nShown below is a portion of a positional index in the format: t erm: doc1:/an}\u230ara\u230bketle{tposition1,\nposition2, . . ./an}\u230ara\u230bketri}ht; doc2:/an}\u230ara\u230bketle{tposition1, position2, . . . /an}\u230ara\u230bketri}ht; etc.\nangels : 2:/an}\u230ara\u230bketle{t36,174,252,651/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t12,22,102,432/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t17/an}\u230ara\u230bketri}ht;\nfools: 2:/an}\u230ara\u230bketle{t1,17,74,222/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t8,78,108,458/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t3,13,23,193/an}\u230ara\u230bketri}ht;\nfear: 2:/an}\u230ara\u230bketle{t87,704,722,901/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t13,43,113,433/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t18,328,528/an}\u230ara\u230bketri}ht;\nin: 2:/an}\u230ara\u230bketle{t3,37,76,444,851/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t10,20,110,470,500/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t5,15,25,195/an}\u230ara\u230bketri}ht;\nrush: 2:/an}\u230ara\u230bketle{t2,66,194,321,702/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t9,69,149,429,569/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t4,14,404/an}\u230ara\u230bketri}ht;\nto: 2:/an}\u230ara\u230bketle{t47,86,234,999/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t14,24,774,944/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t199,319,599,709/an}\u230ara\u230bketri}ht;\ntread : 2:/an}\u230ara\u230bketle{t57,94,333/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t15,35,155/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t20,320/an}\u230ara\u230bketri}ht;\nwhere : 2:/an}\u230ara\u230bketle{t67,124,393,1001/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t11,41,101,421,431/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t16,36,736/an}\u230ara\u230bketri}ht;\nWhich document(s) if any match each of the following queries , where each expression\nwithin quotes is a phrase query?\na.\u201cfools rushin\u201d\nb.\u201cfools rushin\u201d AND\u201cangels fear to tread\u201d\nExercise 2.10 [\u22c6]\nConsider the following fragment of a positional index with t he format:\nword: document: /an}\u230ara\u230bketle{tposition, position, . . . /an}\u230ara\u230bketri}ht; document:/an}\u230ara\u230bketle{tposition, . . ./an}\u230ara\u230bketri}ht\n. . .\nGates : 1:/an}\u230ara\u230bketle{t3/an}\u230ara\u230bketri}ht; 2:/an}\u230ara\u230bketle{t6/an}\u230ara\u230bketri}ht; 3:/an}\u230ara\u230bketle{t2,17/an}\u230ara\u230bketri}ht; 4:/an}\u230ara\u230bketle{t1/an}\u230ara\u230bketri}ht;\nIBM: 4:/an}\u230ara\u230bketle{t3/an}\u230ara\u230bketri}ht; 7:/an}\u230ara\u230bketle{t14/an}\u230ara\u230bketri}ht;\nMicrosoft : 1:/an}\u230ara\u230bketle{t1/an}\u230ara\u230bketri}ht; 2:/an}\u230ara\u230bketle{t1,21/an}\u230ara\u230bketri}ht; 3:/an}\u230ara\u230bketle{t3/an}\u230ara\u230bketri}ht; 5:/an}\u230ara\u230bketle{t16,22,51/an}\u230ara\u230bketri}ht;\nThe/koperator, word1 / kword2 \ufb01nds occurrences of word1 within kwords of word2 (on\neither side), where kis a positive integer argument. Thus k=1 demands that word1\nbe adjacent to word2 .\na.Describe the set of documents that satisfy the query Gates /2 Microsoft .\nb.Describe each set of values for kfor which the query Gates / kMicrosoft returns a\ndifferent set of documents as the answer.\nOnline edition (c)\n2009 Cambridge UP2.5 References and further reading 45\nExercise 2.11 [\u22c6\u22c6]\nConsider the general procedure for merging two positional p ostings lists for a given\ndocument, to determine the document positions where a docum ent satis\ufb01es a / k\nclause (in general there can be multiple positions at which e ach term occurs in a sin-\ngle document). We begin with a pointer to the position of occu rrence of each term\nand move each pointer along the list of occurrences in the doc ument, checking as we\ndo so whether we have a hit for / k. Each move of either pointer counts as a step. Let\nLdenote the total number of occurrences of the two terms in the document. What is\nthe big-O complexity of the merge procedure, if we wish to hav e postings including\npositions in the result?\nExercise 2.12 [\u22c6\u22c6]\nConsider the adaptation of the basic algorithm for intersec tion of two postings lists\n(Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\nqueries. A naive algorithm for this operation could be O(PLmax2), where Pis the\nsum of the lengths of the postings lists (i.e., the sum of docu ment frequencies) and\nLmaxis the maximum length of a document (in tokens).\na.Go through this algorithm carefully and explain how it works .\nb.What is the complexity of this algorithm? Justify your answe r carefully.\nc.For certain queries and data distributions, would another a lgorithm be more ef\ufb01-\ncient? What complexity does it have?\nExercise 2.13 [\u22c6\u22c6]\nSuppose we wish to use a postings intersection procedure to d etermine simply the\nlist of documents that satisfy a / kclause, rather than returning the list of positions,\nas in Figure 2.12 (page 42). For simplicity, assume k\u22652. Let Ldenote the total\nnumber of occurrences of the two terms in the document collec tion (i.e., the sum of\ntheir collection frequencies). Which of the following is tr ue? Justify your answer.\na.The merge can be accomplished in a number of steps linear in Land independent\nofk, and we can ensure that each pointer moves only to the right.\nb.The merge can be accomplished in a number of steps linear in Land independent\nofk, but a pointer may be forced to move non-monotonically (i.e. , to sometimes\nback up)\nc.The merge can require kLsteps in some cases.\nExercise 2.14 [\u22c6\u22c6]\nHow could an IR system combine use of a positional index and us e of stop words?\nWhat is the potential problem, and how could it be handled?\n2.5 References and further reading\nExhaustive discussion of the character-level processing o f East Asian lan- EAST ASIAN\nLANGUAGES guages can be found in Lunde (1998 ). Character bigram indexes are perhaps\nthe most standard approach to indexing Chinese, although so me systems use\nword segmentation. Due to differences in the language and wr iting system,\nword segmentation is most usual for Japanese ( Luk and Kwok 2002 ,Kishida\nOnline edition (c)\n2009 Cambridge UP46 2 The term vocabulary and postings lists\net al. 2005 ). The structure of a character k-gram index over unsegmented text\ndiffers from that in Section 3.2.2 (page 54): there the k-gram dictionary points\nto postings lists of entries in the regular dictionary, wher eas here it points\ndirectly to document postings lists. For further discussio n of Chinese word\nsegmentation, see Sproat et al. (1996 ),Sproat and Emerson (2003 ),Tseng et al.\n(2005 ), and Gao et al. (2005 ).\nLita et al. (2003 ) present a method for truecasing. Natural language pro-\ncessing work on computational morphology is presented in ( Sproat 1992 ,\nBeesley and Karttunen 2003 ).\nLanguage identi\ufb01cation was perhaps \ufb01rst explored in crypto graphy; for\nexample, Konheim (1981 ) presents a character-level k-gram language identi-\n\ufb01cation algorithm. While other methods such as looking for p articular dis-\ntinctive function words and letter combinations have been u sed, with the\nadvent of widespread digital text, many people have explore d the charac-\ntern-gram technique, and found it to be highly successful ( Beesley 1998 ,\nDunning 1994 ,Cavnar and Trenkle 1994 ). Written language identi\ufb01cation\nis regarded as a fairly easy problem, while spoken language i denti\ufb01cation\nremains more dif\ufb01cult; see Hughes et al. (2006 ) for a recent survey.\nExperiments on and discussion of the positive and negative i mpact of\nstemming in English can be found in the following works: Salton (1989 ),Har-\nman (1991 ),Krovetz (1995 ),Hull (1996 ).Hollink et al. (2004 ) provide detailed\nresults for the effectiveness of language-speci\ufb01c methods on 8 European lan-\nguages. In terms of percent change in mean average precision (see page 159)\nover a baseline system, diacritic removal gains up to 23% (be ing especially\nhelpful for Finnish, French, and Swedish). Stemming helped markedly for\nFinnish (30% improvement) and Spanish (10% improvement), b ut for most\nlanguages, including English, the gain from stemming was in the range 0\u2013\n5%, and results from a lemmatizer were poorer still. Compoun d splitting\ngained 25% for Swedish and 15% for German, but only 4% for Dutc h. Rather\nthan language-particular methods, indexing character k-grams (as we sug-\ngested for Chinese) could often give as good or better result s: using within-\nword character 4-grams rather than words gave gains of 37% in Finnish, 27%\nin Swedish, and 20% in German, while even being slightly posi tive for other\nlanguages, such as Dutch, Spanish, and English. Tomlinson (2003 ) presents\nbroadly similar results. Bar-Ilan and Gutman (2005 ) suggest that, at the\ntime of their study (2003), the major commercial web search e ngines suffered\nfrom lacking decent language-particular processing; for e xample, a query on\nwww.google.fr forl\u2019\u00e9lectricit\u00e9 did not separate off the article l\u2019but only matched\npages with precisely this string of article+noun.\nThe classic presentation of skip pointers for IR can be found inMoffat and SKIP LIST\nZobel (1996 ). Extended techniques are discussed in Boldi and Vigna (2005 ).\nThe main paper in the algorithms literature is Pugh (1990 ), which uses mul-\ntilevel skip pointers to give expected O(logP)list access (the same expected\nOnline edition (c)\n2009 Cambridge UP2.5 References and further reading 47\nef\ufb01ciency as using a tree data structure) with less implemen tational complex-\nity. In practice, the effectiveness of using skip pointers d epends on various\nsystem parameters. Moffat and Zobel (1996 ) report conjunctive queries run-\nning about \ufb01ve times faster with the use of skip pointers, but Bahle et al.\n(2002 , p. 217) report that, with modern CPUs, using skip lists inst ead slows\ndown search because it expands the size of the postings list ( i.e., disk I/O\ndominates performance). In contrast, Strohman and Croft (2007 ) again show\ngood performance gains from skipping, in a system architect ure designed to\noptimize for the large memory spaces and multiple cores of re cent CPUs.\nJohnson et al. (2006 ) report that 11.7% of all queries in two 2002 web query\nlogs contained phrase queries, though Kammenhuber et al. (2006 ) report\nonly 3% phrase queries for a different data set. Silverstein et al. (1999 ) note\nthat many queries without explicit phrase operators are act ually implicit\nphrase searches.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 49\n3Dictionaries and tolerant\nretrieval\nIn Chapters 1and 2we developed the ideas underlying inverted indexes\nfor handling Boolean and proximity queries. Here, we develo p techniques\nthat are robust to typographical errors in the query, as well as alternative\nspellings. In Section 3.1we develop data structures that help the search\nfor terms in the vocabulary in an inverted index. In Section 3.2we study\nthe idea of a wildcard query : a query such as *a*e*i*o*u* , which seeks doc- WILDCARD QUERY\numents containing any term that includes all the \ufb01ve vowels i n sequence.\nThe*symbol indicates any (possibly empty) string of characters . Users pose\nsuch queries to a search engine when they are uncertain about how to spell\na query term, or seek documents containing variants of a quer y term; for in-\nstance, the query automat* would seek documents containing any of the terms\nautomatic,automation andautomated .\nWe then turn to other forms of imprecisely posed queries, foc using on\nspelling errors in Section 3.3. Users make spelling errors either by accident,\nor because the term they are searching for (e.g., Herman ) has no unambiguous\nspelling in the collection. We detail a number of techniques for correcting\nspelling errors in queries, one term at a time as well as for an entire string\nof query terms. Finally, in Section 3.4we study a method for seeking vo-\ncabulary terms that are phonetically close to the query term (s). This can be\nespecially useful in cases like the Herman example, where the user may not\nknow how a proper name is spelled in documents in the collecti on.\nBecause we will develop many variants of inverted indexes in this chapter,\nwe will use sometimes the phrase standard inverted index to mean the inverted\nindex developed in Chapters 1and 2, in which each vocabulary term has a\npostings list with the documents in the collection.\n3.1 Search structures for dictionaries\nGiven an inverted index and a query, our \ufb01rst task is to determ ine whether\neach query term exists in the vocabulary and if so, identify t he pointer to the\nOnline edition (c)\n2009 Cambridge UP50 3 Dictionaries and tolerant retrieval\ncorresponding postings. This vocabulary lookup operation uses a classical\ndata structure called the dictionary and has two broad class es of solutions:\nhashing, and search trees. In the literature of data structu res, the entries in\nthe vocabulary (in our case, terms) are often referred to as keys. The choice\nof solution (hashing, or search trees) is governed by a numbe r of questions:\n(1) How many keys are we likely to have? (2) Is the number likel y to remain\nstatic, or change a lot \u2013 and in the case of changes, are we like ly to only have\nnew keys inserted, or to also have some keys in the dictionary be deleted? (3)\nWhat are the relative frequencies with which various keys wi ll be accessed?\nHashing has been used for dictionary lookup in some search en gines. Each\nvocabulary term (key) is hashed into an integer over a large e nough space\nthat hash collisions are unlikely; collisions if any are res olved by auxiliary\nstructures that can demand care to maintain.1At query time, we hash each\nquery term separately and following a pointer to the corresp onding post-\nings, taking into account any logic for resolving hash colli sions. There is no\neasy way to \ufb01nd minor variants of a query term (such as the acce nted and\nnon-accented versions of a word like resume ), since these could be hashed to\nvery different integers. In particular, we cannot seek (for instance) all terms\nbeginning with the pre\ufb01x automat , an operation that we will require below\nin Section 3.2. Finally, in a setting (such as the Web) where the size of the\nvocabulary keeps growing, a hash function designed for curr ent needs may\nnot suf\ufb01ce in a few years\u2019 time.\nSearch trees overcome many of these issues \u2013 for instance, th ey permit us\nto enumerate all vocabulary terms beginning with automat . The best-known\nsearch tree is the binary tree , in which each internal node has two children. BINARY TREE\nThe search for a term begins at the root of the tree. Each inter nal node (in-\ncluding the root) represents a binary test, based on whose ou tcome the search\nproceeds to one of the two sub-trees below that node. Figure 3.1gives an ex-\nample of a binary search tree used for a dictionary. Ef\ufb01cient search (with a\nnumber of comparisons that is O(logM)) hinges on the tree being balanced:\nthe numbers of terms under the two sub-trees of any node are ei ther equal\nor differ by one. The principal issue here is that of rebalanc ing: as terms are\ninserted into or deleted from the binary search tree, it need s to be rebalanced\nso that the balance property is maintained.\nTo mitigate rebalancing, one approach is to allow the number of sub-trees\nunder an internal node to vary in a \ufb01xed interval. A search tre e commonly\nused for a dictionary is the B-tree \u2013 a search tree in which every internal node B-TREE\nhas a number of children in the interval [a,b], where aand bare appropriate\npositive integers; Figure 3.2shows an example with a=2 and b=4. Each\nbranch under an internal node again represents a test for a ra nge of char-\n1. So-called perfect hash functions are designed to preclud e collisions, but are rather more com-\nplicated both to implement and to compute.\nOnline edition (c)\n2009 Cambridge UP3.2 Wildcard queries 51\n\u25eeFigure 3.1 A binary search tree. In this example the branch at the root pa rtitions\nvocabulary terms into two subtrees, those whose \ufb01rst letter is between aandm, and\nthe rest.\nacter sequences, as in the binary tree example of Figure 3.1. A B-tree may\nbe viewed as \u201ccollapsing\u201d multiple levels of the binary tree into one; this\nis especially advantageous when some of the dictionary is di sk-resident, in\nwhich case this collapsing serves the function of pre-fetch ing imminent bi-\nnary tests. In such cases, the integers aand bare determined by the sizes of\ndisk blocks. Section 3.5contains pointers to further background on search\ntrees and B-trees.\nIt should be noted that unlike hashing, search trees demand t hat the char-\nacters used in the document collection have a prescribed ord ering; for in-\nstance, the 26 letters of the English alphabet are always lis ted in the speci\ufb01c\norderAthrough Z. Some Asian languages such as Chinese do not always\nhave a unique ordering, although by now all languages (inclu ding Chinese\nand Japanese) have adopted a standard ordering system for th eir character\nsets.\n3.2 Wildcard queries\nWildcard queries are used in any of the following situations : (1) the user\nis uncertain of the spelling of a query term (e.g., Sydney vs.Sidney , which\nOnline edition (c)\n2009 Cambridge UP52 3 Dictionaries and tolerant retrieval\n\u25eeFigure 3.2 A B-tree. In this example every internal node has between 2 an d 4\nchildren.\nleads to the wildcard query S*dney ); (2) the user is aware of multiple vari-\nants of spelling a term and (consciously) seeks documents co ntaining any of\nthe variants (e.g., color vs.colour ); (3) the user seeks documents containing\nvariants of a term that would be caught by stemming, but is uns ure whether\nthe search engine performs stemming (e.g., judicial vs.judiciary , leading to the\nwildcard query judicia* ); (4) the user is uncertain of the correct rendition of a\nforeign word or phrase (e.g., the query Universit* Stuttgart ).\nA query such as mon* is known as a trailing wildcard query , because the * WILDCARD QUERY\nsymbol occurs only once, at the end of the search string. A sea rch tree on\nthe dictionary is a convenient way of handling trailing wild card queries: we\nwalk down the tree following the symbols m,o andnin turn, at which point\nwe can enumerate the set Wof terms in the dictionary with the pre\ufb01x mon.\nFinally, we use|W|lookups on the standard inverted index to retrieve all\ndocuments containing any term in W.\nBut what about wildcard queries in which the *symbol is not constrained\nto be at the end of the search string? Before handling this gen eral case, we\nmention a slight generalization of trailing wildcard queri es. First, consider\nleading wildcard queries , or queries of the form *mon . Consider a reverse B-tree\non the dictionary \u2013 one in which each root-to-leaf path of the B-tree corre-\nsponds to a term in the dictionary written backwards: thus, the term lemon\nwould, in the B-tree, be represented by the path root-n-o-m-e-l . A walk down\nthe reverse B-tree then enumerates all terms Rin the vocabulary with a given\npre\ufb01x.\nIn fact, using a regular B-tree together with a reverse B-tre e, we can handle\nan even more general case: wildcard queries in which there is a single*sym-\nbol, such as se*mon . To do this, we use the regular B-tree to enumerate the set\nWof dictionary terms beginning with the pre\ufb01x se, then the reverse B-tree to\nOnline edition (c)\n2009 Cambridge UP3.2 Wildcard queries 53\nenumerate the set Rof terms ending with the suf\ufb01x mon. Next, we take the\nintersection W\u2229Rof these two sets, to arrive at the set of terms that begin\nwith the pre\ufb01x seand end with the suf\ufb01x mon. Finally, we use the standard\ninverted index to retrieve all documents containing any ter ms in this inter-\nsection. We can thus handle wildcard queries that contain a s ingle*symbol\nusing two B-trees, the normal B-tree and a reverse B-tree.\n3.2.1 General wildcard queries\nWe now study two techniques for handling general wildcard qu eries. Both\ntechniques share a common strategy: express the given wildc ard query qwas\na Boolean query Qon a specially constructed index, such that the answer to\nQis a superset of the set of vocabulary terms matching qw. Then, we check\neach term in the answer to Qagainst qw, discarding those vocabulary terms\nthat do not match qw. At this point we have the vocabulary terms matching\nqwand can resort to the standard inverted index.\nPermuterm indexes\nOur \ufb01rst special index for general wildcard queries is the permuterm index , PERMUTERM INDEX\na form of inverted index. First, we introduce a special symbo l$into our\ncharacter set, to mark the end of a term. Thus, the term hello is shown here as\nthe augmented term hello$ . Next, we construct a permuterm index, in which\nthe various rotations of each term (augmented with $) all link to the original\nvocabulary term. Figure 3.3gives an example of such a permuterm index\nentry for the term hello.\nWe refer to the set of rotated terms in the permuterm index as t heper-\nmuterm vocabulary .\nHow does this index help us with wildcard queries? Consider t he wildcard\nquerym*n. The key is to rotate such a wildcard query so that the *symbol\nappears at the end of the string \u2013 thus the rotated wildcard qu ery becomes\nn$m* . Next, we look up this string in the permuterm index, where se eking\nn$m* (via a search tree) leads to rotations of (among others) the t ermsman\nandmoron .\nNow that the permuterm index enables us to identify the origi nal vocab-\nulary terms matching a wildcard query, we look up these terms in the stan-\ndard inverted index to retrieve matching documents. We can t hus handle\nany wildcard query with a single *symbol. But what about a query such as\n\ufb01*mo*er ? In this case we \ufb01rst enumerate the terms in the dictionary th at are\nin the permuterm index of er$\ufb01* . Not all such dictionary terms will have\nthe string moin the middle - we \ufb01lter these out by exhaustive enumera-\ntion, checking each candidate to see if it contains mo. In this example, the\nterm\ufb01shmonger would survive this \ufb01ltering but \ufb01libuster would not. We then\nOnline edition (c)\n2009 Cambridge UP54 3 Dictionaries and tolerant retrieval\n\u25eeFigure 3.3 A portion of a permuterm index.\nrun the surviving terms through the standard inverted index for document\nretrieval. One disadvantage of the permuterm index is that i ts dictionary\nbecomes quite large, including as it does all rotations of ea ch term.\nNotice the close interplay between the B-tree and the permut erm index\nabove. Indeed, it suggests that the structure should perhap s be viewed as\na permuterm B-tree. However, we follow traditional termino logy here in\ndescribing the permuterm index as distinct from the B-tree t hat allows us to\nselect the rotations with a given pre\ufb01x.\n3.2.2 k-gram indexes for wildcard queries\nWhereas the permuterm index is simple, it can lead to a consid erable blowup\nfrom the number of rotations per term; for a dictionary of Eng lish terms, this\ncan represent an almost ten-fold space increase. We now pres ent a second\ntechnique, known as the k-gram index, for processing wildcard queries. We\nwill also use k-gram indexes in Section 3.3.4 . A k-gram is a sequence of k\ncharacters. Thus cas,astandstlare all 3-grams occurring in the term castle .\nWe use a special character $to denote the beginning or end of a term, so the\nfull set of 3-grams generated for castle is:$ca,cas,ast,stl,tle,le$.\nIn ak-gram index , the dictionary contains all k-grams that occur in any term k-GRAM INDEX\nin the vocabulary. Each postings list points from a k-gram to all vocabulary\nOnline edition (c)\n2009 Cambridge UP3.2 Wildcard queries 55\netr beetroot metric petrify retrieval- - - -\n\u25eeFigure 3.4 Example of a postings list in a 3-gram index. Here the 3-gram etris\nillustrated. Matching vocabulary terms are lexicographic ally ordered in the postings.\nterms containing that k-gram. For instance, the 3-gram etrwould point to vo-\ncabulary terms such as metric andretrieval . An example is given in Figure 3.4.\nHow does such an index help us with wildcard queries? Conside r the\nwildcard query re*ve . We are seeking documents containing any term that\nbegins with reand ends with ve. Accordingly, we run the Boolean query $re\nANDve$. This is looked up in the 3-gram index and yields a list of matc hing\nterms such as relive ,remove andretrieve . Each of these matching terms is then\nlooked up in the standard inverted index to yield documents m atching the\nquery.\nThere is however a dif\ufb01culty with the use of k-gram indexes, that demands\none further step of processing. Consider using the 3-gram in dex described\nabove for the query red*. Following the process described above, we \ufb01rst\nissue the Boolean query $re AND red to the 3-gram index. This leads to a\nmatch on terms such as retired , which contain the conjunction of the two 3-\ngrams$reandred, yet do not match the original wildcard query red*.\nTo cope with this, we introduce a post-\ufb01ltering step, in which the terms enu-\nmerated by the Boolean query on the 3-gram index are checked i ndividually\nagainst the original query red*. This is a simple string-matching operation\nand weeds out terms such as retired that do not match the original query.\nTerms that survive are then searched in the standard inverte d index as usual.\nWe have seen that a wildcard query can result in multiple term s being\nenumerated, each of which becomes a single-term query on the standard in-\nverted index. Search engines do allow the combination of wil dcard queries\nusing Boolean operators, for example, re*dAND fe*ri . What is the appropriate\nsemantics for such a query? Since each wildcard query turns i nto a disjunc-\ntion of single-term queries, the appropriate interpretati on of this example\nis that we have a conjunction of disjunctions: we seek all doc uments that\ncontain any term matching re*d andany term matching fe*ri.\nEven without Boolean combinations of wildcard queries, the processing of\na wildcard query can be quite expensive, because of the added lookup in the\nspecial index, \ufb01ltering and \ufb01nally the standard inverted in dex. A search en-\ngine may support such rich functionality, but most commonly , the capability\nis hidden behind an interface (say an \u201cAdvanced Query\u201d inter face) that most\nOnline edition (c)\n2009 Cambridge UP56 3 Dictionaries and tolerant retrieval\nusers never use. Exposing such functionality in the search i nterface often en-\ncourages users to invoke it even when they do not require it (s ay, by typing\na pre\ufb01x of their query followed by a *), increasing the proces sing load on the\nsearch engine.\n?Exercise 3.1\nIn the permuterm index, each permuterm vocabulary term poin ts to the original vo-\ncabulary term(s) from which it was derived. How many origina l vocabulary terms\ncan there be in the postings list of a permuterm vocabulary te rm?\nExercise 3.2\nWrite down the entries in the permuterm index dictionary tha t are generated by the\ntermmama .\nExercise 3.3\nIf you wanted to search for s*ng in a permuterm wildcard index , what key(s) would\none do the lookup on?\nExercise 3.4\nRefer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the\npostings are lexicographically ordered. Why is this orderi ng useful?\nExercise 3.5\nConsider again the query \ufb01*mo*er from Section 3.2.1 . What Boolean query on a bigram\nindex would be generated for this query? Can you think of a ter m that matches the\npermuterm query in Section 3.2.1 , but does not satisfy this Boolean query?\nExercise 3.6\nGive an example of a sentence that falsely matches the wildca rd query mon*h if the\nsearch were to simply use a conjunction of bigrams.\n3.3 Spelling correction\nWe next look at the problem of correcting spelling errors in q ueries. For in-\nstance, we may wish to retrieve documents containing the ter mcarrot when\nthe user types the query carot . Google reports ( http://www.google.com/jobs/britney.html )\nthat the following are all treated as misspellings of the que rybritney spears:\nbritian spears, britney\u2019s spears, brandy spears andprittany spears . We look at two\nsteps to solving this problem: the \ufb01rst based on edit distance and the second\nbased on k-gram overlap . Before getting into the algorithmic details of these\nmethods, we \ufb01rst review how search engines provide spell-co rrection as part\nof a user experience.\nOnline edition (c)\n2009 Cambridge UP3.3 Spelling correction 57\n3.3.1 Implementing spelling correction\nThere are two basic principles underlying most spelling cor rection algorithms.\n1.Of various alternative correct spellings for a mis-spelled query, choose\nthe \u201cnearest\u201d one. This demands that we have a notion of nearn ess or\nproximity between a pair of queries. We will develop these pr oximity\nmeasures in Section 3.3.3 .\n2.When two correctly spelled queries are tied (or nearly tied) , select the one\nthat is more common. For instance, grunt andgrant both seem equally\nplausible as corrections for grnt. Then, the algorithm should choose the\nmore common of grunt andgrant as the correction. The simplest notion\nof more common is to consider the number of occurrences of the term\nin the collection; thus if grunt occurs more often than grant , it would be\nthe chosen correction. A different notion of more common is e mployed\nin many search engines, especially on the web. The idea is to u se the\ncorrection that is most common among queries typed in by othe r users.\nThe idea here is that if grunt is typed as a query more often than grant , then\nit is more likely that the user who typed grntintended to type the query\ngrunt .\nBeginning in Section 3.3.3 we describe notions of proximity between queries,\nas well as their ef\ufb01cient computation. Spelling correction algorithms build on\nthese computations of proximity; their functionality is th en exposed to users\nin one of several ways:\n1.On the query carot always retrieve documents containing carot as well as\nany \u201cspell-corrected\u201d version of carot , including carrot andtarot.\n2.As in ( 1) above, but only when the query term carot is not in the dictionary.\n3.As in ( 1) above, but only when the original query returned fewer than a\npreset number of documents (say fewer than \ufb01ve documents).\n4.When the original query returns fewer than a preset number of docu-\nments, the search interface presents a spelling suggestion to the end user:\nthis suggestion consists of the spell-corrected query term (s). Thus, the\nsearch engine might respond to the user: \u201cDid you mean carrot ?\u201d\n3.3.2 Forms of spelling correction\nWe focus on two speci\ufb01c forms of spelling correction that we r efer to as\nisolated-term correction and context-sensitive correction. In isolated-term cor-\nrection, we attempt to correct a single query term at a time \u2013 e ven when we\nOnline edition (c)\n2009 Cambridge UP58 3 Dictionaries and tolerant retrieval\nhave a multiple-term query. The carot example demonstrates this type of cor-\nrection. Such isolated-term correction would fail to detec t, for instance, that\nthe query \ufb02ewformHeathrow contains a mis-spelling of the term from \u2013 because\neach term in the query is correctly spelled in isolation.\nWe begin by examining two techniques for addressing isolate d-term cor-\nrection: edit distance, and k-gram overlap. We then proceed to context-\nsensitive correction.\n3.3.3 Edit distance\nGiven two character strings s1and s2, the edit distance between them is the EDIT DISTANCE\nminimum number of edit operations required to transform s1into s2. Most\ncommonly, the edit operations allowed for this purpose are: (i) insert a char-\nacter into a string; (ii) delete a character from a string and (iii) replace a char-\nacter of a string by another character; for these operations , edit distance is\nsometimes known as Levenshtein distance . For example, the edit distance be- LEVENSHTEIN\nDISTANCE tweencatanddogis 3. In fact, the notion of edit distance can be generalized\nto allowing different weights for different kinds of edit op erations, for in-\nstance a higher weight may be placed on replacing the charact ersby the\ncharacter p, than on replacing it by the character a(the latter being closer to s\non the keyboard). Setting weights in this way depending on th e likelihood of\nletters substituting for each other is very effective in pra ctice (see Section 3.4\nfor the separate issue of phonetic similarity). However, th e remainder of our\ntreatment here will focus on the case in which all edit operat ions have the\nsame weight.\nIt is well-known how to compute the (weighted) edit distance between\ntwo strings in time O(|s1|\u00d7|s2|), where|si|denotes the length of a string si.\nThe idea is to use the dynamic programming algorithm in Figur e3.5, where\nthe characters in s1and s2are given in array form. The algorithm \ufb01lls the\n(integer) entries in a matrix mwhose two dimensions equal the lengths of\nthe two strings whose edit distances is being computed; the (i,j)entry of the\nmatrix will hold (after the algorithm is executed) the edit d istance between\nthe strings consisting of the \ufb01rst icharacters of s1and the \ufb01rst jcharacters\nofs2. The central dynamic programming step is depicted in Lines 8 -10 of\nFigure 3.5, where the three quantities whose minimum is taken correspo nd\nto substituting a character in s1, inserting a character in s1and inserting a\ncharacter in s2.\nFigure 3.6shows an example Levenshtein distance computation of Fig-\nure3.5. The typical cell [i,j]has four entries formatted as a 2 \u00d72 cell. The\nlower right entry in each cell is the min of the other three, co rresponding to\nthe main dynamic programming step in Figure 3.5. The other three entries\nare the three entries m[i\u22121,j\u22121] +0 or 1 depending on whether s1[i] =\nOnline edition (c)\n2009 Cambridge UP3.3 Spelling correction 59\nEDITDISTANCE (s1,s2)\n1int m[i,j] =0\n2fori\u21901to|s1|\n3dom[i, 0] =i\n4forj\u21901to|s2|\n5dom[0,j] =j\n6fori\u21901to|s1|\n7do for j\u21901to|s2|\n8 dom[i,j] =min{m[i\u22121,j\u22121] +if(s1[i] =s2[j])then 0 else 1\ufb01,\n9 m[i\u22121,j] +1,\n10 m[i,j\u22121] +1}\n11 return m[|s1|,|s2|]\n\u25eeFigure 3.5 Dynamic programming algorithm for computing the edit dista nce be-\ntween strings s1and s2.\nf a s t\n0 11 22 33 44\nc1\n112\n2123\n2234\n3345\n44\na2\n222\n3213\n3134\n2245\n33\nt3\n333\n4332\n4223\n3224\n32\ns4\n444\n5443\n5323\n4233\n33\n\u25eeFigure 3.6 Example Levenshtein distance computation. The 2 \u00d72 cell in the [i,j]\nentry of the table shows the three numbers whose minimum yiel ds the fourth. The\ncells in italics determine the edit distance in this example .\ns2[j],m[i\u22121,j] +1 and m[i,j\u22121] +1. The cells with numbers in italics depict\nthe path by which we determine the Levenshtein distance.\nThe spelling correction problem however demands more than c omputing\nedit distance: given a set Sof strings (corresponding to terms in the vocab-\nulary) and a query string q, we seek the string(s) in Vof least edit distance\nfrom q. We may view this as a decoding problem, in which the codeword s\n(the strings in V) are prescribed in advance. The obvious way of doing this\nis to compute the edit distance from qto each string in V, before selecting the\nOnline edition (c)\n2009 Cambridge UP60 3 Dictionaries and tolerant retrieval\nstring(s) of minimum edit distance. This exhaustive search is inordinately\nexpensive. Accordingly, a number of heuristics are used in p ractice to ef\ufb01-\nciently retrieve vocabulary terms likely to have low edit di stance to the query\nterm(s).\nThe simplest such heuristic is to restrict the search to dict ionary terms be-\nginning with the same letter as the query string; the hope wou ld be that\nspelling errors do not occur in the \ufb01rst character of the quer y. A more sophis-\nticated variant of this heuristic is to use a version of the pe rmuterm index,\nin which we omit the end-of-word symbol $. Consider the set of all rota-\ntions of the query string q. For each rotation rfrom this set, we traverse the\nB-tree into the permuterm index, thereby retrieving all dic tionary terms that\nhave a rotation beginning with r. For instance, if qismase and we consider\nthe rotation r=sema , we would retrieve dictionary terms such as semantic\nandsemaphore that do not have a small edit distance to q. Unfortunately, we\nwould miss more pertinent dictionary terms such as mare andmane . To ad-\ndress this, we re\ufb01ne this rotation scheme: for each rotation , we omit a suf\ufb01x\nof\u2113characters before performing the B-tree traversal. This en sures that each\nterm in the set Rof terms retrieved from the dictionary includes a \u201clong\u201d\nsubstring in common with q. The value of \u2113could depend on the length of q.\nAlternatively, we may set it to a \ufb01xed constant such as 2.\n3.3.4 k-gram indexes for spelling correction\nTo further limit the set of vocabulary terms for which we comp ute edit dis-\ntances to the query term, we now show how to invoke the k-gram index of\nSection 3.2.2 (page 54) to assist with retrieving vocabulary terms with low\nedit distance to the query q. Once we retrieve such terms, we can then \ufb01nd\nthe ones of least edit distance from q.\nIn fact, we will use the k-gram index to retrieve vocabulary terms that\nhave many k-grams in common with the query. We will argue that for rea-\nsonable de\ufb01nitions of \u201cmany k-grams in common,\u201d the retrieval process is\nessentially that of a single scan through the postings for th ek-grams in the\nquery string q.\nThe 2-gram (or bigram ) index in Figure 3.7shows (a portion of) the post-\nings for the three bigrams in the query bord. Suppose we wanted to retrieve\nvocabulary terms that contained at least two of these three b igrams. A single\nscan of the postings (much as in Chapter 1) would let us enumerate all such\nterms; in the example of Figure 3.7we would enumerate aboard, boardroom\nandborder .\nThis straightforward application of the linear scan inters ection of postings\nimmediately reveals the shortcoming of simply requiring ma tched vocabu-\nlary terms to contain a \ufb01xed number of k-grams from the query q: terms\nlikeboardroom , an implausible \u201ccorrection\u201d of bord, get enumerated. Conse-\nOnline edition (c)\n2009 Cambridge UP3.3 Spelling correction 61\nrd aboard ardent boardroom borderor border lord morbid sordidbo aboard about boardroom border\n- - - -- - - -- - - -\n\u25eeFigure 3.7 Matching at least two of the three 2-grams in the query bord.\nquently, we require more nuanced measures of the overlap in k-grams be-\ntween a vocabulary term and q. The linear scan intersection can be adapted\nwhen the measure of overlap is the Jaccard coef\ufb01cient for measuring the over- JACCARD COEFFICIENT\nlap between two sets Aand B, de\ufb01ned to be|A\u2229B|/|A\u222aB|. The two sets we\nconsider are the set of k-grams in the query q, and the set of k-grams in a vo-\ncabulary term. As the scan proceeds, we proceed from one voca bulary term\ntto the next, computing on the \ufb02y the Jaccard coef\ufb01cient betwe enqand t. If\nthe coef\ufb01cient exceeds a preset threshold, we add tto the output; if not, we\nmove on to the next term in the postings. To compute the Jaccar d coef\ufb01cient,\nwe need the set of k-grams in qand t.\nSince we are scanning the postings for all k-grams in q, we immediately\nhave these k-grams on hand. What about the k-grams of t? In principle,\nwe could enumerate these on the \ufb02y from t; in practice this is not only slow\nbut potentially infeasible since, in all likelihood, the po stings entries them-\nselves do not contain the complete string tbut rather some encoding of t. The\ncrucial observation is that to compute the Jaccard coef\ufb01cie nt, we only need\nthe length of the string t. To see this, recall the example of Figure 3.7and\nconsider the point when the postings scan for query q=bord reaches term\nt=boardroom . We know that two bigrams match. If the postings stored the\n(pre-computed) number of bigrams in boardroom (namely, 8), we have all the\ninformation we require to compute the Jaccard coef\ufb01cient to be 2/(8+3\u22122);\nthe numerator is obtained from the number of postings hits (2 , fromboand\nrd) while the denominator is the sum of the number of bigrams in bord and\nboardroom , less the number of postings hits.\nWe could replace the Jaccard coef\ufb01cient by other measures th at allow ef-\n\ufb01cient on the \ufb02y computation during postings scans. How do we use these\nOnline edition (c)\n2009 Cambridge UP62 3 Dictionaries and tolerant retrieval\nfor spelling correction? One method that has some empirical support is to\n\ufb01rst use the k-gram index to enumerate a set of candidate vocabulary terms\nthat are potential corrections of q. We then compute the edit distance from q\nto each term in this set, selecting terms from the set with sma ll edit distance\ntoq.\n3.3.5 Context sensitive spelling correction\nIsolated-term correction would fail to correct typographi cal errors such as\n\ufb02ew form Heathrow , where all three query terms are correctly spelled. When\na phrase such as this retrieves few documents, a search engin e may like to\noffer the corrected query \ufb02ewfromHeathrow . The simplest way to do this is to\nenumerate corrections of each of the three query terms (usin g the methods\nleading up to Section 3.3.4 ) even though each query term is correctly spelled,\nthen try substitutions of each correction in the phrase. For the example \ufb02ew\nform Heathrow , we enumerate such phrases as \ufb02ed form Heathrow and\ufb02ew fore\nHeathrow . For each such substitute phrase, the search engine runs the query\nand determines the number of matching results.\nThis enumeration can be expensive if we \ufb01nd many corrections of the in-\ndividual terms, since we could encounter a large number of co mbinations of\nalternatives. Several heuristics are used to trim this spac e. In the example\nabove, as we expand the alternatives for \ufb02ew andform, we retain only the\nmost frequent combinations in the collection or in the query logs, which con-\ntain previous queries by users. For instance, we would retai n\ufb02ew from as an\nalternative to try and extend to a three-term corrected quer y, but perhaps not\n\ufb02ed fore or\ufb02ea form . In this example, the biword \ufb02ed fore is likely to be rare\ncompared to the biword \ufb02ewfrom . Then, we only attempt to extend the list of\ntop biwords (such as \ufb02ew from ), to corrections of Heathrow . As an alternative\nto using the biword statistics in the collection, we may use t he logs of queries\nissued by users; these could of course include queries with s pelling errors.\n?Exercise 3.7\nIf|si|denotes the length of string si, show that the edit distance between s1and s2is\nnever more than max {|s1|,|s2|}.\nExercise 3.8\nCompute the edit distance between paris andalice. Write down the 5 \u00d75 array of\ndistances between all pre\ufb01xes as computed by the algorithm i n Figure 3.5.\nExercise 3.9\nWrite pseudocode showing the details of computing on the \ufb02y t he Jaccard coef\ufb01cient\nwhile scanning the postings of the k-gram index, as mentioned on page 61.\nExercise 3.10\nCompute the Jaccard coef\ufb01cients between the query bord and each of the terms in\nFigure 3.7that contain the bigram or.\nOnline edition (c)\n2009 Cambridge UP3.4 Phonetic correction 63\nExercise 3.11\nConsider the four-term query catched in the rye and suppose that each of the query\nterms has \ufb01ve alternative terms suggested by isolated-term correction. How many\npossible corrected phrases must we consider if we do not trim the space of corrected\nphrases, but instead try all six variants for each of the term s?\nExercise 3.12\nFor each of the pre\ufb01xes of the query \u2014 catched ,catchedin andcatchedinthe \u2014 we have\na number of substitute pre\ufb01xes arising from each term and its alternatives. Suppose\nthat we were to retain only the top 10 of these substitute pre\ufb01 xes, as measured by\nits number of occurrences in the collection. We eliminate th e rest from consideration\nfor extension to longer pre\ufb01xes: thus, if batched in is not one of the 10 most common\n2-term queries in the collection, we do not consider any exte nsion of batchedin as pos-\nsibly leading to a correction of catched intherye . How many of the possible substitute\npre\ufb01xes are we eliminating at each phase?\nExercise 3.13\nAre we guaranteed that retaining and extending only the 10 co mmonest substitute\npre\ufb01xes of catchedin will lead to one of the 10 commonest substitute pre\ufb01xes of catched\ninthe ?\n3.4 Phonetic correction\nOur \ufb01nal technique for tolerant retrieval has to do with phonetic correction:\nmisspellings that arise because the user types a query that s ounds like the tar-\nget term. Such algorithms are especially applicable to sear ches on the names\nof people. The main idea here is to generate, for each term, a \u201c phonetic hash\u201d\nso that similar-sounding terms hash to the same value. The id ea owes its\norigins to work in international police departments from th e early 20th cen-\ntury, seeking to match names for wanted criminals despite th e names being\nspelled differently in different countries. It is mainly us ed to correct phonetic\nmisspellings in proper nouns.\nAlgorithms for such phonetic hashing are commonly collecti vely known as\nsoundex algorithms. However, there is an original soundex algorith m, with SOUNDEX\nvarious variants, built on the following scheme:\n1.Turn every term to be indexed into a 4-character reduced form . Build an\ninverted index from these reduced forms to the original term s; call this\nthe soundex index.\n2.Do the same with query terms.\n3.When the query calls for a soundex match, search this soundex index.\nThe variations in different soundex algorithms have to do wi th the conver-\nsion of terms to 4-character forms. A commonly used conversi on results in\na 4-character code, with the \ufb01rst character being a letter of the alphabet and\nthe other three being digits between 0 and 9.\nOnline edition (c)\n2009 Cambridge UP64 3 Dictionaries and tolerant retrieval\n1.Retain the \ufb01rst letter of the term.\n2.Change all occurrences of the following letters to \u20190\u2019 (zero ): \u2019A\u2019, E\u2019, \u2019I\u2019, \u2019O\u2019,\n\u2019U\u2019, \u2019H\u2019, \u2019W\u2019, \u2019Y\u2019.\n3.Change letters to digits as follows:\nB, F, P , V to 1.\nC, G, J, K, Q, S, X, Z to 2.\nD,T to 3.\nL to 4.\nM, N to 5.\nR to 6.\n4.Repeatedly remove one out of each pair of consecutive identi cal digits.\n5.Remove all zeros from the resulting string. Pad the resultin g string with\ntrailing zeros and return the \ufb01rst four positions, which wil l consist of a\nletter followed by three digits.\nFor an example of a soundex map, Hermann maps to H655. Given a query\n(sayherman ), we compute its soundex code and then retrieve all vocabula ry\nterms matching this soundex code from the soundex index, bef ore running\nthe resulting query on the standard inverted index.\nThis algorithm rests on a few observations: (1) vowels are vi ewed as inter-\nchangeable, in transcribing names; (2) consonants with sim ilar sounds (e.g.,\nD and T) are put in equivalence classes. This leads to related names often\nhaving the same soundex codes. While these rules work for man y cases,\nespecially European languages, such rules tend to be writin g system depen-\ndent. For example, Chinese names can be written in Wade-Gile s or Pinyin\ntranscription. While soundex works for some of the differen ces in the two\ntranscriptions, for instance mapping both Wade-Giles hsand Pinyin xto 2,\nit fails in other cases, for example Wade-Giles jand Pinyin rare mapped\ndifferently.\n?Exercise 3.14\nFind two differently spelled proper nouns whose soundex cod es are the same.\nExercise 3.15\nFind two phonetically similar proper nouns whose soundex co des are different.\nOnline edition (c)\n2009 Cambridge UP3.5 References and further reading 65\n3.5 References and further reading\nKnuth (1997 ) is a comprehensive source for information on search trees, in-\ncluding B-trees and their use in searching through dictiona ries.\nGar\ufb01eld (1976 ) gives one of the \ufb01rst complete descriptions of the permuter m\nindex. Ferragina and Venturini (2007 ) give an approach to addressing the\nspace blowup in permuterm indexes.\nOne of the earliest formal treatments of spelling correctio n was due to\nDamerau (1964 ). The notion of edit distance that we have used is due to Lev-\nenshtein (1965 ) and the algorithm in Figure 3.5is due to Wagner and Fischer\n(1974 ).Peterson (1980 ) and Kukich (1992 ) developed variants of methods\nbased on edit distances, culminating in a detailed empirica l study of sev-\neral methods by Zobel and Dart (1995 ), which shows that k-gram indexing\nis very effective for \ufb01nding candidate mismatches, but shou ld be combined\nwith a more \ufb01ne-grained technique such as edit distance to de termine the\nmost likely misspellings. Gus\ufb01eld (1997 ) is a standard reference on string\nalgorithms such as edit distance.\nProbabilistic models (\u201cnoisy channel\u201d models) for spellin g correction were\npioneered by Kernighan et al. (1990 ) and further developed by Brill and\nMoore (2000 ) and Toutanova and Moore (2002 ). In these models, the mis-\nspelled query is viewed as a probabilistic corruption of a co rrect query. They\nhave a similar mathematical basis to the language model meth ods presented\nin Chapter 12, and also provide ways of incorporating phonetic similarit y,\ncloseness on the keyboard, and data from the actual spelling mistakes of\nusers. Many would regard them as the state-of-the-art appro ach. Cucerzan\nand Brill (2004 ) show how this work can be extended to learning spelling\ncorrection models based on query reformulations in search e ngine logs.\nThe soundex algorithm is attributed to Margaret K. Odell and Robert C.\nRusselli (from U.S. patents granted in 1918 and 1922); the ve rsion described\nhere draws on Bourne and Ford (1961 ).Zobel and Dart (1996 ) evaluate var-\nious phonetic matching algorithms, \ufb01nding that a variant of the soundex\nalgorithm performs poorly for general spelling correction , but that other al-\ngorithms based on the phonetic similarity of term pronuncia tions perform\nwell.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 67\n4 Index construction\nIn this chapter, we look at how to construct an inverted index . We call this\nprocess index construction orindexing ; the process or machine that performs it INDEXING\ntheindexer . The design of indexing algorithms is governed by hardware c on- INDEXER\nstraints. We therefore begin this chapter with a review of th e basics of com-\nputer hardware that are relevant for indexing. We then intro duce blocked\nsort-based indexing (Section 4.2), an ef\ufb01cient single-machine algorithm de-\nsigned for static collections that can be viewed as a more sca lable version of\nthe basic sort-based indexing algorithm we introduced in Ch apter 1. Sec-\ntion 4.3describes single-pass in-memory indexing, an algorithm th at has\neven better scaling properties because it does not hold the v ocabulary in\nmemory. For very large collections like the web, indexing ha s to be dis-\ntributed over computer clusters with hundreds or thousands of machines.\nWe discuss this in Section 4.4. Collections with frequent changes require dy-\nnamic indexing introduced in Section 4.5so that changes in the collection are\nimmediately re\ufb02ected in the index. Finally, we cover some co mplicating is-\nsues that can arise in indexing \u2013 such as security and indexes for ranked\nretrieval \u2013 in Section 4.6.\nIndex construction interacts with several topics covered i n other chapters.\nThe indexer needs raw text, but documents are encoded in many ways (see\nChapter 2). Indexers compress and decompress intermediate \ufb01les and t he\n\ufb01nal index (see Chapter 5). In web search, documents are not on a local\n\ufb01le system, but have to be spidered or crawled (see Chapter 20). In enter-\nprise search, most documents are encapsulated in varied con tent manage-\nment systems, email applications, and databases. We give so me examples\nin Section 4.7. Although most of these applications can be accessed via htt p,\nnative Application Programming Interfaces (APIs) are usua lly more ef\ufb01cient.\nThe reader should be aware that building the subsystem that f eeds raw text\nto the indexing process can in itself be a challenging proble m.\nOnline edition (c)\n2009 Cambridge UP68 4 Index construction\n\u25eeTable 4.1 Typical system parameters in 2007. The seek time is the time n eeded\nto position the disk head in a new position. The transfer time per byte is the rate of\ntransfer from disk to memory when the head is in the right posi tion.\nSymbol Statistic Value\ns average seek time 5 ms =5\u00d710\u22123s\nb transfer time per byte 0.02 \u00b5s=2\u00d710\u22128s\nprocessor\u2019s clock rate 109s\u22121\np lowlevel operation\n(e.g., compare & swap a word) 0.01 \u00b5s=10\u22128s\nsize of main memory several GB\nsize of disk space 1 TB or more\n4.1 Hardware basics\nWhen building an information retrieval (IR) system, many de cisions are based\non the characteristics of the computer hardware on which the system runs.\nWe therefore begin this chapter with a brief review of comput er hardware.\nPerformance characteristics typical of systems in 2007 are shown in Table 4.1.\nA list of hardware basics that we need in this book to motivate IR system\ndesign follows.\n\u2022Access to data in memory is much faster than access to data on d isk. It\ntakes a few clock cycles (perhaps 5 \u00d710\u22129seconds) to access a byte in\nmemory, but much longer to transfer it from disk (about 2 \u00d710\u22128sec-\nonds). Consequently, we want to keep as much data as possible in mem-\nory, especially those data that we need to access frequently . We call the\ntechnique of keeping frequently used disk data in main memor ycaching . CACHING\n\u2022When doing a disk read or write, it takes a while for the disk he ad to\nmove to the part of the disk where the data are located. This ti me is called\ntheseek time and it averages 5 ms for typical disks. No data are being SEEK TIME\ntransferred during the seek. To maximize data transfer rate s, chunks of\ndata that will be read together should therefore be stored co ntiguously on\ndisk. For example, using the numbers in Table 4.1it may take as little as\n0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is\nstored as one chunk, but up to 0.2 +100\u00d7(5\u00d710\u22123) = 0.7 seconds if it\nis stored in 100 noncontiguous chunks because we need to move the disk\nhead up to 100 times.\n\u2022Operating systems generally read and write entire blocks. T hus, reading\na single byte from disk can take as much time as reading the ent ire block.\nOnline edition (c)\n2009 Cambridge UP4.2 Blocked sort-based indexing 69\nBlock sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We c all the part\nof main memory where a block being read or written is stored a buffer . BUFFER\n\u2022Data transfers from disk to memory are handled by the system b us, not by\nthe processor. This means that the processor is available to process data\nduring disk I/O. We can exploit this fact to speed up data tran sfers by\nstoring compressed data on disk. Assuming an ef\ufb01cient decom pression\nalgorithm, the total time of reading and then decompressing compressed\ndata is usually less than reading uncompressed data.\n\u2022Servers used in IR systems typically have several gigabytes (GB) of main\nmemory, sometimes tens of GB. Available disk space is severa l orders of\nmagnitude larger.\n4.2 Blocked sort-based indexing\nThe basic steps in constructing a nonpositional index are de picted in Fig-\nure1.4(page 8). We \ufb01rst make a pass through the collection assembling all\nterm\u2013docID pairs. We then sort the pairs with the term as the d ominant key\nand docID as the secondary key. Finally, we organize the docI Ds for each\nterm into a postings list and compute statistics like term an d document fre-\nquency. For small collections, all this can be done in memory . In this chapter,\nwe describe methods for large collections that require the u se of secondary\nstorage.\nTo make index construction more ef\ufb01cient, we represent term s as termIDs\n(instead of strings as we did in Figure 1.4), where each termID is a unique TERM ID\nserial number. We can build the mapping from terms to termIDs on the \ufb02y\nwhile we are processing the collection; or, in a two-pass app roach, we com-\npile the vocabulary in the \ufb01rst pass and construct the invert ed index in the\nsecond pass. The index construction algorithms described i n this chapter all\ndo a single pass through the data. Section 4.7gives references to multipass\nalgorithms that are preferable in certain applications, fo r example, when disk\nspace is scarce.\nWe work with the Reuters-RCV1 collection as our model collection in this REUTERS -RCV1\nchapter, a collection with roughly 1 GB of text. It consists o f about 800,000\ndocuments that were sent over the Reuters newswire during a 1 -year pe-\nriod between August 20, 1996, and August 19, 1997. A typical d ocument is\nshown in Figure 4.1, but note that we ignore multimedia information like\nimages in this book and are only concerned with text. Reuters -RCV1 covers\na wide range of international topics, including politics, b usiness, sports, and\n(as in this example) science. Some key statistics of the coll ection are shown\nin Table 4.2.\nOnline edition (c)\n2009 Cambridge UP70 4 Index construction\n\u25eeTable 4.2 Collection statistics for Reuters-RCV1. Values are rounde d for the com-\nputations in this book. The unrounded values are: 806,791 do cuments, 222 tokens\nper document, 391,523 (distinct) terms, 6.04 bytes per toke n with spaces and punc-\ntuation, 4.5 bytes per token without spaces and punctuation , 7.5 bytes per term, and\n96,969,056 tokens. The numbers in this table correspond to t he third line (\u201ccase fold-\ning\u201d) in Table 5.1(page 87).\nSymbol Statistic Value\nN documents 800,000\nLave avg. # tokens per document 200\nM terms 400,000\navg. # bytes per token (incl. spaces/punct.) 6\navg. # bytes per token (without spaces/punct.) 4.5\navg. # bytes per term 7.5\nT tokens 100,000,000\nREUTERS\nExtreme conditions create rare Antarctic cloudsYou are here:  Home > News > Science > Article\nGo to a Section:      U.S.     International      Business      Markets Politics Entertainment Technology\nTue Aug 1, 2006 3:20am ET\nEmail This Article | Print This Article | Reprints\nSYDNEY (Reuters) - Rare, mother-of-pearl colored clouds\ncaused by extreme weather conditions above Antarctica are a\npossible indication of global warming, Australian scientists said on\nTuesday.\nKnown as nacreous clouds, the spectacular formations showing delicate\nwisps of colors were photographed in the sky over an Australian\nmeteorological base at Mawson Station on July 25.Sports Oddly Enough\n[-] Text [+]\n\u25eeFigure 4.1 Document from the Reuters newswire.\nReuters-RCV1 has 100 million tokens. Collecting all termID \u2013docID pairs of\nthe collection using 4 bytes each for termID and docID theref ore requires 0.8\nGB of storage. Typical collections today are often one or two orders of mag-\nnitude larger than Reuters-RCV1. You can easily see how such collections\noverwhelm even large computers if we try to sort their termID \u2013docID pairs\nin memory. If the size of the intermediate \ufb01les during index c onstruction is\nwithin a small factor of available memory, then the compress ion techniques\nintroduced in Chapter 5can help; however, the postings \ufb01le of many large\ncollections cannot \ufb01t into memory even after compression.\nWith main memory insuf\ufb01cient, we need to use an external sorting algo- EXTERNAL SORTING\nALGORITHM rithm , that is, one that uses disk. For acceptable speed, the centr al require-\nOnline edition (c)\n2009 Cambridge UP4.2 Blocked sort-based indexing 71\nBSBI NDEX CONSTRUCTION ()\n1n\u21900\n2while (all documents have not been processed)\n3don\u2190n+1\n4 block\u2190PARSE NEXTBLOCK()\n5 BSBI-I NVERT (block)\n6 W RITE BLOCK TODISK(block ,fn)\n7 M ERGE BLOCKS (f1, . . . , fn;fmerged)\n\u25eeFigure 4.2 Blocked sort-based indexing. The algorithm stores inverte d blocks in\n\ufb01les f1, . . . , fnand the merged index in fmerged .\nment of such an algorithm is that it minimize the number of ran dom disk\nseeks during sorting \u2013 sequential disk reads are far faster t han seeks as we\nexplained in Section 4.1. One solution is the blocked sort-based indexing algo- BLOCKED SORT -BASED\nINDEXING ALGORITHM rithm orBSBI in Figure 4.2. BSBI (i) segments the collection into parts of equal\nsize, (ii) sorts the termID\u2013docID pairs of each part in memor y, (iii) stores in-\ntermediate sorted results on disk, and (iv) merges all inter mediate results\ninto the \ufb01nal index.\nThe algorithm parses documents into termID\u2013docID pairs and accumu-\nlates the pairs in memory until a block of a \ufb01xed size is full (P ARSE NEXTBLOCK\nin Figure 4.2). We choose the block size to \ufb01t comfortably into memory to\npermit a fast in-memory sort. The block is then inverted and w ritten to disk.\nInversion involves two steps. First, we sort the termID\u2013docID pairs. N ext, INVERSION\nwe collect all termID\u2013docID pairs with the same termID into a postings list,\nwhere a posting is simply a docID. The result, an inverted index for the block POSTING\nwe have just read, is then written to disk. Applying this to Re uters-RCV1 and\nassuming we can \ufb01t 10 million termID\u2013docID pairs into memory , we end up\nwith ten blocks, each an inverted index of one part of the coll ection.\nIn the \ufb01nal step, the algorithm simultaneously merges the te n blocks into\none large merged index. An example with two blocks is shown in Figure 4.3,\nwhere we use dito denote the ithdocument of the collection. To do the merg-\ning, we open all block \ufb01les simultaneously, and maintain sma ll read buffers\nfor the ten blocks we are reading and a write buffer for the \ufb01na l merged in-\ndex we are writing. In each iteration, we select the lowest te rmID that has\nnot been processed yet using a priority queue or a similar dat a structure. All\npostings lists for this termID are read and merged, and the me rged list is\nwritten back to disk. Each read buffer is re\ufb01lled from its \ufb01le when necessary.\nHow expensive is BSBI? Its time complexity is \u0398(TlogT)because the step\nwith the highest time complexity is sorting and Tis an upper bound for the\nnumber of items we must sort (i.e., the number of termID\u2013docI D pairs). But\nOnline edition (c)\n2009 Cambridge UP72 4 Index construction\nbrutus d1,d3\ncaesar d1,d2,d4\nnoble d5\nwith d1,d2,d3,d5brutus d6,d7\ncaesar d8,d9\njulius d10\nkilled d8postings lists\nto be merged\nbrutus d1,d3,d6,d7\ncaesar d1,d2,d4,d8,d9\njulius d10\nkilled d8\nnoble d5\nwith d1,d2,d3,d5merged\npostings lists\ndisk\n\u25eeFigure 4.3 Merging in blocked sort-based indexing. Two blocks (\u201cposti ngs lists to\nbe merged\u201d) are loaded from disk into memory, merged in memor y (\u201cmerged post-\nings lists\u201d) and written back to disk. We show terms instead o f termIDs for better\nreadability.\nthe actual indexing time is usually dominated by the time it t akes to parse the\ndocuments (P ARSE NEXTBLOCK ) and to do the \ufb01nal merge (M ERGE BLOCKS ).\nExercise 4.6asks you to compute the total index construction time for RCV 1\nthat includes these steps as well as inverting the blocks and writing them to\ndisk.\nNotice that Reuters-RCV1 is not particularly large in an age when one or\nmore GB of memory are standard on personal computers. With ap propriate\ncompression (Chapter 5), we could have created an inverted index for RCV1\nin memory on a not overly beefy server. The techniques we have described\nare needed, however, for collections that are several order s of magnitude\nlarger.\n?Exercise 4.1\nIf we need Tlog2Tcomparisons (where Tis the number of termID\u2013docID pairs) and\ntwo disk seeks for each comparison, how much time would index construction for\nReuters-RCV1 take if we used disk instead of memory for stora ge and an unopti-\nmized sorting algorithm (i.e., not an external sorting algo rithm)? Use the system\nparameters in Table 4.1.\nExercise 4.2 [\u22c6]\nHow would you create the dictionary in blocked sort-based in dexing on the \ufb02y to\navoid an extra pass through the data?\nOnline edition (c)\n2009 Cambridge UP4.3 Single-pass in-memory indexing 73\nSPIMI-I NVERT (token _stream )\n1output _f ile=NEWFILE()\n2dictionary =NEWHASH()\n3while (free memory available)\n4dotoken\u2190next(token _stream )\n5 ifterm(token)/\u2208dictionary\n6 then postings _list=ADDTODICTIONARY (dictionary ,term(token))\n7 else postings _list=GETPOSTINGS LIST(dictionary ,term(token))\n8 iff ull(postings _list)\n9 then postings _list=DOUBLE POSTINGS LIST(dictionary ,term(token))\n10 A DDTOPOSTINGS LIST(postings _list,docID (token))\n11 sorted _terms\u2190SORTTERMS(dictionary )\n12 W RITE BLOCK TODISK(sorted _terms ,dictionary ,output _f ile)\n13 return output _f ile\n\u25eeFigure 4.4 Inversion of a block in single-pass in-memory indexing\n4.3 Single-pass in-memory indexing\nBlocked sort-based indexing has excellent scaling propert ies, but it needs\na data structure for mapping terms to termIDs. For very large collections,\nthis data structure does not \ufb01t into memory. A more scalable a lternative is\nsingle-pass in-memory indexing orSPIMI . SPIMI uses terms instead of termIDs, SINGLE -PASS\nIN-MEMORY INDEXING writes each block\u2019s dictionary to disk, and then starts a new dictionary for the\nnext block. SPIMI can index collections of any size as long as there is enough\ndisk space available.\nThe SPIMI algorithm is shown in Figure 4.4. The part of the algorithm that\nparses documents and turns them into a stream of term\u2013docID p airs, which\nwe call tokens here, has been omitted. SPIMI-I NVERT is called repeatedly on\nthe token stream until the entire collection has been proces sed.\nTokens are processed one by one (line 4) during each successi ve call of\nSPIMI-I NVERT . When a term occurs for the \ufb01rst time, it is added to the\ndictionary (best implemented as a hash), and a new postings l ist is created\n(line 6). The call in line 7 returns this postings list for sub sequent occurrences\nof the term.\nA difference between BSBI and SPIMI is that SPIMI adds a posti ng di-\nrectly to its postings list (line 10). Instead of \ufb01rst collec ting all termID\u2013docID\npairs and then sorting them (as we did in BSBI), each postings list is dynamic\n(i.e., its size is adjusted as it grows) and it is immediately available to collect\npostings. This has two advantages: It is faster because ther e is no sorting\nrequired, and it saves memory because we keep track of the ter m a postings\nOnline edition (c)\n2009 Cambridge UP74 4 Index construction\nlist belongs to, so the termIDs of postings need not be stored . As a result, the\nblocks that individual calls of SPIMI-I NVERT can process are much larger\nand the index construction process as a whole is more ef\ufb01cien t.\nBecause we do not know how large the postings list of a term wil l be when\nwe \ufb01rst encounter it, we allocate space for a short postings l ist initially and\ndouble the space each time it is full (lines 8\u20139). This means t hat some mem-\nory is wasted, which counteracts the memory savings from the omission of\ntermIDs in intermediate data structures. However, the over all memory re-\nquirements for the dynamically constructed index of a block in SPIMI are\nstill lower than in BSBI.\nWhen memory has been exhausted, we write the index of the bloc k (which\nconsists of the dictionary and the postings lists) to disk (l ine 12). We have to\nsort the terms (line 11) before doing this because we want to w rite postings\nlists in lexicographic order to facilitate the \ufb01nal merging step. If each block\u2019s\npostings lists were written in unsorted order, merging bloc ks could not be\naccomplished by a simple linear scan through each block.\nEach call of SPIMI-I NVERT writes a block to disk, just as in BSBI. The last\nstep of SPIMI (corresponding to line 7 in Figure 4.2; not shown in Figure 4.4)\nis then to merge the blocks into the \ufb01nal inverted index.\nIn addition to constructing a new dictionary structure for e ach block and\neliminating the expensive sorting step, SPIMI has a third im portant compo-\nnent: compression. Both the postings and the dictionary ter ms can be stored\ncompactly on disk if we employ compression. Compression inc reases the ef-\n\ufb01ciency of the algorithm further because we can process even larger blocks,\nand because the individual blocks require less space on disk . We refer readers\nto the literature for this aspect of the algorithm (Section 4.7).\nThe time complexity of SPIMI is \u0398(T)because no sorting of tokens is re-\nquired and all operations are at most linear in the size of the collection.\n4.4 Distributed indexing\nCollections are often so large that we cannot perform index c onstruction ef\ufb01-\nciently on a single machine. This is particularly true of the World Wide Web\nfor which we need large computer clusters1to construct any reasonably sized\nweb index. Web search engines, therefore, use distributed indexing algorithms\nfor index construction. The result of the construction proc ess is a distributed\nindex that is partitioned across several machines \u2013 either a ccording to term\nor according to document. In this section, we describe distr ibuted indexing\nfor a term-partitioned index. Most large search engines pre fer a document-\n1. A cluster in this chapter is a group of tightly coupled comp uters that work together closely.\nThis sense of the word is different from the use of cluster as a group of documents that are\nsemantically similar in Chapters 16\u201318.\nOnline edition (c)\n2009 Cambridge UP4.4 Distributed indexing 75\npartitioned index (which can be easily generated from a term -partitioned\nindex). We discuss this topic further in Section 20.3 (page 454).\nThe distributed index construction method we describe in th is section is an\napplication of MapReduce , a general architecture for distributed computing. MAPREDUCE\nMapReduce is designed for large computer clusters. The poin t of a cluster is\nto solve large computing problems on cheap commodity machin es or nodes\nthat are built from standard parts (processor, memory, disk ) as opposed to on\na supercomputer with specialized hardware. Although hundr eds or thou-\nsands of machines are available in such clusters, individua l machines can\nfail at any time. One requirement for robust distributed ind exing is, there-\nfore, that we divide the work up into chunks that we can easily assign and\n\u2013 in case of failure \u2013 reassign. A master node directs the process of assigning MASTER NODE\nand reassigning tasks to individual worker nodes.\nThe map and reduce phases of MapReduce split up the computing job\ninto chunks that standard machines can process in a short tim e. The various\nsteps of MapReduce are shown in Figure 4.5and an example on a collection\nconsisting of two documents is shown in Figure 4.6. First, the input data,\nin our case a collection of web pages, are split into n splits where the size of SPLITS\nthe split is chosen to ensure that the work can be distributed evenly (chunks\nshould not be too large) and ef\ufb01ciently (the total number of c hunks we need\nto manage should not be too large); 16 or 64 MB are good sizes in distributed\nindexing. Splits are not preassigned to machines, but are in stead assigned\nby the master node on an ongoing basis: As a machine \ufb01nishes pr ocessing\none split, it is assigned the next one. If a machine dies or bec omes a laggard\ndue to hardware problems, the split it is working on is simply reassigned to\nanother machine.\nIn general, MapReduce breaks a large computing problem into smaller\nparts by recasting it in terms of manipulation of key-value pairs . For index- KEY-VALUE PAIRS\ning, a key-value pair has the form (termID,docID). In distri buted indexing,\nthe mapping from terms to termIDs is also distributed and the refore more\ncomplex than in single-machine indexing. A simple solution is to maintain\na (perhaps precomputed) mapping for frequent terms that is c opied to all\nnodes and to use terms directly (instead of termIDs) for infr equent terms.\nWe do not address this problem here and assume that all nodes s hare a con-\nsistent term\u2192termID mapping.\nThe map phase of MapReduce consists of mapping splits of the input data MAP PHASE\nto key-value pairs. This is the same parsing task we also enco untered in BSBI\nand SPIMI, and we therefore call the machines that execute th e map phase\nparsers . Each parser writes its output to local intermediate \ufb01les, t hesegment PARSER\nSEGMENT FILE\ufb01les(shown as a-fg-p q-z in Figure 4.5).\nFor the reduce phase , we want all values for a given key to be stored close REDUCE PHASE\ntogether, so that they can be read and processed quickly. Thi s is achieved by\nOnline edition (c)\n2009 Cambridge UP76 4 Index construction\nmaster assign\nmap\nphasereduce\nphaseassign\nparsersplits\nparser\nparserinver terpostings\ninver ter\ninver tera-f\ng-p\nq-za-f g-p q-z\na-f g-p q-z\na-f\nsegment\nfilesg-p q-z\n\u25eeFigure 4.5 An example of distributed indexing with MapReduce. Adapted from\nDean and Ghemawat (2004 ).\npartitioning the keys into jterm partitions and having the parsers write key-\nvalue pairs for each term partition into a separate segment \ufb01 le. In Figure 4.5,\nthe term partitions are according to \ufb01rst letter: a\u2013f, g\u2013p, q \u2013z, and j=3. (We\nchose these key ranges for ease of exposition. In general, ke y ranges need not\ncorrespond to contiguous terms or termIDs.) The term partit ions are de\ufb01ned\nby the person who operates the indexing system (Exercise 4.10). The parsers\nthen write corresponding segment \ufb01les, one for each term par tition. Each\nterm partition thus corresponds to rsegments \ufb01les, where ris the number\nof parsers. For instance, Figure 4.5shows three a\u2013f segment \ufb01les of the a\u2013f\npartition, corresponding to the three parsers shown in the \ufb01 gure.\nCollecting all values (here: docIDs) for a given key (here: t ermID) into one\nlist is the task of the inverters in the reduce phase. The master assigns each INVERTER\nterm partition to a different inverter \u2013 and, as in the case of parsers, reas-\nsigns term partitions in case of failing or slow inverters. E ach term partition\n(corresponding to rsegment \ufb01les, one on each parser) is processed by one in-\nverter. We assume here that segment \ufb01les are of a size that a si ngle machine\ncan handle (Exercise 4.9). Finally, the list of values is sorted for each key and\nwritten to the \ufb01nal sorted postings list (\u201cpostings\u201d in the \ufb01 gure). (Note that\npostings in Figure 4.6include term frequencies, whereas each posting in the\nother sections of this chapter is simply a docID without term frequency in-\nformation.) The data \ufb02ow is shown for a\u2013f in Figure 4.5. This completes the\nconstruction of the inverted index.\nOnline edition (c)\n2009 Cambridge UP4.4 Distributed indexing 77\nSchema of map and reduce functions\nmap: input list(k,v)\nreduce: ( k,list(v)) output\nInstantiation of the schema for index construction\nmap: web collection list(termID ,docID)\nreduce: ( \u00a0termID ,1list(docID) , \u00a0termID  ,2list(docID) , ...) (postings list list 1,postings 2,...)\nExample for index construction\nmap: d2:Cdied.d1:C came ,Cc\u2019ed. (\u00a0C,d2,\u00a0died ,d2,\u00a0C,d1,\u00a0came ,d1,\u00a0C,d1,\u00a0\u2329c\u2019ed ,d1\u232a)\nreduce: ( \u00a0C,(d2,d1,d1) ,\u00a0died ,(d2) ,\u00a0came ,(d1) ,\u00a0c\u2019ed,(d1) ) ( \u2329C,(d1:2,d2:1)\u232a,\u00a0\u2329died ,(d2:1)\u232a,\u00a0\u2329came ,(d1:1)\u232a,\u00a0\u2329c\u2019ed,(d1:1)\u232a\u00a0)\u232a \u232a\n\u232a \u232a \u232a \u232a\u232a \u232a \u232a \u232a \u232a\n\u2329\u2329 \u2329\n\u2329 \u2329 \u2329\u2329 \u2329 \u2329 \u2329 \u2329\u2192\n\u2192\n\u2192\n\u2192\n\u2192\n\u2192\n\u25eeFigure 4.6 Map and reduce functions in MapReduce. In general, the map fu nc-\ntion produces a list of key-value pairs. All values for a key a re collected into one\nlist in the reduce phase. This list is then processed further . The instantiations of the\ntwo functions and an example are shown for index constructio n. Because the map\nphase processes documents in a distributed fashion, termID \u2013docID pairs need not be\nordered correctly initially as in this example. The example shows terms instead of\ntermIDs for better readability. We abbreviate Caesar asCandconquered asc\u2019ed.\nParsers and inverters are not separate sets of machines. The master iden-\nti\ufb01es idle machines and assigns tasks to them. The same machi ne can be a\nparser in the map phase and an inverter in the reduce phase. An d there are\noften other jobs that run in parallel with index constructio n, so in between\nbeing a parser and an inverter a machine might do some crawlin g or another\nunrelated task.\nTo minimize write times before inverters reduce the data, ea ch parser writes\nits segment \ufb01les to its local disk . In the reduce phase, the master communi-\ncates to an inverter the locations of the relevant segment \ufb01l es (e.g., of the r\nsegment \ufb01les of the a\u2013f partition). Each segment \ufb01le only req uires one se-\nquential read because all data relevant to a particular inve rter were written\nto a single segment \ufb01le by the parser. This setup minimizes th e amount of\nnetwork traf\ufb01c needed during indexing.\nFigure 4.6shows the general schema of the MapReduce functions. In-\nput and output are often lists of key-value pairs themselves , so that several\nMapReduce jobs can run in sequence. In fact, this was the desi gn of the\nGoogle indexing system in 2004. What we describe in this sect ion corre-\nsponds to only one of \ufb01ve to ten MapReduce operations in that i ndexing\nsystem. Another MapReduce operation transforms the term-p artitioned in-\ndex we just created into a document-partitioned one.\nMapReduce offers a robust and conceptually simple framewor k for imple-\nmenting index construction in a distributed environment. B y providing a\nsemiautomatic method for splitting index construction int o smaller tasks, it\ncan scale to almost arbitrarily large collections, given co mputer clusters of\nOnline edition (c)\n2009 Cambridge UP78 4 Index construction\nsuf\ufb01cient size.\n?Exercise 4.3\nFor n=15 splits, r=10 segments, and j=3 term partitions, how long would\ndistributed index creation take for Reuters-RCV1 in a MapRe duce architecture? Base\nyour assumptions about cluster machines on Table 4.1.\n4.5 Dynamic indexing\nThus far, we have assumed that the document collection is sta tic. This is \ufb01ne\nfor collections that change infrequently or never (e.g., th e Bible or Shake-\nspeare). But most collections are modi\ufb01ed frequently with d ocuments being\nadded, deleted, and updated. This means that new terms need t o be added\nto the dictionary, and postings lists need to be updated for e xisting terms.\nThe simplest way to achieve this is to periodically reconstr uct the index\nfrom scratch. This is a good solution if the number of changes over time is\nsmall and a delay in making new documents searchable is accep table \u2013 and\nif enough resources are available to construct a new index wh ile the old one\nis still available for querying.\nIf there is a requirement that new documents be included quic kly, one solu-\ntion is to maintain two indexes: a large main index and a small auxiliary index AUXILIARY INDEX\nthat stores new documents. The auxiliary index is kept in mem ory. Searches\nare run across both indexes and results merged. Deletions ar e stored in an in-\nvalidation bit vector. We can then \ufb01lter out deleted documen ts before return-\ning the search result. Documents are updated by deleting and reinserting\nthem.\nEach time the auxiliary index becomes too large, we merge it i nto the main\nindex. The cost of this merging operation depends on how we st ore the index\nin the \ufb01le system. If we store each postings list as a separate \ufb01le, then the\nmerge simply consists of extending each postings list of the main index by\nthe corresponding postings list of the auxiliary index. In t his scheme, the\nreason for keeping the auxiliary index is to reduce the numbe r of disk seeks\nrequired over time. Updating each document separately requ ires up to Mave\ndisk seeks, where Maveis the average size of the vocabulary of documents in\nthe collection. With an auxiliary index, we only put additio nal load on the\ndisk when we merge auxiliary and main indexes.\nUnfortunately, the one-\ufb01le-per-postings-list scheme is i nfeasible because\nmost \ufb01le systems cannot ef\ufb01ciently handle very large number s of \ufb01les. The\nsimplest alternative is to store the index as one large \ufb01le, t hat is, as a concate-\nnation of all postings lists. In reality, we often choose a co mpromise between\nthe two extremes (Section 4.7). To simplify the discussion, we choose the\nsimple option of storing the index as one large \ufb01le here.\nOnline edition (c)\n2009 Cambridge UP4.5 Dynamic indexing 79\nLM ERGE ADDTOKEN(indexes ,Z0,token)\n1Z0\u2190MERGE(Z0,{token})\n2if|Z0|=n\n3 then for i\u21900to\u221e\n4 do if Ii\u2208indexes\n5 then Zi+1\u2190MERGE(Ii,Zi)\n6 (Zi+1is a temporary index on disk.)\n7 indexes\u2190indexes\u2212{Ii}\n8 else Ii\u2190Zi(Zibecomes the permanent index I i.)\n9 indexes\u2190indexes\u222a{Ii}\n10 B REAK\n11 Z0\u2190\u2205\nLOGARITHMIC MERGE()\n1Z0\u2190\u2205 (Z0is the in-memory index.)\n2indexes\u2190\u2205\n3while true\n4doLM ERGE ADDTOKEN(indexes ,Z0,GETNEXTTOKEN())\n\u25eeFigure 4.7 Logarithmic merging. Each token (termID,docID) is initial ly added to\nin-memory index Z0by LM ERGE ADDTOKEN . LOGARITHMIC MERGE initializes Z0\nand indexes .\nIn this scheme, we process each posting \u230aT/n\u230btimes because we touch it\nduring each of\u230aT/n\u230bmerges where nis the size of the auxiliary index and T\nthe total number of postings. Thus, the overall time complex ity is \u0398(T2/n).\n(We neglect the representation of terms here and consider on ly the docIDs.\nFor the purpose of time complexity, a postings list is simply a list of docIDs.)\nWe can do better than \u0398(T2/n)by introducing log2(T/n)indexes I0,I1,\nI2, . . . of size 20\u00d7n, 21\u00d7n, 22\u00d7n. . . . Postings percolate up this sequence of\nindexes and are processed only once on each level. This schem e is called log- LOGARITHMIC\nMERGING arithmic merging (Figure 4.7). As before, up to npostings are accumulated in\nan in-memory auxiliary index, which we call Z0. When the limit nis reached,\nthe 20\u00d7npostings in Z0are transferred to a new index I0that is created on\ndisk. The next time Z0is full, it is merged with I0to create an index Z1of size\n21\u00d7n. Then Z1is either stored as I1(if there isn\u2019t already an I1) or merged\nwith I1into Z2(ifI1exists); and so on. We service search requests by query-\ning in-memory Z0and all currently valid indexes Iion disk and merging the\nresults. Readers familiar with the binomial heap data struc ture2will recog-\n2. See, for example, ( Cormen et al. 1990 , Chapter 19).\nOnline edition (c)\n2009 Cambridge UP80 4 Index construction\nnize its similarity with the structure of the inverted index es in logarithmic\nmerging.\nOverall index construction time is \u0398(Tlog(T/n))because each posting\nis processed only once on each of the log (T/n)levels. We trade this ef\ufb01-\nciency gain for a slow down of query processing; we now need to merge\nresults from log (T/n)indexes as opposed to just two (the main and auxil-\niary indexes). As in the auxiliary index scheme, we still nee d to merge very\nlarge indexes occasionally (which slows down the search sys tem during the\nmerge), but this happens less frequently and the indexes inv olved in a merge\non average are smaller.\nHaving multiple indexes complicates the maintenance of col lection-wide\nstatistics. For example, it affects the spelling correctio n algorithm in Sec-\ntion 3.3(page 56) that selects the corrected alternative with the most hits.\nWith multiple indexes and an invalidation bit vector, the co rrect number of\nhits for a term is no longer a simple lookup. In fact, all aspec ts of an IR\nsystem \u2013 index maintenance, query processing, distributio n, and so on \u2013 are\nmore complex in logarithmic merging.\nBecause of this complexity of dynamic indexing, some large s earch engines\nadopt a reconstruction-from-scratch strategy. They do not construct indexes\ndynamically. Instead, a new index is built from scratch peri odically. Query\nprocessing is then switched from the new index and the old ind ex is deleted.\n?Exercise 4.4\nForn=2 and 1\u2264T\u226430, perform a step-by-step simulation of the algorithm in\nFigure 4.7. Create a table that shows, for each point in time at which T=2\u2217ktokens\nhave been processed (1 \u2264k\u226415), which of the three indexes I0, . . . , I3are in use. The\n\ufb01rst three lines of the table are given below.\nI3I2I1I0\n20 0 0 0\n40 0 0 1\n60 0 1 0\n4.6 Other types of indexes\nThis chapter only describes construction of nonpositional indexes. Except\nfor the much larger data volume we need to accommodate, the ma in differ-\nence for positional indexes is that (termID, docID, (positi on1, position2, . . . ))\ntriples, instead of (termID, docID) pairs have to be process ed and that tokens\nand postings contain positional information in addition to docIDs. With this\nchange, the algorithms discussed here can all be applied to p ositional in-\ndexes.\nIn the indexes we have considered so far, postings lists are o rdered with\nrespect to docID. As we see in Chapter 5, this is advantageous for compres-\nOnline edition (c)\n2009 Cambridge UP4.6 Other types of indexes 81\nusersdocuments\n0/1\ndoc e., 1 otherwis0 if user can\u2019t read\n\u25eeFigure 4.8 A user-document matrix for access control lists. Element (i,j)is 1 if\nuser ihas access to document jand 0 otherwise. During query processing, a user\u2019s\naccess postings list is intersected with the results list re turned by the text part of the\nindex.\nsion \u2013 instead of docIDs we can compress smaller gaps between IDs, thus\nreducing space requirements for the index. However, this st ructure for the\nindex is not optimal when we build ranked (Chapters 6and 7) \u2013 as opposed to RANKED\nBoolean \u2013 retrieval systems . In ranked retrieval, postings are often ordered ac- RETRIEVAL SYSTEMS\ncording to weight or impact, with the highest-weighted post ings occurring\n\ufb01rst. With this organization, scanning of long postings lis ts during query\nprocessing can usually be terminated early when weights hav e become so\nsmall that any further documents can be predicted to be of low similarity\nto the query (see Chapter 6). In a docID-sorted index, new documents are\nalways inserted at the end of postings lists. In an impact-so rted index (Sec-\ntion 7.1.5 , page 140), the insertion can occur anywhere, thus complicating the\nupdate of the inverted index.\nSecurity is an important consideration for retrieval systems in corp orations. SECURITY\nA low-level employee should not be able to \ufb01nd the salary rost er of the cor-\nporation, but authorized managers need to be able to search f or it. Users\u2019\nresults lists must not contain documents they are barred fro m opening; the\nvery existence of a document can be sensitive information.\nUser authorization is often mediated through access control lists or ACLs. ACCESS CONTROL LISTS\nACLs can be dealt with in an information retrieval system by r epresenting\neach document as the set of users that can access them (Figure 4.8) and then\ninverting the resulting user-document matrix. The inverte d ACL index has,\nfor each user, a \u201cpostings list\u201d of documents they can access \u2013 the user\u2019s ac-\ncess list. Search results are then intersected with this lis t. However, such\nan index is dif\ufb01cult to maintain when access permissions cha nge \u2013 we dis-\ncussed these dif\ufb01culties in the context of incremental inde xing for regular\npostings lists in Section 4.5. It also requires the processi ng of very long post-\nings lists for users with access to large document subsets. U ser membership\nis therefore often veri\ufb01ed by retrieving access informatio n directly from the\n\ufb01le system at query time \u2013 even though this slows down retriev al.\nOnline edition (c)\n2009 Cambridge UP82 4 Index construction\n\u25eeTable 4.3 The \ufb01ve steps in constructing an index for Reuters-RCV1 in bl ocked\nsort-based indexing. Line numbers refer to Figure 4.2.\nStep Time\n1 reading of collection (line 4)\n2 10 initial sorts of 107records each (line 5)\n3 writing of 10 blocks (line 6)\n4 total disk transfer time for merging (line 7)\n5 time of actual merging (line 7)\ntotal\n\u25eeTable 4.4 Collection statistics for a large collection.\nSymbol Statistic Value\nN # documents 1,000,000,000\nLave # tokens per document 1000\nM # distinct terms 44,000,000\nWe discussed indexes for storing and retrieving terms (as op posed to doc-\numents) in Chapter 3.\n?Exercise 4.5\nCan spelling correction compromise document-level securi ty? Consider the case where\na spelling correction is based on documents to which the user does not have access.\n?Exercise 4.6\nTotal index construction time in blocked sort-based indexi ng is broken down in Ta-\nble4.3. Fill out the time column of the table for Reuters-RCV1 assum ing a system\nwith the parameters given in Table 4.1.\nExercise 4.7\nRepeat Exercise 4.6for the larger collection in Table 4.4. Choose a block size that is\nrealistic for current technology (remember that a block sho uld easily \ufb01t into main\nmemory). How many blocks do you need?\nExercise 4.8\nAssume that we have a collection of modest size whose index ca n be constructed with\nthe simple in-memory indexing algorithm in Figure 1.4(page 8). For this collection,\ncompare memory, disk and time requirements of the simple alg orithm in Figure 1.4\nand blocked sort-based indexing.\nExercise 4.9\nAssume that machines in MapReduce have 100 GB of disk space ea ch. Assume fur-\nther that the postings list of the term thehas a size of 200 GB. Then the MapReduce\nalgorithm as described cannot be run to construct the index. How would you modify\nMapReduce so that it can handle this case?\nOnline edition (c)\n2009 Cambridge UP4.7 References and further reading 83\nExercise 4.10\nFor optimal load balancing, the inverters in MapReduce must get segmented postings\n\ufb01les of similar sizes. For a new collection, the distributio n of key-value pairs may not\nbe known in advance. How would you solve this problem?\nExercise 4.11\nApply MapReduce to the problem of counting how often each ter m occurs in a set of\n\ufb01les. Specify map and reduce operations for this task. Write down an example along\nthe lines of Figure 4.6.\nExercise 4.12\nWe claimed (on page 80) that an auxiliary index can impair the quality of collec-\ntion statistics. An example is the term weighting method idf , which is de\ufb01ned as\nlog(N/df i)where Nis the total number of documents and df iis the number of docu-\nments that term ioccurs in (Section 6.2.1 , page 117). Show that even a small auxiliary\nindex can cause signi\ufb01cant error in idf when it is computed on the main index only.\nConsider a rare term that suddenly occurs frequently (e.g., Flossie as inTropical Storm\nFlossie ).\n4.7 References and further reading\nWitten et al. (1999 , Chapter 5) present an extensive treatment of the subject of\nindex construction and additional indexing algorithms wit h different trade-\noffs of memory, disk space, and time. In general, blocked sor t-based indexing\ndoes well on all three counts. However, if conserving memory or disk space\nis the main criterion, then other algorithms may be a better c hoice. See Wit-\nten et al. (1999 ), Tables 5.4 and 5.5; BSBI is closest to \u201csort-based multiwa y\nmerge,\u201d but the two algorithms differ in dictionary structu re and use of com-\npression.\nMoffat and Bell (1995 ) show how to construct an index \u201cin situ,\u201d that\nis, with disk space usage close to what is needed for the \ufb01nal i ndex and\nwith a minimum of additional temporary \ufb01les (cf. also Harman and Candela\n(1990 )). They give Lesk (1988 ) and Somogyi (1990 ) credit for being among\nthe \ufb01rst to employ sorting for index construction.\nThe SPIMI method in Section 4.3is from ( Heinz and Zobel 2003 ). We have\nsimpli\ufb01ed several aspects of the algorithm, including comp ression and the\nfact that each term\u2019s data structure also contains, in addit ion to the postings\nlist, its document frequency and house keeping information . We recommend\nHeinz and Zobel (2003 ) and Zobel and Moffat (2006 ) as up-do-date, in-depth\ntreatments of index construction. Other algorithms with go od scaling prop-\nerties with respect to vocabulary size require several pass es through the data,\ne.g., FAST-INV ( Fox and Lee 1991 ,Harman et al. 1992 ).\nThe MapReduce architecture was introduced by Dean and Ghemawat (2004 ).\nAn open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/ .\nRibeiro-Neto et al. (1999 ) and Melnik et al. (2001 ) describe other approaches\nOnline edition (c)\n2009 Cambridge UP84 4 Index construction\nto distributed indexing. Introductory chapters on distrib uted IR are ( Baeza-\nYates and Ribeiro-Neto 1999 , Chapter 9) and ( Grossman and Frieder 2004 ,\nChapter 8). See also Callan (2000 ).\nLester et al. (2005 ) and B\u00fcttcher and Clarke (2005a ) analyze the proper-\nties of logarithmic merging and compare it with other constr uction methods.\nOne of the \ufb01rst uses of this method was in Lucene ( http://lucene.apache.org ).\nOther dynamic indexing methods are discussed by B\u00fcttcher et al. (2006 ) and\nLester et al. (2006 ). The latter paper also discusses the strategy of replacing\nthe old index by one built from scratch.\nHeinz et al. (2002 ) compare data structures for accumulating the vocabu-\nlary in memory. B\u00fcttcher and Clarke (2005b ) discuss security models for a\ncommon inverted index for multiple users. A detailed charac terization of the\nReuters-RCV1 collection can be found in ( Lewis et al. 2004 ). NIST distributes\nthe collection (see http://trec.nist.gov/data/reuters/reuters.html ).\nGarcia-Molina et al. (1999 , Chapter 2) review computer hardware relevant\nto system design in depth.\nAn effective indexer for enterprise search needs to be able t o communicate\nef\ufb01ciently with a number of applications that hold text data in corporations,\nincluding Microsoft Outlook, IBM\u2019s Lotus software, databa ses like Oracle\nand MySQL, content management systems like Open Text, and en terprise\nresource planning software like SAP .\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 85\n5 Index compression\nChapter 1introduced the dictionary and the inverted index as the cent ral\ndata structures in information retrieval (IR). In this chap ter, we employ a\nnumber of compression techniques for dictionary and invert ed index that\nare essential for ef\ufb01cient IR systems.\nOne bene\ufb01t of compression is immediately clear. We need less disk space.\nAs we will see, compression ratios of 1:4 are easy to achieve, potentially cut-\nting the cost of storing the index by 75%.\nThere are two more subtle bene\ufb01ts of compression. The \ufb01rst is increased\nuse of caching. Search systems use some parts of the dictiona ry and the index\nmuch more than others. For example, if we cache the postings l ist of a fre-\nquently used query term t, then the computations necessary for responding\nto the one-term query tcan be entirely done in memory. With compression,\nwe can \ufb01t a lot more information into main memory. Instead of h aving to\nexpend a disk seek when processing a query with t, we instead access its\npostings list in memory and decompress it. As we will see belo w, there are\nsimple and ef\ufb01cient decompression methods, so that the pena lty of having to\ndecompress the postings list is small. As a result, we are abl e to decrease the\nresponse time of the IR system substantially. Because memor y is a more ex-\npensive resource than disk space, increased speed owing to c aching \u2013 rather\nthan decreased space requirements \u2013 is often the prime motiv ator for com-\npression.\nThe second more subtle advantage of compression is faster tr ansfer of data\nfrom disk to memory. Ef\ufb01cient decompression algorithms run so fast on\nmodern hardware that the total time of transferring a compre ssed chunk of\ndata from disk and then decompressing it is usually less than transferring\nthe same chunk of data in uncompressed form. For instance, we can reduce\ninput/output (I/O) time by loading a much smaller compresse d postings\nlist, even when you add on the cost of decompression. So, in mo st cases,\nthe retrieval system runs faster on compressed postings lis ts than on uncom-\npressed postings lists.\nIf the main goal of compression is to conserve disk space, the n the speed\nOnline edition (c)\n2009 Cambridge UP86 5 Index compression\nof compression algorithms is of no concern. But for improved cache uti-\nlization and faster disk-to-memory transfer, decompressi on speeds must be\nhigh. The compression algorithms we discuss in this chapter are highly ef\ufb01-\ncient and can therefore serve all three purposes of index com pression.\nIn this chapter, we de\ufb01ne a posting as a docID in a postings list. For exam- POSTING\nple, the postings list (6; 20, 45, 100), where 6 is the termID o f the list\u2019s term,\ncontains three postings. As discussed in Section 2.4.2 (page 41), postings in\nmost search systems also contain frequency and position inf ormation; but we\nwill only consider simple docID postings here. See Section 5.4for references\non compressing frequencies and positions.\nThis chapter \ufb01rst gives a statistical characterization of t he distribution of\nthe entities we want to compress \u2013 terms and postings in large collections\n(Section 5.1). We then look at compression of the dictionary, using the di ctionary-\nas-a-string method and blocked storage (Section 5.2). Section 5.3describes\ntwo techniques for compressing the postings \ufb01le, variable b yte encoding and\n\u03b3encoding.\n5.1 Statistical properties of terms in information retriev al\nAs in the last chapter, we use Reuters-RCV1 as our model colle ction (see Ta-\nble4.2, page 70). We give some term and postings statistics for the collecti on\nin Table 5.1. \u201c\u2206%\u201d indicates the reduction in size from the previous line.\n\u201cT%\u201d is the cumulative reduction from un\ufb01ltered.\nThe table shows the number of terms for different levels of pr eprocessing\n(column 2). The number of terms is the main factor in determin ing the size\nof the dictionary. The number of nonpositional postings (co lumn 3) is an\nindicator of the expected size of the nonpositional index of the collection.\nThe expected size of a positional index is related to the numb er of positions\nit must encode (column 4).\nIn general, the statistics in Table 5.1show that preprocessing affects the size\nof the dictionary and the number of nonpositional postings g reatly. Stem-\nming and case folding reduce the number of (distinct) terms b y 17% each\nand the number of nonpositional postings by 4% and 3%, respec tively. The\ntreatment of the most frequent words is also important. The rule of 30 states RULE OF 30\nthat the 30 most common words account for 30% of the tokens in w ritten text\n(31% in the table). Eliminating the 150 most common words fro m indexing\n(as stop words; cf. Section 2.2.2 , page 27) cuts 25% to 30% of the nonpositional\npostings. But, although a stop list of 150 words reduces the n umber of post-\nings by a quarter or more, this size reduction does not carry o ver to the size\nof the compressed index. As we will see later in this chapter, the postings\nlists of frequent words require only a few bits per posting af ter compression.\nThe deltas in the table are in a range typical of large collect ions. Note,\nOnline edition (c)\n2009 Cambridge UP5.1 Statistical properties of terms in information retriev al 87\n\u25eeTable 5.1 The effect of preprocessing on the number of terms, nonposit ional post-\nings, and tokens for Reuters-RCV1. \u201c \u2206%\u201d indicates the reduction in size from the pre-\nvious line, except that \u201c30 stop words\u201d and \u201c150 stop words\u201d b oth use \u201ccase folding\u201d\nas their reference line. \u201cT%\u201d is the cumulative (\u201ctotal\u201d) re duction from un\ufb01ltered. We\nperformed stemming with the Porter stemmer (Chapter 2, page 33).\ntokens ( =number of position\n(distinct) terms nonpositional postings entries in postin gs)\nnumber \u2206% T% number \u2206% T% number \u2206% T%\nun\ufb01ltered 484,494 109,971,179 197,879,290\nno numbers 473,723 \u22122\u22122 100,680,242 \u22128\u22128 179,158,204 \u22129\u22129\ncase folding 391,523 \u221217\u221219 96,969,056 \u22123\u221212 179,158,204 \u22120\u22129\n30 stop words 391,493 \u22120\u221219 83,390,443 \u221214\u221224 121,857,825 \u221231\u221238\n150 stop words 391,373 \u22120\u221219 67,001,847 \u221230\u221239 94,516,599 \u221247\u221252\nstemming 322,383 \u221217\u221233 63,812,300 \u22124\u221242 94,516,599 \u22120\u221252\nhowever, that the percentage reductions can be very differe nt for some text\ncollections. For example, for a collection of web pages with a high proportion\nof French text, a lemmatizer for French reduces vocabulary s ize much more\nthan the Porter stemmer does for an English-only collection because French\nis a morphologically richer language than English.\nThe compression techniques we describe in the remainder of t his chapter\narelossless , that is, all information is preserved. Better compression ratios LOSSLESS\ncan be achieved with lossy compression , which discards some information. LOSSY COMPRESSION\nCase folding, stemming, and stop word elimination are forms of lossy com-\npression. Similarly, the vector space model (Chapter 6) and dimensionality\nreduction techniques like latent semantic indexing (Chapt er18) create com-\npact representations from which we cannot fully restore the original collec-\ntion. Lossy compression makes sense when the \u201clost\u201d informa tion is unlikely\never to be used by the search system. For example, web search i s character-\nized by a large number of documents, short queries, and users who only look\nat the \ufb01rst few pages of results. As a consequence, we can disc ard postings of\ndocuments that would only be used for hits far down the list. T hus, there are\nretrieval scenarios where lossy methods can be used for comp ression without\nany reduction in effectiveness.\nBefore introducing techniques for compressing the diction ary, we want to\nestimate the number of distinct terms Min a collection. It is sometimes said\nthat languages have a vocabulary of a certain size. The secon d edition of\ntheOxford English Dictionary (OED) de\ufb01nes more than 600,000 words. But\nthe vocabulary of most large collections is much larger than the OED. The\nOED does not include most names of people, locations, produc ts, or scienti\ufb01c\nOnline edition (c)\n2009 Cambridge UP88 5 Index compression\n0246 80123456\nlog10 Tlog10 M\n\u25eeFigure 5.1 Heaps\u2019 law. Vocabulary size Mas a function of collection size T\n(number of tokens) for Reuters-RCV1. For these data, the das hed line log10M=\n0.49\u2217log10T+1.64 is the best least-squares \ufb01t. Thus, k=101.64\u224844 and b=0.49.\nentities like genes. These names need to be included in the in verted index,\nso our users can search for them.\n5.1.1 Heaps\u2019 law: Estimating the number of terms\nA better way of getting a handle on MisHeaps\u2019 law , which estimates vocab- HEAPS \u2019LAW\nulary size as a function of collection size:\nM=kTb(5.1)\nwhere Tis the number of tokens in the collection. Typical values for the\nparameters kand bare: 30\u2264k\u2264100 and b\u22480.5. The motivation for\nHeaps\u2019 law is that the simplest possible relationship betwe en collection size\nand vocabulary size is linear in log\u2013log space and the assump tion of linearity\nis usually born out in practice as shown in Figure 5.1for Reuters-RCV1. In\nthis case, the \ufb01t is excellent for T>105=100,000, for the parameter values\nb=0.49 and k=44. For example, for the \ufb01rst 1,000,020 tokens Heaps\u2019 law\nOnline edition (c)\n2009 Cambridge UP5.1 Statistical properties of terms in information retriev al 89\npredicts 38,323 terms:\n44\u00d71,000,0200.49\u224838,323.\nThe actual number is 38,365 terms, very close to the predicti on.\nThe parameter kis quite variable because vocabulary growth depends a\nlot on the nature of the collection and how it is processed. Ca se-folding and\nstemming reduce the growth rate of the vocabulary, whereas i ncluding num-\nbers and spelling errors increase it. Regardless of the valu es of the param-\neters for a particular collection, Heaps\u2019 law suggests that (i) the dictionary\nsize continues to increase with more documents in the collec tion, rather than\na maximum vocabulary size being reached, and (ii) the size of the dictionary\nis quite large for large collections. These two hypotheses h ave been empir-\nically shown to be true of large text collections (Section 5.4). So dictionary\ncompression is important for an effective information retr ieval system.\n5.1.2 Zipf\u2019s law: Modeling the distribution of terms\nWe also want to understand how terms are distributed across d ocuments.\nThis helps us to characterize the properties of the algorith ms for compressing\npostings lists in Section 5.3.\nA commonly used model of the distribution of terms in a collec tion is Zipf\u2019s ZIPF\u2019S LAW\nlaw. It states that, if t1is the most common term in the collection, t2is the\nnext most common, and so on, then the collection frequency cf iof the ith\nmost common term is proportional to 1/ i:\ncfi\u221d1\ni. (5.2)\nSo if the most frequent term occurs cf 1times, then the second most frequent\nterm has half as many occurrences, the third most frequent te rm a third as\nmany occurrences, and so on. The intuition is that frequency decreases very\nrapidly with rank. Equation ( 5.2) is one of the simplest ways of formalizing\nsuch a rapid decrease and it has been found to be a reasonably g ood model.\nEquivalently, we can write Zipf\u2019s law as cf i=cikor as log cf i=logc+\nklogiwhere k=\u22121 and cis a constant to be de\ufb01ned in Section 5.3.2 . It\nis therefore a power law with exponent k=\u22121. See Chapter 19, page 426, POWER LAW\nfor another power law, a law characterizing the distributio n of links on web\npages.\nThe log\u2013log graph in Figure 5.2plots the collection frequency of a term as\na function of its rank for Reuters-RCV1. A line with slope \u20131, corresponding\nto the Zipf function log cf i=logc\u2212logi, is also shown. The \ufb01t of the data\nto the law is not particularly good, but good enough to serve a s a model for\nterm distributions in our calculations in Section 5.3.\nOnline edition (c)\n2009 Cambridge UP90 5 Index compression\n01234560123456 7\nlog10 rank7log10 cf\n\u25eeFigure 5.2 Zipf\u2019s law for Reuters-RCV1. Frequency is plotted as a funct ion of\nfrequency rank for the terms in the collection. The line is th e distribution predicted\nby Zipf\u2019s law (weighted least-squares \ufb01t; intercept is 6.95 ).\n?Exercise 5.1 [\u22c6]\nAssuming one machine word per posting, what is the size of the uncompressed (non-\npositional) index for different tokenizations based on Tab le5.1? How do these num-\nbers compare with Table 5.6?\n5.2 Dictionary compression\nThis section presents a series of dictionary data structure s that achieve in-\ncreasingly higher compression ratios. The dictionary is sm all compared with\nthe postings \ufb01le as suggested by Table 5.1. So why compress it if it is respon-\nsible for only a small percentage of the overall space requir ements of the IR\nsystem?\nOne of the primary factors in determining the response time o f an IR sys-\ntem is the number of disk seeks necessary to process a query. I f parts of the\ndictionary are on disk, then many more disk seeks are necessa ry in query\nevaluation. Thus, the main goal of compressing the dictiona ry is to \ufb01t it in\nmain memory, or at least a large portion of it, to support high query through-\nOnline edition (c)\n2009 Cambridge UP5.2 Dictionary compression 91\nterm document\nfrequencypointer to\npostings list\na 656,265 \u2212\u2192\naachen 65 \u2212\u2192\n. . . . . . . . .\nzulu 221 \u2212\u2192\nspace needed: 20 bytes 4 bytes 4 bytes\n\u25eeFigure 5.3 Storing the dictionary as an array of \ufb01xed-width entries.\nput. Although dictionaries of very large collections \ufb01t int o the memory of a\nstandard desktop machine, this is not true of many other appl ication scenar-\nios. For example, an enterprise search server for a large cor poration may\nhave to index a multiterabyte collection with a comparative ly large vocab-\nulary because of the presence of documents in many different languages.\nWe also want to be able to design search systems for limited ha rdware such\nas mobile phones and onboard computers. Other reasons for wa nting to\nconserve memory are fast startup time and having to share res ources with\nother applications. The search system on your PC must get alo ng with the\nmemory-hogging word processing suite you are using at the sa me time.\n5.2.1 Dictionary as a string\nThe simplest data structure for the dictionary is to sort the vocabulary lex-\nicographically and store it in an array of \ufb01xed-width entrie s as shown in\nFigure 5.3. We allocate 20 bytes for the term itself (because few terms h ave\nmore than twenty characters in English), 4 bytes for its docu ment frequency,\nand 4 bytes for the pointer to its postings list. Four-byte po inters resolve a\n4 gigabytes (GB) address space. For large collections like t he web, we need\nto allocate more bytes per pointer. We look up terms in the arr ay by binary\nsearch. For Reuters-RCV1, we need M\u00d7(20+4+4) = 400,000\u00d728=\n11.2megabytes (MB) for storing the dictionary in this schem e.\nUsing \ufb01xed-width entries for terms is clearly wasteful. The average length\nof a term in English is about eight characters (Table 4.2, page 70), so on av-\nerage we are wasting twelve characters in the \ufb01xed-width sch eme. Also,\nwe have no way of storing terms with more than twenty characte rs like\nhydrochloro\ufb02uorocarbons andsupercalifragilisticexpialidocious . We can overcome\nthese shortcomings by storing the dictionary terms as one lo ng string of char-\nacters, as shown in Figure 5.4. The pointer to the next term is also used to\ndemarcate the end of the current term. As before, we locate te rms in the data\nstructure by way of binary search in the (now smaller) table. This scheme\nsaves us 60% compared to \ufb01xed-width storage \u2013 12 bytes on aver age of the\nOnline edition (c)\n2009 Cambridge UP92 5 Index compression\n... sys tilesyzygeticsyzygialsyzygyszaibelyiteszecinszono .  .  .\nfreq.\n9\n92\n5\n71\n12\n...\n4bytespostings ptr.\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n...\n4bytesterm ptr.\n3bytes...\u2192\n\u2192\n\u2192\n\u2192\n\u2192\n\u25eeFigure 5.4 Dictionary-as-a-string storage. Pointers mark the end of t he preceding\nterm and the beginning of the next. For example, the \ufb01rst thre e terms in this example\naresystile ,syzygetic , andsyzygial .\n20 bytes we allocated for terms before. However, we now also n eed to store\nterm pointers. The term pointers resolve 400,000 \u00d78=3.2\u00d7106positions,\nso they need to be log23.2\u00d7106\u224822 bits or 3 bytes long.\nIn this new scheme, we need 400,000 \u00d7(4+4+3+8) = 7.6 MB for the\nReuters-RCV1 dictionary: 4 bytes each for frequency and pos tings pointer, 3\nbytes for the term pointer, and 8 bytes on average for the term . So we have\nreduced the space requirements by one third from 11.2 to 7.6 M B.\n5.2.2 Blocked storage\nWe can further compress the dictionary by grouping terms in t he string into\nblocks of size kand keeping a term pointer only for the \ufb01rst term of each\nblock (Figure 5.5). We store the length of the term in the string as an ad-\nditional byte at the beginning of the term. We thus eliminate k\u22121 term\npointers, but need an additional kbytes for storing the length of each term.\nFork=4, we save (k\u22121)\u00d73=9 bytes for term pointers, but need an ad-\nditional k=4 bytes for term lengths. So the total space requirements for the\ndictionary of Reuters-RCV1 are reduced by 5 bytes per four-t erm block, or a\ntotal of 400,000\u00d71/4\u00d75=0.5 MB, bringing us down to 7.1 MB.\nOnline edition (c)\n2009 Cambridge UP5.2 Dictionary compression 93\n... 7sys tile9syzygetic8syzygial6syzygy 11szaibelyite6szecin.  .  .\nfreq.\n9\n92\n5\n71\n12\n...postings ptr.\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n...term ptr.\n...\u2192\n\u2192\n\u2192\n\u2192\n\u2192\n\u25eeFigure 5.5 Blocked storage with four terms per block. The \ufb01rst block con sists of\nsystile ,syzygetic ,syzygial , andsyzygy with lengths of seven, nine, eight, and six charac-\nters, respectively. Each term is preceded by a byte encoding its length that indicates\nhow many bytes to skip to reach subsequent terms.\nBy increasing the block size k, we get better compression. However, there\nis a tradeoff between compression and the speed of term looku p. For the\neight-term dictionary in Figure 5.6, steps in binary search are shown as dou-\nble lines and steps in list search as simple lines. We search f or terms in the un-\ncompressed dictionary by binary search (a). In the compress ed dictionary, we\n\ufb01rst locate the term\u2019s block by binary search and then its pos ition within the\nlist by linear search through the block (b). Searching the un compressed dic-\ntionary in (a) takes on average (0+1+2+3+2+1+2+2)/8\u22481.6 steps,\nassuming each term is equally likely to come up in a query. For example,\n\ufb01nding the two terms, aidandbox, takes three and two steps, respectively.\nWith blocks of size k=4 in (b), we need (0+1+2+3+4+1+2+3)/8=2\nsteps on average, \u224825% more. For example, \ufb01nding dentakes one binary\nsearch step and two steps through the block. By increasing k, we can get\nthe size of the compressed dictionary arbitrarily close to t he minimum of\n400,000\u00d7(4+4+1+8) = 6.8 MB, but term lookup becomes prohibitively\nslow for large values of k.\nOne source of redundancy in the dictionary we have not exploi ted yet is\nthe fact that consecutive entries in an alphabetically sort ed list share common\npre\ufb01xes. This observation leads to front coding (Figure 5.7). A common pre\ufb01x FRONT CODING\nOnline edition (c)\n2009 Cambridge UP94 5 Index compression\n(a) aid\nbox\nden\nex\njob\nox\npit\nwin\n(b) aid box den ex\njob ox pit win\n\u25eeFigure 5.6 Search of the uncompressed dictionary (a) and a dictionary c om-\npressed by blocking with k=4 (b).\nOne block in blocked compression ( k=4) . . .\n8automata 8automate 9au tomatic 10automation\n\u21d3\n. . . further compressed with front coding.\n8automat\u2217a1\u22c4e2\u22c4ic3\u22c4i on\n\u25eeFigure 5.7 Front coding. A sequence of terms with identical pre\ufb01x (\u201caut omat\u201d) is\nencoded by marking the end of the pre\ufb01x with \u2217and replacing it with \u22c4in subsequent\nterms. As before, the \ufb01rst byte of each entry encodes the numb er of characters.\nOnline edition (c)\n2009 Cambridge UP5.3 Postings \ufb01le compression 95\n\u25eeTable 5.2 Dictionary compression for Reuters-RCV1.\ndata structure size in MB\ndictionary, \ufb01xed-width 11.2\ndictionary, term pointers into string 7.6\n\u223c, with blocking, k=4 7.1\n\u223c, with blocking & front coding 5.9\nis identi\ufb01ed for a subsequence of the term list and then refer red to with a\nspecial character. In the case of Reuters, front coding save s another 1.2 MB,\nas we found in an experiment.\nOther schemes with even greater compression rely on minimal perfect\nhashing, that is, a hash function that maps Mterms onto [1, . . . , M]without\ncollisions. However, we cannot adapt perfect hashes increm entally because\neach new term causes a collision and therefore requires the c reation of a new\nperfect hash function. Therefore, they cannot be used in a dy namic environ-\nment.\nEven with the best compression scheme, it may not be feasible to store\nthe entire dictionary in main memory for very large text coll ections and for\nhardware with limited memory. If we have to partition the dic tionary onto\npages that are stored on disk, then we can index the \ufb01rst term o f each page\nusing a B-tree. For processing most queries, the search syst em has to go to\ndisk anyway to fetch the postings. One additional seek for re trieving the\nterm\u2019s dictionary page from disk is a signi\ufb01cant, but tolera ble increase in the\ntime it takes to process a query.\nTable 5.2summarizes the compression achieved by the four dictionary\ndata structures.\n?Exercise 5.2\nEstimate the space usage of the Reuters-RCV1 dictionary wit h blocks of size k=8\nand k=16 in blocked dictionary storage.\nExercise 5.3\nEstimate the time needed for term lookup in the compressed di ctionary of Reuters-\nRCV1 with block sizes of k=4 (Figure 5.6, b), k=8, and k=16. What is the\nslowdown compared with k=1 (Figure 5.6, a)?\n5.3 Postings \ufb01le compression\nRecall from Table 4.2(page 70) that Reuters-RCV1 has 800,000 documents,\n200 tokens per document, six characters per token, and 100,0 00,000 post-\nings where we de\ufb01ne a posting in this chapter as a docID in a pos tings\nlist, that is, excluding frequency and position informatio n. These numbers\nOnline edition (c)\n2009 Cambridge UP96 5 Index compression\n\u25eeTable 5.3 Encoding gaps instead of document IDs. For example, we store gaps\n107, 5, 43, . . . , instead of docIDs 283154, 283159, 283202, . . . forcomputer . The \ufb01rst\ndocID is left unchanged (only shown for arachnocentric ).\nencoding postings list\nthe docIDs . . . 283042 283043 283044 283045\ngaps 1 1 1\ncomputer docIDs . . . 283047 283154 283159 283202\ngaps 107 5 43\narachnocentric docIDs 252000 500100\ngaps 252000 248100\ncorrespond to line 3 (\u201ccase folding\u201d) in Table 5.1. Document identi\ufb01ers are\nlog2800,000\u224820 bits long. Thus, the size of the collection is about 800,00 0\u00d7\n200\u00d76 bytes =960 MB and the size of the uncompressed postings \ufb01le is\n100,000,000\u00d720/8=250 MB.\nTo devise a more ef\ufb01cient representation of the postings \ufb01le , one that uses\nfewer than 20 bits per document, we observe that the postings for frequent\nterms are close together. Imagine going through the documen ts of a collec-\ntion one by one and looking for a frequent term like computer . We will \ufb01nd\na document containing computer , then we skip a few documents that do not\ncontain it, then there is again a document with the term and so on (see Ta-\nble5.3). The key idea is that the gaps between postings are short, requiring a\nlot less space than 20 bits to store. In fact, gaps for the most frequent terms\nsuch astheandforare mostly equal to 1. But the gaps for a rare term that\noccurs only once or twice in a collection (e.g., arachnocentric in Table 5.3) have\nthe same order of magnitude as the docIDs and need 20 bits. For an econom-\nical representation of this distribution of gaps, we need a variable encoding\nmethod that uses fewer bits for short gaps.\nTo encode small numbers in less space than large numbers, we l ook at two\ntypes of methods: bytewise compression and bitwise compres sion. As the\nnames suggest, these methods attempt to encode gaps with the minimum\nnumber of bytes and bits, respectively.\n5.3.1 Variable byte codes\nVariable byte (VB) encoding uses an integral number of bytes to encode a gap. VARIABLE BYTE\nENCODING The last 7 bits of a byte are \u201cpayload\u201d and encode part of the ga p. The \ufb01rst\nbit of the byte is a continuation bit .It is set to 1 for the last byte of the encoded CONTINUATION BIT\ngap and to 0 otherwise. To decode a variable byte code, we read a sequence\nof bytes with continuation bit 0 terminated by a byte with con tinuation bit 1.\nWe then extract and concatenate the 7-bit parts. Figure 5.8gives pseudocode\nOnline edition (c)\n2009 Cambridge UP5.3 Postings \ufb01le compression 97\nVBE NCODE NUMBER (n)\n1bytes\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2while true\n3doPREPEND (bytes ,nmod 128 )\n4 ifn<128\n5 then BREAK\n6 n\u2190ndiv 128\n7bytes[LENGTH (bytes)] += 128\n8return bytes\nVBE NCODE (numbers )\n1bytestream\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2for each n\u2208numbers\n3dobytes\u2190VBE NCODE NUMBER (n)\n4 bytestream\u2190EXTEND (bytestream ,bytes)\n5return bytestream\nVBD ECODE (bytestream )\n1numbers\u2190/an}\u230ara\u230bketle{t/an}\u230ara\u230bketri}ht\n2n\u21900\n3fori\u21901toLENGTH (bytestream )\n4do if bytestream [i]<128\n5 then n\u2190128\u00d7n+bytestream [i]\n6 else n\u2190128\u00d7n+ (bytestream [i]\u2212128)\n7 A PPEND (numbers ,n)\n8 n\u21900\n9return numbers\n\u25eeFigure 5.8 VB encoding and decoding. The functions div and mod compute\ninteger division and remainder after integer division, res pectively. P REPEND adds an\nelement to the beginning of a list, for example, P REPEND (/an}\u230ara\u230bketle{t1, 2/an}\u230ara\u230bketri}ht, 3) =/an}\u230ara\u230bketle{t3, 1, 2/an}\u230ara\u230bketri}ht. EXTEND\nextends a list, for example, E XTEND (/an}\u230ara\u230bketle{t1,2/an}\u230ara\u230bketri}ht,/an}\u230ara\u230bketle{t3, 4/an}\u230ara\u230bketri}ht) =/an}\u230ara\u230bketle{t1, 2, 3, 4/an}\u230ara\u230bketri}ht.\n\u25eeTable 5.4 VB encoding. Gaps are encoded using an integral number of byt es.\nThe \ufb01rst bit, the continuation bit, of each byte indicates wh ether the code ends with\nthis byte (1) or not (0).\ndocIDs 824 829 215406\ngaps 5 214577\nVB code 00000110 10111000 10000101 00001101 00001100 10110 001\nOnline edition (c)\n2009 Cambridge UP98 5 Index compression\n\u25eeTable 5.5 Some examples of unary and \u03b3codes. Unary codes are only shown for\nthe smaller numbers. Commas in \u03b3codes are for readability only and are not part of\nthe actual codes.\nnumber unary code length offset \u03b3code\n0 0\n1 10 0 0\n2 110 10 0 10,0\n3 1110 10 1 10,1\n4 11110 110 00 110,00\n9 1111111110 1110 001 1110,001\n13 1110 101 1110,101\n24 11110 1000 11110,1000\n511 111111110 11111111 111111110,11111111\n1025 11111111110 0000000001 11111111110,0000000001\nfor VB encoding and decoding and Table 5.4an example of a VB-encoded\npostings list.1\nWith VB compression, the size of the compressed index for Reu ters-RCV1\nis 116 MB as we veri\ufb01ed in an experiment. This is a more than 50% reduction\nof the size of the uncompressed index (see Table 5.6).\nThe idea of VB encoding can also be applied to larger or smalle r units than\nbytes: 32-bit words, 16-bit words, and 4-bit words or nibbles . Larger words NIBBLE\nfurther decrease the amount of bit manipulation necessary a t the cost of less\neffective (or no) compression. Word sizes smaller than byte s get even better\ncompression ratios at the cost of more bit manipulation. In g eneral, bytes\noffer a good compromise between compression ratio and speed of decom-\npression.\nFor most IR systems variable byte codes offer an excellent tr adeoff between\ntime and space. They are also simple to implement \u2013 most of the alternatives\nreferred to in Section 5.4are more complex. But if disk space is a scarce\nresource, we can achieve better compression ratios by using bit-level encod-\nings, in particular two closely related encodings: \u03b3codes, which we will turn\nto next, and \u03b4codes (Exercise 5.9).\n\u27045.3.2 \u03b3codes\nVB codes use an adaptive number of bytes depending on the size of the gap.\nBit-level codes adapt the length of the code on the \ufb01ner grain edbitlevel. The\n1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of\n0, in practice the origin is usually 1, so that 10000000 encod es 1, 10000101 encodes 6 (not 5 as in\nthe table), and so on.\nOnline edition (c)\n2009 Cambridge UP5.3 Postings \ufb01le compression 99\nsimplest bit-level code is unary code . The unary code of nis a string of n1s UNARY CODE\nfollowed by a 0 (see the \ufb01rst two columns of Table 5.5). Obviously, this is not\na very ef\ufb01cient code, but it will come in handy in a moment.\nHow ef\ufb01cient can a code be in principle? Assuming the 2ngaps Gwith\n1\u2264G\u22642nare all equally likely, the optimal encoding uses nbits for each\nG. So some gaps ( G=2nin this case) cannot be encoded with fewer than\nlog2Gbits. Our goal is to get as close to this lower bound as possibl e.\nA method that is within a factor of optimal is \u03b3encoding .\u03b3codes im- \u03b3ENCODING\nplement variable-length encoding by splitting the represe ntation of a gap G\ninto a pair of length and offset .Offset isGin binary, but with the leading 1\nremoved.2For example, for 13 (binary 1101) offset is 101. Length encodes the\nlength of offset in unary code. For 13, the length of offset is 3 bits, which is 1110\nin unary. The \u03b3code of 13 is therefore 1110101, the concatenation of length\n1110 and offset 101. The right hand column of Table 5.5gives additional\nexamples of \u03b3codes.\nA\u03b3code is decoded by \ufb01rst reading the unary code up to the 0 that t er-\nminates it, for example, the four bits 1110 when decoding 111 0101. Now we\nknow how long the offset is: 3 bits. The offset 101 can then be r ead correctly\nand the 1 that was chopped off in encoding is prepended: 101 \u21921101 = 13.\nThe length of offset is\u230alog2G\u230bbits and the length of length is\u230alog2G\u230b+1\nbits, so the length of the entire code is 2 \u00d7\u230alog2G\u230b+1 bits. \u03b3codes are\nalways of odd length and they are within a factor of 2 of what we claimed\nto be the optimal encoding length log2G. We derived this optimum from\nthe assumption that the 2ngaps between 1 and 2nare equiprobable. But this\nneed not be the case. In general, we do not know the probabilit y distribution\nover gaps a priori.\nThe characteristic of a discrete probability distribution3Pthat determines\nits coding properties (including whether a code is optimal) is its entropy H (P), ENTROPY\nwhich is de\ufb01ned as follows:\nH(P) =\u2212\u2211\nx\u2208XP(x)log2P(x)\nwhere Xis the set of all possible numbers we need to be able to encode\n(and therefore \u2211x\u2208XP(x) = 1.0). Entropy is a measure of uncertainty as\nshown in Figure 5.9for a probability distribution Pover two possible out-\ncomes, namely, X={x1,x2}. Entropy is maximized ( H(P) =1) for P(x1) =\nP(x2) =0.5 when uncertainty about which xiwill appear next is largest; and\n2. We assume here that Ghas no leading 0s. If there are any, they are removed before de leting\nthe leading 1.\n3. Readers who want to review basic concepts of probability t heory may want to consult Rice\n(2006 ) orRoss (2006 ). Note that we are interested in probability distributions over integers (gaps,\nfrequencies, etc.), but that the coding properties of a prob ability distribution are independent of\nwhether the outcomes are integers or something else.\nOnline edition (c)\n2009 Cambridge UP100 5 Index compression\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nP(x1)H(P)\n\u25eeFigure 5.9 Entropy H(P)as a function of P(x1)for a sample space with two\noutcomes x1and x2.\nminimized ( H(P) =0) for P(x1) =1,P(x2) =0 and for P(x1) =0,P(x2) =1\nwhen there is absolute certainty.\nIt can be shown that the lower bound for the expected length E(L)of a\ncode LisH(P)if certain conditions hold (see the references). It can furt her\nbe shown that for 1 <H(P)<\u221e,\u03b3encoding is within a factor of 3 of this\noptimal encoding, approaching 2 for large H(P):\nE(L\u03b3)\nH(P)\u22642+1\nH(P)\u22643.\nWhat is remarkable about this result is that it holds for any p robability distri-\nbution P. So without knowing anything about the properties of the dis tribu-\ntion of gaps, we can apply \u03b3codes and be certain that they are within a factor\nof\u22482 of the optimal code for distributions of large entropy. A co de like \u03b3\ncode with the property of being within a factor of optimal for an arbitrary\ndistribution Pis called universal . UNIVERSAL CODE\nIn addition to universality, \u03b3codes have two other properties that are use-\nful for index compression. First, they are pre\ufb01x free , namely, no \u03b3code is the PREFIX FREE\npre\ufb01x of another. This means that there is always a unique dec oding of a\nsequence of \u03b3codes \u2013 and we do not need delimiters between them, which\nwould decrease the ef\ufb01ciency of the code. The second propert y is that \u03b3\ncodes are parameter free . For many other ef\ufb01cient codes, we have to \ufb01t the PARAMETER FREE\nparameters of a model (e.g., the binomial distribution) to t he distribution\nOnline edition (c)\n2009 Cambridge UP5.3 Postings \ufb01le compression 101\nof gaps in the index. This complicates the implementation of compression\nand decompression. For instance, the parameters need to be s tored and re-\ntrieved. And in dynamic indexing, the distribution of gaps c an change, so\nthat the original parameters are no longer appropriate. The se problems are\navoided with a parameter-free code.\nHow much compression of the inverted index do \u03b3codes achieve? To\nanswer this question we use Zipf\u2019s law, the term distributio n model intro-\nduced in Section 5.1.2 . According to Zipf\u2019s law, the collection frequency cf i\nis proportional to the inverse of the rank i, that is, there is a constant c\u2032such\nthat:\ncfi=c\u2032\ni. (5.3)\nWe can choose a different constant csuch that the fractions c/iare relative\nfrequencies and sum to 1 (that is, c/i=cfi/T):\n1=M\n\u2211\ni=1c\ni=cM\n\u2211\ni=11\ni=c H M (5.4)\nc=1\nHM(5.5)\nwhere Mis the number of distinct terms and HMis the Mth harmonic num-\nber.4Reuters-RCV1 has M=400,000 distinct terms and HM\u2248lnM, so we\nhave\nc=1\nHM\u22481\nlnM=1\nln 400,000\u22481\n13.\nThus the ith term has a relative frequency of roughly 1/ (13i), and the ex-\npected average number of occurrences of term iin a document of length L\nis:\nLc\ni\u2248200\u00d71\n13\ni\u224815\ni\nwhere we interpret the relative frequency as a term occurren ce probability.\nRecall that 200 is the average number of tokens per document i n Reuters-\nRCV1 (Table 4.2).\nNow we have derived term statistics that characterize the di stribution of\nterms in the collection and, by extension, the distribution of gaps in the post-\nings lists. From these statistics, we can calculate the spac e requirements for\nan inverted index compressed with \u03b3encoding. We \ufb01rst stratify the vocab-\nulary into blocks of size Lc=15. On average, term ioccurs 15/ itimes per\n4. Note that, unfortunately, the conventional symbol for bo th entropy and harmonic number is\nH. Context should make clear which is meant in this chapter.\nOnline edition (c)\n2009 Cambridge UP102 5 Index compression\nNdocuments\nLcmost\nfrequent Ngaps of 1 each\nterms\nLcnext most\nfrequent N/2 gaps of 2 each\nterms\nLcnext most\nfrequent N/3 gaps of 3 each\nterms\n. . . . . .\n\u25eeFigure 5.10 Strati\ufb01cation of terms for estimating the size of a \u03b3encoded inverted\nindex.\ndocument. So the average number of occurrences fper document is 1 \u2264ffor\nterms in the \ufb01rst block, corresponding to a total number of Ngaps per term.\nThe average is1\n2\u2264f<1 for terms in the second block, corresponding to\nN/2 gaps per term, and1\n3\u2264f<1\n2for terms in the third block, correspond-\ning to N/3 gaps per term, and so on. (We take the lower bound because it\nsimpli\ufb01es subsequent calculations. As we will see, the \ufb01nal estimate is too\npessimistic, even with this assumption.) We will make the so mewhat unre-\nalistic assumption that all gaps for a given term have the sam e size as shown\nin Figure 5.10. Assuming such a uniform distribution of gaps, we then have\ngaps of size 1 in block 1, gaps of size 2 in block 2, and so on.\nEncoding the N/jgaps of size jwith \u03b3codes, the number of bits needed\nfor the postings list of a term in the jth block (corresponding to one row in\nthe \ufb01gure) is:\nbits-per-row =N\nj\u00d7(2\u00d7\u230alog2j\u230b+1)\n\u22482Nlog2j\nj.\nTo encode the entire block, we need (Lc)\u00b7(2Nlog2j)/jbits. There are M/(Lc)\nblocks, so the postings \ufb01le as a whole will take up:\nM\nLc\n\u2211\nj=12NLc log2j\nj. (5.6)\nOnline edition (c)\n2009 Cambridge UP5.3 Postings \ufb01le compression 103\n\u25eeTable 5.6 Index and dictionary compression for Reuters-RCV1. The com pression\nratio depends on the proportion of actual text in the collect ion. Reuters-RCV1 con-\ntains a large amount of XML markup. Using the two best compres sion schemes, \u03b3\nencoding and blocking with front coding, the ratio compress ed index to collection\nsize is therefore especially small for Reuters-RCV1: (101+5.9)/3600\u22480.03.\ndata structure size in MB\ndictionary, \ufb01xed-width 11.2\ndictionary, term pointers into string 7.6\n\u223c, with blocking, k=4 7.1\n\u223c, with blocking & front coding 5.9\ncollection (text, xml markup etc) 3600.0\ncollection (text) 960.0\nterm incidence matrix 40,000.0\npostings, uncompressed (32-bit words) 400.0\npostings, uncompressed (20 bits) 250.0\npostings, variable byte encoded 116.0\npostings, \u03b3encoded 101.0\nFor Reuters-RCV1,M\nLc\u2248400,000/15\u224827,000 and\n27,000\n\u2211\nj=12\u00d7106\u00d715 log2j\nj\u2248224 MB. (5.7)\nSo the postings \ufb01le of the compressed inverted index for our 9 60 MB collec-\ntion has a size of 224 MB, one fourth the size of the original co llection.\nWhen we run \u03b3compression on Reuters-RCV1, the actual size of the com-\npressed index is even lower: 101 MB, a bit more than one tenth o f the size of\nthe collection. The reason for the discrepancy between pred icted and actual\nvalue is that (i) Zipf\u2019s law is not a very good approximation o f the actual dis-\ntribution of term frequencies for Reuters-RCV1 and (ii) gap s are not uniform.\nThe Zipf model predicts an index size of 251 MB for the unround ed numbers\nfrom Table 4.2. If term frequencies are generated from the Zipf model and\na compressed index is created for these arti\ufb01cial terms, the n the compressed\nsize is 254 MB. So to the extent that the assumptions about the distribution\nof term frequencies are accurate, the predictions of the mod el are correct.\nTable 5.6summarizes the compression techniques covered in this chap ter.\nThe term incidence matrix (Figure 1.1, page 4) for Reuters-RCV1 has size\n400,000\u00d7800,000 =40\u00d78\u00d7109bits or 40 GB.\n\u03b3codes achieve great compression ratios \u2013 about 15% better th an vari-\nable byte codes for Reuters-RCV1. But they are expensive to d ecode. This is\nbecause many bit-level operations \u2013 shifts and masks \u2013 are ne cessary to de-\ncode a sequence of \u03b3codes as the boundaries between codes will usually be\nOnline edition (c)\n2009 Cambridge UP104 5 Index compression\nsomewhere in the middle of a machine word. As a result, query p rocessing is\nmore expensive for \u03b3codes than for variable byte codes. Whether we choose\nvariable byte or \u03b3encoding depends on the characteristics of an application,\nfor example, on the relative weights we give to conserving di sk space versus\nmaximizing query response time.\nThe compression ratio for the index in Table 5.6is about 25%: 400 MB (un-\ncompressed, each posting stored as a 32-bit word) versus 101 MB ( \u03b3) and 116\nMB (VB). This shows that both \u03b3and VB codes meet the objectives we stated\nin the beginning of the chapter. Index compression substant ially improves\ntime and space ef\ufb01ciency of indexes by reducing the amount of disk space\nneeded, increasing the amount of information that can be kep t in the cache,\nand speeding up data transfers from disk to memory.\n?Exercise 5.4 [\u22c6]\nCompute variable byte codes for the numbers in Tables 5.3and 5.5.\nExercise 5.5 [\u22c6]\nCompute variable byte and \u03b3codes for the postings list /an}\u230ara\u230bketle{t777, 17743, 294068, 31251336 /an}\u230ara\u230bketri}ht.\nUse gaps instead of docIDs where possible. Write binary code s in 8-bit blocks.\nExercise 5.6\nConsider the postings list /an}\u230ara\u230bketle{t4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400 /an}\u230ara\u230bketri}htwith a correspond-\ning list of gaps/an}\u230ara\u230bketle{t4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130 /an}\u230ara\u230bketri}ht. Assume that the length of the postings\nlist is stored separately, so the system knows when a posting s list is complete. Us-\ning variable byte encoding: (i) What is the largest gap you ca n encode in 1 byte? (ii)\nWhat is the largest gap you can encode in 2 bytes? (iii) How man y bytes will the\nabove postings list require under this encoding? (Count onl y space for encoding the\nsequence of numbers.)\nExercise 5.7\nA little trick is to notice that a gap cannot be of length 0 and t hat the stuff left to encode\nafter shifting cannot be 0. Based on these observations: (i) Suggest a modi\ufb01cation to\nvariable byte encoding that allows you to encode slightly la rger gaps in the same\namount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What\nis the largest gap you can encode in 2 bytes? (iv) How many byte s will the postings\nlist in Exercise 5.6require under this encoding? (Count only space for encoding the\nsequence of numbers.)\nExercise 5.8 [\u22c6]\nFrom the following sequence of \u03b3-coded gaps, reconstruct \ufb01rst the gap sequence and\nthen the postings sequence: 111000111010101111110110111 1011.\nExercise 5.9\n\u03b3codes are relatively inef\ufb01cient for large numbers (e.g., 10 25 in Table 5.5) as they\nencode the length of the offset in inef\ufb01cient unary code. \u03b4codes differ from \u03b3codes \u03b4CODES\nin that they encode the \ufb01rst part of the code ( length ) in\u03b3code instead of unary code.\nThe encoding of offset is the same. For example, the \u03b4code of 7 is 10,0,11 (again, we\nadd commas for readability). 10,0 is the \u03b3code for length (2 in this case) and the\nencoding of offset (11) is unchanged. (i) Compute the \u03b4codes for the other numbers\nOnline edition (c)\n2009 Cambridge UP5.4 References and further reading 105\n\u25eeTable 5.7 Two gap sequences to be merged in blocked sort-based indexin g\n\u03b3encoded gap sequence of run 1 111011011111100101111111111 0100011111001\n\u03b3encoded gap sequence of run 2 111110100001111110001000111 11110010000011111010101\nin Table 5.5. For what range of numbers is the \u03b4code shorter than the \u03b3code? (ii) \u03b3\ncode beats variable byte code in Table 5.6because the index contains stop words and\nthus many small gaps. Show that variable byte code is more com pact if larger gaps\ndominate. (iii) Compare the compression ratios of \u03b4code and variable byte code for\na distribution of gaps dominated by large gaps.\nExercise 5.10\nGo through the above calculation of index size and explicitl y state all the approxima-\ntions that were made to arrive at Equation ( 5.6).\nExercise 5.11\nFor a collection of your choosing, determine the number of do cuments and terms and\nthe average length of a document. (i) How large is the inverte d index predicted to be\nby Equation ( 5.6)? (ii) Implement an indexer that creates a \u03b3-compressed inverted\nindex for the collection. How large is the actual index? (iii ) Implement an indexer\nthat uses variable byte encoding. How large is the variable b yte encoded index?\nExercise 5.12\nTo be able to hold as many postings as possible in main memory, it is a good idea to\ncompress intermediate index \ufb01les during index constructio n. (i) This makes merging\nruns in blocked sort-based indexing more complicated. As an example, work out the\n\u03b3-encoded merged sequence of the gaps in Table 5.7. (ii) Index construction is more\nspace ef\ufb01cient when using compression. Would you also expec t it to be faster?\nExercise 5.13\n(i) Show that the size of the vocabulary is \ufb01nite according to Zipf\u2019s law and in\ufb01nite\naccording to Heaps\u2019 law. (ii) Can we derive Heaps\u2019 law from Zi pf\u2019s law?\n5.4 References and further reading\nHeaps\u2019 law was discovered by Heaps (1978 ). See also Baeza-Yates and Ribeiro-\nNeto (1999 ). A detailed study of vocabulary growth in large collection s is\n(Williams and Zobel 2005 ). Zipf\u2019s law is due to Zipf (1949 ).Witten and Bell\n(1990 ) investigate the quality of the \ufb01t obtained by the law. Other term distri-\nbution models, including K mixture and two-poisson model, a re discussed\nbyManning and Sch\u00fctze (1999 , Chapter 15). Carmel et al. (2001 ),B\u00fcttcher\nand Clarke (2006 ),Blanco and Barreiro (2007 ), and Ntoulas and Cho (2007 )\nshow that lossy compression can achieve good compression wi th no or no\nsigni\ufb01cant decrease in retrieval effectiveness.\nDictionary compression is covered in detail by Witten et al. (1999 , Chap-\nter 4), which is recommended as additional reading.\nOnline edition (c)\n2009 Cambridge UP106 5 Index compression\nSubsection 5.3.1 is based on ( Scholer et al. 2002 ). The authors \ufb01nd that\nvariable byte codes process queries two times faster than ei ther bit-level\ncompressed indexes or uncompressed indexes with a 30% penal ty in com-\npression ratio compared with the best bit-level compressio n method. They\nalso show that compressed indexes can be superior to uncompr essed indexes\nnot only in disk usage, but also in query processing speed. Co mpared with\nVB codes, \u201cvariable nibble\u201d codes showed 5% to 10% better com pression\nand up to one third worse effectiveness in one experiment ( Anh and Moffat\n2005 ).Trotman (2003 ) also recommends using VB codes unless disk space is\nat a premium. In recent work, Anh and Moffat (2005 ;2006a ) and Zukowski\net al. (2006 ) have constructed word-aligned binary codes that are both f aster\nin decompression and at least as ef\ufb01cient as VB codes. Zhang et al. (2007 ) in-\nvestigate the increased effectiveness of caching when a num ber of different\ncompression techniques for postings lists are used on moder n hardware.\n\u03b4codes (Exercise 5.9) and \u03b3codes were introduced by Elias (1975 ), who\nproved that both codes are universal. In addition, \u03b4codes are asymptotically\noptimal for H(P)\u2192\u221e.\u03b4codes perform better than \u03b3codes if large num-\nbers (greater than 15) dominate. A good introduction to info rmation theory,\nincluding the concept of entropy, is ( Cover and Thomas 1991 ). While Elias\ncodes are only asymptotically optimal, arithmetic codes ( Witten et al. 1999 ,\nSection 2.4) can be constructed to be arbitrarily close to th e optimum H(P)\nfor any P.\nSeveral additional index compression techniques are cover ed by Witten et\nal. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommen d using param- PARAMETERIZED CODE\neterized codes for index compression, codes that explicitly model the prob abil-\nity distribution of gaps for each term. For example, they sho w that Golomb GOLOMB CODES\ncodes achieve better compression ratios than \u03b3codes for large collections.\nMoffat and Zobel (1992 ) compare several parameterized methods, including\nLLRUN ( Fraenkel and Klein 1985 ).\nThe distribution of gaps in a postings list depends on the ass ignment of\ndocIDs to documents. A number of researchers have looked int o assign-\ning docIDs in a way that is conducive to the ef\ufb01cient compress ion of gap\nsequences ( Moffat and Stuiver 1996 ;Blandford and Blelloch 2002 ;Silvestri\net al. 2004 ;Blanco and Barreiro 2006 ;Silvestri 2007 ). These techniques assign\ndocIDs in a small range to documents in a cluster where a clust er can consist\nof all documents in a given time period, on a particular web si te, or sharing\nanother property. As a result, when a sequence of documents f rom a clus-\nter occurs in a postings list, their gaps are small and can be m ore effectively\ncompressed.\nDifferent considerations apply to the compression of term f requencies and\nword positions than to the compression of docIDs in postings lists. See Scho-\nler et al. (2002 ) and Zobel and Moffat (2006 ).Zobel and Moffat (2006 ) is\nrecommended in general as an in-depth and up-to-date tutori al on inverted\nOnline edition (c)\n2009 Cambridge UP5.4 References and further reading 107\nindexes, including index compression.\nThis chapter only looks at index compression for Boolean ret rieval. For\nranked retrieval (Chapter 6), it is advantageous to order postings according\nto term frequency instead of docID. During query processing , the scanning\nof many postings lists can then be terminated early because s maller weights\ndo not change the ranking of the highest ranked kdocuments found so far. It\nis not a good idea to precompute and store weights in the index (as opposed\nto frequencies) because they cannot be compressed as well as integers (see\nSection 7.1.5 , page 140).\nDocument compression can also be important in an ef\ufb01cient information re-\ntrieval system. de Moura et al. (2000 ) and Brisaboa et al. (2007 ) describe\ncompression schemes that allow direct searching of terms an d phrases in the\ncompressed text, which is infeasible with standard text com pression utilities\nlikegzip andcompress .\n?Exercise 5.14 [\u22c6]\nWe have de\ufb01ned unary codes as being \u201c10\u201d: sequences of 1s term inated by a 0. In-\nterchanging the roles of 0s and 1s yields an equivalent \u201c01\u201d u nary code. When this\n01 unary code is used, the construction of a \u03b3code can be stated as follows: (1) Write\nGdown in binary using b=\u230alog2j\u230b+1 bits. (2) Prepend (b\u22121)0s. (i) Encode the\nnumbers in Table 5.5in this alternative \u03b3code. (ii) Show that this method produces\na well-de\ufb01ned alternative \u03b3code in the sense that it has the same length and can be\nuniquely decoded.\nExercise 5.15 [\u22c6 \u22c6 \u22c6 ]\nUnary code is not a universal code in the sense de\ufb01ned above. H owever, there exists\na distribution over gaps for which unary code is optimal. Whi ch distribution is this?\nExercise 5.16\nGive some examples of terms that violate the assumption that gaps all have the same\nsize (which we made when estimating the space requirements o f a\u03b3-encoded index).\nWhat are general characteristics of these terms?\nExercise 5.17\nConsider a term whose postings list has size n, say, n=10,000. Compare the size of\nthe\u03b3-compressed gap-encoded postings list if the distribution of the term is uniform\n(i.e., all gaps have the same size) versus its size when the di stribution is not uniform.\nWhich compressed postings list is smaller?\nExercise 5.18\nWork out the sum in Equation ( 5.7) and show it adds up to about 251 MB. Use the\nnumbers in Table 4.2, but do not round Lc,c, and the number of vocabulary blocks.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 109\n6Scoring, term weighting and the\nvector space model\nThus far we have dealt with indexes that support Boolean quer ies: a docu-\nment either matches or does not match a query. In the case of la rge document\ncollections, the resulting number of matching documents ca n far exceed the\nnumber a human user could possibly sift through. Accordingl y, it is essen-\ntial for a search engine to rank-order the documents matchin g a query. To do\nthis, the search engine computes, for each matching documen t, a score with\nrespect to the query at hand. In this chapter we initiate the s tudy of assigning\na score to a (query, document) pair. This chapter consists of three main ideas.\n1.We introduce parametric and zone indexes in Section 6.1, which serve\ntwo purposes. First, they allow us to index and retrieve docu ments by\nmetadata such as the language in which a document is written. Second,\nthey give us a simple means for scoring (and thereby ranking) documents\nin response to a query.\n2.Next, in Section 6.2we develop the idea of weighting the importance of a\nterm in a document, based on the statistics of occurrence of t he term.\n3.In Section 6.3we show that by viewing each document as a vector of such\nweights, we can compute a score between a query and each docum ent.\nThis view is known as vector space scoring.\nSection 6.4develops several variants of term-weighting for the vector space\nmodel. Chapter 7develops computational aspects of vector space scoring,\nand related topics.\nAs we develop these ideas, the notion of a query will assume mu ltiple\nnuances. In Section 6.1we consider queries in which speci\ufb01c query terms\noccur in speci\ufb01ed regions of a matching document. Beginning Section 6.2we\nwill in fact relax the requirement of matching speci\ufb01c regio ns of a document;\ninstead, we will look at so-called free text queries that sim ply consist of query\nterms with no speci\ufb01cation on their relative order, importa nce or where in a\ndocument they should be found. The bulk of our study of scorin g will be in\nthis latter notion of a query being such a set of terms.\nOnline edition (c)\n2009 Cambridge UP110 6 Scoring, term weighting and the vector space model\n6.1 Parametric and zone indexes\nWe have thus far viewed a document as a sequence of terms. In fa ct, most\ndocuments have additional structure. Digital documents ge nerally encode,\nin machine-recognizable form, certain metadata associated with each docu- METADATA\nment. By metadata, we mean speci\ufb01c forms of data about a docum ent, such\nas its author(s), title and date of publication. This metada ta would generally\ninclude \ufb01elds such as the date of creation and the format of the document, as FIELD\nwell the author and possibly the title of the document. The po ssible values\nof a \ufb01eld should be thought of as \ufb01nite \u2013 for instance, the set o f all dates of\nauthorship.\nConsider queries of the form \u201c\ufb01nd documents authored by Will iam Shake-\nspeare in 1601, containing the phrase alaspoorYorick \u201d. Query processing then\nconsists as usual of postings intersections, except that we may merge post-\nings from standard inverted as well as parametric indexes . There is one para- PARAMETRIC INDEX\nmetric index for each \ufb01eld (say, date of creation); it allows us to select only\nthe documents matching a date speci\ufb01ed in the query. Figure 6.1illustrates\nthe user\u2019s view of such a parametric search. Some of the \ufb01elds may assume\nordered values, such as dates; in the example query above, th e year 1601 is\none such \ufb01eld value. The search engine may support querying r anges on\nsuch ordered values; to this end, a structure like a B-tree ma y be used for the\n\ufb01eld\u2019s dictionary.\nZones are similar to \ufb01elds, except the contents of a zone can be arbi trary ZONE\nfree text. Whereas a \ufb01eld may take on a relatively small set of values, a zone\ncan be thought of as an arbitrary, unbounded amount of text. F or instance,\ndocument titles and abstracts are generally treated as zone s. We may build a\nseparate inverted index for each zone of a document, to suppo rt queries such\nas \u201c\ufb01nd documents with merchant in the title and william in the author list and\nthe phrase gentle rain in the body\u201d. This has the effect of building an index\nthat looks like Figure 6.2. Whereas the dictionary for a parametric index\ncomes from a \ufb01xed vocabulary (the set of languages, or the set of dates), the\ndictionary for a zone index must structure whatever vocabul ary stems from\nthe text of that zone.\nIn fact, we can reduce the size of the dictionary by encoding t he zone in\nwhich a term occurs in the postings. In Figure 6.3for instance, we show how\noccurrences of william in the title and author zones of various documents are\nencoded. Such an encoding is useful when the size of the dicti onary is a\nconcern (because we require the dictionary to \ufb01t in main memo ry). But there\nis another important reason why the encoding of Figure 6.3is useful: the\nef\ufb01cient computation of scores using a technique we will cal lweighted zone WEIGHTED ZONE\nSCORING scoring .\nOnline edition (c)\n2009 Cambridge UP6.1 Parametric and zone indexes 111\n\u25eeFigure 6.1 Parametric search. In this example we have a collection with \ufb01elds al-\nlowing us to select publications by zones such as Author and \ufb01 elds such as Language.\nwilliam.author 2 3 5 8william.title 2 4 8 16william.abstract 11 121 1441 1729\n- - - -- - - -- - - -\n\u25eeFigure 6.2 Basic zone index ; zones are encoded as extensions of diction ary en-\ntries.\nwilliam 2.author,2.title 3.author 4.title 5.author- - - -\n\u25eeFigure 6.3 Zone index in which the zone is encoded in the postings rather than\nthe dictionary.\nOnline edition (c)\n2009 Cambridge UP112 6 Scoring, term weighting and the vector space model\n6.1.1 Weighted zone scoring\nThus far in Section 6.1we have focused on retrieving documents based on\nBoolean queries on \ufb01elds and zones. We now turn to a second app lication of\nzones and \ufb01elds.\nGiven a Boolean query qand a document d, weighted zone scoring assigns\nto the pair (q,d)a score in the interval [0, 1], by computing a linear combina-\ntion of zone scores , where each zone of the document contributes a Boolean\nvalue. More speci\ufb01cally, consider a set of documents each of which has \u2113\nzones. Let g1, . . . , g\u2113\u2208[0, 1]such that \u2211\u2113\ni=1gi=1. For 1\u2264i\u2264\u2113, letsibe the\nBoolean score denoting a match (or absence thereof) between qand the ith\nzone. For instance, the Boolean score from a zone could be 1 if all the query\nterm(s) occur in that zone, and zero otherwise; indeed, it co uld be any Boo-\nlean function that maps the presence of query terms in a zone t o 0, 1. Then,\nthe weighted zone score is de\ufb01ned to be\n\u2113\n\u2211\ni=1gisi. (6.1)\nWeighted zone scoring is sometimes referred to also as ranked Boolean re- RANKED BOOLEAN\nRETRIEVAL trieval .\n\u270eExample 6.1: Consider the query shakespeare in a collection in which each doc-\nument has three zones: author, title and body . The Boolean score function for a zone\ntakes on the value 1 if the query term shakespeare is present in the zone, and zero\notherwise. Weighted zone scoring in such a collection would require three weights\ng1,g2and g3, respectively corresponding to the author, title and body zones. Suppose\nwe set g1=0.2,g2=0.3 and g3=0.5 (so that the three weights add up to 1); this cor-\nresponds to an application in which a match in the author zone is least important to\nthe overall score, the titlezone somewhat more, and the body contributes even more.\nThus if the term shakespeare were to appear in the titleand body zones but not the\nauthor zone of a document, the score of this document would be 0.8.\nHow do we implement the computation of weighted zone scores? A sim-\nple approach would be to compute the score for each document i n turn,\nadding in all the contributions from the various zones. Howe ver, we now\nshow how we may compute weighted zone scores directly from in verted in-\ndexes. The algorithm of Figure 6.4treats the case when the query qis a two-\nterm query consisting of query terms q1and q2, and the Boolean function is\nAND: 1 if both query terms are present in a zone and 0 otherwise . Following\nthe description of the algorithm, we describe the extension to more complex\nqueries and Boolean functions.\nThe reader may have noticed the close similarity between thi s algorithm\nand that in Figure 1.6. Indeed, they represent the same postings traversal,\nexcept that instead of merely adding a document to the set of r esults for\nOnline edition (c)\n2009 Cambridge UP6.1 Parametric and zone indexes 113\nZONE SCORE(q1,q2)\n1 \ufb02oat scores[N] = [ 0]\n2 constant g[\u2113]\n3p1\u2190postings (q1)\n4p2\u2190postings (q2)\n5 // scores[] is an array with a score entry for each document, initialized to zero.\n6 // p1and p2are initialized to point to the beginning of their respectiv e postings.\n7 //Assume g[] is initialized to the respective zone weights .\n8while p1/ne}ationslash=NILand p2/ne}ationslash=NIL\n9do if docID (p1) =docID (p2)\n10 then scores[docID (p1)]\u2190WEIGHTED ZONE(p1,p2,g)\n11 p1\u2190next(p1)\n12 p2\u2190next(p2)\n13 else if docID (p1)<docID (p2)\n14 then p1\u2190next(p1)\n15 else p2\u2190next(p2)\n16 return scores\n\u25eeFigure 6.4 Algorithm for computing the weighted zone score from two pos tings\nlists. Function W EIGHTED ZONE (not shown here) is assumed to compute the inner\nloop of Equation 6.1.\na Boolean AND query, we now compute a score for each such docum ent.\nSome literature refers to the array scores[] above as a set of accumulators . The ACCUMULATOR\nreason for this will be clear as we consider more complex Bool ean functions\nthan the AND; thus we may assign a non-zero score to a document even if it\ndoes not contain all query terms.\n6.1.2 Learning weights\nHow do we determine the weights gifor weighted zone scoring? These\nweights could be speci\ufb01ed by an expert (or, in principle, the user); but in-\ncreasingly, these weights are \u201clearned\u201d using training exa mples that have\nbeen judged editorially. This latter methodology falls und er a general class\nof approaches to scoring and ranking in information retriev al, known as\nmachine-learned relevance . We provide a brief introduction to this topic here MACHINE -LEARNED\nRELEVANCE because weighted zone scoring presents a clean setting for i ntroducing it; a\ncomplete development demands an understanding of machine l earning and\nis deferred to Chapter 15.\n1.We are provided with a set of training examples , each of which is a tu-\nple consisting of a query qand a document d, together with a relevance\nOnline edition (c)\n2009 Cambridge UP114 6 Scoring, term weighting and the vector space model\njudgment for donq. In the simplest form, each relevance judgments is ei-\nther Relevant orNon-relevant . More sophisticated implementations of the\nmethodology make use of more nuanced judgments.\n2.The weights giare then \u201clearned\u201d from these examples, in order that the\nlearned scores approximate the relevance judgments in the t raining exam-\nples.\nFor weighted zone scoring, the process may be viewed as learn ing a lin-\near function of the Boolean match scores contributed by the v arious zones.\nThe expensive component of this methodology is the labor-in tensive assem-\nbly of user-generated relevance judgments from which to lea rn the weights,\nespecially in a collection that changes frequently (such as the Web). We now\ndetail a simple example that illustrates how we can reduce th e problem of\nlearning the weights gito a simple optimization problem.\nWe now consider a simple case of weighted zone scoring, where each doc-\nument has a titlezone and a body zone. Given a query qand a document d, we\nuse the given Boolean match function to compute Boolean vari ables sT(d,q)\nand sB(d,q), depending on whether the title (respectively, body) zone o fd\nmatches query q. For instance, the algorithm in Figure 6.4uses an AND of\nthe query terms for this Boolean function. We will compute a s core between\n0 and 1 for each (document, query) pair using sT(d,q)and sB(d,q)by using\na constant g\u2208[0, 1], as follows:\nscore(d,q) =g\u00b7sT(d,q) + ( 1\u2212g)sB(d,q). (6.2)\nWe now describe how to determine the constant gfrom a set of training ex-\namples , each of which is a triple of the form \u03a6j= (dj,qj,r(dj,qj)). In each\ntraining example, a given training document djand a given training query qj\nare assessed by a human editor who delivers a relevance judgm entr(dj,qj)\nthat is either Relevant orNon-relevant . This is illustrated in Figure 6.5, where\nseven training examples are shown.\nFor each training example \u03a6jwe have Boolean values sT(dj,qj)and sB(dj,qj)\nthat we use to compute a score from ( 6.2)\nscore(dj,qj) =g\u00b7sT(dj,qj) + ( 1\u2212g)sB(dj,qj). (6.3)\nWe now compare this computed score to the human relevance jud gment for\nthe same document-query pair (dj,qj); to this end, we will quantize each\nRelevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose\nthat we de\ufb01ne the error of the scoring function with weight gas\n\u03b5(g,\u03a6j) = ( r(dj,qj)\u2212score(dj,qj))2,\nOnline edition (c)\n2009 Cambridge UP6.1 Parametric and zone indexes 115\nExample DocID Query sTsB Judgment\n\u03a61 37linux 1 1 Relevant\n\u03a62 37penguin 0 1 Non-relevant\n\u03a63 238system 0 1 Relevant\n\u03a64 238penguin 0 0 Non-relevant\n\u03a65 1741kernel 1 1 Relevant\n\u03a66 2094driver 0 1 Relevant\n\u03a67 3191driver 1 0 Non-relevant\n\u25eeFigure 6.5 An illustration of training examples.\nsTsB Score\n0 0 0\n0 1 1\u2212g\n1 0 g\n1 1 1\n\u25eeFigure 6.6 The four possible combinations of sTand sB.\nwhere we have quantized the editorial relevance judgment r(dj,qj)to 0 or 1.\nThen, the total error of a set of training examples is given by\n\u2211\nj\u03b5(g,\u03a6j). (6.4)\nThe problem of learning the constant gfrom the given training examples\nthen reduces to picking the value of gthat minimizes the total error in ( 6.4).\nPicking the best value of gin (6.4) in the formulation of Section 6.1.3 re-\nduces to the problem of minimizing a quadratic function of gover the inter-\nval[0, 1]. This reduction is detailed in Section 6.1.3 .\n\u27046.1.3 The optimal weight g\nWe begin by noting that for any training example \u03a6jfor which sT(dj,qj) =0\nand sB(dj,qj) = 1, the score computed by Equation ( 6.2) is 1\u2212g. In similar\nfashion, we may write down the score computed by Equation ( 6.2) for the\nthree other possible combinations of sT(dj,qj)and sB(dj,qj); this is summa-\nrized in Figure 6.6.\nLetn01r(respectively, n01n) denote the number of training examples for\nwhich sT(dj,qj) =0 and sB(dj,qj) =1 and the editorial judgment is Relevant\n(respectively, Non-relevant ). Then the contribution to the total error in Equa-\ntion ( 6.4) from training examples for which sT(dj,qj) = 0 and sB(dj,qj) = 1\nOnline edition (c)\n2009 Cambridge UP116 6 Scoring, term weighting and the vector space model\nis\n[1\u2212(1\u2212g)]2n01r+ [0\u2212(1\u2212g)]2n01n.\nBy writing in similar fashion the error contributions from t raining examples\nof the other three combinations of values for sT(dj,qj)and sB(dj,qj)(and\nextending the notation in the obvious manner), the total err or corresponding\nto Equation ( 6.4) is\n(n01r+n10n)g2+ (n10r+n01n)(1\u2212g)2+n00r+n11n. (6.5)\nBy differentiating Equation ( 6.5) with respect to gand setting the result to\nzero, it follows that the optimal value of gis\nn10r+n01n\nn10r+n10n+n01r+n01n. (6.6)\n?Exercise 6.1\nWhen using weighted zone scoring, is it necessary for all zon es to use the same Boo-\nlean match function?\nExercise 6.2\nIn Example 6.1above with weights g1=0.2,g2=0.31 and g3=0.49, what are all the\ndistinct score values a document may get?\nExercise 6.3\nRewrite the algorithm in Figure 6.4to the case of more than two query terms.\nExercise 6.4\nWrite pseudocode for the function WeightedZone for the case of two postings lists in\nFigure 6.4.\nExercise 6.5\nApply Equation 6.6to the sample training set in Figure 6.5to estimate the best value\nofgfor this sample.\nExercise 6.6\nFor the value of gestimated in Exercise 6.5, compute the weighted zone score for each\n(query, document) example. How do these scores relate to the relevance judgments\nin Figure 6.5(quantized to 0/1)?\nExercise 6.7\nWhy does the expression for gin (6.6) not involve training examples in which sT(dt,qt)\nand sB(dt,qt)have the same value?\nOnline edition (c)\n2009 Cambridge UP6.2 Term frequency and weighting 117\n6.2 Term frequency and weighting\nThus far, scoring has hinged on whether or not a query term is p resent in\na zone within a document. We take the next logical step: a docu ment or\nzone that mentions a query term more often has more to do with t hat query\nand therefore should receive a higher score. To motivate thi s, we recall the\nnotion of a free text query introduced in Section 1.4: a query in which the\nterms of the query are typed freeform into the search interfa ce, without any\nconnecting search operators (such as Boolean operators). T his query style,\nwhich is extremely popular on the web, views the query as simp ly a set of\nwords. A plausible scoring mechanism then is to compute a sco re that is the\nsum, over the query terms, of the match scores between each qu ery term and\nthe document.\nTowards this end, we assign to each term in a document a weight for that\nterm, that depends on the number of occurrences of the term in the doc-\nument. We would like to compute a score between a query term tand a\ndocument d, based on the weight of tind. The simplest approach is to assign\nthe weight to be equal to the number of occurrences of term tin document d.\nThis weighting scheme is referred to as term frequency and is denoted tf t,d, TERM FREQUENCY\nwith the subscripts denoting the term and the document in ord er.\nFor a document d, the set of weights determined by the tf weights above\n(or indeed any weighting function that maps the number of occ urrences of t\nindto a positive real value) may be viewed as a quantitative dige st of that\ndocument. In this view of a document, known in the literature as the bag BAG OF WORDS\nof words model , the exact ordering of the terms in a document is ignored but\nthe number of occurrences of each term is material (in contra st to Boolean\nretrieval). We only retain information on the number of occu rrences of each\nterm. Thus, the document \u201cMary is quicker than John\u201d is, in th is view, iden-\ntical to the document \u201cJohn is quicker than Mary\u201d. Neverthel ess, it seems\nintuitive that two documents with similar bag of words repre sentations are\nsimilar in content. We will develop this intuition further i n Section 6.3.\nBefore doing so we \ufb01rst study the question: are all words in a d ocument\nequally important? Clearly not; in Section 2.2.2 (page 27) we looked at the\nidea of stop words \u2013 words that we decide not to index at all, and therefore do\nnot contribute in any way to retrieval and scoring.\n6.2.1 Inverse document frequency\nRaw term frequency as above suffers from a critical problem: all terms are\nconsidered equally important when it comes to assessing rel evancy on a\nquery. In fact certain terms have little or no discriminatin g power in de-\ntermining relevance. For instance, a collection of documen ts on the auto\nindustry is likely to have the term auto in almost every document. To this\nOnline edition (c)\n2009 Cambridge UP118 6 Scoring, term weighting and the vector space model\nWord cf df\ntry 10422 8760\ninsurance 10440 3997\n\u25eeFigure 6.7 Collection frequency (cf) and document frequency (df) beha ve differ-\nently, as in this example from the Reuters collection.\nend, we introduce a mechanism for attenuating the effect of t erms that occur\ntoo often in the collection to be meaningful for relevance de termination. An\nimmediate idea is to scale down the term weights of terms with high collec-\ntion frequency, de\ufb01ned to be the total number of occurrences of a term in the\ncollection. The idea would be to reduce the tf weight of a term by a factor\nthat grows with its collection frequency.\nInstead, it is more commonplace to use for this purpose the document fre- DOCUMENT\nFREQUENCY quency dft, de\ufb01ned to be the number of documents in the collection that c on-\ntain a term t. This is because in trying to discriminate between document s\nfor the purpose of scoring it is better to use a document-leve l statistic (such\nas the number of documents containing a term) than to use a col lection-wide\nstatistic for the term. The reason to prefer df to cf is illust rated in Figure 6.7,\nwhere a simple example shows that collection frequency (cf) and document\nfrequency (df) can behave rather differently. In particula r, the cf values for\nbothtryandinsurance are roughly equal, but their df values differ signi\ufb01-\ncantly. Intuitively, we want the few documents that contain insurance to get\na higher boost for a query on insurance than the many documents containing\ntryget from a query on try.\nHow is the document frequency df of a term used to scale its wei ght? De-\nnoting as usual the total number of documents in a collection byN, we de\ufb01ne\ntheinverse document frequency (idf) of a term tas follows: INVERSE DOCUMENT\nFREQUENCY\nidft=logN\ndft. (6.7)\nThus the idf of a rare term is high, whereas the idf of a frequen t term is\nlikely to be low. Figure 6.8gives an example of idf\u2019s in the Reuters collection\nof 806,791 documents; in this example logarithms are to the b ase 10. In fact,\nas we will see in Exercise 6.12, the precise base of the logarithm is not material\nto ranking. We will give on page 227a justi\ufb01cation of the particular form in\nEquation ( 6.7).\n6.2.2 Tf-idf weighting\nWe now combine the de\ufb01nitions of term frequency and inverse d ocument\nfrequency, to produce a composite weight for each term in eac h document.\nOnline edition (c)\n2009 Cambridge UP6.2 Term frequency and weighting 119\nterm dftidft\ncar 18,165 1.65\nauto 6723 2.08\ninsurance 19,241 1.62\nbest 25,235 1.5\n\u25eeFigure 6.8 Example of idf values. Here we give the idf\u2019s of terms with var ious\nfrequencies in the Reuters collection of 806,791 documents .\nThe tf-idf weighting scheme assigns to term ta weight in document dgiven TF-IDF\nby\ntf-idf t,d=tft,d\u00d7idft. (6.8)\nIn other words, tf-idf t,dassigns to term ta weight in document dthat is\n1.highest when toccurs many times within a small number of documents\n(thus lending high discriminating power to those documents );\n2.lower when the term occurs fewer times in a document, or occur s in many\ndocuments (thus offering a less pronounced relevance signa l);\n3.lowest when the term occurs in virtually all documents.\nAt this point, we may view each document as a vector with one component DOCUMENT VECTOR\ncorresponding to each term in the dictionary, together with a weight for each\ncomponent that is given by ( 6.8). For dictionary terms that do not occur in\na document, this weight is zero. This vector form will prove t o be crucial to\nscoring and ranking; we will develop these ideas in Section 6.3. As a \ufb01rst\nstep, we introduce the overlap score measure : the score of a document dis the\nsum, over all query terms, of the number of times each of the qu ery terms\noccurs in d. We can re\ufb01ne this idea so that we add up not the number of\noccurrences of each query term tind, but instead the tf-idf weight of each\nterm in d.\nScore(q,d) =\u2211\nt\u2208qtf-idf t,d. (6.9)\nIn Section 6.3we will develop a more rigorous form of Equation ( 6.9).\n?Exercise 6.8\nWhy is the idf of a term always \ufb01nite?\nExercise 6.9\nWhat is the idf of a term that occurs in every document? Compar e this with the use\nof stop word lists.\nOnline edition (c)\n2009 Cambridge UP120 6 Scoring, term weighting and the vector space model\nDoc1 Doc2 Doc3\ncar 27 4 24\nauto 3 33 0\ninsurance 0 33 29\nbest 14 0 17\n\u25eeFigure 6.9 Table of tf values for Exercise 6.10.\nExercise 6.10\nConsider the table of term frequencies for 3 documents denot ed Doc1, Doc2, Doc3 in\nFigure 6.9. Compute the tf-idf weights for the terms car, auto, insurance, best , for each\ndocument, using the idf values from Figure 6.8.\nExercise 6.11\nCan the tf-idf weight of a term in a document exceed 1?\nExercise 6.12\nHow does the base of the logarithm in ( 6.7) affect the score calculation in ( 6.9)? How\ndoes the base of the logarithm affect the relative scores of t wo documents on a given\nquery?\nExercise 6.13\nIf the logarithm in ( 6.7) is computed base 2, suggest a simple approximation to the id f\nof a term.\n6.3 The vector space model for scoring\nIn Section 6.2(page 117) we developed the notion of a document vector that\ncaptures the relative importance of the terms in a document. The representa-\ntion of a set of documents as vectors in a common vector space i s known as\nthevector space model and is fundamental to a host of information retrieval op- VECTOR SPACE MODEL\nerations ranging from scoring documents on a query, documen t classi\ufb01cation\nand document clustering. We \ufb01rst develop the basic ideas und erlying vector\nspace scoring; a pivotal step in this development is the view (Section 6.3.2 )\nof queries as vectors in the same vector space as the document collection.\n6.3.1 Dot products\nWe denote by /vectorV(d)the vector derived from document d, with one com-\nponent in the vector for each dictionary term. Unless otherw ise speci\ufb01ed,\nthe reader may assume that the components are computed using the tf-idf\nweighting scheme, although the particular weighting schem e is immaterial\nto the discussion that follows. The set of documents in a coll ection then may\nbe viewed as a set of vectors in a vector space, in which there i s one axis for\nOnline edition (c)\n2009 Cambridge UP6.3 The vector space model for scoring 121\n0 101\njealousgossip\n/vectorv(q)/vectorv(d1)\n/vectorv(d2)\n/vectorv(d3)\u03b8\n\u25eeFigure 6.10 Cosine similarity illustrated. sim (d1,d2) =cos\u03b8.\neach term. This representation loses the relative ordering of the terms in each\ndocument; recall our example from Section 6.2(page 117), where we pointed\nout that the documents Mary is quicker than John and John is quicker than Mary\nare identical in such a bag of words representation.\nHow do we quantify the similarity between two documents in th is vector\nspace? A \ufb01rst attempt might consider the magnitude of the vec tor difference\nbetween two document vectors. This measure suffers from a dr awback: two\ndocuments with very similar content can have a signi\ufb01cant ve ctor difference\nsimply because one is much longer than the other. Thus the rel ative distribu-\ntions of terms may be identical in the two documents, but the a bsolute term\nfrequencies of one may be far larger.\nTo compensate for the effect of document length, the standar d way of\nquantifying the similarity between two documents d1and d2is to compute\nthecosine similarity of their vector representations /vectorV(d1)and/vectorV(d2) COSINE SIMILARITY\nsim(d1,d2) =/vectorV(d1)\u00b7/vectorV(d2)\n|/vectorV(d1)||/vectorV(d2)|, (6.10)\nwhere the numerator represents the dot product (also known as the inner prod- DOT PRODUCT\nuct) of the vectors /vectorV(d1)and/vectorV(d2), while the denominator is the product of\ntheir Euclidean lengths . The dot product /vectorx\u00b7/vectoryof two vectors is de\ufb01ned as EUCLIDEAN LENGTH\n\u2211M\ni=1xiyi. Let/vectorV(d)denote the document vector for d, with Mcomponents\n/vectorV1(d). . ./vectorVM(d). The Euclidean length of dis de\ufb01ned to be/radical\uf8ecig\n\u2211M\ni=1/vectorV2\ni(d).\nThe effect of the denominator of Equation ( 6.10) is thus to length-normalize LENGTH -\nNORMALIZATIONthe vectors /vectorV(d1)and/vectorV(d2)to unit vectors /vectorv(d1) =/vectorV(d1)/|/vectorV(d1)|and\nOnline edition (c)\n2009 Cambridge UP122 6 Scoring, term weighting and the vector space model\nDoc1 Doc2 Doc3\ncar 0.88 0.09 0.58\nauto 0.10 0.71 0\ninsurance 0 0.71 0.70\nbest 0.46 0 0.41\n\u25eeFigure 6.11 Euclidean normalized tf values for documents in Figure 6.9.\nterm SaS PaP WH\naffection 115 58 20\njealous 10 7 11\ngossip 2 0 6\n\u25eeFigure 6.12 Term frequencies in three novels. The novels are Austen\u2019s Sense and\nSensibility, Pride and Prejudice and Bront\u00eb\u2019s Wuthering Heights.\n/vectorv(d2) =/vectorV(d2)/|/vectorV(d2)|. We can then rewrite ( 6.10) as\nsim(d1,d2) =/vectorv(d1)\u00b7/vectorv(d2). (6.11)\n\u270eExample 6.2: Consider the documents in Figure 6.9. We now apply Euclidean\nnormalization to the tf values from the table, for each of the three documents in the\ntable. The quantity/radical\uf8ecig\n\u2211M\ni=1/vectorV2\ni(d)has the values 30.56, 46.84 and 41.30 respectively\nfor Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these\ndocuments are shown in Figure 6.11.\nThus, ( 6.11) can be viewed as the dot product of the normalized versions o f\nthe two document vectors. This measure is the cosine of the an gle\u03b8between\nthe two vectors, shown in Figure 6.10. What use is the similarity measure\nsim(d1,d2)? Given a document d(potentially one of the diin the collection),\nconsider searching for the documents in the collection most similar to d. Such\na search is useful in a system where a user may identify a docum ent and\nseek others like it \u2013 a feature available in the results lists of search engines\nas a more like this feature. We reduce the problem of \ufb01nding the document(s)\nmost similar to dto that of \ufb01nding the diwith the highest dot products (sim\nvalues) /vectorv(d)\u00b7/vectorv(di). We could do this by computing the dot products between\n/vectorv(d)and each of /vectorv(d1), . . . ,/vectorv(dN), then picking off the highest resulting sim\nvalues.\n\u270eExample 6.3: Figure 6.12 shows the number of occurrences of three terms ( affection ,\njealous andgossip ) in each of the following three novels: Jane Austen\u2019s Sense and Sensi-\nbility (SaS) and Pride and Prejudice (PaP) and Emily Bront\u00eb\u2019s Wuthering Heights (WH).\nOnline edition (c)\n2009 Cambridge UP6.3 The vector space model for scoring 123\nterm SaS PaP WH\naffection 0.996 0.993 0.847\njealous 0.087 0.120 0.466\ngossip 0.017 0 0.254\n\u25eeFigure 6.13 Term vectors for the three novels of Figure 6.12. These are based on\nraw term frequency only and are normalized as if these were th e only terms in the\ncollection. (Since affection andjealous occur in all three documents, their tf-idf weight\nwould be 0 in most formulations.)\nOf course, there are many other terms occurring in each of the se novels. In this ex-\nample we represent each of these novels as a unit vector in thr ee dimensions, corre-\nsponding to these three terms (only); we use raw term frequen cies here, with no idf\nmultiplier. The resulting weights are as shown in Figure 6.13.\nNow consider the cosine similarities between pairs of the re sulting three-dimensional\nvectors. A simple computation shows that sim( /vectorv(SAS), /vectorv(PAP)) is 0.999, whereas\nsim(/vectorv(SAS), /vectorv(WH)) is 0.888; thus, the two books authored by Austen (SaS an d PaP)\nare considerably closer to each other than to Bront\u00eb\u2019s Wuthering Heights . In fact, the\nsimilarity between the \ufb01rst two is almost perfect (when rest ricted to the three terms\nwe consider). Here we have considered tf weights, but we coul d of course use other\nterm weight functions.\nViewing a collection of Ndocuments as a collection of vectors leads to a\nnatural view of a collection as a term-document matrix : this is an M\u00d7Nmatrix TERM -DOCUMENT\nMATRIX whose rows represent the Mterms (dimensions) of the Ncolumns, each of\nwhich corresponds to a document. As always, the terms being i ndexed could\nbe stemmed before indexing; for instance, jealous andjealousy would under\nstemming be considered as a single dimension. This matrix vi ew will prove\nto be useful in Chapter 18.\n6.3.2 Queries as vectors\nThere is a far more compelling reason to represent documents as vectors:\nwe can also view a query as a vector. Consider the query q=jealous gossip .\nThis query turns into the unit vector /vectorv(q) = ( 0, 0.707, 0.707 )on the three\ncoordinates of Figures 6.12 and 6.13. The key idea now: to assign to each\ndocument da score equal to the dot product\n/vectorv(q)\u00b7/vectorv(d).\nIn the example of Figure 6.13,Wuthering Heights is the top-scoring docu-\nment for this query with a score of 0.509, with Pride and Prejudice a distant\nsecond with a score of 0.085, and Sense and Sensibility last with a score of\n0.074. This simple example is somewhat misleading: the numb er of dimen-\nOnline edition (c)\n2009 Cambridge UP124 6 Scoring, term weighting and the vector space model\nsions in practice will be far larger than three: it will equal the vocabulary size\nM.\nTo summarize, by viewing a query as a \u201cbag of words\u201d, we are abl e to\ntreat it as a very short document. As a consequence, we can use the cosine\nsimilarity between the query vector and a document vector as a measure of\nthe score of the document for that query. The resulting score s can then be\nused to select the top-scoring documents for a query. Thus we have\nscore(q,d) =/vectorV(q)\u00b7/vectorV(d)\n|/vectorV(q)||/vectorV(d)|. (6.12)\nA document may have a high cosine score for a query even if it do es not\ncontain all query terms. Note that the preceding discussion does not hinge\non any speci\ufb01c weighting of terms in the document vector, alt hough for the\npresent we may think of them as either tf or tf-idf weights. In fact, a number\nof weighting schemes are possible for query as well as docume nt vectors, as\nillustrated in Example 6.4and developed further in Section 6.4.\nComputing the cosine similarities between the query vector and each doc-\nument vector in the collection, sorting the resulting score s and selecting the\ntopKdocuments can be expensive \u2014 a single similarity computatio n can\nentail a dot product in tens of thousands of dimensions, dema nding tens of\nthousands of arithmetic operations. In Section 7.1we study how to use an in-\nverted index for this purpose, followed by a series of heuris tics for improving\non this.\n\u270eExample 6.4: We now consider the query bestcarinsurance on a \ufb01ctitious collection\nwith N=1,000,000 documents where the document frequencies of auto, best, car and\ninsurance are respectively 5000, 50000, 10000 and 1000.\nterm query document product\ntf df idf w t,qtf wf w t,d\nauto 0 5000 2.3 0 1 1 0.41 0\nbest 1 50000 1.3 1.3 0 0 0 0\ncar 1 10000 2.0 2.0 1 1 0.41 0.82\ninsurance 1 1000 3.0 3.0 2 2 0.82 2.46\nIn this example the weight of a term in the query is simply the i df (and zero for a\nterm not in the query, such as auto); this is re\ufb02ected in the column header w t,q(the en-\ntry forauto is zero because the query does not contain the term auto). For documents,\nwe use tf weighting with no use of idf but with Euclidean norma lization. The former\nis shown under the column headed wf, while the latter is shown under the column\nheaded w t,d. Invoking ( 6.9) now gives a net score of 0 +0+0.82+2.46=3.28.\n6.3.3 Computing vector scores\nIn a typical setting we have a collection of documents each re presented by a\nvector, a free text query represented by a vector, and a posit ive integer K. We\nOnline edition (c)\n2009 Cambridge UP6.3 The vector space model for scoring 125\nCOSINE SCORE(q)\n1 \ufb02oat Scores [N] =0\n2 Initialize Length [N]\n3for each query term t\n4docalculate w t,qand fetch postings list for t\n5 for each pair(d, tft,d)in postings list\n6 doScores [d] += wft,d\u00d7wt,q\n7 Read the array Length [d]\n8for each d\n9doScores [d] =Scores [d]/Length [d]\n10 return Top Kcomponents of Scores []\n\u25eeFigure 6.14 The basic algorithm for computing vector space scores.\nseek the Kdocuments of the collection with the highest vector space sc ores on\nthe given query. We now initiate the study of determining the Kdocuments\nwith the highest vector space scores for a query. Typically, we seek these\nKtop documents in ordered by decreasing score; for instance m any search\nengines use K=10 to retrieve and rank-order the \ufb01rst page of the ten best\nresults. Here we give the basic algorithm for this computati on; we develop a\nfuller treatment of ef\ufb01cient techniques and approximation s in Chapter 7.\nFigure 6.14 gives the basic algorithm for computing vector space scores .\nThe array Length holds the lengths (normalization factors) for each of the N\ndocuments, whereas the array Scores holds the scores for eac h of the docu-\nments. When the scores are \ufb01nally computed in Step 9, all that remains in\nStep 10 is to pick off the Kdocuments with the highest scores.\nThe outermost loop beginning Step 3 repeats the updating of S cores, iter-\nating over each query term tin turn. In Step 5 we calculate the weight in\nthe query vector for term t. Steps 6-8 update the score of each document by\nadding in the contribution from term t. This process of adding in contribu-\ntions one query term at a time is sometimes known as term-at-a-time scoring TERM -AT-A-TIME\nor accumulation, and the Nelements of the array Scores are therefore known\nasaccumulators . For this purpose, it would appear necessary to store, with ACCUMULATOR\neach postings entry, the weight wf t,dof term tin document d(we have thus\nfar used either tf or tf-idf for this weight, but leave open th e possibility of\nother functions to be developed in Section 6.4). In fact this is wasteful, since\nstoring this weight may require a \ufb02oating point number. Two i deas help alle-\nviate this space problem. First, if we are using inverse docu ment frequency,\nwe need not precompute idf t; it suf\ufb01ces to store N/df tat the head of the\npostings for t. Second, we store the term frequency tf t,dfor each postings en-\ntry. Finally, Step 12 extracts the top Kscores \u2013 this requires a priority queue\nOnline edition (c)\n2009 Cambridge UP126 6 Scoring, term weighting and the vector space model\ndata structure, often implemented using a heap. Such a heap t akes no more\nthan 2 Ncomparisons to construct, following which each of the Ktop scores\ncan be extracted from the heap at a cost of O(logN)comparisons.\nNote that the general algorithm of Figure 6.14 does not prescribe a speci\ufb01c\nimplementation of how we traverse the postings lists of the v arious query\nterms; we may traverse them one term at a time as in the loop beg inning\nat Step 3, or we could in fact traverse them concurrently as in Figure 1.6. In\nsuch a concurrent postings traversal we compute the scores o f one document\nat a time, so that it is sometimes called document-at-a-time scoring. We will DOCUMENT -AT-A-TIME\nsay more about this in Section 7.1.5 .\n?Exercise 6.14\nIf we were to stem jealous andjealousy to a common stem before setting up the vector\nspace, detail how the de\ufb01nitions of tf and idf should be modi\ufb01 ed.\nExercise 6.15\nRecall the tf-idf weights computed in Exercise 6.10. Compute the Euclidean nor-\nmalized document vectors for each of the documents, where ea ch vector has four\ncomponents, one for each of the four terms.\nExercise 6.16\nVerify that the sum of the squares of the components of each of the document vectors\nin Exercise 6.15 is 1 (to within rounding error). Why is this the case?\nExercise 6.17\nWith term weights as computed in Exercise 6.15, rank the three documents by com-\nputed score for the query carinsurance , for each of the following cases of term weight-\ning in the query:\n1.The weight of a term is 1 if present in the query, 0 otherwise.\n2.Euclidean normalized idf.\n6.4 Variant tf-idf functions\nFor assigning a weight for each term in each document, a numbe r of alterna-\ntives to tf and tf-idf have been considered. We discuss some o f the principal\nones here; a more complete development is deferred to Chapte r11. We will\nsummarize these alternatives in Section 6.4.3 (page 128).\n6.4.1 Sublinear tf scaling\nIt seems unlikely that twenty occurrences of a term in a docum ent truly carry\ntwenty times the signi\ufb01cance of a single occurrence. Accord ingly, there has\nbeen considerable research into variants of term frequency that go beyond\ncounting the number of occurrences of a term. A common modi\ufb01c ation is\nOnline edition (c)\n2009 Cambridge UP6.4 Variant tf-idf functions 127\nto use instead the logarithm of the term frequency, which ass igns a weight\ngiven by\nwft,d=/braceleftbigg\n1+log tf t,dif tf t,d>0\n0 otherwise. (6.13)\nIn this form, we may replace tf by some other function wf as in ( 6.13), to\nobtain:\nwf-idf t,d=wft,d\u00d7idft. (6.14)\nEquation ( 6.9) can then be modi\ufb01ed by replacing tf-idf by wf-idf as de\ufb01ned\nin (6.14).\n6.4.2 Maximum tf normalization\nOne well-studied technique is to normalize the tf weights of all terms occur-\nring in a document by the maximum tf in that document. For each document\nd, let tf max(d) = max \u03c4\u2208dtf\u03c4,d, where \u03c4ranges over all terms in d. Then, we\ncompute a normalized term frequency for each term tin document dby\nntft,d=a+ (1\u2212a)tft,d\ntfmax(d), (6.15)\nwhere ais a value between 0 and 1 and is generally set to 0.4, although some\nearly work used the value 0.5. The term ain (6.15) is a smoothing term whose SMOOTHING\nrole is to damp the contribution of the second term \u2013 which may be viewed as\na scaling down of tf by the largest tf value in d. We will encounter smoothing\nfurther in Chapter 13when discussing classi\ufb01cation; the basic idea is to avoid\na large swing in ntf t,dfrom modest changes in tf t,d(say from 1 to 2). The main\nidea of maximum tf normalization is to mitigate the followin g anomaly: we\nobserve higher term frequencies in longer documents, merel y because longer\ndocuments tend to repeat the same words over and over again. T o appreciate\nthis, consider the following extreme example: supposed we w ere to take a\ndocument dand create a new document d\u2032by simply appending a copy of d\nto itself. While d\u2032should be no more relevant to any query than dis, the use\nof (6.9) would assign it twice as high a score as d. Replacing tf-idf t,din (6.9) by\nntf-idf t,deliminates the anomaly in this example. Maximum tf normaliz ation\ndoes suffer from the following issues:\n1.The method is unstable in the following sense: a change in the stop word\nlist can dramatically alter term weightings (and therefore ranking). Thus,\nit is hard to tune.\n2.A document may contain an outlier term with an unusually larg e num-\nber of occurrences of that term, not representative of the co ntent of that\ndocument.\nOnline edition (c)\n2009 Cambridge UP128 6 Scoring, term weighting and the vector space model\nTerm frequency Document frequency Normalization\nn (natural) tf t,d n (no) 1 n (none) 1\nl (logarithm) 1 +log(tft,d) t (idf) logN\ndftc (cosine)1\u221a\nw2\n1+w2\n2+...+w2\nM\na (augmented) 0.5 +0.5\u00d7tft,d\nmax t(tft,d)p (prob idf) max {0, logN\u2212dft\ndft}u (pivoted\nunique)1/u(Section 6.4.4 )\nb (boolean)/braceleftbigg1 if tf t,d>0\n0 otherwiseb (byte size) 1/ CharLength\u03b1,\u03b1<1\nL (log ave)1+log(tft,d)\n1+log(avet\u2208d(tft,d))\n\u25eeFigure 6.15 SMART notation for tf-idf variants. Here CharLength is the number\nof characters in the document.\n3.More generally, a document in which the most frequent term ap pears\nroughly as often as many other terms should be treated differ ently from\none with a more skewed distribution.\n6.4.3 Document and query weighting schemes\nEquation ( 6.12) is fundamental to information retrieval systems that use a ny\nform of vector space scoring. Variations from one vector spa ce scoring method\nto another hinge on the speci\ufb01c choices of weights in the vect ors/vectorV(d)and\n/vectorV(q). Figure 6.15 lists some of the principal weighting schemes in use for\neach of /vectorV(d)and/vectorV(q), together with a mnemonic for representing a spe-\nci\ufb01c combination of weights; this system of mnemonics is som etimes called\nSMART notation, following the authors of an early text retri eval system. The\nmnemonic for representing a combination of weights takes th e form ddd.qqq\nwhere the \ufb01rst triplet gives the term weighting of the docume nt vector, while\nthe second triplet gives the weighting in the query vector. T he \ufb01rst letter in\neach triplet speci\ufb01es the term frequency component of the we ighting, the\nsecond the document frequency component, and the third the f orm of nor-\nmalization used. It is quite common to apply different norma lization func-\ntions to /vectorV(d)and/vectorV(q). For example, a very standard weighting scheme\nislnc.ltc , where the document vector has log-weighted term frequency , no\nidf (for both effectiveness and ef\ufb01ciency reasons), and cos ine normalization,\nwhile the query vector uses log-weighted term frequency, id f weighting, and\ncosine normalization.\nOnline edition (c)\n2009 Cambridge UP6.4 Variant tf-idf functions 129\n\u27046.4.4 Pivoted normalized document length\nIn Section 6.3.1 we normalized each document vector by the Euclidean length\nof the vector, so that all document vectors turned into unit v ectors. In doing\nso, we eliminated all information on the length of the origin al document;\nthis masks some subtleties about longer documents. First, l onger documents\nwill \u2013 as a result of containing more terms \u2013 have higher tf val ues. Second,\nlonger documents contain more distinct terms. These factor s can conspire to\nraise the scores of longer documents, which (at least for som e information\nneeds) is unnatural. Longer documents can broadly be lumped into two cat-\negories: (1) verbose documents that essentially repeat the same content \u2013 in\nthese, the length of the document does not alter the relative weights of dif-\nferent terms; (2) documents covering multiple different to pics, in which the\nsearch terms probably match small segments of the document b ut not all of\nit \u2013 in this case, the relative weights of terms are quite diff erent from a single\nshort document that matches the query terms. Compensating f or this phe-\nnomenon is a form of document length normalization that is in dependent of\nterm and document frequencies. To this end, we introduce a fo rm of normal-\nizing the vector representations of documents in the collec tion, so that the\nresulting \u201cnormalized\u201d documents are not necessarily of un it length. Then,\nwhen we compute the dot product score between a (unit) query v ector and\nsuch a normalized document, the score is skewed to account fo r the effect\nof document length on relevance. This form of compensation f or document\nlength is known as pivoted document length normalization . PIVOTED DOCUMENT\nLENGTH\nNORMALIZATIONConsider a document collection together with an ensemble of queries for\nthat collection. Suppose that we were given, for each query qand for each\ndocument d, a Boolean judgment of whether or not dis relevant to the query\nq; in Chapter 8we will see how to procure such a set of relevance judgments\nfor a query ensemble and a document collection. Given this se t of relevance\njudgments, we may compute a probability of relevance as a function of docu-\nment length, averaged over all queries in the ensemble. The r esulting plot\nmay look like the curve drawn in thick lines in Figure 6.16. To compute this\ncurve, we bucket documents by length and compute the fractio n of relevant\ndocuments in each bucket, then plot this fraction against th e median docu-\nment length of each bucket. (Thus even though the \u201ccurve\u201d in F igure 6.16\nappears to be continuous, it is in fact a histogram of discret e buckets of doc-\nument length.)\nOn the other hand, the curve in thin lines shows what might hap pen with\nthe same documents and query ensemble if we were to use releva nce as pre-\nscribed by cosine normalization Equation ( 6.12) \u2013 thus, cosine normalization\nhas a tendency to distort the computed relevance vis-\u00e0-vis t he true relevance,\nat the expense of longer documents. The thin and thick curves crossover at a\npoint pcorresponding to document length \u2113p, which we refer to as the pivot\nOnline edition (c)\n2009 Cambridge UP130 6 Scoring, term weighting and the vector space model\nDocument lengthRelevance\n\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\n\b\b\b\b\b\b\b\b\n\u2113pp\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\b\b\b\b\b\b\b\b\n-6\n\u25eeFigure 6.16 Pivoted document length normalization.\nlength ; dashed lines mark this point on the x\u2212and y\u2212axes. The idea of\npivoted document length normalization would then be to \u201crot ate\u201d the co-\nsine normalization curve counter-clockwise about pso that it more closely\nmatches thick line representing the relevance vs. document length curve.\nAs mentioned at the beginning of this section, we do so by usin g in Equa-\ntion ( 6.12) a normalization factor for each document vector /vectorV(d)that is not\nthe Euclidean length of that vector, but instead one that is l arger than the Eu-\nclidean length for documents of length less than \u2113p, and smaller for longer\ndocuments.\nTo this end, we \ufb01rst note that the normalizing term for /vectorV(d)in the de-\nnominator of Equation ( 6.12) is its Euclidean length, denoted |/vectorV(d)|. In the\nsimplest implementation of pivoted document length normal ization, we use\na normalization factor in the denominator that is linear in |/vectorV(d)|, but one\nof slope <1 as in Figure 6.17. In this \ufb01gure, the x\u2212axis represents|/vectorV(d)|,\nwhile the y\u2212axis represents possible normalization factors we can use. The\nthin line y=xdepicts the use of cosine normalization. Notice the followi ng\naspects of the thick line representing pivoted length norma lization:\n1.It is linear in the document length and has the form\na|/vectorV(d)|+ (1\u2212a)piv, (6.16)\nOnline edition (c)\n2009 Cambridge UP6.4 Variant tf-idf functions 131\n|/vectorV(d)|Pivoted normalization\ny=x; Cosine\nPivoted\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\npiv\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\u0011\n-6\n\u25eeFigure 6.17 Implementing pivoted document length normalization by lin ear scal-\ning.\nwhere piv is the cosine normalization value at which the two c urves in-\ntersect.\n2.Its slope is a<1 and (3) it crosses the y=xline at piv.\nIt has been argued that in practice, Equation ( 6.16) is well approximated by\naud+ (1\u2212a)piv,\nwhere udis the number of unique terms in document d.\nOf course, pivoted document length normalization is not app ropriate for\nall applications. For instance, in a collection of answers t o frequently asked\nquestions (say, at a customer service website), relevance m ay have little to\ndo with document length. In other cases the dependency may be more com-\nplex than can be accounted for by a simple linear pivoted norm alization. In\nsuch cases, document length can be used as a feature in the mac hine learning\nbased scoring approach of Section 6.1.2 .\n?Exercise 6.18\nOne measure of the similarity of two vectors is the Euclidean distance (orL2distance) EUCLIDEAN DISTANCE\nbetween them:\n|/vectorx\u2212/vectory|=/radicaltp/radicalvertex/radicalvertex/radicalbtM\n\u2211\ni=1(xi\u2212yi)2\nOnline edition (c)\n2009 Cambridge UP132 6 Scoring, term weighting and the vector space model\nquery document\nword tf wf df idf qi=wf-idf tf wf di=normalized wf qi\u00b7di\ndigital 10,000\nvideo 100,000\ncameras 50,000\n\u25eeTable 6.1 Cosine computation for Exercise 6.19.\nGiven a query qand documents d1,d2, . . ., we may rank the documents diin order\nof increasing Euclidean distance from q. Show that if qand the diare all normalized\nto unit vectors, then the rank ordering produced by Euclidea n distance is identical to\nthat produced by cosine similarities.\nExercise 6.19\nCompute the vector space similarity between the query \u201cdigi tal cameras\u201d and the\ndocument \u201cdigital cameras and video cameras\u201d by \ufb01lling out t he empty columns in\nTable 6.1. Assume N=10,000,000, logarithmic term weighting (wf columns) for\nquery and document, idf weighting for the query only and cosi ne normalization for\nthe document only. Treat andas a stop word. Enter term counts in the tf columns.\nWhat is the \ufb01nal similarity score?\nExercise 6.20\nShow that for the query affection , the relative ordering of the scores of the three doc-\numents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous\ngossip .\nExercise 6.21\nIn turning a query into a unit vector in Figure 6.13, we assigned equal weights to each\nof the query terms. What other principled approaches are pla usible?\nExercise 6.22\nConsider the case of a query term that is not in the set of Mindexed terms; thus our\nstandard construction of the query vector results in /vectorV(q)not being in the vector space\ncreated from the collection. How would one adapt the vector s pace representation to\nhandle this case?\nExercise 6.23\nRefer to the tf and idf values for four terms and three documen ts in Exercise 6.10.\nCompute the two top scoring documents on the query best car insurance for each of\nthe following weighing schemes: (i) nnn.atc ; (ii)ntc.atc .\nExercise 6.24\nSuppose that the word coyote does not occur in the collection used in Exercises 6.10\nand 6.23. How would one compute ntc.atc scores for the query coyote insurance ?\nOnline edition (c)\n2009 Cambridge UP6.5 References and further reading 133\n6.5 References and further reading\nChapter 7develops the computational aspects of vector space scoring .Luhn\n(1957 ;1958 ) describes some of the earliest reported applications of te rm weight-\ning. His paper dwells on the importance of medium frequency t erms (terms\nthat are neither too commonplace nor too rare) and may be thou ght of as an-\nticipating tf-idf and related weighting schemes. Sp\u00e4rck Jones (1972 ) builds\non this intuition through detailed experiments showing the use of inverse\ndocument frequency in term weighting. A series of extension s and theoret-\nical justi\ufb01cations of idf are due to Salton and Buckley (1987 )Robertson and\nJones (1976 ),Croft and Harper (1979 ) and Papineni (2001 ). Robertson main-\ntains a web page ( http://www.soi.city.ac.uk/ \u02dcser/idf.html ) containing the history\nof idf, including soft copies of early papers that predated e lectronic versions\nof journal article. Singhal et al. (1996a ) develop pivoted document length\nnormalization. Probabilistic language models (Chapter 11) develop weight-\ning techniques that are more nuanced than tf-idf; the reader will \ufb01nd this\ndevelopment in Section 11.4.3 .\nWe observed that by assigning a weight for each term in a docum ent, a\ndocument may be viewed as a vector of term weights, one for eac h term in\nthe collection. The SMART information retrieval system at C ornell ( Salton\n1971b ) due to Salton and colleagues was perhaps the \ufb01rst to view a do c-\nument as a vector of weights. The basic computation of cosine scores as\ndescribed in Section 6.3.3 is due to Zobel and Moffat (2006 ). The two query\nevaluation strategies term-at-a-time and document-at-a- time are discussed\nbyTurtle and Flood (1995 ).\nThe SMART notation for tf-idf term weighting schemes in Figu re6.15 is\npresented in ( Salton and Buckley 1988 ,Singhal et al. 1995 ;1996b ). Not all\nversions of the notation are consistent; we most closely fol low ( Singhal et al.\n1996b ). A more detailed and exhaustive notation was developed in Moffat\nand Zobel (1998 ), considering a larger palette of schemes for term and doc-\nument frequency weighting. Beyond the notation, Moffat and Zobel (1998 )\nsought to set up a space of feasible weighting functions thro ugh which hill-\nclimbing approaches could be used to begin with weighting sc hemes that\nperformed well, then make local improvements to identify th e best combi-\nnations. However, they report that such hill-climbing meth ods failed to lead\nto any conclusions on the best weighting schemes.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 135\n7Computing scores in a complete\nsearch system\nChapter 6developed the theory underlying term weighting in document s\nfor the purposes of scoring, leading up to vector space model s and the basic\ncosine scoring algorithm of Section 6.3.3 (page 124). In this chapter we be-\ngin in Section 7.1with heuristics for speeding up this computation; many of\nthese heuristics achieve their speed at the risk of not \ufb01ndin g quite the top K\ndocuments matching the query. Some of these heuristics gene ralize beyond\ncosine scoring. With Section 7.1in place, we have essentially all the compo-\nnents needed for a complete search engine. We therefore take a step back\nfrom cosine scoring, to the more general problem of computin g scores in a\nsearch engine. In Section 7.2we outline a complete search engine, includ-\ning indexes and structures to support not only cosine scorin g but also more\ngeneral ranking factors such as query term proximity. We des cribe how all\nof the various pieces \ufb01t together in Section 7.2.4 . We conclude this chapter\nwith Section 7.3, where we discuss how the vector space model for free text\nqueries interacts with common query operators.\n7.1 Ef\ufb01cient scoring and ranking\nWe begin by recapping the algorithm of Figure 6.14. For a query such as q=\njealous gossip , two observations are immediate:\n1.The unit vector /vectorv(q)has only two non-zero components.\n2.In the absence of any weighting for query terms, these non-ze ro compo-\nnents are equal \u2013 in this case, both equal 0.707.\nFor the purpose of ranking the documents matching this query , we are\nreally interested in the relative (rather than absolute) sc ores of the documents\nin the collection. To this end, it suf\ufb01ces to compute the cosi ne similarity from\neach document unit vector /vectorv(d)to/vectorV(q)(in which all non-zero components\nof the query vector are set to 1), rather than to the unit vecto r/vectorv(q). For any\nOnline edition (c)\n2009 Cambridge UP136 7 Computing scores in a complete search system\nFASTCOSINE SCORE(q)\n1 \ufb02oat Scores [N] =0\n2for each d\n3doInitialize Length [d]to the length of doc d\n4for each query term t\n5docalculate w t,qand fetch postings list for t\n6 for each pair(d, tft,d)in postings list\n7 doadd wf t,dtoScores [d]\n8 Read the array Length [d]\n9for each d\n10 doDivide Scores [d]byLength [d]\n11 return Top Kcomponents of Scores []\n\u25eeFigure 7.1 A faster algorithm for vector space scores.\ntwo documents d1,d2\n/vectorV(q)\u00b7/vectorv(d1)>/vectorV(q)\u00b7/vectorv(d2)\u21d4/vectorv(q)\u00b7/vectorv(d1)> /vectorv(q)\u00b7/vectorv(d2). (7.1)\nFor any document d, the cosine similarity /vectorV(q)\u00b7/vectorv(d)is the weighted sum,\nover all terms in the query q, of the weights of those terms in d. This in turn\ncan be computed by a postings intersection exactly as in the a lgorithm of\nFigure 6.14, with line 8 altered since we take wt,qto be 1 so that the multiply-\nadd in that step becomes just an addition; the result is shown in Figure 7.1.\nWe walk through the postings in the inverted index for the ter ms in q, accu-\nmulating the total score for each document \u2013 very much as in pr ocessing a\nBoolean query, except we assign a positive score to each docu ment that ap-\npears in any of the postings being traversed. As mentioned in Section 6.3.3\nwe maintain an idf value for each dictionary term and a tf valu e for each\npostings entry. This scheme computes a score for every docum ent in the\npostings of any of the query terms; the total number of such do cuments may\nbe considerably smaller than N.\nGiven these scores, the \ufb01nal step before presenting results to a user is to\npick out the Khighest-scoring documents. While one could sort the comple te\nset of scores, a better approach is to use a heap to retrieve on ly the top K\ndocuments in order. Where Jis the number of documents with non-zero\ncosine scores, constructing such a heap can be performed in 2 Jcomparison\nsteps, following which each of the Khighest scoring documents can be \u201cread\noff\u201d the heap with log Jcomparison steps.\nOnline edition (c)\n2009 Cambridge UP7.1 Ef\ufb01cient scoring and ranking 137\n7.1.1 Inexact top Kdocument retrieval\nThus far, we have focused on retrieving precisely the Khighest-scoring doc-\numents for a query. We now consider schemes by which we produc eKdoc-\numents that are likely to be among the Khighest scoring documents for a\nquery. In doing so, we hope to dramatically lower the cost of c omputing\ntheKdocuments we output, without materially altering the user\u2019 s perceived\nrelevance of the top Kresults. Consequently, in most applications it suf\ufb01ces\nto retrieve Kdocuments whose scores are very close to those of the Kbest.\nIn the sections that follow we detail schemes that retrieve Ksuch documents\nwhile potentially avoiding computing scores for most of the Ndocuments in\nthe collection.\nSuch inexact top- Kretrieval is not necessarily, from the user\u2019s perspective,\na bad thing. The top Kdocuments by the cosine measure are in any case not\nnecessarily the Kbest for the query: cosine similarity is only a proxy for the\nuser\u2019s perceived relevance. In Sections 7.1.2 \u20137.1.6 below, we give heuristics\nusing which we are likely to retrieve Kdocuments with cosine scores close\nto those of the top Kdocuments. The principal cost in computing the out-\nput stems from computing cosine similarities between the qu ery and a large\nnumber of documents. Having a large number of documents in co ntention\nalso increases the selection cost in the \ufb01nal stage of cullin g the top Kdocu-\nments from a heap. We now consider a series of ideas designed t o eliminate\na large number of documents without computing their cosine s cores. The\nheuristics have the following two-step scheme:\n1.Find a set Aof documents that are contenders, where K<|A|\u226a N.A\ndoes not necessarily contain the Ktop-scoring documents for the query,\nbut is likely to have many documents with scores near those of the top K.\n2.Return the Ktop-scoring documents in A.\nFrom the descriptions of these ideas it will be clear that man y of them require\nparameters to be tuned to the collection and application at h and; pointers\nto experience in setting these parameters may be found at the end of this\nchapter. It should also be noted that most of these heuristic s are well-suited\nto free text queries, but not for Boolean or phrase queries.\n7.1.2 Index elimination\nFor a multi-term query q, it is clear we only consider documents containing at\nleast one of the query terms. We can take this a step further us ing additional\nheuristics:\n1.We only consider documents containing terms whose idf excee ds a preset\nthreshold. Thus, in the postings traversal, we only travers e the postings\nOnline edition (c)\n2009 Cambridge UP138 7 Computing scores in a complete search system\nfor terms with high idf. This has a fairly signi\ufb01cant bene\ufb01t: the post-\nings lists of low-idf terms are generally long; with these re moved from\ncontention, the set of documents for which we compute cosine s is greatly\nreduced. One way of viewing this heuristic: low-idf terms ar e treated as\nstop words and do not contribute to scoring. For instance, on the query\ncatcher in the rye , we only traverse the postings for catcher andrye. The\ncutoff threshold can of course be adapted in a query-depende nt manner.\n2.We only consider documents that contain many (and as a specia l case,\nall) of the query terms. This can be accomplished during the p ostings\ntraversal; we only compute scores for documents containing all (or many)\nof the query terms. A danger of this scheme is that by requirin g all (or\neven many) query terms to be present in a document before cons idering\nit for cosine computation, we may end up with fewer than Kcandidate\ndocuments in the output. This issue will discussed further i n Section 7.2.1 .\n7.1.3 Champion lists\nThe idea of champion lists (sometimes also called fancy lists ortop docs ) is to\nprecompute, for each term tin the dictionary, the set of the rdocuments\nwith the highest weights for t; the value of ris chosen in advance. For tf-\nidf weighting, these would be the rdocuments with the highest tf values for\nterm t. We call this set of rdocuments the champion list for term t.\nNow, given a query qwe create a set Aas follows: we take the union\nof the champion lists for each of the terms comprising q. We now restrict\ncosine computation to only the documents in A. A critical parameter in this\nscheme is the value r, which is highly application dependent. Intuitively, r\nshould be large compared with K, especially if we use any form of the index\nelimination described in Section 7.1.2 . One issue here is that the value ris set\nat the time of index construction, whereas Kis application dependent and\nmay not be available until the query is received; as a result w e may (as in the\ncase of index elimination) \ufb01nd ourselves with a set Athat has fewer than K\ndocuments. There is no reason to have the same value of rfor all terms in the\ndictionary; it could for instance be set to be higher for rare r terms.\n7.1.4 Static quality scores and ordering\nWe now further develop the idea of champion lists, in the some what more\ngeneral setting of static quality scores . In many search engines, we have avail- STATIC QUALITY\nSCORES able a measure of quality g(d)for each document dthat is query-independent\nand thus static . This quality measure may be viewed as a number between\nzero and one. For instance, in the context of news stories on t he web, g(d)\nmay be derived from the number of favorable reviews of the sto ry by web\nOnline edition (c)\n2009 Cambridge UP7.1 Ef\ufb01cient scoring and ranking 139\n\u25eeFigure 7.2 A static quality-ordered index. In this example we assume th at Doc1,\nDoc2 and Doc3 respectively have static quality scores g(1) =0.25, g(2) =0.5,g(3) =\n1.\nsurfers. Section 4.6(page 80) provides further discussion on this topic, as\ndoes Chapter 21in the context of web search.\nThe net score for a document dis some combination of g(d)together with\nthe query-dependent score induced (say) by ( 6.12). The precise combination\nmay be determined by the learning methods of Section 6.1.2 , to be developed\nfurther in Section 15.4.1 ; but for the purposes of our exposition here, let us\nconsider a simple sum:\nnet-score (q,d) =g(d) +/vectorV(q)\u00b7/vectorV(d)\n|/vectorV(q)||/vectorV(d)|. (7.2)\nIn this simple form, the static quality g(d)and the query-dependent score\nfrom ( 6.10) have equal contributions, assuming each is between 0 and 1.\nOther relative weightings are possible; the effectiveness of our heuristics will\ndepend on the speci\ufb01c relative weighting.\nFirst, consider ordering the documents in the postings list for each term by\ndecreasing value of g(d). This allows us to perform the postings intersection\nalgorithm of Figure 1.6. In order to perform the intersection by a single pass\nthrough the postings of each query term, the algorithm of Fig ure1.6relied on\nthe postings being ordered by document IDs. But in fact, we on ly required\nthat all postings be ordered by a single common ordering; her e we rely on the\ng(d)values to provide this common ordering. This is illustrated in Figure 7.2,\nwhere the postings are ordered in decreasing order of g(d).\nThe \ufb01rst idea is a direct extension of champion lists: for a we ll-chosen\nvalue r, we maintain for each term taglobal champion list of the rdocuments\nOnline edition (c)\n2009 Cambridge UP140 7 Computing scores in a complete search system\nwith the highest values for g(d) +tf-idf t,d. The list itself is, like all the post-\nings lists considered so far, sorted by a common order (eithe r by document\nIDs or by static quality). Then at query time, we only compute the net scores\n(7.2) for documents in the union of these global champion lists. I ntuitively,\nthis has the effect of focusing on documents likely to have la rge net scores.\nWe conclude the discussion of global champion lists with one further idea.\nWe maintain for each term ttwo postings lists consisting of disjoint sets of\ndocuments, each sorted by g(d)values. The \ufb01rst list, which we call high,\ncontains the mdocuments with the highest tf values for t. The second list,\nwhich we call low, contains all other documents containing t. When process-\ning a query, we \ufb01rst scan only the high lists of the query terms , computing\nnet scores for any document on the high lists of all (or more th an a certain\nnumber of) query terms. If we obtain scores for Kdocuments in the process,\nwe terminate. If not, we continue the scanning into the low li sts, scoring doc-\numents in these postings lists. This idea is developed furth er in Section 7.2.1 .\n7.1.5 Impact ordering\nIn all the postings lists described thus far, we order the doc uments con-\nsistently by some common ordering: typically by document ID but in Sec-\ntion 7.1.4 by static quality scores. As noted at the end of Section 6.3.3 , such a\ncommon ordering supports the concurrent traversal of all of the query terms\u2019\npostings lists, computing the score for each document as we e ncounter it.\nComputing scores in this manner is sometimes referred to as d ocument-at-a-\ntime scoring. We will now introduce a technique for inexact t op-Kretrieval\nin which the postings are not all ordered by a common ordering , thereby\nprecluding such a concurrent traversal. We will therefore r equire scores to\nbe \u201caccumulated\u201d one term at a time as in the scheme of Figure 6.14, so that\nwe have term-at-a-time scoring.\nThe idea is to order the documents din the postings list of term tby\ndecreasing order of tf t,d. Thus, the ordering of documents will vary from\none postings list to another, and we cannot compute scores by a concurrent\ntraversal of the postings lists of all query terms. Given pos tings lists ordered\nby decreasing order of tf t,d, two ideas have been found to signi\ufb01cantly lower\nthe number of documents for which we accumulate scores: (1) w hen travers-\ning the postings list for a query term t, we stop after considering a pre\ufb01x\nof the postings list \u2013 either after a \ufb01xed number of documents rhave been\nseen, or after the value of tf t,dhas dropped below a threshold; (2) when ac-\ncumulating scores in the outer loop of Figure 6.14, we consider the query\nterms in decreasing order of idf, so that the query terms like ly to contribute\nthe most to the \ufb01nal scores are considered \ufb01rst. This latter i dea too can be\nadaptive at the time of processing a query: as we get to query t erms with\nlower idf, we can determine whether to proceed based on the ch anges in\nOnline edition (c)\n2009 Cambridge UP7.1 Ef\ufb01cient scoring and ranking 141\ndocument scores from processing the previous query term. If these changes\nare minimal, we may omit accumulation from the remaining que ry terms, or\nalternatively process shorter pre\ufb01xes of their postings li sts.\nThese ideas form a common generalization of the methods intr oduced in\nSections 7.1.2 \u20137.1.4 . We may also implement a version of static ordering in\nwhich each postings list is ordered by an additive combinati on of static and\nquery-dependent scores. We would again lose the consistenc y of ordering\nacross postings, thereby having to process query terms one a t time accumu-\nlating scores for all documents as we go along. Depending on t he particular\nscoring function, the postings list for a document may be ord ered by other\nquantities than term frequency; under this more general set ting, this idea is\nknown as impact ordering.\n7.1.6 Cluster pruning\nIncluster pruning we have a preprocessing step during which we cluster the\ndocument vectors. Then at query time, we consider only docum ents in a\nsmall number of clusters as candidates for which we compute c osine scores.\nSpeci\ufb01cally, the preprocessing step is as follows:\n1.Pick\u221a\nNdocuments at random from the collection. Call these leaders .\n2.For each document that is not a leader, we compute its nearest leader.\nWe refer to documents that are not leaders as followers . Intuitively, in the par-\ntition of the followers induced by the use of\u221a\nNrandomly chosen leaders,\nthe expected number of followers for each leader is \u2248N/\u221a\nN=\u221a\nN. Next,\nquery processing proceeds as follows:\n1.Given a query q, \ufb01nd the leader Lthat is closest to q. This entails comput-\ning cosine similarities from qto each of the\u221a\nNleaders.\n2.The candidate set Aconsists of Ltogether with its followers. We compute\nthe cosine scores for all documents in this candidate set.\nThe use of randomly chosen leaders for clustering is fast and likely to re-\n\ufb02ect the distribution of the document vectors in the vector s pace: a region\nof the vector space that is dense in documents is likely to pro duce multi-\nple leaders and thus a \ufb01ner partition into sub-regions. This illustrated in\nFigure 7.3.\nVariations of cluster pruning introduce additional parame ters b1and b2,\nboth of which are positive integers. In the pre-processing s tep we attach\neach follower to its b1closest leaders, rather than a single closest leader. At\nquery time we consider the b2leaders closest to the query q. Clearly, the basic\nscheme above corresponds to the case b1=b2=1. Further, increasing b1or\nOnline edition (c)\n2009 Cambridge UP142 7 Computing scores in a complete search system\n\u25eeFigure 7.3 Cluster pruning.\nb2increases the likelihood of \ufb01nding Kdocuments that are more likely to be\nin the set of true top-scoring Kdocuments, at the expense of more compu-\ntation. We reiterate this approach when describing cluster ing in Chapter 16\n(page 354).\n?Exercise 7.1\nWe suggested above (Figure 7.2) that the postings for static quality ordering be in\ndecreasing order of g(d). Why do we use the decreasing rather than the increasing\norder?\nExercise 7.2\nWhen discussing champion lists, we simply used the rdocuments with the largest tf\nvalues to create the champion list for t. But when considering global champion lists,\nwe used idf as well, identifying documents with the largest v alues of g(d) +tf-idf t,d.\nWhy do we differentiate between these two cases?\nExercise 7.3\nIf we were to only have one-term queries, explain why the use o f global champion\nlists with r=Ksuf\ufb01ces for identifying the Khighest scoring documents. What is a\nsimple modi\ufb01cation to this idea if we were to only have s-term queries for any \ufb01xed\ninteger s>1?\nExercise 7.4\nExplain how the common global ordering by g(d)values in all high and low lists\nhelps make the score computation ef\ufb01cient.\nOnline edition (c)\n2009 Cambridge UP7.2 Components of an information retrieval system 143\nExercise 7.5\nConsider again the data of Exercise 6.23 withnnn.atc for the query-dependent scor-\ning. Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2.\nDetermine under Equation ( 7.2) what ranges of static quality score for Doc3 result in\nit being the \ufb01rst, second or third result for the query best car insurance .\nExercise 7.6\nSketch the frequency-ordered postings for the data in Figur e6.9.\nExercise 7.7\nLet the static quality scores for Doc1, Doc2 and Doc3 in Figur e6.11 be respectively\n0.25, 0.5 and 1. Sketch the postings for impact ordering when each postings list is\nordered by the sum of the static quality score and the Euclide an normalized tf values\nin Figure 6.11.\nExercise 7.8\nThe nearest-neighbor problem in the plane is the following: given a set of Ndata\npoints on the plane, we preprocess them into some data struct ure such that, given\na query point Q, we seek the point in Nthat is closest to Qin Euclidean distance.\nClearly cluster pruning can be used as an approach to the near est-neighbor problem\nin the plane, if we wished to avoid computing the distance fro mQto every one of\nthe query points. Devise a simple example on the plane so that with two leaders, the\nanswer returned by cluster pruning is incorrect (it is not th e data point closest to Q).\n7.2 Components of an information retrieval system\nIn this section we combine the ideas developed so far to descr ibe a rudimen-\ntary search system that retrieves and scores documents. We \ufb01 rst develop\nfurther ideas for scoring, beyond vector spaces. Following this, we will put\ntogether all of these elements to outline a complete system. Because we con-\nsider a complete system, we do not restrict ourselves to vect or space retrieval\nin this section. Indeed, our complete system will have provi sions for vector\nspace as well as other query operators and forms of retrieval . In Section 7.3\nwe will return to how vector space queries interact with othe r query opera-\ntors.\n7.2.1 Tiered indexes\nWe mentioned in Section 7.1.2 that when using heuristics such as index elim-\nination for inexact top- Kretrieval, we may occasionally \ufb01nd ourselves with\na set Aof contenders that has fewer than Kdocuments. A common solution\nto this issue is the user of tiered indexes , which may be viewed as a gener- TIERED INDEXES\nalization of champion lists. We illustrate this idea in Figu re7.4, where we\nrepresent the documents and terms of Figure 6.9. In this example we set a tf\nthreshold of 20 for tier 1 and 10 for tier 2, meaning that the ti er 1 index only\nhas postings entries with tf values exceeding 20, while the t ier 2 index only\nOnline edition (c)\n2009 Cambridge UP144 7 Computing scores in a complete search system\n\u25eeFigure 7.4 Tiered indexes. If we fail to get Kresults from tier 1, query processing\n\u201cfalls back\u201d to tier 2, and so on. Within each tier, postings a re ordered by document\nID.\nhas postings entries with tf values exceeding 10. In this exa mple we have\nchosen to order the postings entries within a tier by documen t ID.\n7.2.2 Query-term proximity\nEspecially for free text queries on the web (Chapter 19), users prefer a doc-\nument in which most or all of the query terms appear close to ea ch other,\nOnline edition (c)\n2009 Cambridge UP7.2 Components of an information retrieval system 145\nbecause this is evidence that the document has text focused o n their query\nintent. Consider a query with two or more query terms, t1,t2, . . . , tk. Let \u03c9\nbe the width of the smallest window in a document dthat contains all the\nquery terms, measured in the number of words in the window. Fo r instance,\nif the document were to simply consist of the sentence The quality of mercy\nis not strained , the smallest window for the query strained mercy would be 4.\nIntuitively, the smaller that \u03c9is, the better that dmatches the query. In cases\nwhere the document does not contain all of the query terms, we can set \u03c9\nto be some enormous number. We could also consider variants i n which\nonly words that are not stop words are considered in computin g\u03c9. Such\nproximity-weighted scoring functions are a departure from pure cosine sim-\nilarity and closer to the \u201csoft conjunctive\u201d semantics that Google and other\nweb search engines evidently use.\nHow can we design such a proximity-weighted scoring function to depend PROXIMITY WEIGHTING\non\u03c9? The simplest answer relies on a \u201chand coding\u201d technique we i ntroduce\nbelow in Section 7.2.3 . A more scalable approach goes back to Section 6.1.2 \u2013\nwe treat the integer \u03c9as yet another feature in the scoring function, whose\nimportance is assigned by machine learning, as will be devel oped further in\nSection 15.4.1 .\n7.2.3 Designing parsing and scoring functions\nCommon search interfaces, particularly for consumer-faci ng search applica-\ntions on the web, tend to mask query operators from the end use r. The intent\nis to hide the complexity of these operators from the largely non-technical au-\ndience for such applications, inviting free text queries . Given such interfaces,\nhow should a search equipped with indexes for various retrie val operators\ntreat a query such as risinginterestrates ? More generally, given the various fac-\ntors we have studied that could affect the score of a document , how should\nwe combine these features?\nThe answer of course depends on the user population, the quer y distri-\nbution and the collection of documents. Typically, a query parser is used to\ntranslate the user-speci\ufb01ed keywords into a query with vari ous operators\nthat is executed against the underlying indexes. Sometimes , this execution\ncan entail multiple queries against the underlying indexes ; for example, the\nquery parser may issue a stream of queries:\n1.Run the user-generated query string as a phrase query. Rank t hem by\nvector space scoring using as query the vector consisting of the 3 terms\nrising interestrates .\n2.If fewer than ten documents contain the phrase rising interestrates , run the\ntwo 2-term phrase queries rising interest andinterest rates ; rank these using\nvector space scoring, as well.\nOnline edition (c)\n2009 Cambridge UP146 7 Computing scores in a complete search system\n3.If we still have fewer than ten results, run the vector space q uery consist-\ning of the three individual query terms.\nEach of these steps (if invoked) may yield a list of scored doc uments, for\neach of which we compute a score. This score must combine cont ributions\nfrom vector space scoring, static quality, proximity weigh ting and potentially\nother factors \u2013 particularly since a document may appear in t he lists from\nmultiple steps. This demands an aggregate scoring function that accumulates EVIDENCE\nACCUMULATION evidence of a document\u2019s relevance from multiple sources. How do we de vise\na query parser and how do we devise the aggregate scoring func tion?\nThe answer depends on the setting. In many enterprise settin gs we have\napplication builders who make use of a toolkit of available s coring opera-\ntors, along with a query parsing layer, with which to manuall y con\ufb01gure\nthe scoring function as well as the query parser. Such applic ation builders\nmake use of the available zones, metadata and knowledge of ty pical doc-\numents and queries to tune the parsing and scoring. In collec tions whose\ncharacteristics change infrequently (in an enterprise app lication, signi\ufb01cant\nchanges in collection and query characteristics typically happen with infre-\nquent events such as the introduction of new document format s or document\nmanagement systems, or a merger with another company). Web s earch on\nthe other hand is faced with a constantly changing document c ollection with\nnew characteristics being introduced all the time. It is als o a setting in which\nthe number of scoring factors can run into the hundreds, maki ng hand-tuned\nscoring a dif\ufb01cult exercise. To address this, it is becoming increasingly com-\nmon to use machine-learned scoring, extending the ideas we i ntroduced in\nSection 6.1.2 , as will be discussed further in Section 15.4.1 .\n7.2.4 Putting it all together\nWe have now studied all the components necessary for a basic s earch system\nthat supports free text queries as well as Boolean, zone and \ufb01 eld queries. We\nbrie\ufb02y review how the various pieces \ufb01t together into an over all system; this\nis depicted in Figure 7.5.\nIn this \ufb01gure, documents stream in from the left for parsing a nd linguis-\ntic processing (language and format detection, tokenizati on and stemming).\nThe resulting stream of tokens feeds into two modules. First , we retain a\ncopy of each parsed document in a document cache. This will en able us\nto generate results snippets: snippets of text accompanyin g each document\nin the results list for a query. This snippet tries to give a su ccinct explana-\ntion to the user of why the document matches the query. The aut omatic\ngeneration of such snippets is the subject of Section 8.7. A second copy\nof the tokens is fed to a bank of indexers that create a bank of i ndexes in-\ncluding zone and \ufb01eld indexes that store the metadata for eac h document,\nOnline edition (c)\n2009 Cambridge UP7.3 Vector space scoring and query operator interaction 147\n\u25eeFigure 7.5 A complete search system. Data paths are shown primarily for a free\ntext query.\n(tiered) positional indexes, indexes for spelling correct ion and other tolerant\nretrieval, and structures for accelerating inexact top- Kretrieval. A free text\nuser query (top center) is sent down to the indexes both direc tly and through\na module for generating spelling-correction candidates. A s noted in Chap-\nter3the latter may optionally be invoked only when the original q uery fails\nto retrieve enough results. Retrieved documents (dark arro w) are passed\nto a scoring module that computes scores based on machine-le arned rank-\ning (MLR), a technique that builds on Section 6.1.2 (to be further developed\nin Section 15.4.1 ) for scoring and ranking documents. Finally, these ranked\ndocuments are rendered as a results page.\n?Exercise 7.9\nExplain how the postings intersection algorithm \ufb01rst intro duced in Section 1.3can be\nadapted to \ufb01nd the smallest integer \u03c9that contains all query terms.\nExercise 7.10\nAdapt this procedure to work when not all query terms are pres ent in a document.\n7.3 Vector space scoring and query operator interaction\nWe introduced the vector space model as a paradigm for free te xt queries.\nWe conclude this chapter by discussing how the vector space s coring model\nOnline edition (c)\n2009 Cambridge UP148 7 Computing scores in a complete search system\nrelates to the query operators we have studied in earlier cha pters. The re-\nlationship should be viewed at two levels: in terms of the exp ressiveness\nof queries that a sophisticated user may pose, and in terms of the index that\nsupports the evaluation of the various retrieval methods. I n building a search\nengine, we may opt to support multiple query operators for an end user. In\ndoing so we need to understand what components of the index ca n be shared\nfor executing various query operators, as well as how to hand le user queries\nthat mix various query operators.\nVector space scoring supports so-called free text retrieva l, in which a query\nis speci\ufb01ed as a set of words without any query operators conn ecting them. It\nallows documents matching the query to be scored and thus ran ked, unlike\nthe Boolean, wildcard and phrase queries studied earlier. C lassically, the\ninterpretation of such free text queries was that at least on e of the query terms\nbe present in any retrieved document. However more recently , web search\nengines such as Google have popularized the notion that a set of terms typed\ninto their query boxes (thus on the face of it, a free text quer y) carries the\nsemantics of a conjunctive query that only retrieves docume nts containing\nall or most query terms.\nBoolean retrieval\nClearly a vector space index can be used to answer Boolean que ries, as long\nas the weight of a term tin the document vector for dis non-zero when-\never toccurs in d. The reverse is not true, since a Boolean index does not by\ndefault maintain term weight information. There is no easy w ay of combin-\ning vector space and Boolean queries from a user\u2019s standpoin t: vector space\nqueries are fundamentally a form of evidence accumulation , where the pres-\nence of more query terms in a document adds to the score of a doc ument.\nBoolean retrieval on the other hand, requires a user to speci fy a formula\nforselecting documents through the presence (or absence) of speci\ufb01c com-\nbinations of keywords, without inducing any relative order ing among them.\nMathematically, it is in fact possible to invoke so-called p-norms to combine\nBoolean and vector space queries, but we know of no system tha t makes use\nof this fact.\nWildcard queries\nWildcard and vector space queries require different indexe s, except at the\nbasic level that both can be implemented using postings and a dictionary\n(e.g., a dictionary of trigrams for wildcard queries). If a s earch engine allows\na user to specify a wildcard operator as part of a free text que ry (for instance,\nthe query rom* restaurant ), we may interpret the wildcard component of the\nquery as spawning multiple terms in the vector space (in this example, rome\nOnline edition (c)\n2009 Cambridge UP7.4 References and further reading 149\nandroman would be two such terms) all of which are added to the query\nvector. The vector space query is then executed as usual, wit h matching\ndocuments being scored and ranked; thus a document containi ng bothrome\nandroma is likely to be scored higher than another containing only on e of\nthem. The exact score ordering will of course depend on the re lative weights\nof each term in matching documents.\nPhrase queries\nThe representation of documents as vectors is fundamentall y lossy: the rel-\native order of terms in a document is lost in the encoding of a d ocument as\na vector. Even if we were to try and somehow treat every biword as a term\n(and thus an axis in the vector space), the weights on differe nt axes not in-\ndependent: for instance the phrase German shepherd gets encoded in the axis\ngerman shepherd , but immediately has a non-zero weight on the axes german\nandshepherd . Further, notions such as idf would have to be extended to suc h\nbiwords. Thus an index built for vector space retrieval cann ot, in general, be\nused for phrase queries. Moreover, there is no way of demandi ng a vector\nspace score for a phrase query \u2014 we only know the relative weig hts of each\nterm in a document.\nOn the query germanshepherd , we could use vector space retrieval to iden-\ntify documents heavy in these two terms, with no way of prescr ibing that\nthey occur consecutively. Phrase retrieval, on the other ha nd, tells us of the\nexistence of the phrase german shepherd in a document, without any indi-\ncation of the relative frequency or weight of this phrase. Wh ile these two\nretrieval paradigms (phrase and vector space) consequentl y have different\nimplementations in terms of indexes and retrieval algorith ms, they can in\nsome cases be combined usefully, as in the three-step exampl e of query pars-\ning in Section 7.2.3 .\n7.4 References and further reading\nHeuristics for fast query processing with early terminatio n are described by\nAnh et al. (2001 ),Garcia et al. (2004 ),Anh and Moffat (2006b ),Persin et al.\n(1996 ). Cluster pruning is investigated by Singitham et al. (2004 ) and by\nChierichetti et al. (2007 ); see also Section 16.6 (page 372). Champion lists are\ndescribed in Persin (1994 ) and (under the name top docs ) in Brown (1995 ), TOP DOCS\nand further developed in Brin and Page (1998 ),Long and Suel (2003 ). While\nthese heuristics are well-suited to free text queries that c an be viewed as vec-\ntors, they complicate phrase queries; see Anh and Moffat (2006c ) for an index\nstructure that supports both weighted and Boolean/phrase s earches. Carmel\net al. (2001 )Clarke et al. (2000 ) and Song et al. (2005 ) treat the use of query\nOnline edition (c)\n2009 Cambridge UP150 7 Computing scores in a complete search system\nterm proximity in assessing relevance. Pioneering work on l earning of rank-\ning functions was done by Fuhr (1989 ),Fuhr and Pfeifer (1994 ),Cooper et al.\n(1994 ),Bartell (1994 ),Bartell et al. (1998 ) and by Cohen et al. (1998 ).\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 151\n8Evaluation in information\nretrieval\nWe have seen in the preceding chapters many alternatives in d esigning an IR\nsystem. How do we know which of these techniques are effectiv e in which\napplications? Should we use stop lists? Should we stem? Shou ld we use in-\nverse document frequency weighting? Information retrieva l has developed\nas a highly empirical discipline, requiring careful and tho rough evaluation to\ndemonstrate the superior performance of novel techniques o n representative\ndocument collections.\nIn this chapter we begin with a discussion of measuring the ef fectiveness\nof IR systems (Section 8.1) and the test collections that are most often used\nfor this purpose (Section 8.2). We then present the straightforward notion of\nrelevant and nonrelevant documents and the formal evaluati on methodol-\nogy that has been developed for evaluating unranked retriev al results (Sec-\ntion 8.3). This includes explaining the kinds of evaluation measure s that\nare standardly used for document retrieval and related task s like text clas-\nsi\ufb01cation and why they are appropriate. We then extend these notions and\ndevelop further measures for evaluating ranked retrieval r esults (Section 8.4)\nand discuss developing reliable and informative test colle ctions (Section 8.5).\nWe then step back to introduce the notion of user utility, and how it is ap-\nproximated by the use of document relevance (Section 8.6). The key utility\nmeasure is user happiness. Speed of response and the size of t he index are\nfactors in user happiness. It seems reasonable to assume tha t relevance of\nresults is the most important factor: blindingly fast, usel ess answers do not\nmake a user happy. However, user perceptions do not always co incide with\nsystem designers\u2019 notions of quality. For example, user hap piness commonly\ndepends very strongly on user interface design issues, incl uding the layout,\nclarity, and responsiveness of the user interface, which ar e independent of\nthe quality of the results returned. We touch on other measur es of the qual-\nity of a system, in particular the generation of high-qualit y result summary\nsnippets, which strongly in\ufb02uence user utility, but are not measured in the\nbasic relevance ranking paradigm (Section 8.7).\nOnline edition (c)\n2009 Cambridge UP152 8 Evaluation in information retrieval\n8.1 Information retrieval system evaluation\nTo measure ad hoc information retrieval effectiveness in th e standard way,\nwe need a test collection consisting of three things:\n1.A document collection\n2.A test suite of information needs, expressible as queries\n3.A set of relevance judgments, standardly a binary assessmen t of either\nrelevant ornonrelevant for each query-document pair.\nThe standard approach to information retrieval system eval uation revolves\naround the notion of relevant and nonrelevant documents. With respect to a RELEVANCE\nuser information need, a document in the test collection is g iven a binary\nclassi\ufb01cation as either relevant or nonrelevant. This deci sion is referred to as\nthegold standard orground truth judgment of relevance. The test document GOLD STANDARD\nGROUND TRUTH collection and suite of information needs have to be of a reas onable size:\nyou need to average performance over fairly large test sets, as results are\nhighly variable over different documents and information n eeds. As a rule\nof thumb, 50 information needs has usually been found to be a s uf\ufb01cient\nminimum.\nRelevance is assessed relative to an information need, nota query. For INFORMATION NEED\nexample, an information need might be:\nInformation on whether drinking red wine is more effective a t reduc-\ning your risk of heart attacks than white wine.\nThis might be translated into a query such as:\nwine ANDred ANDwhite ANDheart ANDattack ANDeffective\nA document is relevant if it addresses the stated informatio n need, not be-\ncause it just happens to contain all the words in the query. Th is distinction is\noften misunderstood in practice, because the information n eed is not overt.\nBut, nevertheless, an information need is present. If a user typespython into a\nweb search engine, they might be wanting to know where they ca n purchase\na pet python. Or they might be wanting information on the prog ramming\nlanguage Python. From a one word query, it is very dif\ufb01cult fo r a system to\nknow what the information need is. But, nevertheless, the us er has one, and\ncan judge the returned results on the basis of their relevanc e to it. To evalu-\nate a system, we require an overt expression of an informatio n need, which\ncan be used for judging returned documents as relevant or non relevant. At\nthis point, we make a simpli\ufb01cation: relevance can reasonab ly be thought\nof as a scale, with some documents highly relevant and others marginally\nso. But for the moment, we will use just a binary decision of re levance. We\nOnline edition (c)\n2009 Cambridge UP8.2 Standard test collections 153\ndiscuss the reasons for using binary relevance judgments an d alternatives in\nSection 8.5.1 .\nMany systems contain various weights (often known as parame ters) that\ncan be adjusted to tune system performance. It is wrong to rep ort results on\na test collection which were obtained by tuning these parame ters to maxi-\nmize performance on that collection. That is because such tu ning overstates\nthe expected performance of the system, because the weights will be set to\nmaximize performance on one particular set of queries rathe r than for a ran-\ndom sample of queries. In such cases, the correct procedure i s to have one\nor more development test collections , and to tune the parameters on the devel- DEVELOPMENT TEST\nCOLLECTION opment test collection. The tester then runs the system with those weights\non the test collection and reports the results on that collec tion as an unbiased\nestimate of performance.\n8.2 Standard test collections\nHere is a list of the most standard test collections and evalu ation series. We\nfocus particularly on test collections for ad hoc informati on retrieval system\nevaluation, but also mention a couple of similar test collec tions for text clas-\nsi\ufb01cation.\nThe Cran\ufb01eld collection. This was the pioneering test collection in allo wing CRANFIELD\nprecise quantitative measures of information retrieval ef fectiveness, but\nis nowadays too small for anything but the most elementary pi lot experi-\nments. Collected in the United Kingdom starting in the late 1 950s, it con-\ntains 1398 abstracts of aerodynamics journal articles, a se t of 225 queries,\nand exhaustive relevance judgments of all (query, document ) pairs.\nText Retrieval Conference (TREC) . The U.S. National Institute of Standards TREC\nand Technology (NIST) has run a large IR test bed evaluation s eries since\n1992. Within this framework, there have been many tracks ove r a range\nof different test collections, but the best known test colle ctions are the\nones used for the TREC Ad Hoc track during the \ufb01rst 8 TREC evalu ations\nbetween 1992 and 1999. In total, these test collections comp rise 6 CDs\ncontaining 1.89 million documents (mainly, but not exclusi vely, newswire\narticles) and relevance judgments for 450 information need s, which are\ncalled topics and speci\ufb01ed in detailed text passages. Individual test col -\nlections are de\ufb01ned over different subsets of this data. The early TRECs\neach consisted of 50 information needs, evaluated over diff erent but over-\nlapping sets of documents. TRECs 6\u20138 provide 150 informatio n needs\nover about 528,000 newswire and Foreign Broadcast Informat ion Service\narticles. This is probably the best subcollection to use in f uture work, be-\ncause it is the largest and the topics are more consistent. Be cause the test\nOnline edition (c)\n2009 Cambridge UP154 8 Evaluation in information retrieval\ndocument collections are so large, there are no exhaustive r elevance judg-\nments. Rather, NIST assessors\u2019 relevance judgments are ava ilable only for\nthe documents that were among the top kreturned for some system which\nwas entered in the TREC evaluation for which the information need was\ndeveloped.\nIn more recent years, NIST has done evaluations on larger doc ument col-\nlections, including the 25 million page GOV2 web page collection. From GOV2\nthe beginning, the NIST test document collections were orde rs of magni-\ntude larger than anything available to researchers previou sly and GOV2\nis now the largest Web collection easily available for resea rch purposes.\nNevertheless, the size of GOV2 is still more than 2 orders of m agnitude\nsmaller than the current size of the document collections in dexed by the\nlarge web search companies.\nNII Test Collections for IR Systems ( NTCIR ). The NTCIR project has built NTCIR\nvarious test collections of similar sizes to the TREC collec tions, focus-\ning on East Asian language and cross-language information retrieval , where CROSS -LANGUAGE\nINFORMATION\nRETRIEVALqueries are made in one language over a document collection c ontaining\ndocuments in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-\nen.html\nCross Language Evaluation Forum ( CLEF ). This evaluation series has con- CLEF\ncentrated on European languages and cross-language inform ation retrieval.\nSee:http://www.clef-campaign.org/\nReuters-21578 and Reuters-RCV1. For text classi\ufb01cation, t he most used test REUTERS\ncollection has been the Reuters-21578 collection of 21578 n ewswire arti-\ncles; see Chapter 13, page 279. More recently, Reuters released the much\nlarger Reuters Corpus Volume 1 (RCV1), consisting of 806,79 1 documents;\nsee Chapter 4, page 69. Its scale and rich annotation makes it a better basis\nfor future research.\n20 Newsgroups . This is another widely used text classi\ufb01cation collection , 20 N EWSGROUPS\ncollected by Ken Lang. It consists of 1000 articles from each of 20 Usenet\nnewsgroups (the newsgroup name being regarded as the catego ry). After\nthe removal of duplicate articles, as it is usually used, it c ontains 18941\narticles.\n8.3 Evaluation of unranked retrieval sets\nGiven these ingredients, how is system effectiveness measu red? The two\nmost frequent and basic measures for information retrieval effectiveness are\nprecision and recall. These are \ufb01rst de\ufb01ned for the simple ca se where an\nOnline edition (c)\n2009 Cambridge UP8.3 Evaluation of unranked retrieval sets 155\nIR system returns a set of documents for a query. We will see la ter how to\nextend these notions to ranked retrieval situations.\nPrecision (P) is the fraction of retrieved documents that are relevant PRECISION\nPrecision =#(relevant items retrieved )\n#(retrieved items )=P(relevant|retrieved ) (8.1)\nRecall (R) is the fraction of relevant documents that are retrieved RECALL\nRecall =#(relevant items retrieved )\n#(relevant items )=P(retrieved|relevant ) (8.2)\nThese notions can be made clear by examining the following co ntingency\ntable:\n(8.3)\nRelevant Nonrelevant\nRetrieved true positives (tp) false positives (fp)\nNot retrieved false negatives (fn) true negatives (tn)\nThen:\nP=tp/(tp+f p) (8.4)\nR=tp/(tp+f n)\nAn obvious alternative that may occur to the reader is to judg e an infor-\nmation retrieval system by its accuracy , that is, the fraction of its classi\ufb01ca- ACCURACY\ntions that are correct. In terms of the contingency table abo ve, accuracy =\n(tp+tn)/(tp+f p+f n+tn). This seems plausible, since there are two ac-\ntual classes, relevant and nonrelevant, and an information retrieval system\ncan be thought of as a two-class classi\ufb01er which attempts to l abel them as\nsuch (it retrieves the subset of documents which it believes to be relevant).\nThis is precisely the effectiveness measure often used for e valuating machine\nlearning classi\ufb01cation problems.\nThere is a good reason why accuracy is not an appropriate meas ure for\ninformation retrieval problems. In almost all circumstanc es, the data is ex-\ntremely skewed: normally over 99.9% of the documents are in t he nonrele-\nvant category. A system tuned to maximize accuracy can appea r to perform\nwell by simply deeming all documents nonrelevant to all quer ies. Even if the\nsystem is quite good, trying to label some documents as relev ant will almost\nalways lead to a high rate of false positives. However, label ing all documents\nas nonrelevant is completely unsatisfying to an informatio n retrieval system\nuser. Users are always going to want to see some documents, an d can be\nOnline edition (c)\n2009 Cambridge UP156 8 Evaluation in information retrieval\nassumed to have a certain tolerance for seeing some false pos itives provid-\ning that they get some useful information. The measures of pr ecision and\nrecall concentrate the evaluation on the return of true posi tives, asking what\npercentage of the relevant documents have been found and how many false\npositives have also been returned.\nThe advantage of having the two numbers for precision and rec all is that\none is more important than the other in many circumstances. T ypical web\nsurfers would like every result on the \ufb01rst page to be relevan t (high preci-\nsion) but have not the slightest interest in knowing let alon e looking at every\ndocument that is relevant. In contrast, various profession al searchers such as\nparalegals and intelligence analysts are very concerned wi th trying to get as\nhigh recall as possible, and will tolerate fairly low precis ion results in order to\nget it. Individuals searching their hard disks are also ofte n interested in high\nrecall searches. Nevertheless, the two quantities clearly trade off against one\nanother: you can always get a recall of 1 (but very low precisi on) by retriev-\ning all documents for all queries! Recall is a non-decreasin g function of the\nnumber of documents retrieved. On the other hand, in a good sy stem, preci-\nsion usually decreases as the number of documents retrieved is increased. In\ngeneral we want to get some amount of recall while tolerating only a certain\npercentage of false positives.\nA single measure that trades off precision versus recall is t heF measure , FMEASURE\nwhich is the weighted harmonic mean of precision and recall:\nF=1\n\u03b11\nP+ (1\u2212\u03b1)1\nR=(\u03b22+1)PR\n\u03b22P+Rwhere \u03b22=1\u2212\u03b1\n\u03b1(8.5)\nwhere \u03b1\u2208[0, 1]and thus \u03b22\u2208[0,\u221e]. The default balanced F measure equally\nweights precision and recall, which means making \u03b1=1/2 or \u03b2=1. It is\ncommonly written as F1, which is short for F\u03b2=1, even though the formula-\ntion in terms of \u03b1more transparently exhibits the F measure as a weighted\nharmonic mean. When using \u03b2=1, the formula on the right simpli\ufb01es to:\nF\u03b2=1=2PR\nP+R(8.6)\nHowever, using an even weighting is not the only choice. Valu es of \u03b2<1\nemphasize precision, while values of \u03b2>1 emphasize recall. For example, a\nvalue of \u03b2=3 or \u03b2=5 might be used if recall is to be emphasized. Recall,\nprecision, and the F measure are inherently measures betwee n 0 and 1, but\nthey are also very commonly written as percentages, on a scal e between 0\nand 100.\nWhy do we use a harmonic mean rather than the simpler average ( arith-\nmetic mean)? Recall that we can always get 100% recall by just returning all\ndocuments, and therefore we can always get a 50% arithmetic m ean by the\nOnline edition (c)\n2009 Cambridge UP8.3 Evaluation of unranked retrieval sets 1570\n2 0\n4 0\n6 0\n8 0\n1 0 00 2 0 4 0 6 0 8 0 1 0 0P r e c i s i o n ( R e c a l l f i x e d a t 7 0 % )\nM i n i m u mM a x i m u mA r i t h m e t i cGe o m e t r i cH a r m o n i c\n\u25eeFigure 8.1 Graph comparing the harmonic mean to other means. The graph\nshows a slice through the calculation of various means of pre cision and recall for\nthe \ufb01xed recall value of 70%. The harmonic mean is always less than either the arith-\nmetic or geometric mean, and often quite close to the minimum of the two numbers.\nWhen the precision is also 70%, all the measures coincide.\nsame process. This strongly suggests that the arithmetic me an is an unsuit-\nable measure to use. In contrast, if we assume that 1 document in 10,000 is\nrelevant to the query, the harmonic mean score of this strate gy is 0.02%. The\nharmonic mean is always less than or equal to the arithmetic m ean and the\ngeometric mean. When the values of two numbers differ greatl y, the har-\nmonic mean is closer to their minimum than to their arithmeti c mean; see\nFigure 8.1.\n?Exercise 8.1 [\u22c6]\nAn IR system returns 8 relevant documents, and 10 nonrelevan t documents. There\nare a total of 20 relevant documents in the collection. What i s the precision of the\nsystem on this search, and what is its recall?\nExercise 8.2 [\u22c6]\nThe balanced F measure (a.k.a. F 1) is de\ufb01ned as the harmonic mean of precision and\nrecall. What is the advantage of using the harmonic mean rath er than \u201caveraging\u201d\n(using the arithmetic mean)?\nOnline edition (c)\n2009 Cambridge UP158 8 Evaluation in information retrieval\n0.00.20.40.60.81.0\n0.0 0.2 0.4 0.6 0.8 1.0\nRecallPrecision \n\u25eeFigure 8.2 Precision/recall graph.\nExercise 8.3 [\u22c6\u22c6]\nDerive the equivalence between the two formulas for F measur e shown in Equa-\ntion ( 8.5), given that \u03b1=1/(\u03b22+1).\n8.4 Evaluation of ranked retrieval results\nPrecision, recall, and the F measure are set-based measures . They are com-\nputed using unordered sets of documents. We need to extend th ese measures\n(or to de\ufb01ne new measures) if we are to evaluate the ranked ret rieval results\nthat are now standard with search engines. In a ranked retrie val context,\nappropriate sets of retrieved documents are naturally give n by the top kre-\ntrieved documents. For each such set, precision and recall v alues can be\nplotted to give a precision-recall curve , such as the one shown in Figure 8.2. PRECISION -RECALL\nCURVE Precision-recall curves have a distinctive saw-tooth shap e: if the (k+1)th\ndocument retrieved is nonrelevant then recall is the same as for the top k\ndocuments, but precision has dropped. If it is relevant, the n both precision\nand recall increase, and the curve jags up and to the right. It is often useful to\nremove these jiggles and the standard way to do this is with an interpolated\nprecision: the interpolated precision p interp at a certain recall level ris de\ufb01ned INTERPOLATED\nPRECISION\nOnline edition (c)\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 159\nRecall Interp.\nPrecision\n0.0 1.00\n0.1 0.67\n0.2 0.63\n0.3 0.55\n0.4 0.45\n0.5 0.41\n0.6 0.36\n0.7 0.29\n0.8 0.13\n0.9 0.10\n1.0 0.08\n\u25eeTable 8.1 Calculation of 11-point Interpolated Average Precision. T his is for the\nprecision-recall curve shown in Figure 8.2.\nas the highest precision found for any recall level r\u2032\u2265r:\npinterp(r) =max\nr\u2032\u2265rp(r\u2032) (8.7)\nThe justi\ufb01cation is that almost anyone would be prepared to l ook at a few\nmore documents if it would increase the percentage of the vie wed set that\nwere relevant (that is, if the precision of the larger set is h igher). Interpolated\nprecision is shown by a thinner line in Figure 8.2. With this de\ufb01nition, the\ninterpolated precision at a recall of 0 is well-de\ufb01ned (Exer cise8.4).\nExamining the entire precision-recall curve is very inform ative, but there\nis often a desire to boil this information down to a few number s, or perhaps\neven a single number. The traditional way of doing this (used for instance\nin the \ufb01rst 8 TREC Ad Hoc evaluations) is the 11-point interpolated average 11- POINT\nINTERPOLATED\nAVERAGE PRECISIONprecision . For each information need, the interpolated precision is m easured\nat the 11 recall levels of 0.0, 0.1, 0.2, . . . , 1.0. For the prec ision-recall curve in\nFigure 8.2, these 11 values are shown in Table 8.1. For each recall level, we\nthen calculate the arithmetic mean of the interpolated prec ision at that recall\nlevel for each information need in the test collection. A com posite precision-\nrecall curve showing 11 points can then be graphed. Figure 8.3shows an\nexample graph of such results from a representative good sys tem at TREC 8.\nIn recent years, other measures have become more common. Mos t stan-\ndard among the TREC community is Mean Average Precision (MAP), which MEAN AVERAGE\nPRECISION provides a single-\ufb01gure measure of quality across recall le vels. Among eval-\nuation measures, MAP has been shown to have especially good d iscrimina-\ntion and stability. For a single information need, Average P recision is the\nOnline edition (c)\n2009 Cambridge UP160 8 Evaluation in information retrieval\n00.20.40.60.81\n0 0.2 0.4 0.6 0.8 1\nRecallPrecision \n\u25eeFigure 8.3 Averaged 11-point precision/recall graph across 50 querie s for a rep-\nresentative TREC system. The Mean Average Precision for thi s system is 0.2553.\naverage of the precision value obtained for the set of top kdocuments exist-\ning after each relevant document is retrieved, and this valu e is then averaged\nover information needs. That is, if the set of relevant docum ents for an in-\nformation need qj\u2208Qis{d1, . . .dmj}and Rjkis the set of ranked retrieval\nresults from the top result until you get to document dk, then\nMAP(Q) =1\n|Q||Q|\n\u2211\nj=11\nmjmj\n\u2211\nk=1Precision (Rjk) (8.8)\nWhen a relevant document is not retrieved at all,1the precision value in the\nabove equation is taken to be 0. For a single information need , the average\nprecision approximates the area under the uninterpolated p recision-recall\ncurve, and so the MAP is roughly the average area under the pre cision-recall\ncurve for a set of queries.\nUsing MAP , \ufb01xed recall levels are not chosen, and there is no i nterpola-\ntion. The MAP value for a test collection is the arithmetic me an of average\n1. A system may not fully order all documents in the collectio n in response to a query or at\nany rate an evaluation exercise may be based on submitting on ly the top kresults for each\ninformation need.\nOnline edition (c)\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 161\nprecision values for individual information needs. (This h as the effect of\nweighting each information need equally in the \ufb01nal reporte d number, even\nif many documents are relevant to some queries whereas very f ew are rele-\nvant to other queries.) Calculated MAP scores normally vary widely across\ninformation needs when measured within a single system, for instance, be-\ntween 0.1 and 0.7. Indeed, there is normally more agreement i n MAP for\nan individual information need across systems than for MAP s cores for dif-\nferent information needs for the same system. This means tha t a set of test\ninformation needs must be large and diverse enough to be repr esentative of\nsystem effectiveness across different queries.\nThe above measures factor in precision at all recall levels. For many promi- PRECISION AT k\nnent applications, particularly web search, this may not be germane to users.\nWhat matters is rather how many good results there are on the \ufb01 rst page or\nthe \ufb01rst three pages. This leads to measuring precision at \ufb01x ed low levels of\nretrieved results, such as 10 or 30 documents. This is referr ed to as \u201cPrecision\natk\u201d, for example \u201cPrecision at 10\u201d. It has the advantage of not r equiring any\nestimate of the size of the set of relevant documents but the d isadvantages\nthat it is the least stable of the commonly used evaluation me asures and that\nit does not average well, since the total number of relevant d ocuments for a\nquery has a strong in\ufb02uence on precision at k.\nAn alternative, which alleviates this problem, is R-precision . It requires R-PRECISION\nhaving a set of known relevant documents Rel, from which we calculate the\nprecision of the top Reldocuments returned. (The set Relmay be incomplete,\nsuch as when Relis formed by creating relevance judgments for the pooled\ntopkresults of particular systems in a set of experiments.) R-pr ecision ad-\njusts for the size of the set of relevant documents: A perfect system could\nscore 1 on this metric for each query, whereas, even a perfect system could\nonly achieve a precision at 20 of 0.4 if there were only 8 docum ents in the\ncollection relevant to an information need. Averaging this measure across\nqueries thus makes more sense. This measure is harder to expl ain to naive\nusers than Precision at kbut easier to explain than MAP. If there are |Rel|\nrelevant documents for a query, we examine the top |Rel|results of a sys-\ntem, and \ufb01nd that rare relevant, then by de\ufb01nition, not only is the precision\n(and hence R-precision) r/|Rel|, but the recall of this result set is also r/|Rel|.\nThus, R-precision turns out to be identical to the break-even point , another BREAK -EVEN POINT\nmeasure which is sometimes used, de\ufb01ned in terms of this equa lity relation-\nship holding. Like Precision at k, R-precision describes only one point on\nthe precision-recall curve, rather than attempting to summ arize effectiveness\nacross the curve, and it is somewhat unclear why you should be interested\nin the break-even point rather than either the best point on t he curve (the\npoint with maximal F-measure) or a retrieval level of intere st to a particular\napplication (Precision at k). Nevertheless, R-precision turns out to be highly\ncorrelated with MAP empirically, despite measuring only a s ingle point on\nOnline edition (c)\n2009 Cambridge UP162 8 Evaluation in information retrieval\n0.00.20.40.60.81.0\n0 0.2 0.4 0.6 0.8 1\n1 \u2212 specificitysensitivity ( = recall) \n\u25eeFigure 8.4 The ROC curve corresponding to the precision-recall curve i n Fig-\nure8.2.\n.\nthe curve.\nAnother concept sometimes used in evaluation is an ROC curve . (\u201cROC\u201d ROC CURVE\nstands for \u201cReceiver Operating Characteristics\u201d, but know ing that doesn\u2019t\nhelp most people.) An ROC curve plots the true positive rate o r sensitiv-\nity against the false positive rate or (1 \u2212speci\ufb01city). Here, sensitivity is just SENSITIVITY\nanother term for recall. The false positive rate is given by f p/(f p+tn). Fig-\nure8.4shows the ROC curve corresponding to the precision-recall c urve in\nFigure 8.2. An ROC curve always goes from the bottom left to the top right of\nthe graph. For a good system, the graph climbs steeply on the l eft side. For\nunranked result sets, speci\ufb01city , given by tn/(f p+tn), was not seen as a very SPECIFICITY\nuseful notion. Because the set of true negatives is always so large, its value\nwould be almost 1 for all information needs (and, correspond ingly, the value\nof the false positive rate would be almost 0). That is, the \u201cin teresting\u201d part of\nFigure 8.2is 0<recall <0.4, a part which is compressed to a small corner\nof Figure 8.4. But an ROC curve could make sense when looking over the\nfull retrieval spectrum, and it provides another way of look ing at the data.\nIn many \ufb01elds, a common aggregate measure is to report the are a under the\nROC curve, which is the ROC analog of MAP. Precision-recall c urves are\nsometimes loosely referred to as ROC curves. This is underst andable, but\nnot accurate.\nA \ufb01nal approach that has seen increasing adoption, especial ly when em-\nployed with machine learning approaches to ranking (see Sec tion 15.4, page 341)\nis measures of cumulative gain , and in particular normalized discounted cumu- CUMULATIVE GAIN\nNORMALIZED\nDISCOUNTED\nCUMULATIVE GAIN\nOnline edition (c)\n2009 Cambridge UP8.4 Evaluation of ranked retrieval results 163\nlative gain (NDCG ). NDCG is designed for situations of non-binary notions NDCG\nof relevance (cf. Section 8.5.1 ). Like precision at k, it is evaluated over some\nnumber kof top search results. For a set of queries Q, let R(j,d)be the rele-\nvance score assessors gave to document dfor query j. Then,\nNDCG (Q,k) =1\n|Q||Q|\n\u2211\nj=1Zkjk\n\u2211\nm=12R(j,m)\u22121\nlog2(1+m), (8.9)\nwhere Zkjis a normalization factor calculated to make it so that a perf ect\nranking\u2019s NDCG at kfor query jis 1. For queries for which k\u2032<kdocuments\nare retrieved, the last summation is done up to k\u2032.\n?Exercise 8.4 [\u22c6]\nWhat are the possible values for interpolated precision at a recall level of 0?\nExercise 8.5 [\u22c6\u22c6]\nMust there always be a break-even point between precision an d recall? Either show\nthere must be or give a counter-example.\nExercise 8.6 [\u22c6\u22c6]\nWhat is the relationship between the value of F1and the break-even point?\nExercise 8.7 [\u22c6\u22c6]\nThe Dice coef\ufb01cient of two sets is a measure of their intersection scaled by their size DICE COEFFICIENT\n(giving a value in the range 0 to 1):\nDice(X,Y) =2|X\u2229Y|\n|X|+|Y|\nShow that the balanced F-measure ( F1) is equal to the Dice coef\ufb01cient of the retrieved\nand relevant document sets.\nExercise 8.8 [\u22c6]\nConsider an information need for which there are 4 relevant d ocuments in the collec-\ntion. Contrast two systems run on this collection. Their top 10 results are judged for\nrelevance as follows (the leftmost item is the top ranked sea rch result):\nSystem 1 R N R N N N N N R R\nSystem 2 N R N N R R R N N N\na.What is the MAP of each system? Which has a higher MAP?\nb.Does this result intuitively make sense? What does it say abo ut what is important\nin getting a good MAP score?\nc.What is the R-precision of each system? (Does it rank the syst ems the same as\nMAP?)\nOnline edition (c)\n2009 Cambridge UP164 8 Evaluation in information retrieval\nExercise 8.9 [\u22c6\u22c6]\nThe following list of Rs and Ns represents relevant (R) and no nrelevant (N) returned\ndocuments in a ranked list of 20 documents retrieved in respo nse to a query from a\ncollection of 10,000 documents. The top of the ranked list (t he document the system\nthinks is most likely to be relevant) is on the left of the list . This list shows 6 relevant\ndocuments. Assume that there are 8 relevant documents in tot al in the collection.\nR R N N N N N N R N R N N N R N N N N R\na.What is the precision of the system on the top 20?\nb.What is the F 1on the top 20?\nc.What is the uninterpolated precision of the system at 25% rec all?\nd.What is the interpolated precision at 33% recall?\ne.Assume that these 20 documents are the complete result set of the system. What\nis the MAP for the query?\nAssume, now, instead, that the system returned the entire 10 ,000 documents in a\nranked list, and these are the \ufb01rst 20 results returned.\nf.What is the largest possible MAP that this system could have?\ng.What is the smallest possible MAP that this system could have ?\nh.In a set of experiments, only the top 20 results are evaluated by hand. The result\nin (e) is used to approximate the range (f)\u2013(g). For this exam ple, how large (in\nabsolute terms) can the error for the MAP be by calculating (e ) instead of (f) and\n(g) for this query?\n8.5 Assessing relevance\nTo properly evaluate a system, your test information needs m ust be germane\nto the documents in the test document collection, and approp riate for pre-\ndicted usage of the system. These information needs are best designed by\ndomain experts. Using random combinations of query terms as an informa-\ntion need is generally not a good idea because typically they will not resem-\nble the actual distribution of information needs.\nGiven information needs and documents, you need to collect r elevance\nassessments. This is a time-consuming and expensive proces s involving hu-\nman beings. For tiny collections like Cran\ufb01eld, exhaustive judgments of rel-\nevance for each query and document pair were obtained. For la rge modern\ncollections, it is usual for relevance to be assessed only fo r a subset of the\ndocuments for each query. The most standard approach is pooling , where rel- POOLING\nevance is assessed over a subset of the collection that is for med from the top\nkdocuments returned by a number of different IR systems (usua lly the ones\nto be evaluated), and perhaps other sources such as the resul ts of Boolean\nkeyword searches or documents found by expert searchers in a n interactive\nprocess.\nOnline edition (c)\n2009 Cambridge UP8.5 Assessing relevance 165\nJudge 2 Relevance\nYes No Total\nJudge 1 Yes 300 20 320\nRelevance No 10 70 80\nTotal 310 90 400\nObserved proportion of the times the judges agreed\nP(A) = ( 300+70)/400=370/400 =0.925\nPooled marginals\nP(nonrelevant ) = ( 80+90)/(400+400) =170/800 =0.2125\nP(relevant ) = ( 320+310)/(400+400) =630/800 =0.7878\nProbability that the two judges agreed by chance\nP(E) =P(nonrelevant )2+P(relevant )2=0.21252+0.78782=0.665\nKappa statistic\n\u03ba= (P(A)\u2212P(E))/(1\u2212P(E)) = ( 0.925\u22120.665)/(1\u22120.665) =0.776\n\u25eeTable 8.2 Calculating the kappa statistic.\nA human is not a device that reliably reports a gold standard j udgment\nof relevance of a document to a query. Rather, humans and thei r relevance\njudgments are quite idiosyncratic and variable. But this is not a problem\nto be solved: in the \ufb01nal analysis, the success of an IR system depends on\nhow good it is at satisfying the needs of these idiosyncratic humans, one\ninformation need at a time.\nNevertheless, it is interesting to consider and measure how much agree-\nment between judges there is on relevance judgments. In the s ocial sciences,\na common measure for agreement between judges is the kappa statistic . It is KAPPA STATISTIC\ndesigned for categorical judgments and corrects a simple ag reement rate for\nthe rate of chance agreement.\nkappa =P(A)\u2212P(E)\n1\u2212P(E)(8.10)\nwhere P(A)is the proportion of the times the judges agreed, and P(E)is the\nproportion of the times they would be expected to agree by cha nce. There\nare choices in how the latter is estimated: if we simply say we are making\na two-class decision and assume nothing more, then the expec ted chance\nagreement rate is 0.5. However, normally the class distribu tion assigned is\nskewed, and it is usual to use marginal statistics to calculate expected agree- MARGINAL\nment.2There are still two ways to do it depending on whether one pool s\n2. For a contingency table, as in Table 8.2, a marginal statistic is formed by summing a row or\ncolumn. The marginal ai.k=\u2211jaijk.\nOnline edition (c)\n2009 Cambridge UP166 8 Evaluation in information retrieval\nthe marginal distribution across judges or uses the margina ls for each judge\nseparately; both forms have been used, but we present the poo led version\nbecause it is more conservative in the presence of systemati c differences in as-\nsessments across judges. The calculations are shown in Tabl e8.2. The kappa\nvalue will be 1 if two judges always agree, 0 if they agree only at the rate\ngiven by chance, and negative if they are worse than random. I f there are\nmore than two judges, it is normal to calculate an average pai rwise kappa\nvalue. As a rule of thumb, a kappa value above 0.8 is taken as go od agree-\nment, a kappa value between 0.67 and 0.8 is taken as fair agree ment, and\nagreement below 0.67 is seen as data providing a dubious basi s for an evalu-\nation, though the precise cutoffs depend on the purposes for which the data\nwill be used.\nInterjudge agreement of relevance has been measured within the TREC\nevaluations and for medical IR collections. Using the above rules of thumb,\nthe level of agreement normally falls in the range of \u201cfair\u201d ( 0.67\u20130.8). The fact\nthat human agreement on a binary relevance judgment is quite modest is one\nreason for not requiring more \ufb01ne-grained relevance labeli ng from the test\nset creator. To answer the question of whether IR evaluation results are valid\ndespite the variation of individual assessors\u2019 judgments, people have exper-\nimented with evaluations taking one or the other of two judge s\u2019 opinions as\nthe gold standard. The choice can make a considerable absolute difference to\nreported scores, but has in general been found to have little impact on the rel-\native effectiveness ranking of either different systems or varia nts of a single\nsystem which are being compared for effectiveness.\n8.5.1 Critiques and justi\ufb01cations of the concept of relevan ce\nThe advantage of system evaluation, as enabled by the standa rd model of\nrelevant and nonrelevant documents, is that we have a \ufb01xed se tting in which\nwe can vary IR systems and system parameters to carry out comp arative ex-\nperiments. Such formal testing is much less expensive and al lows clearer\ndiagnosis of the effect of changing system parameters than d oing user stud-\nies of retrieval effectiveness. Indeed, once we have a forma l measure that\nwe have con\ufb01dence in, we can proceed to optimize effectivene ss by machine\nlearning methods, rather than tuning parameters by hand. Of course, if the\nformal measure poorly describes what users actually want, d oing this will\nnot be effective in improving user satisfaction. Our perspe ctive is that, in\npractice, the standard formal measures for IR evaluation, a lthough a simpli-\n\ufb01cation, are good enough, and recent work in optimizing form al evaluation\nmeasures in IR has succeeded brilliantly. There are numerou s examples of\ntechniques developed in formal evaluation settings, which improve effec-\ntiveness in operational settings, such as the development o f document length\nnormalization methods within the context of TREC (Sections 6.4.4 and 11.4.3 )\nOnline edition (c)\n2009 Cambridge UP8.5 Assessing relevance 167\nand machine learning methods for adjusting parameter weigh ts in scoring\n(Section 6.1.2 ).\nThat is not to say that there are not problems latent within th e abstrac-\ntions used. The relevance of one document is treated as indep endent of the\nrelevance of other documents in the collection. (This assum ption is actually\nbuilt into most retrieval systems \u2013 documents are scored aga inst queries, not\nagainst each other \u2013 as well as being assumed in the evaluatio n methods.)\nAssessments are binary: there aren\u2019t any nuanced assessmen ts of relevance.\nRelevance of a document to an information need is treated as a n absolute,\nobjective decision. But judgments of relevance are subject ive, varying across\npeople, as we discussed above. In practice, human assessors are also imper-\nfect measuring instruments, susceptible to failures of und erstanding and at-\ntention. We also have to assume that users\u2019 information need s do not change\nas they start looking at retrieval results. Any results base d on one collection\nare heavily skewed by the choice of collection, queries, and relevance judg-\nment set: the results may not translate from one domain to ano ther or to a\ndifferent user population.\nSome of these problems may be \ufb01xable. A number of recent evalu ations,\nincluding INEX, some TREC tracks, and NTCIR have adopted an o rdinal\nnotion of relevance with documents divided into 3 or 4 classe s, distinguish-\ning slightly relevant documents from highly relevant docum ents. See Sec-\ntion 10.4 (page 210) for a detailed discussion of how this is implemented in\nthe INEX evaluations.\nOne clear problem with the relevance-based assessment that we have pre-\nsented is the distinction between relevance and marginal relevance : whether MARGINAL RELEVANCE\na document still has distinctive usefulness after the user h as looked at cer-\ntain other documents ( Carbonell and Goldstein 1998 ). Even if a document\nis highly relevant, its information can be completely redun dant with other\ndocuments which have already been examined. The most extrem e case of\nthis is documents that are duplicates \u2013 a phenomenon that is a ctually very\ncommon on the World Wide Web \u2013 but it can also easily occur when sev-\neral documents provide a similar precis of an event. In such c ircumstances,\nmarginal relevance is clearly a better measure of utility to the user. Maximiz-\ning marginal relevance requires returning documents that e xhibit diversity\nand novelty. One way to approach measuring this is by using di stinct facts\nor entities as evaluation units. This perhaps more directly measures true\nutility to the user but doing this makes it harder to create a t est collection.\n?Exercise 8.10 [\u22c6\u22c6]\nBelow is a table showing how two human judges rated the releva nce of a set of 12\ndocuments to a particular information need (0 = nonrelevant , 1 = relevant). Let us as-\nsume that you\u2019ve written an IR system that for this query retu rns the set of documents\n{4, 5, 6, 7, 8}.\nOnline edition (c)\n2009 Cambridge UP168 8 Evaluation in information retrieval\ndocID Judge 1 Judge 2\n1 0 0\n2 0 0\n3 1 1\n4 1 1\n5 1 0\n6 1 0\n7 1 0\n8 1 0\n9 0 1\n10 0 1\n11 0 1\n12 0 1\na.Calculate the kappa measure between the two judges.\nb.Calculate precision, recall, and F1of your system if a document is considered rel-\nevant only if the two judges agree.\nc.Calculate precision, recall, and F1of your system if a document is considered rel-\nevant if either judge thinks it is relevant.\n8.6 A broader perspective: System quality and user utility\nFormal evaluation measures are at some distance from our ult imate interest\nin measures of human utility: how satis\ufb01ed is each user with t he results the\nsystem gives for each information need that they pose? The st andard way to\nmeasure human satisfaction is by various kinds of user studi es. These might\ninclude quantitative measures, both objective, such as tim e to complete a\ntask, as well as subjective, such as a score for satisfaction with the search\nengine, and qualitative measures, such as user comments on t he search in-\nterface. In this section we will touch on other system aspect s that allow quan-\ntitative evaluation and the issue of user utility.\n8.6.1 System issues\nThere are many practical benchmarks on which to rate an infor mation re-\ntrieval system beyond its retrieval quality. These include :\n\u2022How fast does it index, that is, how many documents per hour do es it\nindex for a certain distribution over document lengths? (cf . Chapter 4)\n\u2022How fast does it search, that is, what is its latency as a funct ion of index\nsize?\n\u2022How expressive is its query language? How fast is it on comple x queries?\nOnline edition (c)\n2009 Cambridge UP8.6 A broader perspective: System quality and user utility 169\n\u2022How large is its document collection, in terms of the number o f doc-\numents or the collection having information distributed ac ross a broad\nrange of topics?\nAll these criteria apart from query language expressivenes s are straightfor-\nwardly measurable : we can quantify the speed or size. Various kinds of fea-\nture checklists can make query language expressiveness sem i-precise.\n8.6.2 User utility\nWhat we would really like is a way of quantifying aggregate us er happiness,\nbased on the relevance, speed, and user interface of a system . One part of\nthis is understanding the distribution of people we wish to m ake happy, and\nthis depends entirely on the setting. For a web search engine , happy search\nusers are those who \ufb01nd what they want. One indirect measure o f such users\nis that they tend to return to the same engine. Measuring the r ate of return\nof users is thus an effective metric, which would of course be more effective\nif you could also measure how much these users used other sear ch engines.\nBut advertisers are also users of modern web search engines. They are happy\nif customers click through to their sites and then make purch ases. On an\neCommerce web site, a user is likely to be wanting to purchase something.\nThus, we can measure the time to purchase, or the fraction of s earchers who\nbecome buyers. On a shopfront web site, perhaps both the user \u2019s and the\nstore owner\u2019s needs are satis\ufb01ed if a purchase is made. Never theless, in\ngeneral, we need to decide whether it is the end user\u2019s or the e Commerce\nsite owner\u2019s happiness that we are trying to optimize. Usual ly, it is the store\nowner who is paying us.\nFor an \u201centerprise\u201d (company, government, or academic) int ranet search\nengine, the relevant metric is more likely to be user product ivity: how much\ntime do users spend looking for information that they need. T here are also\nmany other practical criteria concerning such matters as in formation secu-\nrity, which we mentioned in Section 4.6(page 80).\nUser happiness is elusive to measure, and this is part of why t he standard\nmethodology uses the proxy of relevance of search results. T he standard\ndirect way to get at user satisfaction is to run user studies, where people en-\ngage in tasks, and usually various metrics are measured, the participants are\nobserved, and ethnographic interview techniques are used t o get qualitative\ninformation on satisfaction. User studies are very useful i n system design,\nbut they are time consuming and expensive to do. They are also dif\ufb01cult to\ndo well, and expertise is required to design the studies and t o interpret the\nresults. We will not discuss the details of human usability t esting here.\nOnline edition (c)\n2009 Cambridge UP170 8 Evaluation in information retrieval\n8.6.3 Re\ufb01ning a deployed system\nIf an IR system has been built and is being used by a large numbe r of users,\nthe system\u2019s builders can evaluate possible changes by depl oying variant\nversions of the system and recording measures that are indic ative of user\nsatisfaction with one variant vs. others as they are being us ed. This method\nis frequently used by web search engines.\nThe most common version of this is A/B testing , a term borrowed from the A/B TEST\nadvertising industry. For such a test, precisely one thing i s changed between\nthe current system and a proposed system, and a small proport ion of traf-\n\ufb01c (say, 1\u201310% of users) is randomly directed to the variant s ystem, while\nmost users use the current system. For example, if we wish to i nvestigate a\nchange to the ranking algorithm, we redirect a random sample of users to\na variant system and evaluate measures such as the frequency with which\npeople click on the top result, or any result on the \ufb01rst page. (This particular\nanalysis method is referred to as clickthrough log analysis orclickstream min- CLICKTHROUGH LOG\nANALYSIS\nCLICKSTREAM MININGing. It is further discussed as a method of implicit feedback in S ection 9.1.7\n(page 187).)\nThe basis of A/B testing is running a bunch of single variable tests (either\nin sequence or in parallel): for each test only one parameter is varied from the\ncontrol (the current live system). It is therefore easy to se e whether varying\neach parameter has a positive or negative effect. Such testi ng of a live system\ncan easily and cheaply gauge the effect of a change on users, a nd, with a\nlarge enough user base, it is practical to measure even very s mall positive\nand negative effects. In principle, more analytic power can be achieved by\nvarying multiple things at once in an uncorrelated (random) way, and doing\nstandard multivariate statistical analysis, such as multi ple linear regression.\nIn practice, though, A/B testing is widely used, because A/B tests are easy\nto deploy, easy to understand, and easy to explain to managem ent.\n8.7 Results snippets\nHaving chosen or ranked the documents matching a query, we wi sh to pre-\nsent a results list that will be informative to the user. In ma ny cases the\nuser will not want to examine all the returned documents and s o we want\nto make the results list informative enough that the user can do a \ufb01nal rank-\ning of the documents for themselves based on relevance to the ir information\nneed.3The standard way of doing this is to provide a snippet , a short sum- SNIPPET\nmary of the document, which is designed so as to allow the user to decide\nits relevance. Typically, the snippet consists of the docum ent title and a short\n3. There are exceptions, in domains where recall is emphasiz ed. For instance, in many legal\ndisclosure cases, a legal associate will review every document that matches a keyword search.\nOnline edition (c)\n2009 Cambridge UP8.7 Results snippets 171\nsummary, which is automatically extracted. The question is how to design\nthe summary so as to maximize its usefulness to the user.\nThe two basic kinds of summaries are static , which are always the same STATIC SUMMARY\nregardless of the query, and dynamic (or query-dependent), which are cus- DYNAMIC SUMMARY\ntomized according to the user\u2019s information need as deduced from a query.\nDynamic summaries attempt to explain why a particular docum ent was re-\ntrieved for the query at hand.\nA static summary is generally comprised of either or both a su bset of the\ndocument and metadata associated with the document. The sim plest form\nof summary takes the \ufb01rst two sentences or 50 words of a docume nt, or ex-\ntracts particular zones of a document, such as the title and a uthor. Instead of\nzones of a document, the summary can instead use metadata ass ociated with\nthe document. This may be an alternative way to provide an aut hor or date,\nor may include elements which are designed to give a summary, such as the\ndescription metadata which can appear in the meta element of a web\nHTML page. This summary is typically extracted and cached at indexing\ntime, in such a way that it can be retrieved and presented quic kly when dis-\nplaying search results, whereas having to access the actual document content\nmight be a relatively expensive operation.\nThere has been extensive work within natural language proce ssing (NLP)\non better ways to do text summarization . Most such work still aims only to TEXT SUMMARIZATION\nchoose sentences from the original document to present and c oncentrates on\nhow to select good sentences. The models typically combine p ositional fac-\ntors, favoring the \ufb01rst and last paragraphs of documents and the \ufb01rst and last\nsentences of paragraphs, with content factors, emphasizin g sentences with\nkey terms, which have low document frequency in the collecti on as a whole,\nbut high frequency and good distribution across the particu lar document\nbeing returned. In sophisticated NLP approaches, the syste m synthesizes\nsentences for a summary, either by doing full text generatio n or by editing\nand perhaps combining sentences used in the document. For ex ample, it\nmight delete a relative clause or replace a pronoun with the n oun phrase\nthat it refers to. This last class of methods remains in the re alm of research\nand is seldom used for search results: it is easier, safer, an d often even better\nto just use sentences from the original document.\nDynamic summaries display one or more \u201cwindows\u201d on the docum ent,\naiming to present the pieces that have the most utility to the user in evalu-\nating the document with respect to their information need. U sually these\nwindows contain one or several of the query terms, and so are o ften re-\nferred to as keyword-in-context (KWIC) snippets, though sometimes they may KEYWORD -IN-CONTEXT\nstill be pieces of the text such as the title that are selected for their query-\nindependent information value just as in the case of static s ummarization.\nDynamic summaries are generated in conjunction with scorin g. If the query\nis found as a phrase, occurrences of the phrase in the documen t will be\nOnline edition (c)\n2009 Cambridge UP172 8 Evaluation in information retrieval\n. . .In recent years, Papua New Guinea has faced severe economic\ndif\ufb01culties and economic growth has slowed, partly as a result of weak\ngovernance and civil war, and partly as a result of external f actors such as\nthe Bougainville civil war which led to the closure in 1989 of the Panguna\nmine (at that time the most important foreign exchange earne r and\ncontributor to Government \ufb01nances), the Asian \ufb01nancial cri sis, a decline in\nthe prices of gold and copper, and a fall in the production of o il.PNG\u2019s\neconomic development record over the past few years is evide nce that\ngovernance issues underly many of the country\u2019s problems. G ood\ngovernance, which may be de\ufb01ned as the transparent and accou ntable\nmanagement of human, natural, economic and \ufb01nancial resour ces for the\npurposes of equitable and sustainable development, \ufb02ows fr om proper\npublic sector management, ef\ufb01cient \ufb01scal and accounting me chanisms, and\na willingness to make service delivery a priority in practic e. . . .\n\u25eeFigure 8.5 An example of selecting text for a dynamic snippet. This snip pet was\ngenerated for a document in response to the query new guinea economic development .\nThe \ufb01gure shows in bold italic where the selected snippet tex t occurred in the original\ndocument.\nshown as the summary. If not, windows within the document tha t contain\nmultiple query terms will be selected. Commonly these windo ws may just\nstretch some number of words to the left and right of the query terms. This is\na place where NLP techniques can usefully be employed: users prefer snip-\npets that read well because they contain complete phrases.\nDynamic summaries are generally regarded as greatly improv ing the us-\nability of IR systems, but they present a complication for IR system design. A\ndynamic summary cannot be precomputed, but, on the other han d, if a sys-\ntem has only a positional index, then it cannot easily recons truct the context\nsurrounding search engine hits in order to generate such a dy namic sum-\nmary. This is one reason for using static summaries. The stan dard solution\nto this in a world of large and cheap disk drives is to locally c ache all the\ndocuments at index time (notwithstanding that this approac h raises various\nlegal, information security and control issues that are far from resolved) as\nshown in Figure 7.5(page 147). Then, a system can simply scan a document\nwhich is about to appear in a displayed results list to \ufb01nd sni ppets containing\nthe query words. Beyond simply access to the text, producing a good KWIC\nsnippet requires some care. Given a variety of keyword occur rences in a\ndocument, the goal is to choose fragments which are: (i) maxi mally informa-\ntive about the discussion of those terms in the document, (ii ) self-contained\nenough to be easy to read, and (iii) short enough to \ufb01t within t he normally\nstrict constraints on the space available for summaries.\nOnline edition (c)\n2009 Cambridge UP8.8 References and further reading 173\nGenerating snippets must be fast since the system is typical ly generating\nmany snippets for each query that it handles. Rather than cac hing an entire\ndocument, it is common to cache only a generous but \ufb01xed size p re\ufb01x of\nthe document, such as perhaps 10,000 characters. For most co mmon, short\ndocuments, the entire document is thus cached, but huge amou nts of local\nstorage will not be wasted on potentially vast documents. Su mmaries of\ndocuments whose length exceeds the pre\ufb01x size will be based o n material\nin the pre\ufb01x only, which is in general a useful zone in which to look for a\ndocument summary anyway.\nIf a document has been updated since it was last processed by a crawler\nand indexer, these changes will be neither in the cache nor in the index. In\nthese circumstances, neither the index nor the summary will accurately re-\n\ufb02ect the current contents of the document, but it is the diffe rences between\nthe summary and the actual document content that will be more glaringly\nobvious to the end user.\n8.8 References and further reading\nDe\ufb01nition and implementation of the notion of relevance to a query got off\nto a rocky start in 1953. Swanson (1988 ) reports that in an evaluation in that\nyear between two teams, they agreed that 1390 documents were variously\nrelevant to a set of 98 questions, but disagreed on a further 1 577 documents,\nand the disagreements were never resolved.\nRigorous formal testing of IR systems was \ufb01rst completed in t he Cran\ufb01eld\nexperiments, beginning in the late 1950s. A retrospective d iscussion of the\nCran\ufb01eld test collection and experimentation with it can be found in ( Clever-\ndon 1991 ). The other seminal series of early IR experiments were thos e on the\nSMART system by Gerard Salton and colleagues ( Salton 1971b ;1991 ). The\nTREC evaluations are described in detail by Voorhees and Harman (2005 ).\nOnline information is available at http://trec.nist.gov/ . Initially, few researchers\ncomputed the statistical signi\ufb01cance of their experimenta l results, but the IR\ncommunity increasingly demands this ( Hull 1993 ). User studies of IR system\neffectiveness began more recently ( Saracevic and Kantor 1988 ;1996 ).\nThe notions of recall and precision were \ufb01rst used by Kent et al. (1955 ),\nalthough the term precision did not appear until later. The F measure (or, FMEASURE\nrather its complement E=1\u2212F) was introduced by van Rijsbergen (1979 ).\nHe provides an extensive theoretical discussion, which sho ws how adopting\na principle of decreasing marginal relevance (at some point a user will be\nunwilling to sacri\ufb01ce a unit of precision for an added unit of recall) leads to\nthe harmonic mean being the appropriate method for combinin g precision\nand recall (and hence to its adoption rather than the minimum or geometric\nmean).\nOnline edition (c)\n2009 Cambridge UP174 8 Evaluation in information retrieval\nBuckley and Voorhees (2000 ) compare several evaluation measures, in-\ncluding precision at k, MAP , and R-precision, and evaluate the error rate of\neach measure. R-precision was adopted as the of\ufb01cial evalua tion metric in R-PRECISION\nthe TREC HARD track ( Allan 2005 ).Aslam and Yilmaz (2005 ) examine its\nsurprisingly close correlation to MAP , which had been noted in earlier stud-\nies (Tague-Sutcliffe and Blustein 1995 ,Buckley and Voorhees 2000 ). A stan-\ndard program for evaluating IR systems which computes many m easures of\nranked retrieval effectiveness is Chris Buckley\u2019s trec_eval program used\nin the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/ .\nKek\u00e4l\u00e4inen and J\u00e4rvelin (2002 ) argue for the superiority of graded rele-\nvance judgments when dealing with very large document colle ctions, and\nJ\u00e4rvelin and Kek\u00e4l\u00e4inen (2002 ) introduce cumulated gain-based methods for\nIR system evaluation in this context. Sakai (2007 ) does a study of the stabil-\nity and sensitivity of evaluation measures based on graded r elevance judg-\nments from NTCIR tasks, and concludes that NDCG is best for ev aluating\ndocument ranking.\nSchamber et al. (1990 ) examine the concept of relevance, stressing its multi-\ndimensional and context-speci\ufb01c nature, but also arguing t hat it can be mea-\nsured effectively. ( Voorhees 2000 ) is the standard article for examining vari-\nation in relevance judgments and their effects on retrieval system scores and\nranking for the TREC Ad Hoc task. Voorhees concludes that although the\nnumbers change, the rankings are quite stable. Hersh et al. (1994 ) present\nsimilar analysis for a medical IR collection. In contrast, Kek\u00e4l\u00e4inen (2005 )\nanalyze some of the later TRECs, exploring a 4-way relevance judgment and\nthe notion of cumulative gain, arguing that the relevance me asure used does\nsubstantially affect system rankings. See also Harter (1998 ).Zobel (1998 )\nstudies whether the pooling method used by TREC to collect a s ubset of doc-\numents that will be evaluated for relevance is reliable and f air, and concludes\nthat it is.\nThe kappa statistic and its use for language-related purpos es is discussed KAPPA STATISTIC\nbyCarletta (1996 ). Many standard sources (e.g., Siegel and Castellan 1988 )\npresent pooled calculation of the expected agreement, but Di Eugenio and\nGlass (2004 ) argue for preferring the unpooled agreement (though perha ps\npresenting multiple measures). For further discussion of a lternative mea-\nsures of agreement, which may in fact be better, see Lombard et al. (2002 )\nand Krippendorff (2003 ).\nText summarization has been actively explored for many year s. Modern\nwork on sentence selection was initiated by Kupiec et al. (1995 ). More recent\nwork includes ( Barzilay and Elhadad 1997 ) and ( Jing 2000 ), together with\na broad selection of work appearing at the yearly DUC confere nces and at\nother NLP venues. Tombros and Sanderson (1998 ) demonstrate the advan-\ntages of dynamic summaries in the IR context. Turpin et al. (2007 ) address\nhow to generate snippets ef\ufb01ciently.\nOnline edition (c)\n2009 Cambridge UP8.8 References and further reading 175\nClickthrough log analysis is studied in ( Joachims 2002b ,Joachims et al.\n2005 ).\nIn a series of papers, Hersh, Turpin and colleagues show how i mprove-\nments in formal retrieval effectiveness, as evaluated in ba tch experiments, do\nnot always translate into an improved system for users ( Hersh et al. 2000a ;b;\n2001 ,Turpin and Hersh 2001 ;2002 ).\nUser interfaces for IR and human factors such as models of hum an infor-\nmation seeking and usability testing are outside the scope o f what we cover\nin this book. More information on these topics can be found in other text-\nbooks, including ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 10) and ( Korfhage\n1997 ), and collections focused on cognitive aspects ( Spink and Cole 2005 ).\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 177\n9Relevance feedback and query\nexpansion\nIn most collections, the same concept may be referred to usin g different\nwords. This issue, known as synonymy , has an impact on the recall of most SYNONYMY\ninformation retrieval systems. For example, you would want a search for\naircraft to match plane (but only for references to an airplane , not a woodwork-\ning plane), and for a search on thermodynamics to match references to heat in\nappropriate discussions. Users often attempt to address th is problem them-\nselves by manually re\ufb01ning a query, as was discussed in Secti on1.4; in this\nchapter we discuss ways in which a system can help with query r e\ufb01nement,\neither fully automatically or with the user in the loop.\nThe methods for tackling this problem split into two major cl asses: global\nmethods and local methods. Global methods are techniques fo r expanding\nor reformulating query terms independent of the query and re sults returned\nfrom it, so that changes in the query wording will cause the ne w query to\nmatch other semantically similar terms. Global methods inc lude:\n\u2022Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2 )\n\u2022Query expansion via automatic thesaurus generation (Secti on9.2.3 )\n\u2022Techniques like spelling correction (discussed in Chapter 3)\nLocal methods adjust a query relative to the documents that i nitially appear\nto match the query. The basic methods here are:\n\u2022Relevance feedback (Section 9.1)\n\u2022Pseudo relevance feedback, also known as Blind relevance fe edback (Sec-\ntion 9.1.6 )\n\u2022(Global) indirect relevance feedback (Section 9.1.7 )\nIn this chapter, we will mention all of these approaches, but we will concen-\ntrate on relevance feedback, which is one of the most used and most success-\nful approaches.\nOnline edition (c)\n2009 Cambridge UP178 9 Relevance feedback and query expansion\n9.1 Relevance feedback and pseudo relevance feedback\nThe idea of relevance feedback (RF) is to involve the user in the retrieval process RELEVANCE FEEDBACK\nso as to improve the \ufb01nal result set. In particular, the user g ives feedback on\nthe relevance of documents in an initial set of results. The b asic procedure is:\n\u2022The user issues a (short, simple) query.\n\u2022The system returns an initial set of retrieval results.\n\u2022The user marks some returned documents as relevant or nonrel evant.\n\u2022The system computes a better representation of the informat ion need based\non the user feedback.\n\u2022The system displays a revised set of retrieval results.\nRelevance feedback can go through one or more iterations of t his sort. The\nprocess exploits the idea that it may be dif\ufb01cult to formulat e a good query\nwhen you don\u2019t know the collection well, but it is easy to judg e particular\ndocuments, and so it makes sense to engage in iterative query re\ufb01nement\nof this sort. In such a scenario, relevance feedback can also be effective in\ntracking a user\u2019s evolving information need: seeing some do cuments may\nlead users to re\ufb01ne their understanding of the information t hey are seeking.\nImage search provides a good example of relevance feedback. Not only is\nit easy to see the results at work, but this is a domain where a u ser can easily\nhave dif\ufb01culty formulating what they want in words, but can e asily indicate\nrelevant or nonrelevant images. After the user enters an ini tial query for bike\non the demonstration system at:\nhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html\nthe initial results (in this case, images) are returned. In F igure 9.1(a), the\nuser has selected some of them as relevant. These will be used to re\ufb01ne the\nquery, while other displayed results have no effect on the re formulation. Fig-\nure9.1(b) then shows the new top-ranked results calculated after t his round\nof relevance feedback.\nFigure 9.2shows a textual IR example where the user wishes to \ufb01nd out\nabout new applications of space satellites.\n9.1.1 The Rocchio algorithm for relevance feedback\nThe Rocchio Algorithm is the classic algorithm for implemen ting relevance\nfeedback. It models a way of incorporating relevance feedba ck information\ninto the vector space model of Section 6.3.\nOnline edition (c)\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 179\n(a)\n(b)\n\u25eeFigure 9.1 Relevance feedback searching over images. (a) The user view s the\ninitial query results for a query of bike, selects the \ufb01rst, third and fourth result in\nthe top row and the fourth result in the bottom row as relevant , and submits this\nfeedback. (b) The users sees the revised result set. Precisi on is greatly improved.\nFromhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001 ).\nOnline edition (c)\n2009 Cambridge UP180 9 Relevance feedback and query expansion\n(a) Query: New space satellite applications\n(b) + 1. 0.539, 08/13/91, NASA Hasn\u2019t Scrapped Imaging Spectrom eter\n+ 2. 0.533, 07/09/91, NASA Scratches Environment Gear From S atel-\nlite Plan\n3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan , But\nUrges Launches of Smaller Probes\n4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes I ncredi-\nble Feat: Staying Within Budget\n5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Pr o-\nposes Satellites for Climate Research\n6. 0.524, 08/22/90, Report Provides Support for the Critics Of Using\nBig Satellites to Study Climate\n7. 0.516, 04/13/87, Arianespace Receives Satellite Launch Pact\nFrom Telesat Canada\n+ 8. 0.509, 12/02/87, Telecommunications Tale of Two Compan ies\n(c) 2.074 new 15.106 space\n30.816 satellite 5.660 application\n5.991 nasa 5.196 eos\n4.196 launch 3.972 aster\n3.516 instrument 3.446 arianespace\n3.004 bundespost 2.806 ss\n2.790 rocket 2.053 scientist\n2.003 broadcast 1.172 earth\n0.836 oil 0.646 measure\n(d) * 1. 0.513, 07/09/91, NASA Scratches Environment Gear From S atel-\nlite Plan\n* 2. 0.500, 08/13/91, NASA Hasn\u2019t Scrapped Imaging Spectrom eter\n3. 0.493, 08/07/89, When the Pentagon Launches a Secret Sate llite,\nSpace Sleuths Do Some Spy Work of Their Own\n4. 0.493, 07/31/89, NASA Uses \u2018Warm\u2019 Superconductors For Fa st\nCircuit\n* 5. 0.492, 12/02/87, Telecommunications Tale of Two Compan ies\n6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile For\nCommercial Use\n7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Matc h\nthe Soviets In Rocket Launchers\n8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Co st $90\nMillion\n\u25eeFigure 9.2 Example of relevance feedback on a text collection. (a) The i nitial query\n(a). (b) The user marks some relevant documents (shown with a plus sign). (c) The\nquery is then expanded by 18 terms with weights as shown. (d) T he revised top\nresults are then shown. A * marks the documents which were jud ged relevant in the\nrelevance feedback phase.\nOnline edition (c)\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 181\n\u25eeFigure 9.3 The Rocchio optimal query for separating relevant and nonre levant\ndocuments.\nThe underlying theory. We want to \ufb01nd a query vector, denoted as /vectorq, that\nmaximizes similarity with relevant documents while minimi zing similarity\nwith nonrelevant documents. If Cris the set of relevant documents and Cnr\nis the set of nonrelevant documents, then we wish to \ufb01nd:1\n/vectorqopt=arg max\n/vectorq[sim(/vectorq,Cr)\u2212sim(/vectorq,Cnr)], (9.1)\nwhere sim is de\ufb01ned as in Equation 6.10. Under cosine similarity, the optimal\nquery vector /vectorqoptfor separating the relevant and nonrelevant documents is:\n/vectorqopt=1\n|Cr|\u2211\n/vectordj\u2208Cr/vectordj\u22121\n|Cnr|\u2211\n/vectordj\u2208Cnr/vectordj (9.2)\nThat is, the optimal query is the vector difference between t he centroids of the\nrelevant and nonrelevant documents; see Figure 9.3. However, this observa-\ntion is not terribly useful, precisely because the full set o f relevant documents\nis not known: it is what we want to \ufb01nd.\nThe Rocchio (1971 ) algorithm. This was the relevance feedback mecha- ROCCHIO ALGORITHM\n1. In the equation, arg maxxf(x)returns a value of xwhich maximizes the value of the function\nf(x). Similarly, arg minxf(x)returns a value of xwhich minimizes the value of the function\nf(x).\nOnline edition (c)\n2009 Cambridge UP182 9 Relevance feedback and query expansion\n\u25eeFigure 9.4 An application of Rocchio\u2019s algorithm. Some documents have been\nlabeled as relevant and nonrelevant and the initial query ve ctor is moved in response\nto this feedback.\nnism introduced in and popularized by Salton\u2019s SMART system around 1970.\nIn a real IR query context, we have a user query and partial kno wledge of\nknown relevant and nonrelevant documents. The algorithm pr oposes using\nthe modi\ufb01ed query /vectorqm:\n/vectorqm=\u03b1/vectorq0+\u03b21\n|Dr|\u2211\n/vectordj\u2208Dr/vectordj\u2212\u03b31\n|Dnr|\u2211\n/vectordj\u2208Dnr/vectordj (9.3)\nwhere q0is the original query vector, Drand Dnrare the set of known rel-\nevant and nonrelevant documents respectively, and \u03b1,\u03b2, and \u03b3are weights\nattached to each term. These control the balance between tru sting the judged\ndocument set versus the query: if we have a lot of judged docum ents, we\nwould like a higher \u03b2and \u03b3. Starting from q0, the new query moves you\nsome distance toward the centroid of the relevant documents and some dis-\ntance away from the centroid of the nonrelevant documents. T his new query\ncan be used for retrieval in the standard vector space model ( see Section 6.3).\nWe can easily leave the positive quadrant of the vector space by subtracting\noff a nonrelevant document\u2019s vector. In the Rocchio algorit hm, negative term\nweights are ignored. That is, the term weight is set to 0. Figu re9.4shows the\neffect of applying relevance feedback.\nRelevance feedback can improve both recall and precision. B ut, in prac-\ntice, it has been shown to be most useful for increasing recal l in situations\nOnline edition (c)\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 183\nwhere recall is important. This is partly because the techni que expands the\nquery, but it is also partly an effect of the use case: when the y want high\nrecall, users can be expected to take time to review results a nd to iterate on\nthe search. Positive feedback also turns out to be much more v aluable than\nnegative feedback, and so most IR systems set \u03b3<\u03b2. Reasonable values\nmight be \u03b1=1,\u03b2=0.75, and \u03b3=0.15. In fact, many systems, such as\nthe image search system in Figure 9.1, allow only positive feedback, which\nis equivalent to setting \u03b3=0. Another alternative is to use only the marked\nnonrelevant document which received the highest ranking fr om the IR sys-\ntem as negative feedback (here, |Dnr|=1 in Equation ( 9.3)). While many of\nthe experimental results comparing various relevance feed back variants are\nrather inconclusive, some studies have suggested that this variant, called Ide IDE DEC -HI\ndec-hi is the most effective or at least the most consistent perform er.\n\u27049.1.2 Probabilistic relevance feedback\nRather than reweighting the query in a vector space, if a user has told us\nsome relevant and nonrelevant documents, then we can procee d to build a\nclassi\ufb01er. One way of doing this is with a Naive Bayes probabi listic model.\nIfRis a Boolean indicator variable expressing the relevance of a document,\nthen we can estimate P(xt=1|R), the probability of a term tappearing in a\ndocument, depending on whether it is relevant or not, as:\n\u02c6P(xt=1|R=1) =|VR t|/|VR| (9.4)\n\u02c6P(xt=1|R=0) = ( d ft\u2212|VR t|)/(N\u2212|VR|)\nwhere Nis the total number of documents, d ftis the number that contain\nt,VRis the set of known relevant documents, and VR tis the subset of this\nset containing t. Even though the set of known relevant documents is a per-\nhaps small subset of the true set of relevant documents, if we assume that\nthe set of relevant documents is a small subset of the set of al l documents\nthen the estimates given above will be reasonable. This give s a basis for\nanother way of changing the query term weights. We will discu ss such prob-\nabilistic approaches more in Chapters 11and 13, and in particular outline\nthe application to relevance feedback in Section 11.3.4 (page 228). For the\nmoment, observe that using just Equation ( 9.4) as a basis for term-weighting\nis likely insuf\ufb01cient. The equations use only collection st atistics and infor-\nmation about the term distribution within the documents jud ged relevant.\nThey preserve no memory of the original query.\n9.1.3 When does relevance feedback work?\nThe success of relevance feedback depends on certain assump tions. Firstly,\nthe user has to have suf\ufb01cient knowledge to be able to make an i nitial query\nOnline edition (c)\n2009 Cambridge UP184 9 Relevance feedback and query expansion\nwhich is at least somewhere close to the documents they desir e. This is\nneeded anyhow for successful information retrieval in the b asic case, but\nit is important to see the kinds of problems that relevance fe edback cannot\nsolve alone. Cases where relevance feedback alone is not suf \ufb01cient include:\n\u2022Misspellings. If the user spells a term in a different way to t he way it\nis spelled in any document in the collection, then relevance feedback is\nunlikely to be effective. This can be addressed by the spelli ng correction\ntechniques of Chapter 3.\n\u2022Cross-language information retrieval. Documents in anoth er language\nare not nearby in a vector space based on term distribution. R ather, docu-\nments in the same language cluster more closely together.\n\u2022Mismatch of searcher\u2019s vocabulary versus collection vocab ulary. If the\nuser searches for laptop but all the documents use the term notebook com-\nputer , then the query will fail, and relevance feedback is again mo st likely\nineffective.\nSecondly, the relevance feedback approach requires releva nt documents to\nbe similar to each other. That is, they should cluster. Ideal ly, the term dis-\ntribution in all relevant documents will be similar to that i n the documents\nmarked by the users, while the term distribution in all nonre levant docu-\nments will be different from those in relevant documents. Th ings will work\nwell if all relevant documents are tightly clustered around a single proto-\ntype, or, at least, if there are different prototypes, if the relevant documents\nhave signi\ufb01cant vocabulary overlap, while similarities be tween relevant and\nnonrelevant documents are small. Implicitly, the Rocchio r elevance feedback\nmodel treats relevant documents as a single cluster , which it models via the\ncentroid of the cluster. This approach does not work as well i f the relevant\ndocuments are a multimodal class, that is, they consist of se veral clusters of\ndocuments within the vector space. This can happen with:\n\u2022Subsets of the documents using different vocabulary, such a sBurma vs.\nMyanmar\n\u2022A query for which the answer set is inherently disjunctive, s uch asPop\nstarswho once workedat BurgerKing .\n\u2022Instances of a general concept, which often appear as a disju nction of\nmore speci\ufb01c concepts, for example, felines .\nGood editorial content in the collection can often provide a solution to this\nproblem. For example, an article on the attitudes of differe nt groups to the\nsituation in Burma could introduce the terminology used by d ifferent parties,\nthus linking the document clusters.\nOnline edition (c)\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 185\nRelevance feedback is not necessarily popular with users. U sers are often\nreluctant to provide explicit feedback, or in general do not wish to prolong\nthe search interaction. Furthermore, it is often harder to u nderstand why a\nparticular document was retrieved after relevance feedbac k is applied.\nRelevance feedback can also have practical problems. The lo ng queries\nthat are generated by straightforward application of relev ance feedback tech-\nniques are inef\ufb01cient for a typical IR system. This results i n a high computing\ncost for the retrieval and potentially long response times f or the user. A par-\ntial solution to this is to only reweight certain prominent t erms in the relevant\ndocuments, such as perhaps the top 20 terms by term frequency . Some ex-\nperimental results have also suggested that using a limited number of terms\nlike this may give better results ( Harman 1992 ) though other work has sug-\ngested that using more terms is better in terms of retrieved d ocument quality\n(Buckley et al. 1994b ).\n9.1.4 Relevance feedback on the web\nSome web search engines offer a similar/related pages featu re: the user in-\ndicates a document in the results set as exemplary from the st andpoint of\nmeeting his information need and requests more documents li ke it. This can\nbe viewed as a particular simple form of relevance feedback. However, in\ngeneral relevance feedback has been little used in web searc h. One exception\nwas the Excite web search engine, which initially provided f ull relevance\nfeedback. However, the feature was in time dropped, due to la ck of use. On\nthe web, few people use advanced search interfaces and most w ould like to\ncomplete their search in a single interaction. But the lack o f uptake also prob-\nably re\ufb02ects two other factors: relevance feedback is hard t o explain to the\naverage user, and relevance feedback is mainly a recall enha ncing strategy,\nand web search users are only rarely concerned with getting s uf\ufb01cient recall.\nSpink et al. (2000 ) present results from the use of relevance feedback in\nthe Excite search engine. Only about 4% of user query session s used the\nrelevance feedback option, and these were usually exploiti ng the \u201cMore like\nthis\u201d link next to each result. About 70% of users only looked at the \ufb01rst\npage of results and did not pursue things any further. For peo ple who used\nrelevance feedback, results were improved about two thirds of the time.\nAn important more recent thread of work is the use of clickstr eam data\n(what links a user clicks on) to provide indirect relevance f eedback. Use\nof this data is studied in detail in ( Joachims 2002b ,Joachims et al. 2005 ).\nThe very successful use of web link structure (see Chapter 21) can also be\nviewed as implicit feedback, but provided by page authors ra ther than read-\ners (though in practice most authors are also readers).\nOnline edition (c)\n2009 Cambridge UP186 9 Relevance feedback and query expansion\n?Exercise 9.1\nIn Rocchio\u2019s algorithm, what weight setting for \u03b1/\u03b2/\u03b3does a \u201cFind pages like this\none\u201d search correspond to?\nExercise 9.2 [\u22c6]\nGive three reasons why relevance feedback has been little us ed in web search.\n9.1.5 Evaluation of relevance feedback strategies\nInteractive relevance feedback can give very substantial g ains in retrieval\nperformance. Empirically, one round of relevance feedback is often very\nuseful. Two rounds is sometimes marginally more useful. Suc cessful use of\nrelevance feedback requires enough judged documents, othe rwise the pro-\ncess is unstable in that it may drift away from the user\u2019s info rmation need.\nAccordingly, having at least \ufb01ve judged documents is recomm ended.\nThere is some subtlety to evaluating the effectiveness of re levance feed-\nback in a sound and enlightening way. The obvious \ufb01rst strate gy is to start\nwith an initial query q0and to compute a precision-recall graph. Following\none round of feedback from the user, we compute the modi\ufb01ed qu eryqm\nand again compute a precision-recall graph. Here, in both ro unds we assess\nperformance over all documents in the collection, which mak es comparisons\nstraightforward. If we do this, we \ufb01nd spectacular gains fro m relevance feed-\nback: gains on the order of 50% in mean average precision. But unfortunately\nit is cheating. The gains are partly due to the fact that known relevant doc-\numents (judged by the user) are now ranked higher. Fairness d emands that\nwe should only evaluate with respect to documents not seen by the user.\nA second idea is to use documents in the residual collection (the set of doc-\numents minus those assessed relevant) for the second round o f evaluation.\nThis seems like a more realistic evaluation. Unfortunately , the measured per-\nformance can then often be lower than for the original query. This is partic-\nularly the case if there are few relevant documents, and so a f air proportion\nof them have been judged by the user in the \ufb01rst round. The rela tive per-\nformance of variant relevance feedback methods can be valid ly compared,\nbut it is dif\ufb01cult to validly compare performance with and wi thout relevance\nfeedback because the collection size and the number of relev ant documents\nchanges from before the feedback to after it.\nThus neither of these methods is fully satisfactory. A third method is to\nhave two collections, one which is used for the initial query and relevance\njudgments, and the second that is then used for comparative e valuation. The\nperformance of both q0and qmcan be validly compared on the second col-\nlection.\nPerhaps the best evaluation of the utility of relevance feed back is to do user\nstudies of its effectiveness, in particular by doing a time- based comparison:\nOnline edition (c)\n2009 Cambridge UP9.1 Relevance feedback and pseudo relevance feedback 187\nPrecision at k=50\nTerm weighting no RF pseudo RF\nlnc.ltc 64.2% 72.7%\nLnu.ltu 74.2% 87.0%\n\u25eeFigure 9.5 Results showing pseudo relevance feedback greatly improvi ng perfor-\nmance. These results are taken from the Cornell SMART system at TREC 4 ( Buckley\net al. 1995 ), and also contrast the use of two different length normaliz ation schemes\n(L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20\nterms to each query.\nhow fast does a user \ufb01nd relevant documents with relevance fe edback vs.\nanother strategy (such as query reformulation), or alterna tively, how many\nrelevant documents does a user \ufb01nd in a certain amount of time . Such no-\ntions of user utility are fairest and closest to real system u sage.\n9.1.6 Pseudo relevance feedback\nPseudo relevance feedback , also known as blind relevance feedback , provides a PSEUDO RELEVANCE\nFEEDBACK\nBLIND RELEVANCE\nFEEDBACKmethod for automatic local analysis. It automates the manua l part of rele-\nvance feedback, so that the user gets improved retrieval per formance with-\nout an extended interaction. The method is to do normal retri eval to \ufb01nd an\ninitial set of most relevant documents, to then assume that the top kranked\ndocuments are relevant, and \ufb01nally to do relevance feedback as before under\nthis assumption.\nThis automatic technique mostly works. Evidence suggests t hat it tends\nto work better than global analysis (Section 9.2). It has been found to im-\nprove performance in the TREC ad hoc task. See for example the results in\nFigure 9.5. But it is not without the dangers of an automatic process. Fo r\nexample, if the query is about copper mines and the top several documents\nare all about mines in Chile, then there may be query drift in t he direction of\ndocuments on Chile.\n9.1.7 Indirect relevance feedback\nWe can also use indirect sources of evidence rather than expl icit feedback on\nrelevance as the basis for relevance feedback. This is often called implicit (rel- IMPLICIT RELEVANCE\nFEEDBACK evance) feedback . Implicit feedback is less reliable than explicit feedback , but is\nmore useful than pseudo relevance feedback, which contains no evidence of\nuser judgments. Moreover, while users are often reluctant t o provide explicit\nfeedback, it is easy to collect implicit feedback in large qu antities for a high\nvolume system, such as a web search engine.\nOnline edition (c)\n2009 Cambridge UP188 9 Relevance feedback and query expansion\nOn the web, DirectHit introduced the idea of ranking more hig hly docu-\nments that users chose to look at more often. In other words, c licks on links\nwere assumed to indicate that the page was likely relevant to the query. This\napproach makes various assumptions, such as that the docume nt summaries\ndisplayed in results lists (on whose basis users choose whic h documents to\nclick on) are indicative of the relevance of these documents . In the original\nDirectHit search engine, the data about the click rates on pa ges was gathered\nglobally, rather than being user or query speci\ufb01c. This is on e form of the gen-\neral area of clickstream mining . Today, a closely related approach is used in CLICKSTREAM MINING\nranking the advertisements that match a web search query (Ch apter 19).\n9.1.8 Summary\nRelevance feedback has been shown to be very effective at imp roving rele-\nvance of results. Its successful use requires queries for wh ich the set of rele-\nvant documents is medium to large. Full relevance feedback i s often onerous\nfor the user, and its implementation is not very ef\ufb01cient in m ost IR systems.\nIn many cases, other types of interactive retrieval may impr ove relevance by\nabout as much with less work.\nBeyond the core ad hoc retrieval scenario, other uses of rele vance feedback\ninclude:\n\u2022Following a changing information need (e.g., names of car mo dels of in-\nterest change over time)\n\u2022Maintaining an information \ufb01lter (e.g., for a news feed). Su ch \ufb01lters are\ndiscussed further in Chapter 13.\n\u2022Active learning (deciding which examples it is most useful t o know the\nclass of to reduce annotation costs).\n?Exercise 9.3\nUnder what conditions would the modi\ufb01ed query qmin Equation 9.3be the same as\nthe original query q0? In all other cases, is qmcloser than q0to the centroid of the\nrelevant documents?\nExercise 9.4\nWhy is positive feedback likely to be more useful than negati ve feedback to an IR\nsystem? Why might only using one nonrelevant document be mor e effective than\nusing several?\nExercise 9.5\nSuppose that a user\u2019s initial query is cheap CDs cheap DVDs extremely cheap CDs . The\nuser examines two documents, d1and d2. She judges d1, with the content CDs cheap\nsoftware cheap CDs relevant and d2with content cheap thrills DVDs nonrelevant. As-\nsume that we are using direct term frequency (with no scaling and no document\nOnline edition (c)\n2009 Cambridge UP9.2 Global methods for query reformulation 189\nfrequency). There is no need to length-normalize vectors. U sing Rocchio relevance\nfeedback as in Equation ( 9.3) what would the revised query vector be after relevance\nfeedback? Assume \u03b1=1,\u03b2=0.75, \u03b3=0.25.\nExercise 9.6 [\u22c6]\nOmar has implemented a relevance feedback web search system , where he is going\nto do relevance feedback based only on words in the title text returned for a page (for\nef\ufb01ciency). The user is going to rank 3 results. The \ufb01rst user , Jinxing, queries for:\nbananaslug\nand the top three titles returned are:\nbanana slug Ariolimax columbianus\nSanta Cruz mountains banana slug\nSanta Cruz Campus Mascot\nJinxing judges the \ufb01rst two documents relevant, and the thir d nonrelevant. Assume\nthat Omar\u2019s search engine uses term frequency but no length n ormalization nor IDF.\nAssume that he is using the Rocchio relevance feedback mecha nism, with \u03b1=\u03b2=\n\u03b3=1. Show the \ufb01nal revised query that would be run. (Please list the vector elements\nin alphabetical order.)\n9.2 Global methods for query reformulation\nIn this section we more brie\ufb02y discuss three global methods f or expanding a\nquery: by simply aiding the user in doing so, by using a manual thesaurus,\nand through building a thesaurus automatically.\n9.2.1 Vocabulary tools for query reformulation\nVarious user supports in the search process can help the user see how their\nsearches are or are not working. This includes information a bout words that\nwere omitted from the query because they were on stop lists, w hat words\nwere stemmed to, the number of hits on each term or phrase, and whether\nwords were dynamically turned into phrases. The IR system mi ght also sug-\ngest search terms by means of a thesaurus or a controlled voca bulary. A user\ncan also be allowed to browse lists of the terms that are in the inverted index,\nand thus \ufb01nd good terms that appear in the collection.\n9.2.2 Query expansion\nIn relevance feedback, users give additional input on docum ents (by mark-\ning documents in the results set as relevant or not), and this input is used\nto reweight the terms in the query for documents. In query expansion on the QUERY EXPANSION\nother hand, users give additional input on query words or phr ases, possibly\nsuggesting additional query terms. Some search engines (es pecially on the\nOnline edition (c)\n2009 Cambridge UP190 9 Relevance feedback and query expansionY a h o o ! M y Y a h o o ! M a i l W e l c o m e , G u e s t [ S i g n I n ] H e l pS e a r c hp a l m\nW\ne b I m a g e s V i d e o L o c a l S h o p p i n g m o r eO p t i o n sA l s o t r y :S P O N S O R R E S U L T S\np a\nl m t r e e s ,p a\nl m s p r i n g s ,p a\nl m c e n t r o ,p a\nl mt r e o , M o r e . . .Pa\nl m b A T & Ta t t . c o m / w i r e l e s s l G o m o b i l e e f f o r t l e s s l y w i t h t h e P A L M T r e o f r o mA\nT & T ( C i n g u l a r ) .Pa\nl m H a n d h e l d sP a l m . c o m l O r g a n i z e r , P l a n n e r , W i F i , M u s i c B l u e t o o t h , G a m e s ,P h o t o s & V i d e o .Pa\nl m , I n c .M a k e r o f h a n d h e l d P DA\nd e v i c e s t h a t a l l o w m o b i l e u s e r s t o m a n a g es c h e d u l e s , c o n t a c t s , a n d o t h e r p e r s o n a l a n d b u s i n e s s i n f o r m a t i o n .w w w . p a l m . c o m l C a c h e dPa\nl m , I n c . b T r e o a n d C e n t r o s m a r t p h o n e s , h a n d h e l d s ,a n d a c c e s s o r i e sP a l m , I n c . , i n n o v a t o r o f e a s y l t o l u s e m o b i l e p r o d u c t s i n c l u d i n gP a l m \u00ae T r e o _ a n d C e n t r o _ s m a r t p h o n e s , P a l m h a n d h e l d s , s e r v i c e s ,a n d a c c e s s o r i e s .w w w . p a l m . c o m / u s l C a c h e d\nS P O N S O R R E S U L T SH a n d h e l d s a t D e l lS t a y C o n n e c t e d w i t hH\na n d h e l d P C s & P DA\ns .S h o p a t D e l l \u2122 O f f i c i a lS i t e .w w w . D e l l . c o mB u y Pa\nl m C e n t r oC a s e sU\nl t i m a t e s e l e c t i o n o fc a s e s a n d a c c e s s o r i e sf o r b u s i n e s s d e v i c e s .w w w . C a s e s . c o mF r e e P l a m T r e oG e tA\nF r e e P a l m T r e o7 0 0\nW P h o n e . P a r t i c i p a t eT o d a y .E v\na l u a t i o n N a t i o n . c o m /t r e o\n1 \u00aa 1 0\no f a b o u t 5 3 4 ,0 0 0\n,0 0 0\nf o r p a l m ( A b o u t t h i s p a g e )\u00aa 0 . 1 1\ns e c.\n\u25eeFigure 9.6 An example of query expansion in the interface of the Yahoo! w eb\nsearch engine in 2006. The expanded query suggestions appea r just below the \u201cSearch\nResults\u201d bar.\nweb) suggest related queries in response to a query; the user s then opt to use\none of these alternative query suggestions. Figure 9.6shows an example of\nquery suggestion options being presented in the Yahoo! web s earch engine.\nThe central question in this form of query expansion is how to generate al-\nternative or expanded queries for the user. The most common f orm of query\nexpansion is global analysis, using some form of thesaurus. For each term\ntin a query, the query can be automatically expanded with syno nyms and\nrelated words of tfrom the thesaurus. Use of a thesaurus can be combined\nwith ideas of term weighting: for instance, one might weight added terms\nless than original query terms.\nMethods for building a thesaurus for query expansion includ e:\n\u2022Use of a controlled vocabulary that is maintained by human ed itors. Here,\nthere is a canonical term for each concept. The subject headi ngs of tra-\nditional library subject indexes, such as the Library of Con gress Subject\nHeadings, or the Dewey Decimal system are examples of a contr olled\nvocabulary. Use of a controlled vocabulary is quite common f or well-\nresourced domains. A well-known example is the Uni\ufb01ed Medic al Lan-\nguage System (UMLS) used with MedLine for querying the biome dical\nresearch literature. For example, in Figure 9.7,neoplasms was added to a\nOnline edition (c)\n2009 Cambridge UP9.2 Global methods for query reformulation 191\n\u2022 User query: cancer\n\u2022 PubMed query: (\u201cneoplasms\u201d[TIAB] NOT Medline[SB]) OR \u201cneoplasms\u201d[MeSH\nTerms]OR cancer[TextWord]\n\u2022 User query: skin itch\n\u2022 PubMed query: (\u201cskin\u201d[MeSH Terms] OR (\u201cintegumentary system\u201d[TIAB] NOT\nMedline[SB]) OR \u201cintegumentary system\u201d[MeSHTerms] OR ski n[Text Word])AND\n((\u201cpruritus\u201d[TIAB] NOT Medline[SB]) OR \u201cpruritus\u201d[MeSH T erms] OR itch[Text\nWord])\n\u25eeFigure 9.7 Examples of query expansion via the PubMed thesaurus. When a user\nissues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/ ,\ntheir query is mapped on to the Medline vocabulary as shown.\nsearch for cancer . This Medline query expansion also contrasts with the\nYahoo! example. The Yahoo! interface is a case of interactiv e query expan-\nsion, whereas PubMed does automatic query expansion. Unles s the user\nchooses to examine the submitted query, they may not even rea lize that\nquery expansion has occurred.\n\u2022A manual thesaurus. Here, human editors have built up sets of synony-\nmous names for concepts, without designating a canonical te rm. The\nUMLS metathesaurus is one example of a thesaurus. Statistic s Canada\nmaintains a thesaurus of preferred terms, synonyms, broade r terms, and\nnarrower terms for matters on which the government collects statistics,\nsuch as goods and services. This thesaurus is also bilingual English and\nFrench.\n\u2022An automatically derived thesaurus. Here, word co-occurre nce statistics\nover a collection of documents in a domain are used to automat ically in-\nduce a thesaurus; see Section 9.2.3 .\n\u2022Query reformulations based on query log mining. Here, we exp loit the\nmanual query reformulations of other users to make suggesti ons to a new\nuser. This requires a huge query volume, and is thus particul arly appro-\npriate to web search.\nThesaurus-based query expansion has the advantage of not re quiring any\nuser input. Use of query expansion generally increases reca ll and is widely\nused in many science and engineering \ufb01elds. As well as such gl obal analysis\ntechniques, it is also possible to do query expansion by loca l analysis, for\ninstance, by analyzing the documents in the result set. User input is now\nOnline edition (c)\n2009 Cambridge UP192 9 Relevance feedback and query expansion\nWord Nearest neighbors\nabsolutely absurd, whatsoever, totally, exactly, nothing\nbottomed dip, copper, drops, topped, slide, trimmed\ncaptivating shimmer, stunningly, superbly, plucky, witty\ndoghouse dog, porch, crawling, beside, downstairs\nmakeup repellent, lotion, glossy, sunscreen, skin, gel\nmediating reconciliation, negotiate, case, conciliation\nkeeping hoping, bring, wiping, could, some, would\nlithographs drawings, Picasso, Dali, sculptures, Gauguin\npathogens toxins, bacteria, organisms, bacterial, parasite\nsenses grasp, psyche, truly, clumsy, naive, innate\n\u25eeFigure 9.8 An example of an automatically generated thesaurus. This ex ample\nis based on the work in Sch\u00fctze (1998 ), which employs latent semantic indexing (see\nChapter 18).\nusually required, but a distinction remains as to whether th e user is giving\nfeedback on documents or on query terms.\n9.2.3 Automatic thesaurus generation\nAs an alternative to the cost of a manual thesaurus, we could a ttempt to\ngenerate a thesaurus automatically by analyzing a collecti on of documents.\nThere are two main approaches. One is simply to exploit word c ooccurrence.\nWe say that words co-occurring in a document or paragraph are likely to be\nin some sense similar or related in meaning, and simply count text statistics\nto \ufb01nd the most similar words. The other approach is to use a sh allow gram-\nmatical analysis of the text and to exploit grammatical rela tions or grammat-\nical dependencies. For example, we say that entities that ar e grown, cooked,\neaten, and digested, are more likely to be food items. Simply using word\ncooccurrence is more robust (it cannot be misled by parser er rors), but using\ngrammatical relations is more accurate.\nThe simplest way to compute a co-occurrence thesaurus is bas ed on term-\nterm similarities. We begin with a term-document matrix A, where each cell\nAt,dis a weighted count wt,dfor term tand document d, with weighting so\nAhas length-normalized rows. If we then calculate C=AAT, then Cu,vis\na similarity score between terms uand v, with a larger number being better.\nFigure 9.8shows an example of a thesaurus derived in basically this man ner,\nbut with an extra step of dimensionality reduction via Laten t Semantic In-\ndexing, which we discuss in Chapter 18. While some of the thesaurus terms\nare good or at least suggestive, others are marginal or bad. T he quality of the\nassociations is typically a problem. Term ambiguity easily introduces irrel-\nOnline edition (c)\n2009 Cambridge UP9.3 References and further reading 193\nevant statistically correlated terms. For example, a query forApple computer\nmay expand to Apple red fruit computer . In general these thesauri suffer from\nboth false positives and false negatives. Moreover, since t he terms in the au-\ntomatic thesaurus are highly correlated in documents anywa y (and often the\ncollection used to derive the thesaurus is the same as the one being indexed),\nthis form of query expansion may not retrieve many additiona l documents.\nQuery expansion is often effective in increasing recall. Ho wever, there is\na high cost to manually producing a thesaurus and then updati ng it for sci-\nenti\ufb01c and terminological developments within a \ufb01eld. In ge neral a domain-\nspeci\ufb01c thesaurus is required: general thesauri and dictio naries give far too\nlittle coverage of the rich domain-particular vocabularie s of most scienti\ufb01c\n\ufb01elds. However, query expansion may also signi\ufb01cantly decr ease precision,\nparticularly when the query contains ambiguous terms. For e xample, if the\nuser searches for interestrate , expanding the query to interestratefascinateeval-\nuate is unlikely to be useful. Overall, query expansion is less su ccessful than\nrelevance feedback, though it may be as good as pseudo releva nce feedback.\nIt does, however, have the advantage of being much more under standable to\nthe system user.\n?Exercise 9.7\nIfAis simply a Boolean cooccurrence matrix, then what do you get as the entries in\nC?\n9.3 References and further reading\nWork in information retrieval quickly confronted the probl em of variant ex-\npression which meant that the words in a query might not appea r in a doc-\nument, despite it being relevant to the query. An early exper iment about\n1960 cited by Swanson (1988 ) found that only 11 out of 23 documents prop-\nerly indexed under the subject toxicity had any use of a word containing the\nstem toxi. There is also the issue of translation, of users knowing wha t terms\na document will use. Blair and Maron (1985 ) conclude that \u201cit is impossibly\ndif\ufb01cult for users to predict the exact words, word combinat ions, and phrases\nthat are used by all (or most) relevant documents and only (or primarily) by\nthose documents\u201d.\nThe main initial papers on relevance feedback using vector s pace models\nall appear in Salton (1971b ), including the presentation of the Rocchio al-\ngorithm ( Rocchio 1971 ) and the Ide dec-hi variant along with evaluation of\nseveral variants ( Ide 1971 ). Another variant is to regard alldocuments in\nthe collection apart from those judged relevant as nonrelev ant, rather than\nonly ones that are explicitly judged nonrelevant. However, Sch\u00fctze et al.\n(1995 ) and Singhal et al. (1997 ) show that better results are obtained for rout-\ning by using only documents close to the query of interest rat her than all\nOnline edition (c)\n2009 Cambridge UP194 9 Relevance feedback and query expansion\ndocuments. Other later work includes Salton and Buckley (1990 ),Riezler\net al. (2007 ) (a statistical NLP approach to RF) and the recent survey pap er\nRuthven and Lalmas (2003 ).\nThe effectiveness of interactive relevance feedback syste ms is discussed in\n(Salton 1989 ,Harman 1992 ,Buckley et al. 1994b ).Koenemann and Belkin\n(1996 ) do user studies of the effectiveness of relevance feedback .\nTraditionally Roget\u2019s thesaurus has been the best known Eng lish language\nthesaurus ( Roget 1946 ). In recent computational work, people almost always\nuse WordNet ( Fellbaum 1998 ), not only because it is free, but also because of\nits rich link structure. It is available at: http://wordnet.princeton.edu .\nQiu and Frei (1993 ) and Sch\u00fctze (1998 ) discuss automatic thesaurus gener-\nation. Xu and Croft (1996 ) explore using both local and global query expan-\nsion.\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 195\n10 XML retrieval\nInformation retrieval systems are often contrasted with re lational databases.\nTraditionally, IR systems have retrieved information from unstructured text\n\u2013 by which we mean \u201craw\u201d text without markup. Databases are de signed\nfor querying relational data : sets of records that have values for prede\ufb01ned\nattributes such as employee number, title and salary. There are fundamental\ndifferences between information retrieval and database sy stems in terms of\nretrieval model, data structures and query language as show n in Table 10.1.1\nSome highly structured text search problems are most ef\ufb01cie ntly handled\nby a relational database, for example, if the employee table contains an at-\ntribute for short textual job descriptions and you want to \ufb01n d all employees\nwho are involved with invoicing. In this case, the SQL query:\nselect lastname from employees where job_desc like \u2019invoic %\u2019;\nmay be suf\ufb01cient to satisfy your information need with high p recision and\nrecall.\nHowever, many structured data sources containing text are b est modeled\nas structured documents rather than relational data. We cal l the search over\nsuch structured documents structured retrieval . Queries in structured retrieval STRUCTURED\nRETRIEVAL can be either structured or unstructured, but we will assume in this chap-\nter that the collection consists only of structured documen ts. Applications\nof structured retrieval include digital libraries, patent databases, blogs, text\nin which entities like persons and locations have been tagge d (in a process\ncalled named entity tagging) and output from of\ufb01ce suites li ke OpenOf\ufb01ce\nthat save documents as marked up text. In all of these applica tions, we want\nto be able to run queries that combine textual criteria with s tructural criteria.\nExamples of such queries are givemeafull-lengtharticleonfastfouriertransforms\n(digital libraries), givemepatentswhoseclaimsmentionRSApublickeyencrypti on\n1. In most modern database systems, one can enable full-text search for text columns. This\nusually means that an inverted index is created and Boolean o r vector space search enabled,\neffectively combining core database with information retr ieval technologies.\nOnline edition (c)\n2009 Cambridge UP196 10 XML retrieval\nRDB search unstructured retrieval structured retrieval\nobjects records unstructured documents trees with text at leaves\nmodel relational model vector space & others ?\nmain data structure table inverted index ?\nqueries SQL free text queries ?\n\u25eeTable 10.1 RDB (relational database) search, unstructured informati on retrieval\nand structured information retrieval. There is no consensu s yet as to which methods\nwork best for structured retrieval although many researche rs believe that XQuery\n(page 215) will become the standard for structured queries.\nand that cite US patent 4,405,829 (patents), or give me articles about sightseeing\ntours of the Vatican and the Coliseum (entity-tagged text). These three queries\nare structured queries that cannot be answered well by an unr anked retrieval\nsystem. As we argued in Example 1.1(page 15) unranked retrieval models\nlike the Boolean model suffer from low recall. For instance, an unranked\nsystem would return a potentially large number of articles t hat mention the\nVatican, the Coliseum and sightseeing tours without rankin g the ones that\nare most relevant for the query \ufb01rst. Most users are also noto riously bad at\nprecisely stating structural constraints. For instance, u sers may not know\nfor which structured elements the search system supports se arch. In our ex-\nample, the user may be unsure whether to issue the query as sightseeing AND\n(COUNTRY :Vatican OR LANDMARK :Coliseum) , assightseeing AND(STATE:Vatican OR\nBUILDING :Coliseum) or in some other form. Users may also be completely un-\nfamiliar with structured search and advanced search interf aces or unwilling\nto use them. In this chapter, we look at how ranked retrieval m ethods can be\nadapted to structured documents to address these problems.\nWe will only look at one standard for encoding structured doc uments: Ex-\ntensible Markup Language orXML , which is currently the most widely used XML\nsuch standard. We will not cover the speci\ufb01cs that distingui sh XML from\nother types of markup such as HTML and SGML. But most of what we say\nin this chapter is applicable to markup languages in general .\nIn the context of information retrieval, we are only interes ted in XML as\na language for encoding text and documents. A perhaps more wi despread\nuse of XML is to encode non-text data. For example, we may want to export\ndata in XML format from an enterprise resource planning syst em and then\nread them into an analytics program to produce graphs for a pr esentation.\nThis type of application of XML is called data-centric because numerical and DATA -CENTRIC XML\nnon-text attribute-value data dominate and text is usually a small fraction of\nthe overall data. Most data-centric XML is stored in databas es \u2013 in contrast\nto the inverted index-based methods for text-centric XML th at we present in\nthis chapter.\nOnline edition (c)\n2009 Cambridge UP10.1 Basic XML concepts 197\nWe call XML retrieval structured retrieval in this chapter. Some researchers\nprefer the term semistructured retrieval to distinguish XML retrieval from database SEMISTRUCTURED\nRETRIEVAL querying. We have adopted the terminology that is widesprea d in the XML\nretrieval community. For instance, the standard way of refe rring to XML\nqueries is structured queries , not semistructured queries . The term structured\nretrieval is rarely used for database querying and it always refers to X ML\nretrieval in this book.\nThere is a second type of information retrieval problem that is intermediate\nbetween unstructured retrieval and querying a relational d atabase: paramet-\nric and zone search, which we discussed in Section 6.1(page 110). In the\ndata model of parametric and zone search, there are parametr ic \ufb01elds (re-\nlational attributes like date or\ufb01le-size ) and zones \u2013 text attributes that each\ntake a chunk of unstructured text as value, e.g., author and title in Figure 6.1\n(page 111). The data model is \ufb02at, that is, there is no nesting of attrib utes.\nThe number of attributes is small. In contrast, XML document s have the\nmore complex tree structure that we see in Figure 10.2 in which attributes\nare nested. The number of attributes and nodes is greater tha n in parametric\nand zone search.\nAfter presenting the basic concepts of XML in Section 10.1, this chapter\n\ufb01rst discusses the challenges we face in XML retrieval (Sect ion10.2). Next we\ndescribe a vector space model for XML retrieval (Section 10.3). Section 10.4\npresents INEX, a shared task evaluation that has been held fo r a number of\nyears and currently is the most important venue for XML retri eval research.\nWe discuss the differences between data-centric and text-c entric approaches\nto XML in Section 10.5.\n10.1 Basic XML concepts\nAn XML document is an ordered, labeled tree. Each node of the t ree is an\nXML element and is written with an opening and closing tag. An element can XML ELEMENT\nhave one or more XML attributes . In the XML document in Figure 10.1, the XML ATTRIBUTE\nscene element is enclosed by the two tags <scene ...> and</scene> . It\nhas an attribute number with value viiand two child elements, titleand verse .\nFigure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of\ntext, e.g., Shakespeare ,Macbeth , andMacbeth\u2019s castle . The tree\u2019s internal nodes\nencode either the structure of the document ( title,act, and scene ) or metadata\nfunctions ( author ).\nThe standard for accessing and processing XML documents is t he XML\nDocument Object Model or DOM . The DOM represents elements, attributes XML DOM\nand text within elements as nodes in a tree. Figure 10.2 is a simpli\ufb01ed DOM\nrepresentation of the XML document in Figure 10.1.2With a DOM API, we\n2. The representation is simpli\ufb01ed in a number of respects. F or example, we do not show the\nOnline edition (c)\n2009 Cambridge UP198 10 XML retrieval\n<play>\n<author>Shakespeare</author>\n<title>Macbeth</title>\n<act number=\"I\">\n<scene number=\"vii\">\n<title>Macbeth\u2019s castle</title>\n<verse>Will I with wine and wassail ...</verse>\n</scene>\n</act>\n</play>\n\u25eeFigure 10.1 An XML document.\nroot element\nplay\nelement\nauthorelement\nactelement\ntitle\ntext\nShakespearetext\nMacbeth\nattribute\nnumber=\"I\"element\nscene\nattribute\nnumber=\"vii\"element\nverseelement\ntitle\ntext\nWill I with ...text\nMacbeth\u2019s castle\n\u25eeFigure 10.2 The XML document in Figure 10.1 as a simpli\ufb01ed DOM object.\nOnline edition (c)\n2009 Cambridge UP10.1 Basic XML concepts 199\n//article\n[.//yr = 2001 or .//yr = 2002]\n//section\n[about(.,summer holidays)]holidays summersectionarticle\n\u25eeFigure 10.3 An XML query in NEXI format and its partial representation as a tree.\ncan process an XML document by starting at the root element an d then de-\nscending down the tree from parents to children.\nXPath is a standard for enumerating paths in an XML document collec tion. XPATH\nWe will also refer to paths as XML contexts or simply contexts in this chapter. XML CONTEXT\nOnly a small subset of XPath is needed for our purposes. The XP ath expres-\nsionnode selects all nodes of that name. Successive elements of a path are\nseparated by slashes, so act/scene selects all scene elements whose par-\nent is an actelement. Double slashes indicate that an arbitrary number o f\nelements can intervene on a path: play//scene selects all scene elements\noccurring in a play element. In Figure 10.2 this set consists of a single scene el-\nement, which is accessible via the path play,act,scene from the top. An initial\nslash starts the path at the root element. /play/title selects the play\u2019s ti-\ntle in Figure 10.1,/play//title selects a set with two members (the play\u2019s\ntitle and the scene\u2019s title), and /scene/title selects no elements. For no-\ntational convenience, we allow the \ufb01nal element of a path to b e a vocabulary\nterm and separate it from the element path by the symbol #, eve n though this\ndoes not conform to the XPath standard. For example, title#\"Macbeth\"\nselects all titles containing the term Macbeth .\nWe also need the concept of schema in this chapter. A schema puts con- SCHEMA\nstraints on the structure of allowable XML documents for a pa rticular ap-\nplication. A schema for Shakespeare\u2019s plays may stipulate t hat scenes can\nonly occur as children of acts and that only acts and scenes ha ve the num-\nberattribute. Two standards for schemas for XML documents are XML DTD XML DTD\n(document type de\ufb01nition) and XML Schema . Users can only write structured XML S CHEMA\nqueries for an XML retrieval system if they have some minimal knowledge\nabout the schema of the collection.\nrootnode and text is not embedded in textnodes. See http://www.w3.org/DOM/ .\nOnline edition (c)\n2009 Cambridge UP200 10 XML retrieval\nM\u2019scastletitle\nWillI ...versescene\nJuliusCaesartitlebook\nGallicwartitle\nJuliusCaesarauthorbook\nd1q1 q2\n\u25eeFigure 10.4 Tree representation of XML documents and queries.\nA common format for XML queries is NEXI (Narrowed Extended XPath NEXI\nI). We give an example in Figure 10.3. We display the query on four lines for\ntypographical convenience, but it is intended to be read as o ne unit without\nline breaks. In particular, //section is embedded under //article .\nThe query in Figure 10.3 speci\ufb01es a search for sections about the sum-\nmer holidays that are part of articles from 2001 or 2002. As in XPath dou-\nble slashes indicate that an arbitrary number of elements ca n intervene on\na path. The dot in a clause in square brackets refers to the ele ment the\nclause modi\ufb01es. The clause [.//yr = 2001 or .//yr = 2002] mod-\ni\ufb01es//article . Thus, the dot refers to //article in this case. Similarly,\nthe dot in [about(., summer holidays)] refers to the section that the\nclause modi\ufb01es.\nThe two yrconditions are relational attribute constraints. Only art icles\nwhose yrattribute is 2001 or 2002 (or that contain an element whose yr\nattribute is 2001 or 2002) are to be considered. The about clause is a ranking\nconstraint: Sections that occur in the right type of article are to be ranked\naccording to how relevant they are to the topic summerholidays .\nWe usually handle relational attribute constraints by pre\ufb01 ltering or post-\n\ufb01ltering: We simply exclude all elements from the result set that do not meet\nthe relational attribute constraints. In this chapter, we w ill not address how\nto do this ef\ufb01ciently and instead focus on the core informati on retrieval prob-\nlem in XML retrieval, namely how to rank documents according to the rele-\nvance criteria expressed in the about conditions of the NEXI query.\nIf we discard relational attributes, we can represent docum ents as trees\nwith only one type of node: element nodes. In other words, we r emove\nall attribute nodes from the XML document, such as the number attribute in\nFigure 10.1. Figure 10.4 shows a subtree of the document in Figure 10.1 as an\nelement-node tree (labeled d1).\nOnline edition (c)\n2009 Cambridge UP10.2 Challenges in XML retrieval 201\nWe can represent queries as trees in the same way. This is a que ry-by-\nexample approach to query language design because users pos e queries by\ncreating objects that satisfy the same formal description a s documents. In\nFigure 10.4,q1is a search for books whose titles score highly for the keywor ds\nJuliusCaesar .q2is a search for books whose author elements score highly for\nJulius Caesar and whose title elements score highly for Gallic war .3\n10.2 Challenges in XML retrieval\nIn this section, we discuss a number of challenges that make s tructured re-\ntrieval more dif\ufb01cult than unstructured retrieval. Recall from page 195the\nbasic setting we assume in structured retrieval: the collec tion consists of\nstructured documents and queries are either structured (as in Figure 10.3)\nor unstructured (e.g., summerholidays ).\nThe \ufb01rst challenge in structured retrieval is that users wan t us to return\nparts of documents (i.e., XML elements), not entire documen ts as IR systems\nusually do in unstructured retrieval. If we query Shakespea re\u2019s plays for\nMacbeth\u2019s castle , should we return the scene, the act or the entire play in Fig-\nure10.2? In this case, the user is probably looking for the scene. On t he other\nhand, an otherwise unspeci\ufb01ed search for Macbeth should return the play of\nthis name, not a subunit.\nOne criterion for selecting the most appropriate part of a do cument is the\nstructured document retrieval principle : STRUCTURED\nDOCUMENT RETRIEVAL\nPRINCIPLE Structured document retrieval principle. A system should always re-\ntrieve the most speci\ufb01c part of a document answering the quer y.\nThis principle motivates a retrieval strategy that returns the smallest unit\nthat contains the information sought, but does not go below t his level. How-\never, it can be hard to implement this principle algorithmic ally. Consider the\nquerytitle#\"Macbeth\" applied to Figure 10.2. The title of the tragedy,\nMacbeth , and the title of Act I, Scene vii, Macbeth\u2019s castle , are both good hits\nbecause they contain the matching term Macbeth . But in this case, the title of\nthe tragedy, the higher node, is preferred. Deciding which l evel of the tree is\nright for answering a query is dif\ufb01cult.\nParallel to the issue of which parts of a document to return to the user is\nthe issue of which parts of a document to index. In Section 2.1.2 (page 20), we\ndiscussed the need for a document unit or indexing unit in indexing and re- INDEXING UNIT\ntrieval. In unstructured retrieval, it is usually clear wha t the right document\n3. To represent the semantics of NEXI queries fully we would a lso need to designate one node\nin the tree as a \u201ctarget node\u201d, for example, the section in the tree in Figure 10.3. Without the\ndesignation of a target node, the tree in Figure 10.3 is not a search for sections embedded in\narticles (as speci\ufb01ed by NEXI), but a search for articles tha t contain sections.\nOnline edition (c)\n2009 Cambridge UP202 10 XML retrieval\n\u25eeFigure 10.5 Partitioning an XML document into non-overlapping indexin g units.\nunit is: \ufb01les on your desktop, email messages, web pages on th e web etc. In\nstructured retrieval, there are a number of different appro aches to de\ufb01ning\nthe indexing unit.\nOne approach is to group nodes into non-overlapping pseudod ocuments\nas shown in Figure 10.5. In the example, books, chapters and sections have\nbeen designated to be indexing units, but without overlap. F or example, the\nleftmost dashed indexing unit contains only those parts of t he tree domi-\nnated by book that are not already part of other indexing units. The disad-\nvantage of this approach is that pseudodocuments may not mak e sense to\nthe user because they are not coherent units. For instance, t he leftmost in-\ndexing unit in Figure 10.5 merges three disparate elements, the class ,author\nand titleelements.\nWe can also use one of the largest elements as the indexing uni t, for exam-\nple, the book element in a collection of books or the play element for Shake-\nspeare\u2019s works. We can then postprocess search results to \ufb01n d for each book\nor play the subelement that is the best hit. For example, the q ueryMacbeth\u2019s\ncastle may return the play Macbeth , which we can then postprocess to identify\nact I, scene vii as the best-matching subelement. Unfortuna tely, this two-\nstage retrieval process fails to return the best subelement for many queries\nbecause the relevance of a whole book is often not a good predi ctor of the\nrelevance of small subelements within it.\nInstead of retrieving large units and identifying subeleme nts (top down),\nwe can also search all leaves, select the most relevant ones a nd then extend\nthem to larger units in postprocessing (bottom up). For the q ueryMacbeth\u2019s\ncastle in Figure 10.1, we would retrieve the title Macbeth\u2019s castle in the \ufb01rst\npass and then decide in a postprocessing step whether to retu rn the title, the\nscene, the act or the play. This approach has a similar proble m as the last one:\nThe relevance of a leaf element is often not a good predictor o f the relevance\nOnline edition (c)\n2009 Cambridge UP10.2 Challenges in XML retrieval 203\nof elements it is contained in.\nThe least restrictive approach is to index all elements. Thi s is also prob-\nlematic. Many XML elements are not meaningful search result s, e.g., typo-\ngraphical elements like <b>definitely</b> or an ISBN number which\ncannot be interpreted without context. Also, indexing all e lements means\nthat search results will be highly redundant. For the query Macbeth\u2019s castle\nand the document in Figure 10.1, we would return all of the play,act,scene\nand title elements on the path between the root node and Macbeth\u2019s castle .\nThe leaf node would then occur four times in the result set, on ce directly and\nthree times as part of other elements. We call elements that a re contained\nwithin each other nested . Returning redundant nested elements in a list of NESTED ELEMENTS\nreturned hits is not very user-friendly.\nBecause of the redundancy caused by nested elements it is com mon to re-\nstrict the set of elements that are eligible to be returned. R estriction strategies\ninclude:\n\u2022discard all small elements\n\u2022discard all element types that users do not look at (this requ ires a working\nXML retrieval system that logs this information)\n\u2022discard all element types that assessors generally do not ju dge to be rele-\nvant (if relevance assessments are available)\n\u2022only keep element types that a system designer or librarian h as deemed\nto be useful search results\nIn most of these approaches, result sets will still contain n ested elements.\nThus, we may want to remove some elements in a postprocessing step to re-\nduce redundancy. Alternatively, we can collapse several ne sted elements in\nthe results list and use highlighting of query terms to draw the user\u2019s atten-\ntion to the relevant passages. If query terms are highlighte d, then scanning a\nmedium-sized element (e.g., a section) takes little more ti me than scanning a\nsmall subelement (e.g., a paragraph). Thus, if the section a nd the paragraph\nboth occur in the results list, it is suf\ufb01cient to show the sec tion. An additional\nadvantage of this approach is that the paragraph is presente d together with\nits context (i.e., the embedding section). This context may be helpful in in-\nterpreting the paragraph (e.g., the source of the informati on reported) even\nif the paragraph on its own satis\ufb01es the query.\nIf the user knows the schema of the collection and is able to sp ecify the\ndesired type of element, then the problem of redundancy is al leviated as few\nnested elements have the same type. But as we discussed in the introduction,\nusers often don\u2019t know what the name of an element in the colle ction is (Is the\nVatican acountry or a city?) or they may not know how to compose structured\nqueries at all.\nOnline edition (c)\n2009 Cambridge UP204 10 XML retrieval\nGatesbook\nGatesauthorbook\nGatescreatorbook\nGateslastname\nBill\ufb01rstnameauthorbook\nq3 q4 d2 d3\n\u25eeFigure 10.6 Schema heterogeneity: intervening nodes and mismatched na mes.\nA challenge in XML retrieval related to nesting is that we may need to\ndistinguish different contexts of a term when we compute ter m statistics for\nranking, in particular inverse document frequency (idf) st atistics as de\ufb01ned\nin Section 6.2.1 (page 117). For example, the term Gates under the node author\nis unrelated to an occurrence under a content node like section if used to refer\nto the plural of gate. It makes little sense to compute a single document\nfrequency for Gates in this example.\nOne solution is to compute idf for XML-context/term pairs, e .g., to com-\npute different idf weights for author#\"Gates\" andsection#\"Gates\" .\nUnfortunately, this scheme will run into sparse data proble ms \u2013 that is, many\nXML-context pairs occur too rarely to reliably estimate df ( see Section 13.2,\npage 260, for a discussion of sparseness). A compromise is only to con -\nsider the parent node xof the term and not the rest of the path from the\nroot to xto distinguish contexts. There are still con\ufb02ations of cont exts that\nare harmful in this scheme. For instance, we do not distingui sh names of\nauthors and names of corporations if both have the parent nod ename . But\nmost important distinctions, like the example contrast author#\"Gates\" vs.\nsection#\"Gates\" , will be respected.\nIn many cases, several different XML schemas occur in a colle ction since\nthe XML documents in an IR application often come from more th an one\nsource. This phenomenon is called schema heterogeneity orschema diversity SCHEMA\nHETEROGENEITY and presents yet another challenge. As illustrated in Figur e10.6 comparable\nelements may have different names: creator ind2vs.author ind3. In other\ncases, the structural organization of the schemas may be dif ferent: Author\nOnline edition (c)\n2009 Cambridge UP10.2 Challenges in XML retrieval 205\nnames are direct descendants of the node author inq3, but there are the in-\ntervening nodes \ufb01rstname and lastname ind3. If we employ strict matching\nof trees, then q3will retrieve neither d2nord3although both documents are\nrelevant. Some form of approximate matching of element name s in combina-\ntion with semi-automatic matching of different document st ructures can help\nhere. Human editing of correspondences of elements in diffe rent schemas\nwill usually do better than automatic methods.\nSchema heterogeneity is one reason for query-document mism atches like\nq3/d2and q3/d3. Another reason is that users often are not familiar with the\nelement names and the structure of the schemas of collection s they search\nas mentioned. This poses a challenge for interface design in XML retrieval.\nIdeally, the user interface should expose the tree structur e of the collection\nand allow users to specify the elements they are querying. If we take this\napproach, then designing the query interface in structured retrieval is more\ncomplex than a search box for keyword queries in unstructure d retrieval.\nWe can also support the user by interpreting all parent-chil d relationships\nin queries as descendant relationships with any number of in tervening nodes\nallowed. We call such queries extended queries . The tree in Figure 10.3 and q4EXTENDED QUERY\nin Figure 10.6 are examples of extended queries. We show edges that are\ninterpreted as descendant relationships as dashed arrows. Inq4, a dashed\narrow connects book andGates . As a pseudo-XPath notation for q4, we adopt\nbook//#\"Gates\" : a book that somewhere in its structure contains the word\nGates where the path from the book node to Gates can be arbitrarily long.\nThe pseudo-XPath notation for the extended query that in add ition speci\ufb01es\nthatGates occurs in a section of the book isbook//section//#\"Gates\" .\nIt is convenient for users to be able to issue such extended qu eries without\nhaving to specify the exact structural con\ufb01guration in whic h a query term\nshould occur \u2013 either because they do not care about the exact con\ufb01guration\nor because they do not know enough about the schema of the coll ection to be\nable to specify it.\nIn Figure 10.7, the user is looking for a chapter entitled FFT (q5). Sup-\npose there is no such chapter in the collection, but that ther e are references to\nbooks on FFT ( d4). A reference to a book on FFT is not exactly what the user\nis looking for, but it is better than returning nothing. Exte nded queries do not\nhelp here. The extended query q6also returns nothing. This is a case where\nwe may want to interpret the structural constraints speci\ufb01e d in the query as\nhints as opposed to as strict conditions. As we will discuss i n Section 10.4,\nusers prefer a relaxed interpretation of structural constr aints: Elements that\ndo not meet structural constraints perfectly should be rank ed lower, but they\nshould not be omitted from search results.\nOnline edition (c)\n2009 Cambridge UP206 10 XML retrieval\nFFTtitlechapter\nFFTtitlechapter\nFFTtitle\nencryptiontitlereferences chapterbook\nq5 q6 d4\n\u25eeFigure 10.7 A structural mismatch between two queries and a document.\n10.3 A vector space model for XML retrieval\nIn this section, we present a simple vector space model for XM L retrieval.\nIt is not intended to be a complete description of a state-of- the-art system.\nInstead, we want to give the reader a \ufb02avor of how documents ca n be repre-\nsented and retrieved in XML retrieval.\nTo take account of structure in retrieval in Figure 10.4, we want a book\nentitled Julius Caesar to be a match for q1and no match (or a lower weighted\nmatch) for q2. In unstructured retrieval, there would be a single dimensi on\nof the vector space for Caesar . In XML retrieval, we must separate the title\nwordCaesar from the author name Caesar . One way of doing this is to have\neach dimension of the vector space encode a word together wit h its position\nwithin the XML tree.\nFigure 10.8 illustrates this representation. We \ufb01rst take each text nod e\n(which in our setup is always a leaf) and break it into multipl e nodes, one for\neach word. So the leaf node Bill Gates is split into two leaves BillandGates .\nNext we de\ufb01ne the dimensions of the vector space to be lexicalized subtrees\nof documents \u2013 subtrees that contain at least one vocabulary term. A sub-\nset of these possible lexicalized subtrees is shown in the \ufb01g ure, but there are\nothers \u2013 e.g., the subtree corresponding to the whole docume nt with the leaf\nnodeGates removed. We can now represent queries and documents as vec-\ntors in this space of lexicalized subtrees and compute match es between them.\nThis means that we can use the vector space formalism from Cha pter 6for\nXML retrieval. The main difference is that the dimensions of vector space\nOnline edition (c)\n2009 Cambridge UP10.3 A vector space model for XML retrieval 207\n\u25eeFigure 10.8 A mapping of an XML document (left) to a set of lexicalized sub trees\n(right).\nin unstructured retrieval are vocabulary terms whereas the y are lexicalized\nsubtrees in XML retrieval.\nThere is a tradeoff between the dimensionality of the space a nd accuracy\nof query results. If we trivially restrict dimensions to voc abulary terms, then\nwe have a standard vector space retrieval system that will re trieve many\ndocuments that do not match the structure of the query (e.g., Gates in the\ntitle as opposed to the author element). If we create a separa te dimension\nfor each lexicalized subtree occurring in the collection, t he dimensionality of\nthe space becomes too large. A compromise is to index all path s that end\nin a single vocabulary term, in other words, all XML-context /term pairs.\nWe call such an XML-context/term pair a structural term and denote it by STRUCTURAL TERM\n/an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}ht: a pair of XML-context cand vocabulary term t. The document in\nFigure 10.8 has nine structural terms. Seven are shown (e.g., \"Bill\" and\nAuthor#\"Bill\" ) and two are not shown: /Book/Author#\"Bill\" and\n/Book/Author#\"Gates\" . The tree with the leaves BillandGates is a lexical-\nized subtree that is not a structural term. We use the previou sly introduced\npseudo-XPath notation for structural terms.\nAs we discussed in the last section users are bad at rememberi ng details\nabout the schema and at constructing queries that comply wit h the schema.\nWe will therefore interpret all queries as extended queries \u2013 that is, there can\nbe an arbitrary number of intervening nodes in the document f or any parent-\nchild node pair in the query. For example, we interpret q5in Figure 10.7 as\nq6.\nBut we still prefer documents that match the query structure closely by\nOnline edition (c)\n2009 Cambridge UP208 10 XML retrieval\ninserting fewer additional nodes. We ensure that retrieval results respect this\npreference by computing a weight for each match. A simple mea sure of the\nsimilarity of a path cqin a query and a path cdin a document is the following\ncontext resemblance function C R: CONTEXT\nRESEMBLANCE\nCR(cq,cd) =/braceleft\uf8ecigg1+|cq|\n1+|cd|ifcqmatches cd\n0 if cqdoes not match cd(10.1)\nwhere|cq|and|cd|are the number of nodes in the query path and document\npath, respectively, and cqmatches cdiff we can transform cqinto cdby in-\nserting additional nodes. Two examples from Figure 10.6 are C R(cq4,cd2) =\n3/4=0.75 and C R(cq4,cd3) = 3/5=0.6 where cq4,cd2and cd3are the rele-\nvant paths from top to leaf node in q4,d2and d3, respectively. The value of\nCR(cq,cd)is 1.0 if qand dare identical.\nThe \ufb01nal score for a document is computed as a variant of the co sine mea-\nsure (Equation ( 6.10), page 121), which we call S IMNOMERGE for reasons\nthat will become clear shortly. S IMNOMERGE is de\ufb01ned as follows:\nSIMNOMERGE(q,d) =\u2211\nck\u2208B\u2211\ncl\u2208BCR(ck,cl)\u2211\nt\u2208Vweight (q,t,ck)weight (d,t,cl)/radical\uf8ecig\n\u2211c\u2208B,t\u2208Vweight2(d,t,c)(10.2)\nwhere Vis the vocabulary of non-structural terms; Bis the set of all XML con-\ntexts; and weight (q,t,c)and weight (d,t,c)are the weights of term tin XML\ncontext cin query qand document d, respectively. We compute the weights\nusing one of the weightings from Chapter 6, such as idf t\u00b7wft,d. The inverse\ndocument frequency idf tdepends on which elements we use to compute\ndftas discussed in Section 10.2. The similarity measure S IMNOMERGE(q,d)\nis not a true cosine measure since its value can be larger than 1.0 (Exer-\ncise 10.11 ). We divide by/radical\uf8ecig\n\u2211c\u2208B,t\u2208Vweight2(d,t,c)to normalize for doc-\nument length (Section 6.3.1 , page 121). We have omitted query length nor-\nmalization to simplify the formula. It has no effect on ranki ng since, for\na given query, the normalizer/radical\uf8ecig\n\u2211c\u2208B,t\u2208Vweight2(q,t,c)is the same for all\ndocuments.\nThe algorithm for computing S IMNOMERGE for all documents in the col-\nlection is shown in Figure 10.9. The array normalizer in Figure 10.9 contains/radical\uf8ecig\n\u2211c\u2208B,t\u2208Vweight2(d,t,c)from Equation ( 10.2) for each document.\nWe give an example of how S IMNOMERGE computes query-document\nsimilarities in Figure 10.10 ./an}\u230ara\u230bketle{tc1,t/an}\u230ara\u230bketri}htis one of the structural terms in the query.\nWe successively retrieve all postings lists for structural terms/an}\u230ara\u230bketle{tc\u2032,t/an}\u230ara\u230bketri}htwith the\nsame vocabulary term t. Three example postings lists are shown. For the\n\ufb01rst one, we have C R(c1,c1) = 1.0 since the two contexts are identical. The\nOnline edition (c)\n2009 Cambridge UP10.3 A vector space model for XML retrieval 209\nSCORE DOCUMENTS WITHSIMNOMERGE(q,B,V,N,normalizer )\n1forn\u21901toN\n2doscore[n]\u21900\n3for each/an}\u230ara\u230bketle{tcq,t/an}\u230ara\u230bketri}ht\u2208q\n4dowq\u2190WEIGHT (q,t,cq)\n5 for each c\u2208B\n6 do if CR(cq,c)>0\n7 then postings\u2190GETPOSTINGS (/an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}ht)\n8 for each posting\u2208postings\n9 dox\u2190CR(cq,c)\u2217wq\u2217weight (posting )\n10 score[docID (posting )] += x\n11 forn\u21901toN\n12 doscore[n]\u2190score[n]/normalizer [n]\n13 return score\n\u25eeFigure 10.9 The algorithm for scoring documents with S IMNOMERGE .\nquery\n/an}\u230ara\u230bketle{tc1,t/an}\u230ara\u230bketri}ht\nCR(c1,c1)=1.0\nCR(c1,c2)=0\nCR(c1,c3)=0.63inverted index\n/an}\u230ara\u230bketle{tc1,t/an}\u230ara\u230bketri}ht\u2212\u2192/an}\u230ara\u230bketle{td1, 0.5/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td4, 0.1/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td9, 0.2/an}\u230ara\u230bketri}ht. . .\n/an}\u230ara\u230bketle{tc2,t/an}\u230ara\u230bketri}ht\u2212\u2192/an}\u230ara\u230bketle{td2, 0.25/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td3, 0.1/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td12, 0.9/an}\u230ara\u230bketri}ht. . .\n/an}\u230ara\u230bketle{tc3,t/an}\u230ara\u230bketri}ht\u2212\u2192/an}\u230ara\u230bketle{td3, 0.7/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td6, 0.8/an}\u230ara\u230bketri}ht/an}\u230ara\u230bketle{td9, 0.6/an}\u230ara\u230bketri}ht. . .\n\u25eeFigure 10.10 Scoring of a query with one structural term in S IMNOMERGE .\nnext context has no context resemblance with c1: CR(c1,c2) =0 and the cor-\nresponding postings list is ignored. The context match of c1with c3is 0.63>0\nand it will be processed. In this example, the highest rankin g document is d9\nwith a similarity of 1.0 \u00d70.2+0.63\u00d70.6=0.578. To simplify the \ufb01gure, the\nquery weight of/an}\u230ara\u230bketle{tc1,t/an}\u230ara\u230bketri}htis assumed to be 1.0.\nThe query-document similarity function in Figure 10.9 is called S IMNOMERGE\nbecause different XML contexts are kept separate for the pur pose of weight-\ning. An alternative similarity function is S IMMERGE which relaxes the match-\ning conditions of query and document further in the followin g three ways.\nOnline edition (c)\n2009 Cambridge UP210 10 XML retrieval\n\u2022We collect the statistics used for computing weight (q,t,c)and weight (d,t,c)\nfrom allcontexts that have a non-zero resemblance to c(as opposed to just\nfrom cas in S IMNOMERGE ). For instance, for computing the document\nfrequency of the structural term atl#\"recognition\" , we also count\noccurrences of recognition in XML contexts fm/atl ,article//atl etc.\n\u2022We modify Equation ( 10.2) by merging all structural terms in the docu-\nment that have a non-zero context resemblance to a given quer y struc-\ntural term. For example, the contexts /play/act/scene/title and\n/play/title in the document will be merged when matching against\nthe query term /play/title#\"Macbeth\" .\n\u2022The context resemblance function is further relaxed: Conte xts have a non-\nzero resemblance in many cases where the de\ufb01nition of C Rin Equation ( 10.1)\nreturns 0.\nSee the references in Section 10.6 for details.\nThese three changes alleviate the problem of sparse term sta tistics dis-\ncussed in Section 10.2 and increase the robustness of the matching function\nagainst poorly posed structural queries. The evaluation of SIMNOMERGE\nand S IMMERGE in the next section shows that the relaxed matching condi-\ntions of S IMMERGE increase the effectiveness of XML retrieval.\n?Exercise 10.1\nConsider computing df for a structural term as the number of t imes that the structural\nterm occurs under a particular parent node. Assume the follo wing: the structural\nterm/an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}ht=author#\"Herbert\" occurs once as the child of the node squib ; there are\n10squib nodes in the collection; /an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}htoccurs 1000 times as the child of article ; there are\n1,000,000 article nodes in the collection. The idf weight of /an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}htthen is log210/1\u22483.3\nwhen occurring as the child of squib and log21,000,000/1000\u224810.0 when occurring\nas the child of article . (i) Explain why this is not an appropriate weighting for /an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}ht.\nWhy should/an}\u230ara\u230bketle{tc,t/an}\u230ara\u230bketri}htnot receive a weight that is three times higher in articles th an in\nsquibs? (ii) Suggest a better way of computing idf.\nExercise 10.2\nWrite down all the structural terms occurring in the XML docu ment in Figure 10.8.\nExercise 10.3\nHow many structural terms does the document in Figure 10.1 yield?\n10.4 Evaluation of XML retrieval\nThe premier venue for research on XML retrieval is the INEX (INitiative for INEX\ntheEvaluation of XML retrieval ) program, a collaborative effort that has pro-\nduced reference collections, sets of queries, and relevanc e judgments. A\nyearly INEX meeting is held to present and discuss research r esults. The\nOnline edition (c)\n2009 Cambridge UP10.4 Evaluation of XML retrieval 211\n12,107 number of documents\n494 MB size\n1995\u20132002 time of publication of articles\n1,532 average number of XML nodes per document\n6.9 average depth of a node\n30 number of CAS topics\n30 number of CO topics\n\u25eeTable 10.2 INEX 2002 collection statistics.\nIEEE Transac-\ntion on Pat-\ntern Analysisjournal title\nActivity\nrecognitionarticle title\nThis work fo-\ncuses on . . .paragraph\nIntroductiontitlefront matter sectionbodyarticle\n\u25eeFigure 10.11 Simpli\ufb01ed schema of the documents in the INEX collection.\nINEX 2002 collection consisted of about 12,000 articles fro m IEEE journals.\nWe give collection statistics in Table 10.2 and show part of the schema of\nthe collection in Figure 10.11 . The IEEE journal collection was expanded in\n2005. Since 2006 INEX uses the much larger English Wikipedia as a test col-\nlection. The relevance of documents is judged by human asses sors using the\nmethodology introduced in Section 8.1(page 152), appropriately modi\ufb01ed\nfor structured documents as we will discuss shortly.\nTwo types of information needs or topics in INEX are content- only or CO\ntopics and content-and-structure (CAS) topics. CO topics are regular key- CO TOPICS\nword queries as in unstructured information retrieval. CAS topics have struc- CAS TOPICS\ntural constraints in addition to keywords. We already encou ntered an exam-\nOnline edition (c)\n2009 Cambridge UP212 10 XML retrieval\nple of a CAS topic in Figure 10.3. The keywords in this case are summer and\nholidays and the structural constraints specify that the keywords oc cur in a\nsection that in turn is part of an article and that this articl e has an embedded\nyear attribute with value 2001 or 2002.\nSince CAS queries have both structural and content criteria , relevance as-\nsessments are more complicated than in unstructured retrie val. INEX 2002\nde\ufb01ned component coverage and topical relevance as orthogo nal dimen-\nsions of relevance. The component coverage dimension evaluates whether the COMPONENT\nCOVERAGE element retrieved is \u201cstructurally\u201d correct, i.e., neithe r too low nor too high\nin the tree. We distinguish four cases:\n\u2022Exact coverage (E). The information sought is the main topic of the com-\nponent and the component is a meaningful unit of information .\n\u2022Too small (S). The information sought is the main topic of the component,\nbut the component is not a meaningful (self-contained) unit of informa-\ntion.\n\u2022Too large (L). The information sought is present in the compo nent, but is\nnot the main topic.\n\u2022No coverage (N). The information sought is not a topic of the c omponent.\nThe topical relevance dimension also has four levels: highly relevant (3), TOPICAL RELEVANCE\nfairly relevant (2), marginally relevant (1) and nonreleva nt (0). Components\nare judged on both dimensions and the judgments are then comb ined into\na digit-letter code. 2S is a fairly relevant component that i s too small and\n3E is a highly relevant component that has exact coverage. In theory, there\nare 16 combinations of coverage and relevance, but many cann ot occur. For\nexample, a nonrelevant component cannot have exact coverag e, so the com-\nbination 3N is not possible.\nThe relevance-coverage combinations are quantized as foll ows:\nQ(rel,cov) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f31.00 if (rel,cov) =3E\n0.75 if (rel,cov)\u2208{2E, 3L}\n0.50 if (rel,cov)\u2208{1E, 2L, 2S}\n0.25 if (rel,cov)\u2208{1S, 1L}\n0.00 if (rel,cov) =0N\nThis evaluation scheme takes account of the fact that binary relevance judg-\nments, which are standard in unstructured information retr ieval (Section 8.5.1 ,\npage 166), are not appropriate for XML retrieval. A 2S component prov ides\nincomplete information and may be dif\ufb01cult to interpret wit hout more con-\ntext, but it does answer the query partially. The quantizati on function Q\ndoes not impose a binary choice relevant/nonrelevant and in stead allows us\nto grade the component as partially relevant.\nOnline edition (c)\n2009 Cambridge UP10.4 Evaluation of XML retrieval 213\nalgorithm average precision\nSIMNOMERGE 0.242\nSIMMERGE 0.271\n\u25eeTable 10.3 INEX 2002 results of the vector space model in Section 10.3 for content-\nand-structure (CAS) queries and the quantization function Q.\nThe number of relevant components in a retrieved set Aof components\ncan then be computed as:\n#(relevant items retrieved ) =\u2211\nc\u2208AQ(rel(c),cov(c))\nAs an approximation, the standard de\ufb01nitions of precision, recall and F from\nChapter 8can be applied to this modi\ufb01ed de\ufb01nition of relevant items re -\ntrieved, with some subtleties because we sum graded as oppos ed to binary\nrelevance assessments. See the references on focused retri eval in Section 10.6\nfor further discussion.\nOne \ufb02aw of measuring relevance this way is that overlap is not accounted\nfor. We discussed the concept of marginal relevance in the co ntext of un-\nstructured retrieval in Section 8.5.1 (page 166). This problem is worse in\nXML retrieval because of the problem of multiple nested elem ents occur-\nring in a search result as we discussed on page 203. Much of the recent focus\nat INEX has been on developing algorithms and evaluation mea sures that\nreturn non-redundant results lists and evaluate them prope rly. See the refer-\nences in Section 10.6.\nTable 10.3 shows two INEX 2002 runs of the vector space system we de-\nscribed in Section 10.3. The better run is the S IMMERGE run, which incor-\nporates few structural constraints and mostly relies on key word matching.\nSIMMERGE \u2019s median average precision (where the median is with respec t to\naverage precision numbers over topics) is only 0.147. Effec tiveness in XML\nretrieval is often lower than in unstructured retrieval sin ce XML retrieval is\nharder. Instead of just \ufb01nding a document, we have to \ufb01nd the s ubpart of a\ndocument that is most relevant to the query. Also, XML retrie val effective-\nness \u2013 when evaluated as described here \u2013 can be lower than uns tructured\nretrieval effectiveness on a standard evaluation because g raded judgments\nlower measured performance. Consider a system that returns a document\nwith graded relevance 0.6 and binary relevance 1 at the top of the retrieved\nlist. Then, interpolated precision at 0.00 recall (cf. page 158) is 1.0 on a binary\nevaluation, but can be as low as 0.6 on a graded evaluation.\nTable 10.3 gives us a sense of the typical performance of XML retrieval,\nbut it does not compare structured with unstructured retrie val. Table 10.4\ndirectly shows the effect of using structure in retrieval. T he results are for a\nOnline edition (c)\n2009 Cambridge UP214 10 XML retrieval\ncontent only full structure improvement\nprecision at 5 0.2000 0.3265 63.3%\nprecision at 10 0.1820 0.2531 39.1%\nprecision at 20 0.1700 0.1796 5.6%\nprecision at 30 0.1527 0.1531 0.3%\n\u25eeTable 10.4 A comparison of content-only and full-structure search in I NEX\n2003/2004.\nlanguage-model-based system (cf. Chapter 12) that is evaluated on a subset\nof CAS topics from INEX 2003 and 2004. The evaluation metric i s precision\natkas de\ufb01ned in Chapter 8(page 161). The discretization function used for\nthe evaluation maps highly relevant elements (roughly corr esponding to the\n3E elements de\ufb01ned for Q) to 1 and all other elements to 0. The content-\nonly system treats queries and documents as unstructured ba gs of words.\nThe full-structure model ranks elements that satisfy struc tural constraints\nhigher than elements that do not. For instance, for the query in Figure 10.3\nan element that contains the phrase summer holidays in a section will be rated\nhigher than one that contains it in an abstract .\nThe table shows that structure helps increase precision at t he top of the\nresults list. There is a large increase of precision at k=5 and at k=10. There\nis almost no improvement at k=30. These results demonstrate the bene\ufb01ts\nof structured retrieval. Structured retrieval imposes add itional constraints on\nwhat to return and documents that pass the structural \ufb01lter a re more likely\nto be relevant. Recall may suffer because some relevant docu ments will be\n\ufb01ltered out, but for precision-oriented tasks structured r etrieval is superior.\n10.5 Text-centric vs. data-centric XML retrieval\nIn the type of structured retrieval we cover in this chapter, XML structure\nserves as a framework within which we match the text of the que ry with the\ntext of the XML documents. This exempli\ufb01es a system that is op timized for\ntext-centric XML . While both text and structure are important, we give higher TEXT -CENTRIC XML\npriority to text. We do this by adapting unstructured retrie val methods to\nhandling additional structural constraints. The premise o f our approach is\nthat XML document retrieval is characterized by (i) long tex t \ufb01elds (e.g., sec-\ntions of a document), (ii) inexact matching, and (iii) relev ance-ranked results.\nRelational databases do not deal well with this use case.\nIn contrast, data-centric XML mainly encodes numerical and non-text attribute- DATA -CENTRIC XML\nvalue data. When querying data-centric XML, we want to impos e exact\nmatch conditions in most cases. This puts the emphasis on the structural\naspects of XML documents and queries. An example is:\nOnline edition (c)\n2009 Cambridge UP10.5 Text-centric vs. data-centric XML retrieval 215\nFind employees whose salary is the same this month as it was 12 months\nago.\nThis query requires no ranking. It is purely structural and a n exact matching\nof the salaries in the two time periods is probably suf\ufb01cient to meet the user\u2019s\ninformation need.\nText-centric approaches are appropriate for data that are e ssentially text\ndocuments, marked up as XML to capture document structure. T his is be-\ncoming a de facto standard for publishing text databases sin ce most text\ndocuments have some form of interesting structure \u2013 paragra phs, sections,\nfootnotes etc. Examples include assembly manuals, issues o f journals, Shake-\nspeare\u2019s collected works and newswire articles.\nData-centric approaches are commonly used for data collect ions with com-\nplex structures that mainly contain non-text data. A text-c entric retrieval\nengine will have a hard time with proteomic data in bioinform atics or with\nthe representation of a city map that (together with street n ames and other\ntextual descriptions) forms a navigational database.\nTwo other types of queries that are dif\ufb01cult to handle in a tex t-centric struc-\ntured retrieval model are joins and ordering constraints. T he query for em-\nployees with unchanged salary requires a join. The followin g query imposes\nan ordering constraint:\nRetrieve the chapter of the book Introduction to algorithms that follows\nthe chapter Binomial heaps .\nThis query relies on the ordering of elements in XML \u2013 in this c ase the order-\ning of chapter elements underneath the book node. There are p owerful query\nlanguages for XML that can handle numerical attributes, joi ns and ordering\nconstraints. The best known of these is XQuery, a language pr oposed for\nstandardization by the W3C. It is designed to be broadly appl icable in all ar-\neas where XML is used. Due to its complexity, it is challengin g to implement\nan XQuery-based ranked retrieval system with the performan ce characteris-\ntics that users have come to expect in information retrieval . This is currently\none of the most active areas of research in XML retrieval.\nRelational databases are better equipped to handle many str uctural con-\nstraints, particularly joins (but ordering is also dif\ufb01cul t in a database frame-\nwork \u2013 the tuples of a relation in the relational calculus are not ordered). For\nthis reason, most data-centric XML retrieval systems are ex tensions of rela-\ntional databases (see the references in Section 10.6). If text \ufb01elds are short,\nexact matching meets user needs and retrieval results in for m of unordered\nsets are acceptable, then using a relational database for XM L retrieval is ap-\npropriate.\nOnline edition (c)\n2009 Cambridge UP216 10 XML retrieval\n10.6 References and further reading\nThere are many good introductions to XML, including ( Harold and Means\n2004 ). Table 10.1 is inspired by a similar table in ( van Rijsbergen 1979 ). Sec-\ntion 10.4 follows the overview of INEX 2002 by G\u00f6vert and Kazai (2003 ),\npublished in the proceedings of the meeting ( Fuhr et al. 2003a ). The pro-\nceedings of the four following INEX meetings were published asFuhr et al.\n(2003b ),Fuhr et al. (2005 ),Fuhr et al. (2006 ), and Fuhr et al. (2007 ). An up-\ntodate overview article is Fuhr and Lalmas (2007 ). The results in Table 10.4\nare from ( Kamps et al. 2006 ).Chu-Carroll et al. (2006 ) also present evidence\nthat XML queries increase precision compared with unstruct ured queries.\nInstead of coverage and relevance, INEX now evaluates on the related but\ndifferent dimensions of exhaustivity and speci\ufb01city ( Lalmas and Tombros\n2007 ).Trotman et al. (2006 ) relate the tasks investigated at INEX to real world\nuses of structured retrieval such as structured book search on internet book-\nstore sites.\nThe structured document retrieval principle is due to Chiaramella et al.\n(1996 ). Figure 10.5 is from ( Fuhr and Gro\u00dfjohann 2004 ).Rahm and Bernstein\n(2001 ) give a survey of automatic schema matching that is applicab le to XML.\nThe vector-space based XML retrieval method in Section 10.3 is essentially\nIBM Haifa\u2019s JuruXML system as presented by Mass et al. (2003 ) and Carmel\net al. (2003 ).Schlieder and Meuss (2002 ) and Grabs and Schek (2002 ) describe\nsimilar approaches. Carmel et al. (2003 ) represent queries as XML fragments . XML FRAGMENT\nThe trees that represent XML queries in this chapter are all X ML fragments,\nbut XML fragments also permit the operators +,\u2212and phrase on content\nnodes.\nWe chose to present the vector space model for XML retrieval b ecause it\nis simple and a natural extension of the unstructured vector space model\nin Chapter 6. But many other unstructured retrieval methods have been\napplied to XML retrieval with at least as much success as the v ector space\nmodel. These methods include language models (cf. Chapter 12, e.g., Kamps\net al. (2004 ),List et al. (2005 ),Ogilvie and Callan (2005 )), systems that use\na relational database as a backend ( Mihajlovi\u00b4 c et al. 2005 ,Theobald et al.\n2005 ;2008 ), probabilistic weighting ( Lu et al. 2007 ), and fusion ( Larson 2005 ).\nThere is currently no consensus as to what the best approach t o XML retrieval\nis.\nMost early work on XML retrieval accomplished relevance ran king by fo-\ncusing on individual terms, including their structural con texts, in query and\ndocument. As in unstructured information retrieval, there is a trend in more\nrecent work to model relevance ranking as combining evidenc e from dis-\nparate measurements about the query, the document and their match. The\ncombination function can be tuned manually ( Arvola et al. 2005 ,Sigurbj\u00f6rns-\nson et al. 2004 ) or trained using machine learning methods ( Vittaut and Gal-\nOnline edition (c)\n2009 Cambridge UP10.7 Exercises 217\nlinari (2006 ), cf. Section 15.4.1 , page 341).\nAn active area of XML retrieval research is focused retrieval (Trotman et al. FOCUSED RETRIEVAL\n2007 ), which aims to avoid returning nested elements that share o ne or more\ncommon subelements (cf. discussion in Section 10.2, page 203). There is ev-\nidence that users dislike redundancy caused by nested eleme nts ( Betsi et al.\n2006 ). Focused retrieval requires evaluation measures that pen alize redun-\ndant results lists ( Kazai and Lalmas 2006 ,Lalmas et al. 2007 ).Trotman and\nGeva (2006 ) argue that XML retrieval is a form of passage retrieval . In passage PASSAGE RETRIEVAL\nretrieval ( Salton et al. 1993 ,Hearst and Plaunt 1993 ,Zobel et al. 1995 ,Hearst\n1997 ,Kaszkiel and Zobel 1997 ), the retrieval system returns short passages\ninstead of documents in response to a user query. While eleme nt bound-\naries in XML documents are cues for identifying good segment boundaries\nbetween passages, the most relevant passage often does not c oincide with an\nXML element.\nIn the last several years, the query format at INEX has been th e NEXI stan-\ndard proposed by Trotman and Sigurbj\u00f6rnsson (2004 ). Figure 10.3 is from\ntheir paper. O\u2019Keefe and Trotman (2004 ) give evidence that users cannot reli-\nably distinguish the child and descendant axes. This justi\ufb01 es only permitting\ndescendant axes in NEXI (and XML fragments). These structur al constraints\nwere only treated as \u201chints\u201d in recent INEXes. Assessors can judge an ele-\nment highly relevant even though it violates one of the struc tural constraints\nspeci\ufb01ed in a NEXI query.\nAn alternative to structured query languages like NEXI is a m ore sophisti-\ncated user interface for query formulation ( Tannier and Geva 2005 ,van Zwol\net al. 2006 ,Woodley and Geva 2006 ).\nA broad overview of XML retrieval that covers database as wel l as IR ap-\nproaches is given by Amer-Yahia and Lalmas (2006 ) and an extensive refer-\nence list on the topic can be found in ( Amer-Yahia et al. 2005 ). Chapter 6\nofGrossman and Frieder (2004 ) is a good introduction to structured text re-\ntrieval from a database perspective. The proposed standard for XQuery is\navailable at http://www.w3.org/TR/xquery/ including an extension for full-text\nqueries ( Amer-Yahia et al. 2006 ):http://www.w3.org/TR/xquery-full-text/ . Work\nthat has looked at combining the relational database and the unstructured\ninformation retrieval approaches includes ( Fuhr and R\u00f6lleke 1997 ), (Navarro\nand Baeza-Yates 1997 ), (Cohen 1998 ), and ( Chaudhuri et al. 2006 ).\n10.7 Exercises\n?Exercise 10.4\nFind a reasonably sized XML document collection (or a collec tion using a markup lan-\nguage different from XML like HTML) on the web and download it . Jon Bosak\u2019s XML\nedition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or\nthe \ufb01rst 10,000 documents of the Wikipedia are good choices. Create three CAS topics\nOnline edition (c)\n2009 Cambridge UP218 10 XML retrieval\nof the type shown in Figure 10.3 that you would expect to do better than analogous\nCO topics. Explain why an XML retrieval system would be able t o exploit the XML\nstructure of the documents to achieve better retrieval resu lts on the topics than an\nunstructured retrieval system.\nExercise 10.5\nFor the collection and the topics in Exercise 10.4, (i) are there pairs of elements e1and\ne2, with e2a subelement of e1such that both answer one of the topics? Find one case\neach where (ii) e1(iii)e2is the better answer to the query.\nExercise 10.6\nImplement the (i) S IMMERGE (ii) S IMNOMERGE algorithm in Section 10.3 and run it\nfor the collection and the topics in Exercise 10.4. (iii) Evaluate the results by assigning\nbinary relevance judgments to the \ufb01rst \ufb01ve documents of the t hree retrieved lists for\neach algorithm. Which algorithm performs better?\nExercise 10.7\nAre all of the elements in Exercise 10.4 appropriate to be returned as hits to a user or\nare there elements (as in the example <b>definitely</b> on page 203) that you\nwould exclude?\nExercise 10.8\nWe discussed the tradeoff between accuracy of results and di mensionality of the vec-\ntor space on page 207. Give an example of an information need that we can answer\ncorrectly if we index all lexicalized subtrees, but cannot a nswer if we only index struc-\ntural terms.\nExercise 10.9\nIf we index all structural terms, what is the size of the index as a function of text size?\nExercise 10.10\nIf we index all lexicalized subtrees, what is the size of the i ndex as a function of text\nsize?\nExercise 10.11\nGive an example of a query-document pair for which S IMNOMERGE(q,d)is larger\nthan 1.0.\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 219\n11Probabilistic information\nretrieval\nDuring the discussion of relevance feedback in Section 9.1.2 , we observed\nthat if we have some known relevant and nonrelevant document s, then we\ncan straightforwardly start to estimate the probability of a term tappearing\nin a relevant document P(t|R=1), and that this could be the basis of a\nclassi\ufb01er that decides whether documents are relevant or no t. In this chapter,\nwe more systematically introduce this probabilistic appro ach to IR, which\nprovides a different formal basis for a retrieval model and r esults in different\ntechniques for setting term weights.\nUsers start with information needs , which they translate into query repre-\nsentations . Similarly, there are documents , which are converted into document\nrepresentations (the latter differing at least by how text is tokenized, but p er-\nhaps containing fundamentally less information, as when a n on-positional\nindex is used). Based on these two representations, a system tries to de-\ntermine how well documents satisfy information needs. In th e Boolean or\nvector space models of IR, matching is done in a formally de\ufb01n ed but seman-\ntically imprecise calculus of index terms. Given only a quer y, an IR system\nhas an uncertain understanding of the information need. Giv en the query\nand document representations, a system has an uncertain gue ss of whether\na document has content relevant to the information need. Pro bability theory\nprovides a principled foundation for such reasoning under u ncertainty. This\nchapter provides one answer as to how to exploit this foundat ion to estimate\nhow likely it is that a document is relevant to an information need.\nThere is more than one possible retrieval model which has a pr obabilistic\nbasis. Here, we will introduce probability theory and the Pr obability Rank-\ning Principle (Sections 11.1\u201311.2), and then concentrate on the Binary Inde-\npendence Model (Section 11.3), which is the original and still most in\ufb02uential\nprobabilistic retrieval model. Finally, we will introduce related but extended\nmethods which use term counts, including the empirically su ccessful Okapi\nBM25 weighting scheme, and Bayesian Network models for IR (S ection 11.4).\nIn Chapter 12, we then present the alternative probabilistic language mo del-\nOnline edition (c)\n2009 Cambridge UP220 11 Probabilistic information retrieval\ning approach to IR, which has been developed with considerab le success in\nrecent years.\n11.1 Review of basic probability theory\nWe hope that the reader has seen a little basic probability th eory previously.\nWe will give a very quick review; some references for further reading appear\nat the end of the chapter. A variable Arepresents an event (a subset of the\nspace of possible outcomes). Equivalently, we can represen t the subset via a\nrandom variable , which is a function from outcomes to real numbers; the sub- RANDOM VARIABLE\nset is the domain over which the random variable Ahas a particular value.\nOften we will not know with certainty whether an event is true in the world.\nWe can ask the probability of the event 0 \u2264P(A)\u22641. For two events Aand\nB, the joint event of both events occurring is described by the joint probabil-\nityP(A,B). The conditional probability P(A|B)expresses the probability of\nevent Agiven that event Boccurred. The fundamental relationship between\njoint and conditional probabilities is given by the chain rule : CHAIN RULE\nP(A,B) =P(A\u2229B) =P(A|B)P(B) =P(B|A)P(A) (11.1)\nWithout making any assumptions, the probability of a joint e vent equals the\nprobability of one of the events multiplied by the probabili ty of the other\nevent conditioned on knowing the \ufb01rst event happened.\nWriting P(A)for the complement of an event, we similarly have:\nP(A,B) =P(B|A)P(A) (11.2)\nProbability theory also has a partition rule , which says that if an event Bcan PARTITION RULE\nbe divided into an exhaustive set of disjoint subcases, then the probability of\nBis the sum of the probabilities of the subcases. A special cas e of this rule\ngives that:\nP(B) =P(A,B) +P(A,B) (11.3)\nFrom these we can derive Bayes\u2019 Rule for inverting conditional probabili- BAYES \u2019 RULE\nties:\nP(A|B) =P(B|A)P(A)\nP(B)=/bracketleft\uf8ecigg\nP(B|A)\n\u2211X\u2208{A,A}P(B|X)P(X)/bracketright\uf8ecigg\nP(A) (11.4)\nThis equation can also be thought of as a way of updating proba bilities. We\nstart off with an initial estimate of how likely the event Ais when we do\nnot have any other information; this is the prior probability P (A). Bayes\u2019 rule PRIOR PROBABILITY\nlets us derive a posterior probability P (A|B)after having seen the evidence B, POSTERIOR\nPROBABILITY\nOnline edition (c)\n2009 Cambridge UP11.2 The Probability Ranking Principle 221\nbased on the likelihood ofBoccurring in the two cases that Adoes or does not\nhold.1\nFinally, it is often useful to talk about the odds of an event, which provide ODDS\na kind of multiplier for how probabilities change:\nOdds: O(A) =P(A)\nP(A)=P(A)\n1\u2212P(A)(11.5)\n11.2 The Probability Ranking Principle\n11.2.1 The 1/0 loss case\nWe assume a ranked retrieval setup as in Section 6.3, where there is a collec-\ntion of documents, the user issues a query, and an ordered lis t of documents\nis returned. We also assume a binary notion of relevance as in Chapter 8. For\na query qand a document din the collection, let Rd,qbe an indicator random\nvariable that says whether dis relevant with respect to a given query q. That\nis, it takes on a value of 1 when the document is relevant and 0 o therwise. In\ncontext we will often write just RforRd,q.\nUsing a probabilistic model, the obvious order in which to pr esent doc-\numents to the user is to rank documents by their estimated pro bability of\nrelevance with respect to the information need: P(R=1|d,q). This is the ba-\nsis of the Probability Ranking Principle (PRP) ( van Rijsbergen 1979 , 113\u2013114): PROBABILITY\nRANKING PRINCIPLE\n\u201cIf a reference retrieval system\u2019s response to each request is a ranking\nof the documents in the collection in order of decreasing pro bability\nof relevance to the user who submitted the request, where the prob-\nabilities are estimated as accurately as possible on the bas is of what-\never data have been made available to the system for this purp ose, the\noverall effectiveness of the system to its user will be the be st that is\nobtainable on the basis of those data.\u201d\nIn the simplest case of the PRP , there are no retrieval costs o r other utility\nconcerns that would differentially weight actions or error s. You lose a point\nfor either returning a nonrelevant document or failing to re turn a relevant\ndocument (such a binary situation where you are evaluated on your accuracy\nis called 1/0 loss ). The goal is to return the best possible results as the top k 1/0 LOSS\ndocuments, for any value of kthe user chooses to examine. The PRP then\nsays to simply rank all documents in decreasing order of P(R=1|d,q). If\na set of retrieval results is to be returned, rather than an or dering, the Bayes BAYES OPTIMAL\nDECISION RULE\n1. The term likelihood is just a synonym of probability . It is the probability of an event or data\naccording to a model. The term is usually used when people are thinking of holding the data\n\ufb01xed, while varying the model.\nOnline edition (c)\n2009 Cambridge UP222 11 Probabilistic information retrieval\nOptimal Decision Rule , the decision which minimizes the risk of loss, is to\nsimply return documents that are more likely relevant than n onrelevant:\ndis relevant iff P(R=1|d,q)>P(R=0|d,q) (11.6)\nTheorem 11.1. The PRP is optimal, in the sense that it minimizes the expecte d loss\n(also known as the Bayes risk ) under 1/0 loss. BAYES RISK\nThe proof can be found in Ripley (1996 ). However, it requires that all proba-\nbilities are known correctly. This is never the case in pract ice. Nevertheless,\nthe PRP still provides a very useful foundation for developi ng models of IR.\n11.2.2 The PRP with retrieval costs\nSuppose, instead, that we assume a model of retrieval costs. LetC1be the\ncost of not retrieving a relevant document and C0the cost of retrieval of a\nnonrelevant document. Then the Probability Ranking Princi ple says that if\nfor a speci\ufb01c document dand for all documents d\u2032not yet retrieved\nC0\u00b7P(R=0|d)\u2212C1\u00b7P(R=1|d)\u2264C0\u00b7P(R=0|d\u2032)\u2212C1\u00b7P(R=1|d\u2032) (11.7)\nthen dis the next document to be retrieved. Such a model gives a form al\nframework where we can model differential costs of false pos itives and false\nnegatives and even system performance issues at the modelin g stage, rather\nthan simply at the evaluation stage, as we did in Section 8.6(page 168). How-\never, we will not further consider loss/utility models in th is chapter.\n11.3 The Binary Independence Model\nThe Binary Independence Model (BIM) we present in this section is the model BINARY\nINDEPENDENCE\nMODELthat has traditionally been used with the PRP. It introduces some simple as-\nsumptions, which make estimating the probability function P(R|d,q)prac-\ntical. Here, \u201cbinary\u201d is equivalent to Boolean: documents a nd queries are\nboth represented as binary term incidence vectors. That is, a document d\nis represented by the vector /vectorx= (x1, . . . , xM)where xt=1 if term tis\npresent in document dand xt=0 if tis not present in d. With this rep-\nresentation, many possible documents have the same vector r epresentation.\nSimilarly, we represent qby the incidence vector /vectorq(the distinction between\nqand/vectorqis less central since commonly qis in the form of a set of words).\n\u201cIndependence\u201d means that terms are modeled as occurring in documents\nindependently. The model recognizes no association betwee n terms. This\nassumption is far from correct, but it nevertheless often gi ves satisfactory\nresults in practice; it is the \u201cnaive\u201d assumption of Naive Ba yes models, dis-\ncussed further in Section 13.4 (page 265). Indeed, the Binary Independence\nOnline edition (c)\n2009 Cambridge UP11.3 The Binary Independence Model 223\nModel is exactly the same as the multivariate Bernoulli Naiv e Bayes model\npresented in Section 13.3 (page 263). In a sense this assumption is equivalent\nto an assumption of the vector space model, where each term is a dimension\nthat is orthogonal to all other terms.\nWe will \ufb01rst present a model which assumes that the user has a s ingle\nstep information need. As discussed in Chapter 9, seeing a range of results\nmight let the user re\ufb01ne their information need. Fortunatel y, as mentioned\nthere, it is straightforward to extend the Binary Independe nce Model so as to\nprovide a framework for relevance feedback, and we present t his model in\nSection 11.3.4 .\nTo make a probabilistic retrieval strategy precise, we need to estimate how\nterms in documents contribute to relevance, speci\ufb01cally, w e wish to know\nhow term frequency, document frequency, document length, a nd other statis-\ntics that we can compute in\ufb02uence judgments about document r elevance,\nand how they can be reasonably combined to estimate the proba bility of doc-\nument relevance. We then order documents by decreasing esti mated proba-\nbility of relevance.\nWe assume here that the relevance of each document is indepen dent of the\nrelevance of other documents. As we noted in Section 8.5.1 (page 166), this\nis incorrect: the assumption is especially harmful in pract ice if it allows a\nsystem to return duplicate or near duplicate documents. Und er the BIM, we\nmodel the probability P(R|d,q)that a document is relevant via the probabil-\nity in terms of term incidence vectors P(R|/vectorx,/vectorq). Then, using Bayes rule, we\nhave:\nP(R=1|/vectorx,/vectorq) =P(/vectorx|R=1,/vectorq)P(R=1|/vectorq)\nP(/vectorx|/vectorq)(11.8)\nP(R=0|/vectorx,/vectorq) =P(/vectorx|R=0,/vectorq)P(R=0|/vectorq)\nP(/vectorx|/vectorq)\nHere, P(/vectorx|R=1,/vectorq)and P(/vectorx|R=0,/vectorq)are the probability that if a relevant or\nnonrelevant, respectively, document is retrieved, then th at document\u2019s rep-\nresentation is /vectorx. You should think of this quantity as de\ufb01ned with respect to\na space of possible documents in a domain. How do we compute al l these\nprobabilities? We never know the exact probabilities, and s o we have to use\nestimates: Statistics about the actual document collectio n are used to estimate\nthese probabilities. P(R=1|/vectorq)and P(R=0|/vectorq)indicate the prior probability\nof retrieving a relevant or nonrelevant document respectiv ely for a query /vectorq.\nAgain, if we knew the percentage of relevant documents in the collection,\nthen we could use this number to estimate P(R=1|/vectorq)and P(R=0|/vectorq). Since\na document is either relevant or nonrelevant to a query, we mu st have that:\nP(R=1|/vectorx,/vectorq) +P(R=0|/vectorx,/vectorq) =1 (11.9)\nOnline edition (c)\n2009 Cambridge UP224 11 Probabilistic information retrieval\n11.3.1 Deriving a ranking function for query terms\nGiven a query q, we wish to order returned documents by descending P(R=\n1|d,q). Under the BIM, this is modeled as ordering by P(R=1|/vectorx,/vectorq). Rather\nthan estimating this probability directly, because we are i nterested only in the\nranking of documents, we work with some other quantities whi ch are easier\nto compute and which give the same ordering of documents. In p articular,\nwe can rank documents by their odds of relevance (as the odds o f relevance\nis monotonic with the probability of relevance). This makes things easier,\nbecause we can ignore the common denominator in ( 11.8), giving:\nO(R|/vectorx,/vectorq) =P(R=1|/vectorx,/vectorq)\nP(R=0|/vectorx,/vectorq)=P(R=1|/vectorq)P(/vectorx|R=1,/vectorq)\nP(/vectorx|/vectorq)\nP(R=0|/vectorq)P(/vectorx|R=0,/vectorq)\nP(/vectorx|/vectorq)=P(R=1|/vectorq)\nP(R=0|/vectorq)\u00b7P(/vectorx|R=1,/vectorq)\nP(/vectorx|R=0,/vectorq)(11.10)\nThe left term in the rightmost expression of Equation ( 11.10 ) is a constant for\na given query. Since we are only ranking documents, there is t hus no need\nfor us to estimate it. The right-hand term does, however, req uire estimation,\nand this initially appears to be dif\ufb01cult: How can we accurat ely estimate the\nprobability of an entire term incidence vector occurring? I t is at this point that\nwe make the Naive Bayes conditional independence assumption that the presence NAIVE BAYES\nASSUMPTION or absence of a word in a document is independent of the presen ce or absence\nof any other word (given the query):\nP(/vectorx|R=1,/vectorq)\nP(/vectorx|R=0,/vectorq)=M\n\u220f\nt=1P(xt|R=1,/vectorq)\nP(xt|R=0,/vectorq)(11.11)\nSo:\nO(R|/vectorx,/vectorq) =O(R|/vectorq)\u00b7M\n\u220f\nt=1P(xt|R=1,/vectorq)\nP(xt|R=0,/vectorq)(11.12)\nSince each xtis either 0 or 1, we can separate the terms to give:\nO(R|/vectorx,/vectorq) =O(R|/vectorq)\u00b7\u220f\nt:xt=1P(xt=1|R=1,/vectorq)\nP(xt=1|R=0,/vectorq)\u00b7\u220f\nt:xt=0P(xt=0|R=1,/vectorq)\nP(xt=0|R=0,/vectorq)(11.13)\nHenceforth, let pt=P(xt=1|R=1,/vectorq)be the probability of a term appear-\ning in a document relevant to the query, and ut=P(xt=1|R=0,/vectorq)be the\nprobability of a term appearing in a nonrelevant document. T hese quantities\ncan be visualized in the following contingency table where t he columns add\nto 1:\n(11.14)\ndocument relevant ( R=1) nonrelevant ( R=0)\nTerm present xt=1 pt ut\nTerm absent xt=0 1\u2212pt 1\u2212ut\nOnline edition (c)\n2009 Cambridge UP11.3 The Binary Independence Model 225\nLet us make an additional simplifying assumption that terms not occur-\nring in the query are equally likely to occur in relevant and n onrelevant doc-\numents: that is, if qt=0 then pt=ut. (This assumption can be changed,\nas when doing relevance feedback in Section 11.3.4 .) Then we need only\nconsider terms in the products that appear in the query, and s o,\nO(R|/vectorq,/vectorx) =O(R|/vectorq)\u00b7\u220f\nt:xt=qt=1pt\nut\u00b7\u220f\nt:xt=0,qt=11\u2212pt\n1\u2212ut(11.15)\nThe left product is over query terms found in the document and the right\nproduct is over query terms not found in the document.\nWe can manipulate this expression by including the query ter ms found in\nthe document into the right product, but simultaneously div iding through\nby them in the left product, so the value is unchanged. Then we have:\nO(R|/vectorq,/vectorx) =O(R|/vectorq)\u00b7\u220f\nt:xt=qt=1pt(1\u2212ut)\nut(1\u2212pt)\u00b7\u220f\nt:qt=11\u2212pt\n1\u2212ut(11.16)\nThe left product is still over query terms found in the docume nt, but the right\nproduct is now over all query terms. That means that this righ t product is a\nconstant for a particular query, just like the odds O(R|/vectorq). So the only quantity\nthat needs to be estimated to rank documents for relevance to a query is the\nleft product. We can equally rank documents by the logarithm of this term,\nsince log is a monotonic function. The resulting quantity us ed for ranking is\ncalled the Retrieval Status Value (RSV) in this model: RETRIEVAL STATUS\nVALUE\nRSV d=log \u220f\nt:xt=qt=1pt(1\u2212ut)\nut(1\u2212pt)=\u2211\nt:xt=qt=1logpt(1\u2212ut)\nut(1\u2212pt)(11.17)\nSo everything comes down to computing the RSV . De\ufb01ne ct:\nct=logpt(1\u2212ut)\nut(1\u2212pt)=logpt\n(1\u2212pt)+log1\u2212ut\nut(11.18)\nThe ctterms are log odds ratios for the terms in the query. We have th e\nodds of the term appearing if the document is relevant ( pt/(1\u2212pt)) and the\nodds of the term appearing if the document is nonrelevant ( ut/(1\u2212ut)). The\nodds ratio is the ratio of two such odds, and then we \ufb01nally take the log of that ODDS RATIO\nquantity. The value will be 0 if a term has equal odds of appear ing in relevant\nand nonrelevant documents, and positive if it is more likely to appear in\nrelevant documents. The ctquantities function as term weights in the model,\nand the document score for a query is RSV d=\u2211xt=qt=1ct. Operationally, we\nsum them in accumulators for query terms appearing in docume nts, just as\nfor the vector space model calculations discussed in Sectio n7.1(page 135).\nWe now turn to how we estimate these ctquantities for a particular collection\nand query.\nOnline edition (c)\n2009 Cambridge UP226 11 Probabilistic information retrieval\n11.3.2 Probability estimates in theory\nFor each term t, what would these ctnumbers look like for the whole collec-\ntion? ( 11.19 ) gives a contingency table of counts of documents in the coll ec-\ntion, where df tis the number of documents that contain term t:\n(11.19)\ndocuments relevant nonrelevant Total\nTerm present xt=1 s dft\u2212s dft\nTerm absent xt=0 S\u2212s(N\u2212dft)\u2212(S\u2212s)N\u2212dft\nTotal S N\u2212S N\nUsing this, pt=s/Sand ut= (dft\u2212s)/(N\u2212S)and\nct=K(N, dft,S,s) =logs/(S\u2212s)\n(dft\u2212s)/((N\u2212dft)\u2212(S\u2212s))(11.20)\nTo avoid the possibility of zeroes (such as if every or no rele vant document\nhas a particular term) it is fairly standard to add1\n2to each of the quantities\nin the center 4 terms of ( 11.19 ), and then to adjust the marginal counts (the\ntotals) accordingly (so, the bottom right cell totals N+2). Then we have:\n\u02c6ct=K(N, dft,S,s) =log(s+1\n2)/(S\u2212s+1\n2)\n(dft\u2212s+1\n2)/(N\u2212dft\u2212S+s+1\n2)(11.21)\nAdding1\n2in this way is a simple form of smoothing. For trials with cat-\negorical outcomes (such as noting the presence or absence of a term), one\nway to estimate the probability of an event from data is simpl y to count the\nnumber of times an event occurred divided by the total number of trials.\nThis is referred to as the relative frequency of the event. Estimating the prob- RELATIVE FREQUENCY\nability as the relative frequency is the maximum likelihood estimate (orMLE ), MAXIMUM LIKELIHOOD\nESTIMATE\nMLEbecause this value makes the observed data maximally likely . However, if\nwe simply use the MLE, then the probability given to events we happened to\nsee is usually too high, whereas other events may be complete ly unseen and\ngiving them as a probability estimate their relative freque ncy of 0 is both an\nunderestimate, and normally breaks our models, since anyth ing multiplied\nby 0 is 0. Simultaneously decreasing the estimated probabil ity of seen events\nand increasing the probability of unseen events is referred to as smoothing . SMOOTHING\nOne simple way of smoothing is to add a number \u03b1to each of the observed\ncounts. These pseudocounts correspond to the use of a uniform distribution PSEUDOCOUNTS\nover the vocabulary as a Bayesian prior , following Equation ( 11.4). We ini- BAYESIAN PRIOR\ntially assume a uniform distribution over events, where the size of \u03b1denotes\nthe strength of our belief in uniformity, and we then update t he probability\nbased on observed events. Since our belief in uniformity is w eak, we use\nOnline edition (c)\n2009 Cambridge UP11.3 The Binary Independence Model 227\n\u03b1=1\n2. This is a form of maximum a posteriori (MAP ) estimation, where we MAXIMUM A\nPOSTERIORI\nMAPchoose the most likely point value for probabilities based o n the prior and\nthe observed evidence, following Equation ( 11.4). We will further discuss\nmethods of smoothing estimated counts to give probability m odels in Sec-\ntion 12.2.2 (page 243); the simple method of adding1\n2to each observed count\nwill do for now.\n11.3.3 Probability estimates in practice\nUnder the assumption that relevant documents are a very smal l percentage\nof the collection, it is plausible to approximate statistic s for nonrelevant doc-\numents by statistics from the whole collection. Under this a ssumption, ut\n(the probability of term occurrence in nonrelevant documen ts for a query) is\ndft/Nand\nlog[(1\u2212ut)/ut] =log[(N\u2212dft)/df t]\u2248logN/df t (11.22)\nIn other words, we can provide a theoretical justi\ufb01cation fo r the most fre-\nquently used form of idf weighting, which we saw in Section 6.2.1 .\nThe approximation technique in Equation ( 11.22 ) cannot easily be extended\nto relevant documents. The quantity ptcan be estimated in various ways:\n1.We can use the frequency of term occurrence in known relevant docu-\nments (if we know some). This is the basis of probabilistic ap proaches to\nrelevance feedback weighting in a feedback loop, discussed in the next\nsubsection.\n2.Croft and Harper (1979 ) proposed using a constant in their combination\nmatch model. For instance, we might assume that ptis constant over all\nterms xtin the query and that pt=0.5. This means that each term has\neven odds of appearing in a relevant document, and so the ptand(1\u2212pt)\nfactors cancel out in the expression for RSV . Such an estimate is weak, but\ndoesn\u2019t disagree violently with our hopes for the search ter ms appearing\nin many but not all relevant documents. Combining this metho d with our\nearlier approximation for ut, the document ranking is determined simply\nby which query terms occur in documents scaled by their idf we ighting.\nFor short documents (titles or abstracts) in situations in w hich iterative\nsearching is undesirable, using this weighting term alone c an be quite\nsatisfactory, although in many other circumstances we woul d like to do\nbetter.\n3.Greiff (1998 ) argues that the constant estimate of ptin the Croft and Harper\n(1979 ) model is theoretically problematic and not observed empir ically: as\nmight be expected, ptis shown to rise with df t. Based on his data analysis,\na plausible proposal would be to use the estimate pt=1\n3+2\n3dft/N.\nOnline edition (c)\n2009 Cambridge UP228 11 Probabilistic information retrieval\nIterative methods of estimation, which combine some of the a bove ideas,\nare discussed in the next subsection.\n11.3.4 Probabilistic approaches to relevance feedback\nWe can use (pseudo-)relevance feedback, perhaps in an itera tive process of\nestimation, to get a more accurate estimate of pt. The probabilistic approach\nto relevance feedback works as follows:\n1.Guess initial estimates of ptand ut. This can be done using the probability\nestimates of the previous section. For instance, we can assu me that ptis\nconstant over all xtin the query, in particular, perhaps taking pt=1\n2.\n2.Use the current estimates of ptand utto determine a best guess at the set\nof relevant documents R={d:Rd,q=1}. Use this model to retrieve a set\nof candidate relevant documents, which we present to the use r.\n3.We interact with the user to re\ufb01ne the model of R. We do this by learn-\ning from the user relevance judgments for some subset of docu ments V.\nBased on relevance judgments, Vis partitioned into two subsets: VR=\n{d\u2208V,Rd,q=1}\u2282 Rand VNR ={d\u2208V,Rd,q=0}, which is disjoint\nfrom R.\n4.We reestimate ptand uton the basis of known relevant and nonrelevant\ndocuments. If the sets VRand VNR are large enough, we may be able\nto estimate these quantities directly from these documents as maximum\nlikelihood estimates:\npt=|VR t|/|VR| (11.23)\n(where VR tis the set of documents in VR containing xt). In practice,\nwe usually need to smooth these estimates. We can do this by ad ding\n1\n2to both the count |VR t|and to the number of relevant documents not\ncontaining the term, giving:\npt=|VR t|+1\n2\n|VR|+1(11.24)\nHowever, the set of documents judged by the user ( V) is usually very\nsmall, and so the resulting statistical estimate is quite un reliable (noisy),\neven if the estimate is smoothed. So it is often better to comb ine the new\ninformation with the original guess in a process of Bayesian updating. In\nthis case we have:\np(k+1)\nt=|VR t|+\u03bap(k)\nt\n|VR|+\u03ba(11.25)\nOnline edition (c)\n2009 Cambridge UP11.3 The Binary Independence Model 229\nHere p(k)\ntis the kthestimate for ptin an iterative updating process and\nis used as a Bayesian prior in the next iteration with a weight ing of \u03ba.\nRelating this equation back to Equation ( 11.4) requires a bit more proba-\nbility theory than we have presented here (we need to use a bet a distribu-\ntion prior, conjugate to the Bernoulli random variable Xt). But the form\nof the resulting equation is quite straightforward: rather than uniformly\ndistributing pseudocounts, we now distribute a total of \u03bapseudocounts\naccording to the previous estimate, which acts as the prior d istribution.\nIn the absence of other evidence (and assuming that the user i s perhaps\nindicating roughly 5 relevant or nonrelevant documents) th en a value\nof around \u03ba=5 is perhaps appropriate. That is, the prior is strongly\nweighted so that the estimate does not change too much from th e evi-\ndence provided by a very small number of documents.\n5.Repeat the above process from step 2, generating a successio n of approxi-\nmations to Rand hence pt, until the user is satis\ufb01ed.\nIt is also straightforward to derive a pseudo-relevance fee dback version of\nthis algorithm, where we simply pretend that VR=V. More brie\ufb02y:\n1.Assume initial estimates for ptand utas above.\n2.Determine a guess for the size of the relevant document set. I f unsure, a\nconservative (too small) guess is likely to be best. This mot ivates use of a\n\ufb01xed size set Vof highest ranked documents.\n3.Improve our guesses for ptand ut. We choose from the methods of Equa-\ntions ( 11.23 ) and ( 11.25 ) for re-estimating pt, except now based on the set\nVinstead of VR. If we let Vtbe the subset of documents in Vcontaining\nxtand use add1\n2smoothing, we get:\npt=|Vt|+1\n2\n|V|+1(11.26)\nand if we assume that documents that are not retrieved are non relevant\nthen we can update our utestimates as:\nut=dft\u2212|Vt|+1\n2\nN\u2212|V|+1(11.27)\n4.Go to step 2 until the ranking of the returned results converg es.\nOnce we have a real estimate for ptthen the ctweights used in the RSV\nvalue look almost like a tf-idf value. For instance, using Eq uation ( 11.18 ),\nOnline edition (c)\n2009 Cambridge UP230 11 Probabilistic information retrieval\nEquation ( 11.22 ), and Equation ( 11.26 ), we have:\nct=log/bracketleftbiggpt\n1\u2212pt\u00b71\u2212ut\nut/bracketrightbigg\n\u2248log/bracketleft\uf8ecigg\n|Vt|+1\n2\n|V|\u2212|Vt|+1\u00b7N\ndft/bracketright\uf8ecigg\n(11.28)\nBut things aren\u2019t quite the same: pt/(1\u2212pt)measures the (estimated) pro-\nportion of relevant documents that the term toccurs in, not term frequency.\nMoreover, if we apply log identities:\nct=log|Vt|+1\n2\n|V|\u2212|Vt|+1+logN\ndft(11.29)\nwe see that we are now adding the two log scaled components rather than\nmultiplying them.\n?Exercise 11.1\nWork through the derivation of Equation ( 11.20 ) from Equations ( 11.18 ) and ( 11.19 ).\nExercise 11.2\nWhat are the differences between standard vector space tf-i df weighting and the BIM\nprobabilistic retrieval model (in the case where no documen t relevance information\nis available)?\nExercise 11.3 [\u22c6\u22c6]\nLetXtbe a random variable indicating whether the term tappears in a document.\nSuppose we have |R|relevant documents in the document collection and that Xt=1\ninsof the documents. Take the observed data to be just these obse rvations of Xtfor\neach document in R. Show that the MLE for the parameter pt=P(Xt=1|R=1,/vectorq),\nthat is, the value for ptwhich maximizes the probability of the observed data, is\npt=s/|R|.\nExercise 11.4\nDescribe the differences between vector space relevance fe edback and probabilistic\nrelevance feedback.\n11.4 An appraisal and some extensions\n11.4.1 An appraisal of probabilistic models\nProbabilistic methods are one of the oldest formal models in IR. Already\nin the 1970s they were held out as an opportunity to place IR on a \ufb01rmer\ntheoretical footing, and with the resurgence of probabilis tic methods in com-\nputational linguistics in the 1990s, that hope has returned , and probabilis-\ntic methods are again one of the currently hottest topics in I R. Traditionally,\nprobabilistic IR has had neat ideas but the methods have neve r won on per-\nformance. Getting reasonable approximations of the needed probabilities for\nOnline edition (c)\n2009 Cambridge UP11.4 An appraisal and some extensions 231\na probabilistic IR model is possible, but it requires some ma jor assumptions.\nIn the BIM these are:\n\u2022a Boolean representation of documents/queries/relevance\n\u2022term independence\n\u2022terms not in the query don\u2019t affect the outcome\n\u2022document relevance values are independent\nIt is perhaps the severity of the modeling assumptions that m akes achieving\ngood performance dif\ufb01cult. A general problem seems to be tha t probabilistic\nmodels either require partial relevance information or els e only allow for\nderiving apparently inferior term weighting models.\nThings started to change in the 1990s when the BM25 weighting scheme,\nwhich we discuss in the next section, showed very good perfor mance, and\nstarted to be adopted as a term weighting scheme by many group s. The\ndifference between \u201cvector space\u201d and \u201cprobabilistic\u201d IR s ystems is not that\ngreat: in either case, you build an information retrieval sc heme in the exact\nsame way that we discussed in Chapter 7. For a probabilistic IR system, it\u2019s\njust that, at the end, you score queries not by cosine similar ity and tf-idf in\na vector space, but by a slightly different formula motivate d by probability\ntheory. Indeed, sometimes people have changed an existing v ector-space\nIR system into an effectively probabilistic system simply b y adopted term\nweighting formulas from probabilistic models. In this sect ion, we brie\ufb02y\npresent three extensions of the traditional probabilistic model, and in the next\nchapter, we look at the somewhat different probabilistic la nguage modeling\napproach to IR.\n11.4.2 Tree-structured dependencies between terms\nSome of the assumptions of the BIM can be removed. For example , we can\nremove the assumption that terms are independent. This assu mption is very\nfar from true in practice. A case that particularly violates this assumption is\nterm pairs like Hong andKong , which are strongly dependent. But dependen-\ncies can occur in various complex con\ufb01gurations, such as bet ween the set of\ntermsNew,York,England ,City,Stock ,Exchange , andUniversity .van Rijsbergen\n(1979 ) proposed a simple, plausible model which allowed a tree str ucture of\nterm dependencies, as in Figure 11.1. In this model each term can be directly\ndependent on only one other term, giving a tree structure of d ependencies.\nWhen it was invented in the 1970s, estimation problems held b ack the practi-\ncal success of this model, but the idea was reinvented as the T ree Augmented\nNaive Bayes model by Friedman and Goldszmidt (1996 ), who used it with\nsome success on various machine learning data sets.\nOnline edition (c)\n2009 Cambridge UP232 11 Probabilistic information retrieval\nx1\nx2\nx3 x4 x5\nx6 x7\n\u25eeFigure 11.1 A tree of dependencies between terms. In this graphical mode l rep-\nresentation, a term xiis directly dependent on a term xkif there is an arrow xk\u2192xi.\n11.4.3 Okapi BM25: a non-binary model\nThe BIM was originally designed for short catalog records an d abstracts of\nfairly consistent length, and it works reasonably in these c ontexts, but for\nmodern full-text search collections, it seems clear that a m odel should pay\nattention to term frequency and document length, as in Chapt er6. The BM25 BM25 WEIGHTS\nweighting scheme , often called Okapi weighting , after the system in which it was OKAPI WEIGHTING\n\ufb01rst implemented, was developed as a way of building a probab ilistic model\nsensitive to these quantities while not introducing too man y additional pa-\nrameters into the model ( Sp\u00e4rck Jones et al. 2000 ). We will not develop the\nfull theory behind the model here, but just present a series o f forms that\nbuild up to the standard form now used for document scoring. T he simplest\nscore for document dis just idf weighting of the query terms present, as in\nEquation ( 11.22 ):\nRSV d=\u2211\nt\u2208qlogN\ndft(11.30)\nSometimes, an alternative version of idf is used. If we start with the formula\nin Equation ( 11.21 ) but in the absence of relevance feedback information we\nestimate that S=s=0, then we get an alternative idf formulation as follows:\nRSV d=\u2211\nt\u2208qlogN\u2212dft+1\n2\ndft+1\n2(11.31)\nOnline edition (c)\n2009 Cambridge UP11.4 An appraisal and some extensions 233\nThis variant behaves slightly strangely: if a term occurs in over half the doc-\numents in the collection then this model gives a negative ter m weight, which\nis presumably undesirable. But, assuming the use of a stop li st, this normally\ndoesn\u2019t happen, and the value for each summand can be given a \ufb02 oor of 0.\nWe can improve on Equation ( 11.30 ) by factoring in the frequency of each\nterm and document length:\nRSV d=\u2211\nt\u2208qlog/bracketleftbiggN\ndft/bracketrightbigg\n\u00b7(k1+1)tftd\nk1((1\u2212b) +b\u00d7(Ld/Lave)) + tftd(11.32)\nHere, tf tdis the frequency of term tin document d, and Ldand Laveare the\nlength of document dand the average document length for the whole col-\nlection. The variable k1is a positive tuning parameter that calibrates the\ndocument term frequency scaling. A k1value of 0 corresponds to a binary\nmodel (no term frequency), and a large value corresponds to u sing raw term\nfrequency. bis another tuning parameter (0 \u2264b\u22641) which determines\nthe scaling by document length: b=1 corresponds to fully scaling the term\nweight by the document length, while b=0 corresponds to no length nor-\nmalization.\nIf the query is long, then we might also use similar weighting for query\nterms. This is appropriate if the queries are paragraph long information\nneeds, but unnecessary for short queries.\nRSV d=\u2211\nt\u2208q/bracketleftbigg\nlogN\ndft/bracketrightbigg\n\u00b7(k1+1)tftd\nk1((1\u2212b) +b\u00d7(Ld/Lave)) + tftd\u00b7(k3+1)tftq\nk3+tftq(11.33)\nwith tf tqbeing the frequency of term tin the query q, and k3being another\npositive tuning parameter that this time calibrates term fr equency scaling\nof the query. In the equation presented, there is no length no rmalization of\nqueries (it is as if b=0 here). Length normalization of the query is unnec-\nessary because retrieval is being done with respect to a sing le \ufb01xed query.\nThe tuning parameters of these formulas should ideally be se t to optimize\nperformance on a development test collection (see page 153). That is, we\ncan search for values of these parameters that maximize perf ormance on a\nseparate development test collection (either manually or w ith optimization\nmethods such as grid search or something more advanced), and then use\nthese parameters on the actual test collection. In the absen ce of such opti-\nmization, experiments have shown reasonable values are to s etk1and k3to\na value between 1.2 and 2 and b=0.75.\nIf we have relevance judgments available, then we can use the full form of\n(11.21 ) in place of the approximation log (N/df t)introduced in ( 11.22 ):\nRSV d=\u2211\nt\u2208qlog/bracketleft\uf8ecigg/bracketleft\uf8ecigg\n(|VR t|+1\n2)/(|VNR t|+1\n2)\n(dft\u2212|VR t|+1\n2)/(N\u2212dft\u2212|VR|+|VR t|+1\n2)/bracketright\uf8ecigg\n(11.34)\nOnline edition (c)\n2009 Cambridge UP234 11 Probabilistic information retrieval\n\u00d7(k1+1)tftd\nk1((1\u2212b) +b(Ld/Lave)) + tftd\u00d7(k3+1)tftq\nk3+tftq/bracketrightbigg\nHere, VR t,NVR t, and VRare used as in Section 11.3.4 . The \ufb01rst part of the\nexpression re\ufb02ects relevance feedback (or just idf weighti ng if no relevance\ninformation is available), the second implements document term frequency\nand document length scaling, and the third considers term fr equency in the\nquery.\nRather than just providing a term weighting method for terms in a user\u2019s\nquery, relevance feedback can also involve augmenting the q uery (automat-\nically or with manual review) with some (say, 10\u201320) of the to p terms in the\nknown-relevant documents as ordered by the relevance facto r\u02c6ctfrom Equa-\ntion ( 11.21 ), and the above formula can then be used with such an augmente d\nquery vector /vectorq.\nThe BM25 term weighting formulas have been used quite widely and quite\nsuccessfully across a range of collections and search tasks . Especially in the\nTREC evaluations, they performed well and were widely adopt ed by many\ngroups. See Sp\u00e4rck Jones et al. (2000 ) for extensive motivation and discussion\nof experimental results.\n11.4.4 Bayesian network approaches to IR\nTurtle and Croft (1989 ;1991 ) introduced into information retrieval the use\nofBayesian networks (Jensen and Jensen 2001 ), a form of probabilistic graph- BAYESIAN NETWORKS\nical model. We skip the details because fully introducing th e formalism of\nBayesian networks would require much too much space, but con ceptually,\nBayesian networks use directed graphs to show probabilisti c dependencies\nbetween variables, as in Figure 11.1, and have led to the development of so-\nphisticated algorithms for propagating in\ufb02uence so as to al low learning and\ninference with arbitrary knowledge within arbitrary direc ted acyclic graphs.\nTurtle and Croft used a sophisticated network to better mode l the complex\ndependencies between a document and a user\u2019s information ne ed.\nThe model decomposes into two parts: a document collection n etwork and\na query network. The document collection network is large, b ut can be pre-\ncomputed: it maps from documents to terms to concepts. The co ncepts are\na thesaurus-based expansion of the terms appearing in the do cument. The\nquery network is relatively small but a new network needs to b e built each\ntime a query comes in, and then attached to the document netwo rk. The\nquery network maps from query terms, to query subexpression s (built us-\ning probabilistic or \u201cnoisy\u201d versions of AND and ORoperators), to the user\u2019s\ninformation need.\nThe result is a \ufb02exible probabilistic network which can gene ralize vari-\nous simpler Boolean and probabilistic models. Indeed, this is the primary\nOnline edition (c)\n2009 Cambridge UP11.5 References and further reading 235\ncase of a statistical ranked retrieval model that naturally supports structured\nquery operators. The system allowed ef\ufb01cient large-scale r etrieval, and was\nthe basis of the InQuery text retrieval system, built at the U niversity of Mas-\nsachusetts. This system performed very well in TREC evaluat ions and for a\ntime was sold commercially. On the other hand, the model stil l used various\napproximations and independence assumptions to make param eter estima-\ntion and computation possible. There has not been much follo w-on work\nalong these lines, but we would note that this model was actua lly built very\nearly on in the modern era of using Bayesian networks, and the re have been\nmany subsequent developments in the theory, and the time is p erhaps right\nfor a new generation of Bayesian network-based information retrieval sys-\ntems.\n11.5 References and further reading\nLonger introductions to probability theory can be found in m ost introduc-\ntory probability and statistics books, such as ( Grinstead and Snell 1997 ,Rice\n2006 ,Ross 2006 ). An introduction to Bayesian utility theory can be found in\n(Ripley 1996 ).\nThe probabilistic approach to IR originated in the UK in the 1 950s. The\n\ufb01rst major presentation of a probabilistic model is Maron and Kuhns (1960 ).\nRobertson and Jones (1976 ) introduce the main foundations of the BIM and\nvan Rijsbergen (1979 ) presents in detail the classic BIM probabilistic model.\nThe idea of the PRP is variously attributed to S. E. Robertson , M. E. Maron\nand W. S. Cooper (the term \u201cProbabilistic Ordering Principl e\u201d is used in\nRobertson and Jones (1976 ), but PRP dominates in later work). Fuhr (1992 )\nis a more recent presentation of probabilistic IR, which inc ludes coverage of\nother approaches such as probabilistic logics and Bayesian networks. Crestani\net al. (1998 ) is another survey. Sp\u00e4rck Jones et al. (2000 ) is the de\ufb01nitive pre-\nsentation of probabilistic IR experiments by the \u201cLondon sc hool\u201d, and Robert-\nson(2005 ) presents a retrospective on the group\u2019s participation in T REC eval-\nuations, including detailed discussion of the Okapi BM25 sc oring function\nand its development. Robertson et al. (2004 ) extend BM25 to the case of mul-\ntiple weighted \ufb01elds.\nThe open-source Indri search engine, which is distributed w ith the Lemur\ntoolkit (http://www.lemurproject.org/ ) merges ideas from Bayesian inference net-\nworks and statistical language modeling approaches (see Ch apter 12), in par-\nticular preserving the former\u2019s support for structured que ry operators.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 237\n12Language models for information\nretrieval\nA common suggestion to users for coming up with good queries i s to think\nof words that would likely appear in a relevant document, and to use those\nwords as the query. The language modeling approach to IR dire ctly models\nthat idea: a document is a good match to a query if the document model\nis likely to generate the query, which will in turn happen if t he document\ncontains the query words often. This approach thus provides a different real-\nization of some of the basic ideas for document ranking which we saw in Sec-\ntion 6.2(page 117). Instead of overtly modeling the probability P(R=1|q,d)\nof relevance of a document dto a query q, as in the traditional probabilis-\ntic approach to IR (Chapter 11), the basic language modeling approach in-\nstead builds a probabilistic language model Mdfrom each document d, and\nranks documents based on the probability of the model genera ting the query:\nP(q|Md).\nIn this chapter, we \ufb01rst introduce the concept of language mo dels (Sec-\ntion 12.1) and then describe the basic and most commonly used language\nmodeling approach to IR, the Query Likelihood Model (Sectio n12.2). Af-\nter some comparisons between the language modeling approac h and other\napproaches to IR (Section 12.3), we \ufb01nish by brie\ufb02y describing various ex-\ntensions to the language modeling approach (Section 12.4).\n12.1 Language models\n12.1.1 Finite automata and language models\nWhat do we mean by a document model generating a query? A tradi tional\ngenerative model of a language, of the kind familiar from formal language GENERATIVE MODEL\ntheory, can be used either to recognize or to generate string s. For example,\nthe \ufb01nite automaton shown in Figure 12.1 can generate strings that include\nthe examples shown. The full set of strings that can be genera ted is called\nthelanguage of the automaton.1LANGUAGE\nOnline edition (c)\n2009 Cambridge UP238 12 Language models for information retrieval\nI wishI wish\nI wish I wish\nI wish I wish I wish\nI wish I wish I wish I wish I wish I wish\n. . .\nCANNOT GENERATE : wish I wish\n\u25eeFigure 12.1 A simple \ufb01nite automaton and some of the strings in the langua ge it\ngenerates.\u2192shows the start state of the automaton and a double circle ind icates a\n(possible) \ufb01nishing state.\nq1\nP(STOP|q1) =0.2the 0.2\na 0.1\nfrog 0.01\ntoad 0.01\nsaid 0.03\nlikes 0.02\nthat 0.04\n. . . . . .\n\u25eeFigure 12.2 A one-state \ufb01nite automaton that acts as a unigram language m odel.\nWe show a partial speci\ufb01cation of the state emission probabi lities.\nIf instead each node has a probability distribution over gen erating differ-\nent terms, we have a language model. The notion of a language m odel is\ninherently probabilistic. A language model is a function that puts a probability LANGUAGE MODEL\nmeasure over strings drawn from some vocabulary. That is, fo r a language\nmodel Mover an alphabet \u03a3:\n\u2211\ns\u2208\u03a3\u2217P(s) =1 (12.1)\nOne simple kind of language model is equivalent to a probabil istic \ufb01nite\nautomaton consisting of just a single node with a single prob ability distri-\nbution over producing different terms, so that \u2211t\u2208VP(t) = 1, as shown\nin Figure 12.2. After generating each word, we decide whether to stop or\nto loop around and then produce another word, and so the model also re-\nquires a probability of stopping in the \ufb01nishing state. Such a model places a\nprobability distribution over any sequence of words. By con struction, it also\nprovides a model for generating text according to its distri bution.\n1. Finite automata can have outputs attached to either their states or their arcs; we use states\nhere, because that maps directly on to the way probabilistic automata are usually formalized.\nOnline edition (c)\n2009 Cambridge UP12.1 Language models 239\nModel M1 Model M2\nthe 0.2 the 0.15\na 0.1 a 0.12\nfrog 0.01 frog 0.0002\ntoad 0.01 toad 0.0001\nsaid 0.03 said 0.03\nlikes 0.02 likes 0.04\nthat 0.04 that 0.04\ndog 0.005 dog 0.01\ncat 0.003 cat 0.015\nmonkey 0.001 monkey 0.002\n. . . . . . . . . . . .\n\u25eeFigure 12.3 Partial speci\ufb01cation of two unigram language models.\n\u270eExample 12.1: To \ufb01nd the probability of a word sequence, we just multiply th e\nprobabilities which the model gives to each word in the seque nce, together with the\nprobability of continuing or stopping after producing each word. For example,\nP(frog said that toad likes frog ) = ( 0.01\u00d70.03\u00d70.04\u00d70.01\u00d70.02\u00d70.01) (12.2)\n\u00d7(0.8\u00d70.8\u00d70.8\u00d70.8\u00d70.8\u00d70.8\u00d70.2)\n\u2248 0.000000000001573\nAs you can see, the probability of a particular string/docum ent, is usually a very\nsmall number! Here we stopped after generating frogthe second time. The \ufb01rst line of\nnumbers are the term emission probabilities, and the second line gives the probabil-\nity of continuing or stopping after generating each word. An explicit stop probability\nis needed for a \ufb01nite automaton to be a well-formed language m odel according to\nEquation ( 12.1). Nevertheless, most of the time, we will omit to include STOP and\n(1\u2212STOP)probabilities (as do most other authors). To compare two mod els for a\ndata set, we can calculate their likelihood ratio , which results from simply dividing the LIKELIHOOD RATIO\nprobability of the data according to one model by the probabi lity of the data accord-\ning to the other model. Providing that the stop probability i s \ufb01xed, its inclusion will\nnot alter the likelihood ratio that results from comparing t he likelihood of two lan-\nguage models generating a string. Hence, it will not alter th e ranking of documents.2\nNevertheless, formally, the numbers will no longer truly be probabilities, but only\nproportional to probabilities. See Exercise 12.4.\n\u270eExample 12.2: Suppose, now, that we have two language models M1and M2,\nshown partially in Figure 12.3. Each gives a probability estimate to a sequence of\n2. In the IR context that we are leading up to, taking the stop p robability to be \ufb01xed across\nmodels seems reasonable. This is because we are generating q ueries, and the length distribution\nof queries is \ufb01xed and independent of the document from which we are generating the language\nmodel.\nOnline edition (c)\n2009 Cambridge UP240 12 Language models for information retrieval\nterms, as already illustrated in Example 12.1. The language model that gives the\nhigher probability to the sequence of terms is more likely to have generated the term\nsequence. This time, we will omit STOP probabilities from our calculations. For the\nsequence shown, we get:\n(12.3) s frog said that toad likes that dog\nM10.01 0.03 0.04 0.01 0.02 0.04 0.005\nM20.0002 0.03 0.04 0.0001 0.04 0.04 0.01\nP(s|M1) =0.00000000000048\nP(s|M2) =0.000000000000000384\nand we see that P(s|M1)>P(s|M2). We present the formulas here in terms of prod-\nucts of probabilities, but, as is common in probabilistic ap plications, in practice it is\nusually best to work with sums of log probabilities (cf. page 258).\n12.1.2 Types of language models\nHow do we build probabilities over sequences of terms? We can always\nuse the chain rule from Equation ( 11.1) to decompose the probability of a\nsequence of events into the probability of each successive e vent conditioned\non earlier events:\nP(t1t2t3t4) =P(t1)P(t2|t1)P(t3|t1t2)P(t4|t1t2t3) (12.4)\nThe simplest form of language model simply throws away all co nditioning\ncontext, and estimates each term independently. Such a mode l is called a\nunigram language model : UNIGRAM LANGUAGE\nMODEL\nPuni(t1t2t3t4) =P(t1)P(t2)P(t3)P(t4) (12.5)\nThere are many more complex kinds of language models, such as bigram BIGRAM LANGUAGE\nMODEL language models , which condition on the previous term,\nPbi(t1t2t3t4) =P(t1)P(t2|t1)P(t3|t2)P(t4|t3) (12.6)\nand even more complex grammar-based language models such as proba-\nbilistic context-free grammars. Such models are vital for t asks like speech\nrecognition, spelling correction, and machine translatio n, where you need\nthe probability of a term conditioned on surrounding contex t. However,\nmost language-modeling work in IR has used unigram language models.\nIR is not the place where you most immediately need complex la nguage\nmodels, since IR does not directly depend on the structure of sentences to\nthe extent that other tasks like speech recognition do. Unig ram models are\noften suf\ufb01cient to judge the topic of a text. Moreover, as we s hall see, IR lan-\nguage models are frequently estimated from a single documen t and so it is\nOnline edition (c)\n2009 Cambridge UP12.1 Language models 241\nquestionable whether there is enough training data to do mor e. Losses from\ndata sparseness (see the discussion on page 260) tend to outweigh any gains\nfrom richer models. This is an example of the bias-variance t radeoff (cf. Sec-\ntion 14.6, page 308): With limited training data, a more constrained model\ntends to perform better. In addition, unigram models are mor e ef\ufb01cient to\nestimate and apply than higher-order models. Nevertheless , the importance\nof phrase and proximity queries in IR in general suggests tha t future work\nshould make use of more sophisticated language models, and s ome has be-\ngun to (see Section 12.5, page 252). Indeed, making this move parallels the\nmodel of van Rijsbergen in Chapter 11(page 231).\n12.1.3 Multinomial distributions over words\nUnder the unigram language model the order of words is irrele vant, and so\nsuch models are often called \u201cbag of words\u201d models, as discus sed in Chap-\nter6(page 117). Even though there is no conditioning on preceding context ,\nthis model nevertheless still gives the probability of a par ticular ordering of\nterms. However, any other ordering of this bag of terms will h ave the same\nprobability. So, really, we have a multinomial distribution over words. So long MULTINOMIAL\nDISTRIBUTION as we stick to unigram models, the language model name and mot ivation\ncould be viewed as historical rather than necessary. We coul d instead just\nrefer to the model as a multinomial model. From this perspect ive, the equa-\ntions presented above do not present the multinomial probab ility of a bag of\nwords, since they do not sum over all possible orderings of th ose words, as\nis done by the multinomial coef\ufb01cient (the \ufb01rst term on the ri ght-hand side)\nin the standard presentation of a multinomial model:\nP(d) =Ld!\ntft1,d!tft2,d!\u00b7\u00b7\u00b7tftM,d!P(t1)tft1,dP(t2)tft2,d\u00b7\u00b7\u00b7P(tM)tftM,d(12.7)\nHere, Ld=\u22111\u2264i\u2264Mtfti,dis the length of document d,Mis the size of the term\nvocabulary, and the products are now over the terms in the voc abulary, not\nthe positions in the document. However, just as with STOP probabilities, in\npractice we can also leave out the multinomial coef\ufb01cient in our calculations,\nsince, for a particular bag of words, it will be a constant, an d so it has no effect\non the likelihood ratio of two different models generating a particular bag of\nwords. Multinomial distributions also appear in Section 13.2 (page 258).\nThe fundamental problem in designing language models is tha t we do not\nknow what exactly we should use as the model Md. However, we do gener-\nally have a sample of text that is representative of that mode l. This problem\nmakes a lot of sense in the original, primary uses of language models. For ex-\nample, in speech recognition, we have a training sample of (s poken) text. But\nwe have to expect that, in the future, users will use differen t words and in\nOnline edition (c)\n2009 Cambridge UP242 12 Language models for information retrieval\ndifferent sequences, which we have never observed before, a nd so the model\nhas to generalize beyond the observed data to allow unknown w ords and se-\nquences. This interpretation is not so clear in the IR case, w here a document\nis \ufb01nite and usually \ufb01xed. The strategy we adopt in IR is as fol lows. We\npretend that the document dis only a representative sample of text drawn\nfrom a model distribution, treating it like a \ufb01ne-grained to pic. We then esti-\nmate a language model from this sample, and use that model to c alculate the\nprobability of observing any word sequence, and, \ufb01nally, we rank documents\naccording to their probability of generating the query.\n?Exercise 12.1 [\u22c6]\nIncluding stop probabilities in the calculation, what will the sum of the probability\nestimates of all strings in the language of length 1 be? Assum e that you generate a\nword and then decide whether to stop or not (i.e., the null str ing is not part of the\nlanguage).\nExercise 12.2 [\u22c6]\nIf the stop probability is omitted from calculations, what w ill the sum of the scores\nassigned to strings in the language of length 1 be?\nExercise 12.3 [\u22c6]\nWhat is the likelihood ratio of the document according to M1and M2in Exam-\nple12.2?\nExercise 12.4 [\u22c6]\nNo explicit STOP probability appeared in Example 12.2. Assuming that the STOP\nprobability of each model is 0.1, does this change the likeli hood ratio of a document\naccording to the two models?\nExercise 12.5 [\u22c6\u22c6]\nHow might a language model be used in a spelling correction sy stem? In particular,\nconsider the case of context-sensitive spelling correctio n, and correcting incorrect us-\nages of words, such as their inAre you their? (See Section 3.5(page 65) for pointers to\nsome literature on this topic.)\n12.2 The query likelihood model\n12.2.1 Using query likelihood language models in IR\nLanguage modeling is a quite general formal approach to IR, w ith many vari-\nant realizations. The original and basic method for using la nguage models\nin IR is the query likelihood model . In it, we construct from each document d QUERY LIKELIHOOD\nMODEL in the collection a language model Md. Our goal is to rank documents by\nP(d|q), where the probability of a document is interpreted as the li kelihood\nthat it is relevant to the query. Using Bayes rule (as introdu ced in Section 11.1,\npage 220), we have:\nP(d|q) =P(q|d)P(d)/P(q)\nOnline edition (c)\n2009 Cambridge UP12.2 The query likelihood model 243\nP(q)is the same for all documents, and so can be ignored. The prior prob-\nability of a document P(d)is often treated as uniform across all dand so it\ncan also be ignored, but we could implement a genuine prior wh ich could in-\nclude criteria like authority, length, genre, newness, and number of previous\npeople who have read the document. But, given these simpli\ufb01c ations, we\nreturn results ranked by simply P(q|d), the probability of the query qunder\nthe language model derived from d. The Language Modeling approach thus\nattempts to model the query generation process: Documents a re ranked by\nthe probability that a query would be observed as a random sam ple from the\nrespective document model.\nThe most common way to do this is using the multinomial unigra m lan-\nguage model, which is equivalent to a multinomial Naive Baye s model (page 263),\nwhere the documents are the classes, each treated in the esti mation as a sep-\narate \u201clanguage\u201d. Under this model, we have that:\nP(q|Md) =Kq\u220f\nt\u2208VP(t|Md)tft,d (12.8)\nwhere, again Kq=Ld!/(tft1,d!tft2,d!\u00b7\u00b7\u00b7tftM,d!)is the multinomial coef\ufb01cient\nfor the query q, which we will henceforth ignore, since it is a constant for a\nparticular query.\nFor retrieval based on a language model (henceforth LM), we t reat the\ngeneration of queries as a random process. The approach is to\n1.Infer a LM for each document.\n2.Estimate P(q|Mdi), the probability of generating the query according to\neach of these document models.\n3.Rank the documents according to these probabilities.\nThe intuition of the basic model is that the user has a prototy pe document in\nmind, and generates a query based on words that appear in this document.\nOften, users have a reasonable idea of terms that are likely t o occur in doc-\numents of interest and they will choose query terms that dist inguish these\ndocuments from others in the collection.3Collection statistics are an integral\npart of the language model, rather than being used heuristic ally as in many\nother approaches.\n12.2.2 Estimating the query generation probability\nIn this section we describe how to estimate P(q|Md). The probability of pro-\nducing the query given the LM Mdof document dusing maximum likelihood\n3. Of course, in other cases, they do not. The answer to this wi thin the language modeling\napproach is translation language models, as brie\ufb02y discuss ed in Section 12.4.\nOnline edition (c)\n2009 Cambridge UP244 12 Language models for information retrieval\nestimation (MLE) and the unigram assumption is:\n\u02c6P(q|Md) =\u220f\nt\u2208q\u02c6Pmle(t|Md) =\u220f\nt\u2208qtft,d\nLd(12.9)\nwhere Mdis the language model of document d, tft,dis the (raw) term fre-\nquency of term tin document d, and Ldis the number of tokens in docu-\nment d. That is, we just count up how often each word occurred, and di vide\nthrough by the total number of words in the document d. This is the same\nmethod of calculating an MLE as we saw in Section 11.3.2 (page 226), but\nnow using a multinomial over word counts.\nThe classic problem with using language models is one of esti mation (the\n\u02c6symbol on the P\u2019s is used above to stress that the model is esti mated):\nterms appear very sparsely in documents. In particular, som e words will\nnot have appeared in the document at all, but are possible wor ds for the in-\nformation need, which the user may have used in the query. If w e estimate\n\u02c6P(t|Md) = 0 for a term missing from a document d, then we get a strict\nconjunctive semantics: documents will only give a query non -zero probabil-\nity if all of the query terms appear in the document. Zero prob abilities are\nclearly a problem in other uses of language models, such as wh en predicting\nthe next word in a speech recognition application, because m any words will\nbe sparsely represented in the training data. It may seem rat her less clear\nwhether this is problematic in an IR application. This could be thought of\nas a human-computer interface issue: vector space systems h ave generally\npreferred more lenient matching, though recent web search d evelopments\nhave tended more in the direction of doing searches with such conjunctive\nsemantics. Regardless of the approach here, there is a more g eneral prob-\nlem of estimation: occurring words are also badly estimated ; in particular,\nthe probability of words occurring once in the document is no rmally over-\nestimated, since their one occurrence was partly by chance. The answer to\nthis (as we saw in Section 11.3.2 , page 226) is smoothing. But as people have\ncome to understand the LM approach better, it has become appa rent that the\nrole of smoothing in this model is not only to avoid zero proba bilities. The\nsmoothing of terms actually implements major parts of the te rm weighting\ncomponent (Exercise 12.8). It is not just that an unsmoothed model has con-\njunctive semantics; an unsmoothed model works badly becaus e it lacks parts\nof the term weighting component.\nThus, we need to smooth probabilities in our document langua ge mod-\nels: to discount non-zero probabilities and to give some pro bability mass to\nunseen words. There\u2019s a wide space of approaches to smoothin g probabil-\nity distributions to deal with this problem. In Section 11.3.2 (page 226), we\nalready discussed adding a number (1, 1/2, or a small \u03b1) to the observed\nOnline edition (c)\n2009 Cambridge UP12.2 The query likelihood model 245\ncounts and renormalizing to give a probability distributio n.4In this sec-\ntion we will mention a couple of other smoothing methods, whi ch involve\ncombining observed counts with a more general reference pro bability distri-\nbution. The general approach is that a non-occurring term sh ould be possi-\nble in a query, but its probability should be somewhat close t o but no more\nlikely than would be expected by chance from the whole collec tion. That is,\nif tf t,d=0 then\n\u02c6P(t|Md)\u2264cft/T\nwhere cf tis the raw count of the term in the collection, and Tis the raw size\n(number of tokens) of the entire collection. A simple idea th at works well in\npractice is to use a mixture between a document-speci\ufb01c mult inomial distri-\nbution and a multinomial distribution estimated from the en tire collection:\n\u02c6P(t|d) =\u03bb\u02c6Pmle(t|Md) + ( 1\u2212\u03bb)\u02c6Pmle(t|Mc) (12.10)\nwhere 0 <\u03bb<1 and Mcis a language model built from the entire doc-\nument collection. This mixes the probability from the docum ent with the\ngeneral collection frequency of the word. Such a model is ref erred to as a\nlinear interpolation language model.5Correctly setting \u03bbis important to the LINEAR\nINTERPOLATION good performance of this model.\nAn alternative is to use a language model built from the whole collection\nas a prior distribution in a Bayesian updating process (rather than a uniform BAYESIAN SMOOTHING\ndistribution, as we saw in Section 11.3.2 ). We then get the following equation:\n\u02c6P(t|d) =tft,d+\u03b1\u02c6P(t|Mc)\nLd+\u03b1(12.11)\nBoth of these smoothing methods have been shown to perform we ll in IR\nexperiments; we will stick with the linear interpolation sm oothing method\nfor the rest of this section. While different in detail, they are both conceptu-\nally similar: in both cases the probability estimate for a wo rd present in the\ndocument combines a discounted MLE and a fraction of the esti mate of its\nprevalence in the whole collection, while for words not pres ent in a docu-\nment, the estimate is just a fraction of the estimate of the pr evalence of the\nword in the whole collection.\nThe role of smoothing in LMs for IR is not simply or principall y to avoid es-\ntimation problems. This was not clear when the models were \ufb01r st proposed,\nbut it is now understood that smoothing is essential to the go od properties\n4. In the context of probability theory, (re)normalization refers to summing numbers that cover\nan event space and dividing them through by their sum, so that the result is a probability distri-\nbution which sums to 1. This is distinct from both the concept of term normalization in Chapter 2\nand the concept of length normalization in Chapter 6, which is done with a L2norm.\n5. It is also referred to as Jelinek-Mercer smoothing.\nOnline edition (c)\n2009 Cambridge UP246 12 Language models for information retrieval\nof the models. The reason for this is explored in Exercise 12.8. The extent\nof smoothing in these two models is controlled by the \u03bband \u03b1parameters: a\nsmall value of \u03bbor a large value of \u03b1means more smoothing. This parameter\ncan be tuned to optimize performance using a line search (or, for the linear\ninterpolation model, by other methods, such as the expectat ion maximimiza-\ntion algorithm; see Section 16.5, page 368). The value need not be a constant.\nOne approach is to make the value a function of the query size. This is useful\nbecause a small amount of smoothing (a \u201cconjunctive-like\u201d s earch) is more\nsuitable for short queries, while a lot of smoothing is more s uitable for long\nqueries.\nTo summarize, the retrieval ranking for a query qunder the basic LM for\nIR we have been considering is given by:\nP(d|q)\u221dP(d)\u220f\nt\u2208q((1\u2212\u03bb)P(t|Mc) +\u03bbP(t|Md)) (12.12)\nThis equation captures the probability that the document th at the user had\nin mind was in fact d.\n\u270eExample 12.3: Suppose the document collection contains two documents:\n\u2022d1: Xyzzy reports a pro\ufb01t but revenue is down\n\u2022d2: Quorus narrows quarter loss but revenue decreases further\nThe model will be MLE unigram models from the documents and co llection, mixed\nwith \u03bb=1/2.\nSuppose the query is revenue down . Then:\nP(q|d1) = [( 1/8+2/16)/2]\u00d7[(1/8+1/16)/2] (12.13)\n= 1/8\u00d73/32=3/256\nP(q|d2) = [( 1/8+2/16)/2]\u00d7[(0/8+1/16)/2]\n= 1/8\u00d71/32=1/256\nSo, the ranking is d1>d2.\n12.2.3 Ponte and Croft\u2019s Experiments\nPonte and Croft (1998 ) present the \ufb01rst experiments on the language model-\ning approach to information retrieval. Their basic approac h is the model that\nwe have presented until now. However, we have presented an ap proach\nwhere the language model is a mixture of two multinomials, mu ch as in\n(Miller et al. 1999 ,Hiemstra 2000 ) rather than Ponte and Croft\u2019s multivari-\nate Bernoulli model. The use of multinomials has been standa rd in most\nsubsequent work in the LM approach and experimental results in IR, as\nwell as evidence from text classi\ufb01cation which we consider i n Section 13.3\nOnline edition (c)\n2009 Cambridge UP12.2 The query likelihood model 247\nPrecision\nRec. tf-idf LM %chg\n0.0 0.7439 0.7590 +2.0\n0.1 0.4521 0.4910 +8.6\n0.2 0.3514 0.4045 +15.1 *\n0.3 0.2761 0.3342 +21.0 *\n0.4 0.2093 0.2572 +22.9 *\n0.5 0.1558 0.2061 +32.3 *\n0.6 0.1024 0.1405 +37.1 *\n0.7 0.0451 0.0760 +68.7 *\n0.8 0.0160 0.0432 +169.6 *\n0.9 0.0033 0.0063 +89.3\n1.0 0.0028 0.0050 +76.9\nAve 0.1868 0.2233 +19.55 *\n\u25eeFigure 12.4 Results of a comparison of tf-idf with language modeling (LM ) term\nweighting by Ponte and Croft (1998 ). The version of tf-idf from the INQUERY IR sys-\ntem includes length normalization of tf. The table gives an e valuation according to\n11-point average precision with signi\ufb01cance marked with a * according to a Wilcoxon\nsigned rank test. The language modeling approach always doe s better in these exper-\niments, but note that where the approach shows signi\ufb01cant ga ins is at higher levels\nof recall.\n(page 263), suggests that it is superior. Ponte and Croft argued stron gly for\nthe effectiveness of the term weights that come from the lang uage modeling\napproach over traditional tf-idf weights. We present a subs et of their results\nin Figure 12.4 where they compare tf-idf to language modeling by evaluatin g\nTREC topics 202\u2013250 over TREC disks 2 and 3. The queries are se ntence-\nlength natural language queries. The language modeling app roach yields\nsigni\ufb01cantly better results than their baseline tf-idf bas ed term weighting ap-\nproach. And indeed the gains shown here have been extended in subsequent\nwork.\n?Exercise 12.6 [\u22c6]\nConsider making a language model from the following trainin g text:\nthe martian has landed on the latin pop sensation ricky marti n\na.Under a MLE-estimated unigram probability model, what are P(the)and P(martian )?\nb.Under a MLE-estimated bigram model, what are P(sensation|pop)and P(pop|the)?\nOnline edition (c)\n2009 Cambridge UP248 12 Language models for information retrieval\nExercise 12.7 [\u22c6\u22c6]\nSuppose we have a collection that consists of the 4 documents given in the below\ntable.\ndocID Document text\n1 click go the shears boys click click click\n2 click click\n3 metal here\n4 metal shears click here\nBuild a query likelihood language model for this document co llection. Assume a\nmixture model between the documents and the collection, wit h both weighted at 0.5.\nMaximum likelihood estimation (mle) is used to estimate bot h as unigram models.\nWork out the model probabilities of the queries click,shears , and hence clickshears for\neach document, and use those probabilities to rank the docum ents returned by each\nquery. Fill in these probabilities in the below table:\nQuery Doc 1 Doc 2 Doc 3 Doc 4\nclick\nshears\nclick shears\nWhat is the \ufb01nal ranking of the documents for the query click shears ?\nExercise 12.8 [\u22c6\u22c6]\nUsing the calculations in Exercise 12.7 as inspiration or as examples where appro-\npriate, write one sentence each describing the treatment th at the model in Equa-\ntion ( 12.10 ) gives to each of the following quantities. Include whether it is present\nin the model or not and whether the effect is raw or scaled.\na.Term frequency in a document\nb.Collection frequency of a term\nc.Document frequency of a term\nd.Length normalization of a term\nExercise 12.9 [\u22c6\u22c6]\nIn the mixture model approach to the query likelihood model ( Equation ( 12.12 )), the\nprobability estimate of a term is based on the term frequency of a word in a document,\nand the collection frequency of the word. Doing this certain ly guarantees that each\nterm of a query (in the vocabulary) has a non-zero chance of be ing generated by each\ndocument. But it has a more subtle but important effect of imp lementing a form of\nterm weighting, related to what we saw in Chapter 6. Explain how this works. In\nparticular, include in your answer a concrete numeric examp le showing this term\nweighting at work.\n12.3 Language modeling versus other approaches in IR\nThe language modeling approach provides a novel way of looki ng at the\nproblem of text retrieval, which links it with a lot of recent work in speech\nOnline edition (c)\n2009 Cambridge UP12.3 Language modeling versus other approaches in IR 249\nand language processing. As Ponte and Croft (1998 ) emphasize, the language\nmodeling approach to IR provides a different approach to sco ring matches\nbetween queries and documents, and the hope is that the proba bilistic lan-\nguage modeling foundation improves the weights that are use d, and hence\nthe performance of the model. The major issue is estimation o f the docu-\nment model, such as choices of how to smooth it effectively. T he model\nhas achieved very good retrieval results. Compared to other probabilistic\napproaches, such as the BIM from Chapter 11, the main difference initially\nappears to be that the LM approach does away with explicitly m odeling rel-\nevance (whereas this is the central variable evaluated in th e BIM approach).\nBut this may not be the correct way to think about things, as so me of the\npapers in Section 12.5 further discuss. The LM approach assumes that docu-\nments and expressions of information needs are objects of th e same type, and\nassesses their match by importing the tools and methods of la nguage mod-\neling from speech and natural language processing. The resu lting model is\nmathematically precise, conceptually simple, computatio nally tractable, and\nintuitively appealing. This seems similar to the situation with XML retrieval\n(Chapter 10): there the approaches that assume queries and documents ar e\nobjects of the same type are also among the most successful.\nOn the other hand, like all IR models, you can also raise objec tions to the\nmodel. The assumption of equivalence between document and i nformation\nneed representation is unrealistic. Current LM approaches use very simple\nmodels of language, usually unigram models. Without an expl icit notion of\nrelevance, relevance feedback is dif\ufb01cult to integrate int o the model, as are\nuser preferences. It also seems necessary to move beyond a un igram model\nto accommodate notions of phrase or passage matching or Bool ean retrieval\noperators. Subsequent work in the LM approach has looked at a ddressing\nsome of these concerns, including putting relevance back in to the model and\nallowing a language mismatch between the query language and the docu-\nment language.\nThe model has signi\ufb01cant relations to traditional tf-idf mo dels. Term fre-\nquency is directly represented in tf-idf models, and much re cent work has\nrecognized the importance of document length normalizatio n. The effect of\ndoing a mixture of document generation probability with col lection gener-\nation probability is a little like idf: terms rare in the gene ral collection but\ncommon in some documents will have a greater in\ufb02uence on the r anking of\ndocuments. In most concrete realizations, the models share treating terms as\nif they were independent. On the other hand, the intuitions a re probabilistic\nrather than geometric, the mathematical models are more pri ncipled rather\nthan heuristic, and the details of how statistics like term f requency and doc-\nument length are used differ. If you are concerned mainly wit h performance\nnumbers, recent work has shown the LM approach to be very effe ctive in re-\ntrieval experiments, beating tf-idf and BM25 weights. Neve rtheless, there is\nOnline edition (c)\n2009 Cambridge UP250 12 Language models for information retrieval\nQuery Query model P(t|Query )\nDocument Doc. model P(t|Document )(a)\n(b)(c)\n\u25eeFigure 12.5 Three ways of developing the language modeling approach: (a ) query\nlikelihood, (b) document likelihood, and (c) model compari son.\nperhaps still insuf\ufb01cient evidence that its performance so greatly exceeds that\nof a well-tuned traditional vector space retrieval system a s to justify chang-\ning an existing implementation.\n12.4 Extended language modeling approaches\nIn this section we brie\ufb02y mention some of the work that extend s the basic\nlanguage modeling approach.\nThere are other ways to think of using the language modeling i dea in IR\nsettings, and many of them have been tried in subsequent work . Rather than\nlooking at the probability of a document language model Mdgenerating the\nquery, you can look at the probability of a query language mod elMqgener-\nating the document. The main reason that doing things in this direction and\ncreating a document likelihood model is less appealing is that there is much less DOCUMENT\nLIKELIHOOD MODEL text available to estimate a language model based on the quer y text, and so\nthe model will be worse estimated, and will have to depend mor e on being\nsmoothed with some other language model. On the other hand, i t is easy to\nsee how to incorporate relevance feedback into such a model: you can ex-\npand the query with terms taken from relevant documents in th e usual way\nand hence update the language model Mq(Zhai and Lafferty 2001a ). Indeed,\nwith appropriate modeling choices, this approach leads to t he BIM model of\nChapter 11. The relevance model of Lavrenko and Croft (2001 ) is an instance\nof a document likelihood model, which incorporates pseudo- relevance feed-\nback into a language modeling approach. It achieves very str ong empirical\nresults.\nRather than directly generating in either direction, we can make a lan-\nguage model from both the document and query, and then ask how different\nthese two language models are from each other. Lafferty and Zhai (2001 ) lay\nOnline edition (c)\n2009 Cambridge UP12.4 Extended language modeling approaches 251\nout these three ways of thinking about the problem, which we s how in Fig-\nure12.5, and develop a general risk minimization approach for docum ent\nretrieval. For instance, one way to model the risk of returni ng a document d\nas relevant to a query qis to use the Kullback-Leibler (KL) divergence between KULLBACK -LEIBLER\nDIVERGENCE their respective language models:\nR(d;q) =KL(Md/\u230aard\u230alMq) =\u2211\nt\u2208VP(t|Mq)logP(t|Mq)\nP(t|Md)(12.14)\nKL divergence is an asymmetric divergence measure originat ing in informa-\ntion theory, which measures how bad the probability distrib ution Mqis at\nmodeling Md(Cover and Thomas 1991 ,Manning and Sch\u00fctze 1999 ).Laf-\nferty and Zhai (2001 ) present results suggesting that a model comparison\napproach outperforms both query-likelihood and document- likelihood ap-\nproaches. One disadvantage of using KL divergence as a ranki ng function\nis that scores are not comparable across queries. This does n ot matter for ad\nhoc retrieval, but is important in other applications such a s topic tracking.\nKraaij and Spitters (2003 ) suggest an alternative proposal which models sim-\nilarity as a normalized log-likelihood ratio (or, equivale ntly, as a difference\nbetween cross-entropies).\nBasic LMs do not address issues of alternate expression, tha t is, synonymy,\nor any deviation in use of language between queries and docum ents. Berger\nand Lafferty (1999 ) introduce translation models to bridge this query-docume nt\ngap. A translation model lets you generate query words not in a document by TRANSLATION MODEL\ntranslation to alternate terms with similar meaning. This a lso provides a ba-\nsis for performing cross-language IR. We assume that the tra nslation model\ncan be represented by a conditional probability distributi onT(\u00b7|\u00b7)between\nvocabulary terms. The form of the translation query generat ion model is\nthen:\nP(q|Md) =\u220f\nt\u2208q\u2211\nv\u2208VP(v|Md)T(t|v) (12.15)\nThe term P(v|Md)is the basic document language model, and the term T(t|v)\nperforms translation. This model is clearly more computati onally intensive\nand we need to build a translation model. The translation mod el is usually\nbuilt using separate resources (such as a traditional thesa urus or bilingual\ndictionary or a statistical machine translation system\u2019s t ranslation diction-\nary), but can be built using the document collection if there are pieces of\ntext that naturally paraphrase or summarize other pieces of text. Candi-\ndate examples are documents and their titles or abstracts, o r documents and\nanchor-text pointing to them in a hypertext environment.\nBuilding extended LM approaches remains an active area of re search. In\ngeneral, translation models, relevance feedback models, a nd model compar-\nOnline edition (c)\n2009 Cambridge UP252 12 Language models for information retrieval\nison approaches have all been demonstrated to improve perfo rmance over\nthe basic query likelihood LM.\n12.5 References and further reading\nFor more details on the basic concepts of probabilistic lang uage models and\ntechniques for smoothing, see either Manning and Sch\u00fctze (1999 , Chapter 6)\norJurafsky and Martin (2008 , Chapter 4).\nThe important initial papers that originated the language m odeling ap-\nproach to IR are: ( Ponte and Croft 1998 ,Hiemstra 1998 ,Berger and Lafferty\n1999 ,Miller et al. 1999 ). Other relevant papers can be found in the next sev-\neral years of SIGIR proceedings. ( Croft and Lafferty 2003 ) contains a col-\nlection of papers from a workshop on language modeling appro aches and\nHiemstra and Kraaij (2005 ) review one prominent thread of work on using\nlanguage modeling approaches for TREC tasks. Zhai and Lafferty (2001b )\nclarify the role of smoothing in LMs for IR and present detail ed empirical\ncomparisons of different smoothing methods. Zaragoza et al. (2003 ) advo-\ncate using full Bayesian predictive distributions rather t han MAP point es-\ntimates, but while they outperform Bayesian smoothing, the y fail to outper-\nform a linear interpolation. Zhai and Lafferty (2002 ) argue that a two-stage\nsmoothing model with \ufb01rst Bayesian smoothing followed by li near interpo-\nlation gives a good model of the task, and performs better and more stably\nthan a single form of smoothing. A nice feature of the LM appro ach is that it\nprovides a convenient and principled way to put various kind s of prior infor-\nmation into the model; Kraaij et al. (2002 ) demonstrate this by showing the\nvalue of link information as a prior in improving web entry pa ge retrieval\nperformance. As brie\ufb02y discussed in Chapter 16(page 353),Liu and Croft\n(2004 ) show some gains by smoothing a document LM with estimates fr om\na cluster of similar documents; Tao et al. (2006 ) report larger gains by doing\ndocument-similarity based smoothing.\nHiemstra and Kraaij (2005 ) present TREC results showing a LM approach\nbeating use of BM25 weights. Recent work has achieved some ga ins by\ngoing beyond the unigram model, providing the higher order m odels are\nsmoothed with lower order models ( Gao et al. 2004 ,Cao et al. 2005 ), though\nthe gains to date remain modest. Sp\u00e4rck Jones (2004 ) presents a critical view-\npoint on the rationale for the language modeling approach, b utLafferty and\nZhai (2003 ) argue that a uni\ufb01ed account can be given of the probabilisti c\nsemantics underlying both the language modeling approach p resented in\nthis chapter and the classical probabilistic information r etrieval approach of\nChapter 11. The Lemur Toolkit ( http://www.lemurproject.org/ ) provides a \ufb02exi-\nble open source framework for investigating language model ing approaches\nto IR.\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 253\n13Text classi\ufb01cation and Naive\nBayes\nThus far, this book has mainly discussed the process of ad hoc retrieval , where\nusers have transient information needs that they try to addr ess by posing\none or more queries to a search engine. However, many users ha ve ongoing\ninformation needs. For example, you might need to track deve lopments in\nmulticore computer chips . One way of doing this is to issue the query multi-\ncoreANDcomputer ANDchip against an index of recent newswire articles each\nmorning. In this and the following two chapters we examine th e question:\nHow can this repetitive task be automated? To this end, many s ystems sup-\nport standing queries . A standing query is like any other query except that it STANDING QUERY\nis periodically executed on a collection to which new docume nts are incre-\nmentally added over time.\nIf your standing query is just multicore ANDcomputer ANDchip, you will tend\nto miss many relevant new articles which use other terms such asmulticore\nprocessors . To achieve good recall, standing queries thus have to be re\ufb01 ned\nover time and can gradually become quite complex. In this exa mple, using a\nBoolean search engine with stemming, you might end up with a q uery like\n(multicore ORmulti-core) AND(chipORprocessor ORmicroprocessor) .\nTo capture the generality and scope of the problem space to wh ich stand-\ning queries belong, we now introduce the general notion of a classi\ufb01cation CLASSIFICATION\nproblem. Given a set of classes , we seek to determine which class(es) a given\nobject belongs to. In the example, the standing query serves to divide new\nnewswire articles into the two classes: documentsaboutmulticorecomputerchips\nanddocuments not about multicore computer chips . We refer to this as two-class\nclassi\ufb01cation . Classi\ufb01cation using standing queries is also called routing or ROUTING\n\ufb01ltering and will be discussed further in Section 15.3.1 (page 335). FILTERING\nA class need not be as narrowly focused as the standing query multicore\ncomputer chips . Often, a class is a more general subject area like China orcoffee .\nSuch more general classes are usually referred to as topics , and the classi\ufb01ca-\ntion task is then called text classi\ufb01cation ,text categorization ,topic classi\ufb01cation , TEXT CLASSIFICATION\nortopic spotting . An example for China appears in Figure 13.1. Standing\nqueries and topics differ in their degree of speci\ufb01city, but the methods for\nOnline edition (c)\n2009 Cambridge UP254 13 Text classi\ufb01cation and Naive Bayes\nsolving routing, \ufb01ltering, and text classi\ufb01cation are esse ntially the same. We\ntherefore include routing and \ufb01ltering under the rubric of t ext classi\ufb01cation\nin this and the following chapters.\nThe notion of classi\ufb01cation is very general and has many appl ications within\nand beyond information retrieval (IR). For instance, in com puter vision, a\nclassi\ufb01er may be used to divide images into classes such as landscape ,por-\ntrait, and neither . We focus here on examples from information retrieval such\nas:\n\u2022Several of the preprocessing steps necessary for indexing a s discussed in\nChapter 2: detecting a document\u2019s encoding (ASCII, Unicode UTF-8 etc ;\npage 20); word segmentation (Is the white space between two letters a\nword boundary or not? page 24 ) ; truecasing (page 30); and identifying\nthe language of a document (page 46).\n\u2022The automatic detection of spam pages (which then are not inc luded in\nthe search engine index).\n\u2022The automatic detection of sexually explicit content (whic h is included in\nsearch results only if the user turns an option such as SafeSe arch off).\n\u2022Sentiment detection or the automatic classi\ufb01cation of a movie or product SENTIMENT DETECTION\nreview as positive or negative. An example application is a u ser search-\ning for negative reviews before buying a camera to make sure i t has no\nundesirable features or quality problems.\n\u2022Personal email sorting . A user may have folders like talk announcements , EMAIL SORTING\nelectronic bills ,email from family and friends , and so on, and may want a\nclassi\ufb01er to classify each incoming email and automaticall y move it to the\nappropriate folder. It is easier to \ufb01nd messages in sorted fo lders than in\na very large inbox. The most common case of this application i s a spam\nfolder that holds all suspected spam messages.\n\u2022Topic-speci\ufb01c or vertical search. Vertical search engines restrict searches to VERTICAL SEARCH\nENGINE a particular topic. For example, the query computer science on a vertical\nsearch engine for the topic China will return a list of Chinese computer\nscience departments with higher precision and recall than t he query com-\nputerscienceChina on a general purpose search engine. This is because the\nvertical search engine does not include web pages in its inde x that contain\nthe term china in a different sense (e.g., referring to a hard white ceramic ),\nbut does include relevant pages even if they do not explicitl y mention the\ntermChina .\n\u2022Finally, the ranking function in ad hoc information retriev al can also be\nbased on a document classi\ufb01er as we will explain in Section 15.4 (page 341).\nOnline edition (c)\n2009 Cambridge UP255\nThis list shows the general importance of classi\ufb01cation in I R. Most retrieval\nsystems today contain multiple components that use some for m of classi\ufb01er.\nThe classi\ufb01cation task we will use as an example in this book i s text classi\ufb01-\ncation.\nA computer is not essential for classi\ufb01cation. Many classi\ufb01 cation tasks\nhave traditionally been solved manually. Books in a library are assigned\nLibrary of Congress categories by a librarian. But manual cl assi\ufb01cation is\nexpensive to scale. The multicore computer chips example illustrates one al-\nternative approach: classi\ufb01cation by the use of standing qu eries \u2013 which can\nbe thought of as rules \u2013 most commonly written by hand. As in our exam- RULES IN TEXT\nCLASSIFICATION ple(multicore ORmulti-core) AND(chipORprocessor ORmicroprocessor) , rules are\nsometimes equivalent to Boolean expressions.\nA rule captures a certain combination of keywords that indic ates a class.\nHand-coded rules have good scaling properties, but creatin g and maintain-\ning them over time is labor intensive. A technically skilled person (e.g., a\ndomain expert who is good at writing regular expressions) ca n create rule\nsets that will rival or exceed the accuracy of the automatica lly generated clas-\nsi\ufb01ers we will discuss shortly; however, it can be hard to \ufb01nd someone with\nthis specialized skill.\nApart from manual classi\ufb01cation and hand-crafted rules, th ere is a third\napproach to text classi\ufb01cation, namely, machine learning- based text classi\ufb01-\ncation. It is the approach that we focus on in the next several chapters. In\nmachine learning, the set of rules or, more generally, the de cision criterion of\nthe text classi\ufb01er, is learned automatically from training data. This approach\nis also called statistical text classi\ufb01cation if the learning method is statistical. STATISTICAL TEXT\nCLASSIFICATION In statistical text classi\ufb01cation, we require a number of go od example docu-\nments (or training documents) for each class. The need for ma nual classi\ufb01-\ncation is not eliminated because the training documents com e from a person\nwho has labeled them \u2013 where labeling refers to the process of annotating LABELING\neach document with its class. But labeling is arguably an eas ier task than\nwriting rules. Almost anybody can look at a document and deci de whether\nor not it is related to China. Sometimes such labeling is alre ady implicitly\npart of an existing work\ufb02ow. For instance, you may go through the news\narticles returned by a standing query each morning and give r elevance feed-\nback (cf. Chapter 9) by moving the relevant articles to a special folder like\nmulticore-processors .\nWe begin this chapter with a general introduction to the text classi\ufb01cation\nproblem including a formal de\ufb01nition (Section 13.1); we then cover Naive\nBayes, a particularly simple and effective classi\ufb01cation m ethod (Sections 13.2\u2013\n13.4). All of the classi\ufb01cation algorithms we study represent do cuments in\nhigh-dimensional spaces. To improve the ef\ufb01ciency of these algorithms, it\nis generally desirable to reduce the dimensionality of thes e spaces; to this\nend, a technique known as feature selection is commonly applied in text clas-\nOnline edition (c)\n2009 Cambridge UP256 13 Text classi\ufb01cation and Naive Bayes\nsi\ufb01cation as discussed in Section 13.5. Section 13.6 covers evaluation of text\nclassi\ufb01cation. In the following chapters, Chapters 14and 15, we look at two\nother families of classi\ufb01cation methods, vector space clas si\ufb01ers and support\nvector machines.\n13.1 The text classi\ufb01cation problem\nIn text classi\ufb01cation, we are given a description d\u2208Xof a document, where\nXis the document space ; and a \ufb01xed set of classes C={c1,c2, . . . , cJ}. Classes DOCUMENT SPACE\nCLASS are also called categories orlabels . Typically, the document space Xis some\ntype of high-dimensional space, and the classes are human de \ufb01ned for the\nneeds of an application, as in the examples China and documents that talk\nabout multicore computer chips above. We are given a training set Dof labeled TRAINING SET\ndocuments/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht,where/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht\u2208X\u00d7C. For example:\n/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht=/an}\u230ara\u230bketle{tBeijing joins the World Trade Organization, China/an}\u230ara\u230bketri}ht\nfor the one-sentence document Beijing joins the World Trade Organization and\nthe class (or label) China .\nUsing a learning method orlearning algorithm , we then wish to learn a clas- LEARNING METHOD\nsi\ufb01er or classi\ufb01cation function \u03b3that maps documents to classes: CLASSIFIER\n\u03b3:X\u2192C (13.1)\nThis type of learning is called supervised learning because a supervisor (the SUPERVISED LEARNING\nhuman who de\ufb01nes the classes and labels training documents) serves as a\nteacher directing the learning process. We denote the super vised learning\nmethod by \u0393and write \u0393(D) =\u03b3. The learning method \u0393takes the training\nsetDas input and returns the learned classi\ufb01cation function \u03b3.\nMost names for learning methods \u0393are also used for classi\ufb01ers \u03b3. We\ntalk about the Naive Bayes (NB) learning method \u0393when we say that \u201cNaive\nBayes is robust,\u201d meaning that it can be applied to many diffe rent learning\nproblems and is unlikely to produce classi\ufb01ers that fail cat astrophically. But\nwhen we say that \u201cNaive Bayes had an error rate of 20%,\u201d we are d escribing\nan experiment in which a particular NB classi\ufb01er \u03b3(which was produced by\nthe NB learning method) had a 20% error rate in an application .\nFigure 13.1 shows an example of text classi\ufb01cation from the Reuters-RCV 1\ncollection, introduced in Section 4.2, page 69. There are six classes ( UK,China ,\n. . . ,sports ), each with three training documents. We show a few mnemonic\nwords for each document\u2019s content. The training set provide s some typical\nexamples for each class, so that we can learn the classi\ufb01cati on function \u03b3.\nOnce we have learned \u03b3, we can apply it to the test set (ortest data ), for ex- TEST SET\nample, the new document \ufb01rst private Chinese airline whose class is unknown.\nOnline edition (c)\n2009 Cambridge UP13.1 The text classi\ufb01cation problem 257\nclasses:\ntraining\nset:test\nset:regions industries subject areas\u03b3(d\u2032) =China\n\ufb01rst\nprivate\nChinese\nairlineUK China poultry coffee elections sports\nLondoncongestion\nBig BenParliament\nthe QueenWindsorBeijingOlympics\nGreatWalltourism\ncommunistMaochickenfeed\nduckspate\nturkeybird\ufb02ubeansroasting\nrobustaarabica\nharvestKenyavotesrecount\nrun-offseat\ncampaignTV adsbaseballdiamond\nsoccerforward\ncaptainteamd\u2032\n\u25eeFigure 13.1 Classes, training set, and test set in text classi\ufb01cation .\nIn Figure 13.1, the classi\ufb01cation function assigns the new document to cla ss\n\u03b3(d) =China , which is the correct assignment.\nThe classes in text classi\ufb01cation often have some interesti ng structure such\nas the hierarchy in Figure 13.1. There are two instances each of region cate-\ngories, industry categories, and subject area categories. A hierarchy can be\nan important aid in solving a classi\ufb01cation problem; see Sec tion 15.3.2 for\nfurther discussion. Until then, we will make the assumption in the text clas-\nsi\ufb01cation chapters that the classes form a set with no subset relationships\nbetween them.\nDe\ufb01nition ( 13.1) stipulates that a document is a member of exactly one\nclass. This is not the most appropriate model for the hierarc hy in Figure 13.1.\nFor instance, a document about the 2008 Olympics should be a m ember of\ntwo classes: the China class and the sports class. This type of classi\ufb01cation\nproblem is referred to as an any-of problem and we will return to it in Sec-\ntion 14.5 (page 306). For the time being, we only consider one-of problems\nwhere a document is a member of exactly one class.\nOur goal in text classi\ufb01cation is high accuracy on test data o rnew data \u2013 for\nexample, the newswire articles that we will encounter tomor row morning\nin the multicore chip example. It is easy to achieve high accu racy on the\ntraining set (e.g., we can simply memorize the labels). But h igh accuracy on\nthe training set in general does not mean that the classi\ufb01er w ill work well on\nOnline edition (c)\n2009 Cambridge UP258 13 Text classi\ufb01cation and Naive Bayes\nnew data in an application. When we use the training set to lea rn a classi\ufb01er\nfor test data, we make the assumption that training data and t est data are\nsimilar or from the same distribution . We defer a precise de\ufb01nition of this\nnotion to Section 14.6 (page 308).\n13.2 Naive Bayes text classi\ufb01cation\nThe \ufb01rst supervised learning method we introduce is the multinomial Naive MULTINOMIAL NAIVE\nBAYES Bayes ormultinomial NB model, a probabilistic learning method. The proba-\nbility of a document dbeing in class cis computed as\nP(c|d)\u221dP(c)\u220f\n1\u2264k\u2264ndP(tk|c) (13.2)\nwhere P(tk|c)is the conditional probability of term tkoccurring in a docu-\nment of class c.1We interpret P(tk|c)as a measure of how much evidence\ntkcontributes that cis the correct class. P(c)is the prior probability of a\ndocument occurring in class c. If a document\u2019s terms do not provide clear\nevidence for one class versus another, we choose the one that has a higher\nprior probability./an}\u230ara\u230bketle{tt1,t2, . . . , tnd/an}\u230ara\u230bketri}htare the tokens in dthat are part of the vocab-\nulary we use for classi\ufb01cation and ndis the number of such tokens in d. For\nexample,/an}\u230ara\u230bketle{tt1,t2, . . . , tnd/an}\u230ara\u230bketri}htfor the one-sentence document Beijing and Taipei join\nthe WTO might be/an}\u230ara\u230bketle{tBeijing ,Taipei ,join,WTO/an}\u230ara\u230bketri}ht, with nd=4, if we treat the terms\nandandtheas stop words.\nIn text classi\ufb01cation, our goal is to \ufb01nd the bestclass for the document. The\nbest class in NB classi\ufb01cation is the most likely or maximum a posteriori (MAP) MAXIMUM A\nPOSTERIORI CLASS class cmap:\ncmap=arg max\nc\u2208C\u02c6P(c|d) =arg max\nc\u2208C\u02c6P(c)\u220f\n1\u2264k\u2264nd\u02c6P(tk|c). (13.3)\nWe write \u02c6PforPbecause we do not know the true values of the parameters\nP(c)and P(tk|c), but estimate them from the training set as we will see in a\nmoment.\nIn Equation ( 13.3), many conditional probabilities are multiplied, one for\neach position 1\u2264k\u2264nd. This can result in a \ufb02oating point under\ufb02ow.\nIt is therefore better to perform the computation by adding l ogarithms of\nprobabilities instead of multiplying probabilities. The c lass with the highest\nlog probability score is still the most probable; log (xy) = log(x) +log(y)\nand the logarithm function is monotonic. Hence, the maximiz ation that is\n1. We will explain in the next section why P(c|d)is proportional to ( \u221d), not equal to the quantity\non the right.\nOnline edition (c)\n2009 Cambridge UP13.2 Naive Bayes text classi\ufb01cation 259\nactually done in most implementations of NB is:\ncmap=arg max\nc\u2208C[log\u02c6P(c) +\u2211\n1\u2264k\u2264ndlog\u02c6P(tk|c)]. (13.4)\nEquation ( 13.4) has a simple interpretation. Each conditional parameter\nlog\u02c6P(tk|c)is a weight that indicates how good an indicator tkis for c. Sim-\nilarly, the prior log \u02c6P(c)is a weight that indicates the relative frequency of\nc. More frequent classes are more likely to be the correct clas s than infre-\nquent classes. The sum of log prior and term weights is then a m easure of\nhow much evidence there is for the document being in the class , and Equa-\ntion ( 13.4) selects the class for which we have the most evidence.\nWe will initially work with this intuitive interpretation o f the multinomial\nNB model and defer a formal derivation to Section 13.4.\nHow do we estimate the parameters \u02c6P(c)and \u02c6P(tk|c)? We \ufb01rst try the\nmaximum likelihood estimate (MLE; Section 11.3.2 , page 226), which is sim-\nply the relative frequency and corresponds to the most likel y value of each\nparameter given the training data. For the priors this estim ate is:\n\u02c6P(c) =Nc\nN, (13.5)\nwhere Ncis the number of documents in class cand Nis the total number of\ndocuments.\nWe estimate the conditional probability \u02c6P(t|c)as the relative frequency of\nterm tin documents belonging to class c:\n\u02c6P(t|c) =Tct\n\u2211t\u2032\u2208VTct\u2032, (13.6)\nwhere Tctis the number of occurrences of tin training documents from class\nc, including multiple occurrences of a term in a document. We h ave made the\npositional independence assumption here, which we will discuss in more detail\nin the next section: Tctis a count of occurrences in all positions kin the doc-\numents in the training set. Thus, we do not compute different estimates for\ndifferent positions and, for example, if a word occurs twice in a document,\nin positions k1and k2, then \u02c6P(tk1|c) = \u02c6P(tk2|c).\nThe problem with the MLE estimate is that it is zero for a term\u2013 class combi-\nnation that did not occur in the training data. If the term WTO in the training\ndata only occurred in China documents, then the MLE estimates for the other\nclasses, for example UK, will be zero:\n\u02c6P(WTO|UK) =0.\nNow, the one-sentence document Britain is a member of the WTO will get a\nconditional probability of zero for UKbecause we are multiplying the condi-\ntional probabilities for all terms in Equation ( 13.2). Clearly, the model should\nOnline edition (c)\n2009 Cambridge UP260 13 Text classi\ufb01cation and Naive Bayes\nTRAIN MULTINOMIAL NB(C,D)\n1V\u2190EXTRACT VOCABULARY (D)\n2N\u2190COUNT DOCS(D)\n3for each c\u2208C\n4doNc\u2190COUNT DOCSINCLASS(D,c)\n5 prior[c]\u2190Nc/N\n6 text c\u2190CONCATENATE TEXTOFALLDOCSINCLASS(D,c)\n7 for each t\u2208V\n8 doTct\u2190COUNT TOKENS OFTERM(text c,t)\n9 for each t\u2208V\n10 docondprob [t][c]\u2190Tct+1\n\u2211t\u2032(Tct\u2032+1)\n11 return V,prior ,condprob\nAPPLY MULTINOMIAL NB(C,V,prior ,condprob ,d)\n1W\u2190EXTRACT TOKENS FROM DOC(V,d)\n2for each c\u2208C\n3doscore[c]\u2190logprior[c]\n4 for each t\u2208W\n5 doscore[c] += logcondprob [t][c]\n6return arg maxc\u2208Cscore[c]\n\u25eeFigure 13.2 Naive Bayes algorithm (multinomial model): Training and te sting.\nassign a high probability to the UKclass because the term Britain occurs. The\nproblem is that the zero probability for WTO cannot be \u201cconditioned away,\u201d\nno matter how strong the evidence for the class UKfrom other features. The\nestimate is 0 because of sparseness : The training data are never large enough SPARSENESS\nto represent the frequency of rare events adequately, for ex ample, the fre-\nquency of WTO occurring in UKdocuments.\nTo eliminate zeros, we use add-one orLaplace smoothing , which simply adds ADD -ONE SMOOTHING\none to each count (cf. Section 11.3.2 ):\n\u02c6P(t|c) =Tct+1\n\u2211t\u2032\u2208V(Tct\u2032+1)=Tct+1\n(\u2211t\u2032\u2208VTct\u2032) +B, (13.7)\nwhere B=|V|is the number of terms in the vocabulary. Add-one smoothing\ncan be interpreted as a uniform prior (each term occurs once f or each class)\nthat is then updated as evidence from the training data comes in. Note that\nthis is a prior probability for the occurrence of a term as opposed to the prior\nprobability of a class which we estimate in Equation ( 13.5) on the document\nlevel.\nOnline edition (c)\n2009 Cambridge UP13.2 Naive Bayes text classi\ufb01cation 261\n\u25eeTable 13.1 Data for parameter estimation examples.\ndocID words in document in c=China ?\ntraining set 1 Chinese Beijing Chinese yes\n2 Chinese Chinese Shanghai yes\n3 Chinese Macao yes\n4 Tokyo Japan Chinese no\ntest set 5 Chinese Chinese Chinese Tokyo Japan ?\n\u25eeTable 13.2 Training and test times for NB.\nmode time complexity\ntraining \u0398(|D|Lave+|C||V|)\ntesting \u0398(La+|C|Ma) =\u0398(|C|Ma)\nWe have now introduced all the elements we need for training a nd apply-\ning an NB classi\ufb01er. The complete algorithm is described in F igure 13.2.\n\u270eExample 13.1: For the example in Table 13.1, the multinomial parameters we\nneed to classify the test document are the priors \u02c6P(c) =3/4 and \u02c6P(c) =1/4 and the\nfollowing conditional probabilities:\n\u02c6P(Chinese|c) = ( 5+1)/(8+6) =6/14=3/7\n\u02c6P(Tokyo|c) = \u02c6P(Japan|c) = ( 0+1)/(8+6) =1/14\n\u02c6P(Chinese|c) = ( 1+1)/(3+6) =2/9\n\u02c6P(Tokyo|c) = \u02c6P(Japan|c) = ( 1+1)/(3+6) =2/9\nThe denominators are (8+6)and(3+6)because the lengths of text cand text care 8\nand 3, respectively, and because the constant Bin Equation ( 13.7) is 6 as the vocabu-\nlary consists of six terms.\nWe then get:\n\u02c6P(c|d5)\u221d3/4\u00b7(3/7)3\u00b71/14\u00b71/14\u22480.0003.\n\u02c6P(c|d5)\u221d1/4\u00b7(2/9)3\u00b72/9\u00b72/9\u22480.0001.\nThus, the classi\ufb01er assigns the test document to c=China . The reason for this clas-\nsi\ufb01cation decision is that the three occurrences of the posi tive indicator Chinese ind5\noutweigh the occurrences of the two negative indicators Japan andTokyo .\nWhat is the time complexity of NB? The complexity of computin g the pa-\nrameters is \u0398(|C||V|)because the set of parameters consists of |C||V|con-\nditional probabilities and |C|priors. The preprocessing necessary for com-\nputing the parameters (extracting the vocabulary, countin g terms, etc.) can\nbe done in one pass through the training data. The time comple xity of this\nOnline edition (c)\n2009 Cambridge UP262 13 Text classi\ufb01cation and Naive Bayes\ncomponent is therefore \u0398(|D|Lave), where|D|is the number of documents\nand Laveis the average length of a document.\nWe use \u0398(|D|Lave)as a notation for \u0398(T)here, where Tis the length of the\ntraining collection. This is nonstandard; \u0398(.)is not de\ufb01ned for an average.\nWe prefer expressing the time complexity in terms of Dand Lavebecause\nthese are the primary statistics used to characterize train ing collections.\nThe time complexity of A PPLY MULTINOMIAL NB in Figure 13.2 is\u0398(|C|La).\nLaand Maare the numbers of tokens and types, respectively, in the tes t doc-\nument. A PPLY MULTINOMIAL NB can be modi\ufb01ed to be \u0398(La+|C|Ma)(Ex-\nercise 13.8). Finally, assuming that the length of test documents is bou nded,\n\u0398(La+|C|Ma) =\u0398(|C|Ma)because La<b|C|Mafor a \ufb01xed constant b.2\nTable 13.2 summarizes the time complexities. In general, we have |C||V|<\n|D|Lave, so both training and testing complexity are linear in the ti me it takes\nto scan the data. Because we have to look at the data at least on ce, NB can be\nsaid to have optimal time complexity. Its ef\ufb01ciency is one re ason why NB is\na popular text classi\ufb01cation method.\n13.2.1 Relation to multinomial unigram language model\nThe multinomial NB model is formally identical to the multin omial unigram\nlanguage model (Section 12.2.1 , page 242). In particular, Equation ( 13.2) is\na special case of Equation ( 12.12 ) from page 243, which we repeat here for\n\u03bb=1:\nP(d|q)\u221dP(d)\u220f\nt\u2208qP(t|Md). (13.8)\nThe document din text classi\ufb01cation (Equation ( 13.2)) takes the role of the\nquery in language modeling (Equation ( 13.8)) and the classes cin text clas-\nsi\ufb01cation take the role of the documents din language modeling. We used\nEquation ( 13.8) to rank documents according to the probability that they ar e\nrelevant to the query q. In NB classi\ufb01cation, we are usually only interested\nin the top-ranked class.\nWe also used MLE estimates in Section 12.2.2 (page 243) and encountered\nthe problem of zero estimates owing to sparse data (page 244); but instead\nof add-one smoothing, we used a mixture of two distributions to address the\nproblem there. Add-one smoothing is closely related to add-1\n2smoothing in\nSection 11.3.4 (page 228).\n?Exercise 13.1\nWhy is|C||V|<|D|Lavein Table 13.2 expected to hold for most text collections?\n2. Our assumption here is that the length of test documents is bounded. Lawould exceed\nb|C|Mafor extremely long test documents.\nOnline edition (c)\n2009 Cambridge UP13.3 The Bernoulli model 263\nTRAIN BERNOULLI NB(C,D)\n1V\u2190EXTRACT VOCABULARY (D)\n2N\u2190COUNT DOCS(D)\n3for each c\u2208C\n4doNc\u2190COUNT DOCSINCLASS(D,c)\n5 prior[c]\u2190Nc/N\n6 for each t\u2208V\n7 doNct\u2190COUNT DOCSINCLASS CONTAINING TERM(D,c,t)\n8 condprob [t][c]\u2190(Nct+1)/(Nc+2)\n9return V,prior ,condprob\nAPPLY BERNOULLI NB(C,V,prior ,condprob ,d)\n1Vd\u2190EXTRACT TERMS FROM DOC(V,d)\n2for each c\u2208C\n3doscore[c]\u2190logprior[c]\n4 for each t\u2208V\n5 do if t\u2208Vd\n6 then score[c] += logcondprob [t][c]\n7 else score[c] += log(1\u2212condprob [t][c])\n8return arg maxc\u2208Cscore[c]\n\u25eeFigure 13.3 NB algorithm (Bernoulli model): Training and testing. The a dd-one\nsmoothing in Line 8 (top) is in analogy to Equation ( 13.7) with B=2.\n13.3 The Bernoulli model\nThere are two different ways we can set up an NB classi\ufb01er. The model we in-\ntroduced in the previous section is the multinomial model. I t generates one\nterm from the vocabulary in each position of the document, wh ere we as-\nsume a generative model that will be discussed in more detail in Section 13.4\n(see also page 237).\nAn alternative to the multinomial model is the multivariate Bernoulli model\norBernoulli model . It is equivalent to the binary independence model of Sec- BERNOULLI MODEL\ntion 11.3 (page 222), which generates an indicator for each term of the vo-\ncabulary, either 1 indicating presence of the term in the doc ument or 0 indi-\ncating absence. Figure 13.3 presents training and testing algorithms for the\nBernoulli model. The Bernoulli model has the same time compl exity as the\nmultinomial model.\nThe different generation models imply different estimatio n strategies and\ndifferent classi\ufb01cation rules. The Bernoulli model estima tes\u02c6P(t|c)as the frac-\ntion of documents of class cthat contain term t(Figure 13.3, TRAIN BERNOULLI -\nOnline edition (c)\n2009 Cambridge UP264 13 Text classi\ufb01cation and Naive Bayes\nNB, line 8). In contrast, the multinomial model estimates \u02c6P(t|c)as the frac-\ntion of tokens orfraction of positions in documents of class cthat contain term\nt(Equation ( 13.7)). When classifying a test document, the Bernoulli model\nuses binary occurrence information, ignoring the number of occurrences,\nwhereas the multinomial model keeps track of multiple occur rences. As a\nresult, the Bernoulli model typically makes many mistakes w hen classifying\nlong documents. For example, it may assign an entire book to t he class China\nbecause of a single occurrence of the term China .\nThe models also differ in how nonoccurring terms are used in c lassi\ufb01ca-\ntion. They do not affect the classi\ufb01cation decision in the mu ltinomial model;\nbut in the Bernoulli model the probability of nonoccurrence is factored in\nwhen computing P(c|d)(Figure 13.3, APPLY BERNOULLI NB, Line 7). This is\nbecause only the Bernoulli NB model models absence of terms e xplicitly.\n\u270eExample 13.2: Applying the Bernoulli model to the example in Table 13.1, we\nhave the same estimates for the priors as before: \u02c6P(c) = 3/4, \u02c6P(c) = 1/4. The\nconditional probabilities are:\n\u02c6P(Chinese|c) = ( 3+1)/(3+2) =4/5\n\u02c6P(Japan|c) = \u02c6P(Tokyo|c) = ( 0+1)/(3+2) =1/5\n\u02c6P(Beijing|c) = \u02c6P(Macao|c) = \u02c6P(Shanghai|c) = ( 1+1)/(3+2) =2/5\n\u02c6P(Chinese|c) = ( 1+1)/(1+2) =2/3\n\u02c6P(Japan|c) = \u02c6P(Tokyo|c) = ( 1+1)/(1+2) =2/3\n\u02c6P(Beijing|c) = \u02c6P(Macao|c) = \u02c6P(Shanghai|c) = ( 0+1)/(1+2) =1/3\nThe denominators are (3+2)and(1+2)because there are three documents in c\nand one document in cand because the constant Bin Equation ( 13.7) is 2 \u2013 there are\ntwo cases to consider for each term, occurrence and nonoccur rence.\nThe scores of the test document for the two classes are\n\u02c6P(c|d5)\u221d \u02c6P(c)\u00b7\u02c6P(Chinese|c)\u00b7\u02c6P(Japan|c)\u00b7\u02c6P(Tokyo|c)\n\u00b7(1\u2212\u02c6P(Beijing|c))\u00b7(1\u2212\u02c6P(Shanghai|c))\u00b7(1\u2212\u02c6P(Macao|c))\n= 3/4\u00b74/5\u00b71/5\u00b71/5\u00b7(1\u22122/5)\u00b7(1\u22122/5)\u00b7(1\u22122/5)\n\u2248 0.005\nand, analogously,\n\u02c6P(c|d5)\u221d1/4\u00b72/3\u00b72/3\u00b72/3\u00b7(1\u22121/3)\u00b7(1\u22121/3)\u00b7(1\u22121/3)\n\u2248 0.022\nThus, the classi\ufb01er assigns the test document to c=not-China . When looking only\nat binary occurrence and not at term frequency, Japan andTokyo are indicators for c\n(2/3>1/5) and the conditional probabilities of Chinese forcand care not different\nenough (4/5 vs. 2/3) to affect the classi\ufb01cation decision.\nOnline edition (c)\n2009 Cambridge UP13.4 Properties of Naive Bayes 265\n13.4 Properties of Naive Bayes\nTo gain a better understanding of the two models and the assum ptions they\nmake, let us go back and examine how we derived their classi\ufb01c ation rules in\nChapters 11and 12. We decide class membership of a document by assigning\nit to the class with the maximum a posteriori probability (cf . Section 11.3.2 ,\npage 226), which we compute as follows:\ncmap=arg max\nc\u2208CP(c|d)\n=arg max\nc\u2208CP(d|c)P(c)\nP(d)(13.9)\n=arg max\nc\u2208CP(d|c)P(c), (13.10)\nwhere Bayes\u2019 rule (Equation ( 11.4), page 220) is applied in ( 13.9) and we drop\nthe denominator in the last step because P(d)is the same for all classes and\ndoes not affect the argmax.\nWe can interpret Equation ( 13.10 ) as a description of the generative process\nwe assume in Bayesian text classi\ufb01cation. To generate a docu ment, we \ufb01rst\nchoose class cwith probability P(c)(top nodes in Figures 13.4 and 13.5). The\ntwo models differ in the formalization of the second step, th e generation of\nthe document given the class, corresponding to the conditio nal distribution\nP(d|c):\nMultinomial P(d|c) = P(/an}\u230ara\u230bketle{tt1, . . . , tk, . . . , tnd/an}\u230ara\u230bketri}ht|c) (13.11)\nBernoulli P(d|c) = P(/an}\u230ara\u230bketle{te1, . . . , ei, . . . , eM/an}\u230ara\u230bketri}ht|c), (13.12)\nwhere/an}\u230ara\u230bketle{tt1, . . . , tnd/an}\u230ara\u230bketri}htis the sequence of terms as it occurs in d(minus terms\nthat were excluded from the vocabulary) and /an}\u230ara\u230bketle{te1, . . . , ei, . . . , eM/an}\u230ara\u230bketri}htis a binary\nvector of dimensionality Mthat indicates for each term whether it occurs in\ndor not.\nIt should now be clearer why we introduced the document space Xin\nEquation ( 13.1) when we de\ufb01ned the classi\ufb01cation problem. A critical step\nin solving a text classi\ufb01cation problem is to choose the docu ment represen-\ntation./an}\u230ara\u230bketle{tt1, . . . , tnd/an}\u230ara\u230bketri}htand/an}\u230ara\u230bketle{te1, . . . , eM/an}\u230ara\u230bketri}htare two different document representa-\ntions. In the \ufb01rst case, Xis the set of all term sequences (or, more precisely,\nsequences of term tokens). In the second case, Xis{0, 1}M.\nWe cannot use Equations ( 13.11 ) and ( 13.12 ) for text classi\ufb01cation directly.\nFor the Bernoulli model, we would have to estimate 2M|C|different param-\neters, one for each possible combination of Mvalues eiand a class. The\nnumber of parameters in the multinomial case has the same ord er of magni-\nOnline edition (c)\n2009 Cambridge UP266 13 Text classi\ufb01cation and Naive Bayes\nC=China\nX1=Beijing X2=and X3=Taipei X4=join X5=WTO\n\u25eeFigure 13.4 The multinomial NB model.\ntude.3This being a very large quantity, estimating these paramete rs reliably\nis infeasible.\nTo reduce the number of parameters, we make the Naive Bayes conditional CONDITIONAL\nINDEPENDENCE\nASSUMPTIONindependence assumption . We assume that attribute values are independent of\neach other given the class:\nMultinomial P(d|c) = P(/an}\u230ara\u230bketle{tt1, . . . , tnd/an}\u230ara\u230bketri}ht|c) =\u220f\n1\u2264k\u2264ndP(Xk=tk|c) (13.13)\nBernoulli P(d|c) = P(/an}\u230ara\u230bketle{te1, . . . , eM/an}\u230ara\u230bketri}ht|c) =\u220f\n1\u2264i\u2264MP(Ui=ei|c). (13.14)\nWe have introduced two random variables here to make the two d ifferent\ngenerative models explicit. Xkis the random variable for position kin the RANDOM VARIABLE X\ndocument and takes as values terms from the vocabulary. P(Xk=t|c)is the\nprobability that in a document of class cthe term twill occur in position k.UiRANDOM VARIABLE U\nis the random variable for vocabulary term iand takes as values 0 (absence)\nand 1 (presence). \u02c6P(Ui=1|c)is the probability that in a document of class c\nthe term tiwill occur \u2013 in any position and possibly multiple times.\nWe illustrate the conditional independence assumption in F igures 13.4 and 13.5.\nThe class China generates values for each of the \ufb01ve term attributes (multi-\nnomial) or six binary attributes (Bernoulli) with a certain probability, inde-\npendent of the values of the other attributes. The fact that a document in the\nclass China contains the term Taipei does not make it more likely or less likely\nthat it also contains Beijing .\nIn reality, the conditional independence assumption does n ot hold for text\ndata. Terms areconditionally dependent on each other. But as we will dis-\ncuss shortly, NB models perform well despite the conditiona l independence\nassumption.\n3. In fact, if the length of documents is not bounded, the numb er of parameters in the multino-\nmial case is in\ufb01nite.\nOnline edition (c)\n2009 Cambridge UP13.4 Properties of Naive Bayes 267\nUAlaska =0 UBeijing =1 UIndia=0 Ujoin=1 UTaipei =1 UWTO =1C=China\n\u25eeFigure 13.5 The Bernoulli NB model.\nEven when assuming conditional independence, we still have too many\nparameters for the multinomial model if we assume a differen t probability\ndistribution for each position kin the document. The position of a term in a\ndocument by itself does not carry information about the clas s. Although\nthere is a difference between China sues France and France sues China , the\noccurrence of China in position 1 versus position 3 of the document is not\nuseful in NB classi\ufb01cation because we look at each term separ ately. The con-\nditional independence assumption commits us to this way of p rocessing the\nevidence.\nAlso, if we assumed different term distributions for each po sition k, we\nwould have to estimate a different set of parameters for each k. The probabil-\nity ofbean appearing as the \ufb01rst term of a coffee document could be different\nfrom it appearing as the second term, and so on. This again cau ses problems\nin estimation owing to data sparseness.\nFor these reasons, we make a second independence assumption for the\nmultinomial model, positional independence : The conditional probabilities for POSITIONAL\nINDEPENDENCE a term are the same independent of position in the document.\nP(Xk1=t|c) =P(Xk2=t|c)\nfor all positions k1,k2, terms tand classes c. Thus, we have a single dis-\ntribution of terms that is valid for all positions kiand we can use Xas its\nsymbol.4Positional independence is equivalent to adopting the bag o f words\nmodel, which we introduced in the context of ad hoc retrieval in Chapter 6\n(page 117).\nWith conditional and positional independence assumptions , we only need\nto estimate \u0398(M|C|)parameters P(tk|c)(multinomial model) or P(ei|c)(Bernoulli\n4. Our terminology is nonstandard. The random variable Xis a categorical variable, not a multi-\nnomial variable, and the corresponding NB model should perh aps be called a sequence model . We\nhave chosen to present this sequence model and the multinomi al model in Section 13.4.1 as the\nsame model because they are computationally identical.\nOnline edition (c)\n2009 Cambridge UP268 13 Text classi\ufb01cation and Naive Bayes\n\u25eeTable 13.3 Multinomial versus Bernoulli model.\nmultinomial model Bernoulli model\nevent model generation of token generation of document\nrandom variable(s) X=tifftoccurs at given pos Ut=1 iff toccurs in doc\ndocument representation d=/an}\u230ara\u230bketle{tt1, . . . , tk, . . . , tnd/an}\u230ara\u230bketri}ht,tk\u2208V d =/an}\u230ara\u230bketle{te1, . . . , ei, . . . , eM/an}\u230ara\u230bketri}ht,\nei\u2208{0, 1}\nparameter estimation \u02c6P(X=t|c) \u02c6P(Ui=e|c)\ndecision rule: maximize \u02c6P(c)\u220f1\u2264k\u2264nd\u02c6P(X=tk|c) \u02c6P(c)\u220fti\u2208V\u02c6P(Ui=ei|c)\nmultiple occurrences taken into account ignored\nlength of docs can handle longer docs works best for short doc s\n# features can handle more works best with fewer\nestimate for term the \u02c6P(X=the|c)\u22480.05 \u02c6P(Uthe=1|c)\u22481.0\nmodel), one for each term\u2013class combination, rather than a n umber that is\nat least exponential in M, the size of the vocabulary. The independence\nassumptions reduce the number of parameters to be estimated by several\norders of magnitude.\nTo summarize, we generate a document in the multinomial mode l (Fig-\nure13.4) by \ufb01rst picking a class C=cwith P(c)where Cis a random variable RANDOM VARIABLE C\ntaking values from Cas values. Next we generate term tkin position kwith\nP(Xk=tk|c)for each of the ndpositions of the document. The Xkall have\nthe same distribution over terms for a given c. In the example in Figure 13.4,\nwe show the generation of /an}\u230ara\u230bketle{tt1,t2,t3,t4,t5/an}\u230ara\u230bketri}ht=/an}\u230ara\u230bketle{tBeijing ,and,Taipei ,join,WTO/an}\u230ara\u230bketri}ht,\ncorresponding to the one-sentence document Beijing and Taipei join WTO .\nFor a completely speci\ufb01ed document generation model, we wou ld also\nhave to de\ufb01ne a distribution P(nd|c)over lengths. Without it, the multino-\nmial model is a token generation model rather than a document generation\nmodel.\nWe generate a document in the Bernoulli model (Figure 13.5) by \ufb01rst pick-\ning a class C=cwith P(c)and then generating a binary indicator eifor each\nterm tiof the vocabulary (1 \u2264i\u2264M). In the example in Figure 13.5, we\nshow the generation of /an}\u230ara\u230bketle{te1,e2,e3,e4,e5,e6/an}\u230ara\u230bketri}ht=/an}\u230ara\u230bketle{t0, 1, 0, 1, 1, 1/an}\u230ara\u230bketri}ht, corresponding,\nagain, to the one-sentence document Beijing and Taipei join WTO where we\nhave assumed that andis a stop word.\nWe compare the two models in Table 13.3, including estimation equations\nand decision rules.\nNaive Bayes is so called because the independence assumptio ns we have\njust made are indeed very naive for a model of natural languag e. The condi-\ntional independence assumption states that features are in dependent of each\nother given the class. This is hardly ever true for terms in do cuments. In\nmany cases, the opposite is true. The pairs hong andkong orlondon anden-\nOnline edition (c)\n2009 Cambridge UP13.4 Properties of Naive Bayes 269\n\u25eeTable 13.4 Correct estimation implies accurate prediction, but accur ate predic-\ntion does not imply correct estimation.\nc1 c2 class selected\ntrue probability P(c|d) 0.6 0.4 c1\n\u02c6P(c)\u220f1\u2264k\u2264nd\u02c6P(tk|c)(Equation ( 13.13 )) 0.00099 0.00001\nNB estimate \u02c6P(c|d) 0.99 0.01 c1\nglish in Figure 13.7 are examples of highly dependent terms. In addition, the\nmultinomial model makes an assumption of positional indepe ndence. The\nBernoulli model ignores positions in documents altogether because it only\ncares about absence or presence. This bag-of-words model di scards all in-\nformation that is communicated by the order of words in natur al language\nsentences. How can NB be a good text classi\ufb01er when its model o f natural\nlanguage is so oversimpli\ufb01ed?\nThe answer is that even though the probability estimates of NB are of low\nquality, its classi\ufb01cation decisions are surprisingly good. Consider a document\ndwith true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Ta-\nble13.4. Assume that dcontains many terms that are positive indicators for\nc1and many terms that are negative indicators for c2. Thus, when using the\nmultinomial model in Equation ( 13.13 ),\u02c6P(c1)\u220f1\u2264k\u2264nd\u02c6P(tk|c1)will be much\nlarger than \u02c6P(c2)\u220f1\u2264k\u2264nd\u02c6P(tk|c2)(0.00099 vs. 0.00001 in the table). After di-\nvision by 0.001 to get well-formed probabilities for P(c|d), we end up with\none estimate that is close to 1.0 and one that is close to 0.0. T his is common:\nThe winning class in NB classi\ufb01cation usually has a much larg er probabil-\nity than the other classes and the estimates diverge very sig ni\ufb01cantly from\nthe true probabilities. But the classi\ufb01cation decision is b ased on which class\ngets the highest score. It does not matter how accurate the es timates are. De-\nspite the bad estimates, NB estimates a higher probability f orc1and therefore\nassigns dto the correct class in Table 13.4.Correct estimation implies accurate\nprediction, but accurate prediction does not imply correct estimation. NB classi\ufb01ers\nestimate badly, but often classify well.\nEven if it is not the method with the highest accuracy for text , NB has many\nvirtues that make it a strong contender for text classi\ufb01cati on. It excels if there\nare many equally important features that jointly contribut e to the classi\ufb01-\ncation decision. It is also somewhat robust to noise feature s (as de\ufb01ned in\nthe next section) and concept drift \u2013 the gradual change over time of the con- CONCEPT DRIFT\ncept underlying a class like US president from Bill Clinton to George W. Bush\n(see Section 13.7). Classi\ufb01ers like kNN (Section 14.3, page 297) can be care-\nfully tuned to idiosyncratic properties of a particular tim e period. This will\nthen hurt them when documents in the following time period ha ve slightly\nOnline edition (c)\n2009 Cambridge UP270 13 Text classi\ufb01cation and Naive Bayes\n\u25eeTable 13.5 A set of documents for which the NB independence assumptions are\nproblematic.\n(1) He moved from London, Ontario, to London, England.\n(2) He moved from London, England, to London, Ontario.\n(3) He moved from England to London, Ontario.\ndifferent properties.\nThe Bernoulli model is particularly robust with respect to c oncept drift.\nWe will see in Figure 13.8 that it can have decent performance when using\nfewer than a dozen terms. The most important indicators for a class are less\nlikely to change. Thus, a model that only relies on these feat ures is more\nlikely to maintain a certain level of accuracy in concept dri ft.\nNB\u2019s main strength is its ef\ufb01ciency: Training and classi\ufb01ca tion can be ac-\ncomplished with one pass over the data. Because it combines e f\ufb01ciency with\ngood accuracy it is often used as a baseline in text classi\ufb01ca tion research.\nIt is often the method of choice if (i) squeezing out a few extr a percentage\npoints of accuracy is not worth the trouble in a text classi\ufb01c ation application,\n(ii) a very large amount of training data is available and the re is more to be\ngained from training on a lot of data than using a better class i\ufb01er on a smaller\ntraining set, or (iii) if its robustness to concept drift can be exploited.\nIn this book, we discuss NB as a classi\ufb01er for text. The indepe ndence as-\nsumptions do not hold for text. However, it can be shown that N B is an\noptimal classi\ufb01er (in the sense of minimal error rate on new data) for data OPTIMAL CLASSIFIER\nwhere the independence assumptions do hold.\n13.4.1 A variant of the multinomial model\nAn alternative formalization of the multinomial model repr esents each doc-\nument das an M-dimensional vector of counts /an}\u230ara\u230bketle{ttft1,d, . . . , tf tM,d/an}\u230ara\u230bketri}htwhere tf ti,d\nis the term frequency of tiind.P(d|c)is then computed as follows (cf. Equa-\ntion ( 12.8), page 243);\nP(d|c) =P(/an}\u230ara\u230bketle{ttft1,d, . . . , tf tM,d/an}\u230ara\u230bketri}ht|c)\u221d\u220f\n1\u2264i\u2264MP(X=ti|c)tfti,d(13.15)\nNote that we have omitted the multinomial factor. See Equati on (12.8) (page 243).\nEquation ( 13.15 ) is equivalent to the sequence model in Equation ( 13.2) as\nP(X=ti|c)tfti,d=1 for terms that do not occur in d(tfti,d=0) and a term\nthat occurs tf ti,d\u22651 times will contribute tf ti,dfactors both in Equation ( 13.2)\nand in Equation ( 13.15 ).\nOnline edition (c)\n2009 Cambridge UP13.5 Feature selection 271\nSELECT FEATURES (D,c,k)\n1V\u2190EXTRACT VOCABULARY (D)\n2L\u2190[]\n3for each t\u2208V\n4doA(t,c)\u2190COMPUTE FEATURE UTILITY (D,t,c)\n5 A PPEND (L,/an}\u230ara\u230bketle{tA(t,c),t/an}\u230ara\u230bketri}ht)\n6return FEATURES WITHLARGEST VALUES (L,k)\n\u25eeFigure 13.6 Basic feature selection algorithm for selecting the kbest features.\n?Exercise 13.2 [\u22c6]\nWhich of the documents in Table 13.5 have identical and different bag of words rep-\nresentations for (i) the Bernoulli model (ii) the multinomi al model? If there are differ-\nences, describe them.\nExercise 13.3\nThe rationale for the positional independence assumption i s that there is no useful\ninformation in the fact that a term occurs in position kof a document. Find exceptions.\nConsider formulaic documents with a \ufb01xed document structur e.\nExercise 13.4\nTable 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the\ndifference.\n13.5 Feature selection\nFeature selection is the process of selecting a subset of the terms occurring FEATURE SELECTION\nin the training set and using only this subset as features in t ext classi\ufb01ca-\ntion. Feature selection serves two main purposes. First, it makes training\nand applying a classi\ufb01er more ef\ufb01cient by decreasing the siz e of the effective\nvocabulary. This is of particular importance for classi\ufb01er s that, unlike NB,\nare expensive to train. Second, feature selection often inc reases classi\ufb01ca-\ntion accuracy by eliminating noise features. A noise feature is one that, when NOISE FEATURE\nadded to the document representation, increases the classi \ufb01cation error on\nnew data. Suppose a rare term, say arachnocentric , has no information about\na class, say China , but all instances of arachnocentric happen to occur in China\ndocuments in our training set. Then the learning method migh t produce a\nclassi\ufb01er that misassigns test documents containing arachnocentric toChina .\nSuch an incorrect generalization from an accidental proper ty of the training\nset is called over\ufb01tting . OVERFITTING\nWe can view feature selection as a method for replacing a comp lex clas-\nsi\ufb01er (using all features) with a simpler one (using a subset of the features).\nOnline edition (c)\n2009 Cambridge UP272 13 Text classi\ufb01cation and Naive Bayes\nIt may appear counterintuitive at \ufb01rst that a seemingly weak er classi\ufb01er is\nadvantageous in statistical text classi\ufb01cation, but when d iscussing the bias-\nvariance tradeoff in Section 14.6 (page 308), we will see that weaker models\nare often preferable when limited training data are availab le.\nThe basic feature selection algorithm is shown in Figure 13.6. For a given\nclass c, we compute a utility measure A(t,c)for each term of the vocabulary\nand select the kterms that have the highest values of A(t,c). All other terms\nare discarded and not used in classi\ufb01cation. We will introdu ce three different\nutility measures in this section: mutual information, A(t,c) = I(Ut;Cc); the\n\u03c72test, A(t,c) =X2(t,c); and frequency, A(t,c) =N(t,c).\nOf the two NB models, the Bernoulli model is particularly sen sitive to\nnoise features. A Bernoulli NB classi\ufb01er requires some form of feature se-\nlection or else its accuracy will be low.\nThis section mainly addresses feature selection for two-cl ass classi\ufb01cation\ntasks like China versus not-China . Section 13.5.5 brie\ufb02y discusses optimiza-\ntions for systems with more than two classes.\n13.5.1 Mutual information\nA common feature selection method is to compute A(t,c)as the expected\nmutual information (MI) of term tand class c.5MI measures how much in- MUTUAL INFORMATION\nformation the presence/absence of a term contributes to mak ing the correct\nclassi\ufb01cation decision on c. Formally:\nI(U;C) = \u2211\net\u2208{1,0}\u2211\nec\u2208{1,0}P(U=et,C=ec)log2P(U=et,C=ec)\nP(U=et)P(C=ec), (13.16)\nwhere Uis a random variable that takes values et=1 (the document contains\nterm t) and et=0 (the document does not contain t), as de\ufb01ned on page 266,\nand Cis a random variable that takes values ec=1 (the document is in class\nc) and ec=0 (the document is not in class c). We write Utand Ccif it is not\nclear from context which term tand class cwe are referring to.\nForMLEs of the probabilities, Equation ( 13.16 ) is equivalent to Equation ( 13.17 ):\nI(U;C) =N11\nNlog2NN 11\nN1.N.1+N01\nNlog2NN 01\nN0.N.1(13.17)\n+N10\nNlog2NN 10\nN1.N.0+N00\nNlog2NN 00\nN0.N.0\nwhere the Ns are counts of documents that have the values of etand ecthat\nare indicated by the two subscripts. For example, N10is the number of doc-\n5. Take care not to confuse expected mutual information with pointwise mutual information ,\nwhich is de\ufb01ned as log N11/E11where N11and E11are de\ufb01ned as in Equation ( 13.18 ). The\ntwo measures have different properties. See Section 13.7.\nOnline edition (c)\n2009 Cambridge UP13.5 Feature selection 273\numents that contain t(et=1) and are not in c(ec=0).N1.=N10+N11is\nthe number of documents that contain t(et=1) and we count documents\nindependent of class membership ( ec\u2208{0, 1}).N=N00+N01+N10+N11\nis the total number of documents. An example of one of the MLE e stimates\nthat transform Equation ( 13.16 ) into Equation ( 13.17 ) isP(U=1,C=1) =\nN11/N.\n\u270eExample 13.3: Consider the class poultry and the term export in Reuters-RCV1.\nThe counts of the number of documents with the four possible c ombinations of indi-\ncator values are as follows:\nec=epoultry =1ec=epoultry =0\net=eexport=1 N11=49 N10=27,652\net=eexport=0 N01=141 N00=774,106\nAfter plugging these values into Equation ( 13.17 ) we get:\nI(U;C) =49\n801,948log2801,948\u00b749\n(49+27,652 )(49+141)\n+141\n801,948log2801,948\u00b7141\n(141+774,106 )(49+141)\n+27,652\n801,948log2801,948\u00b727,652\n(49+27,652 )(27,652 +774,106 )\n+774,106\n801,948log2801,948\u00b7774,106\n(141+774,106 )(27,652 +774,106 )\n\u2248 0.0001105\nTo select kterms t1, . . . , tkfor a given class, we use the feature selection al-\ngorithm in Figure 13.6: We compute the utility measure as A(t,c) =I(Ut,Cc)\nand select the kterms with the largest values.\nMutual information measures how much information \u2013 in the in formation-\ntheoretic sense \u2013 a term contains about the class. If a term\u2019s distribution is\nthe same in the class as it is in the collection as a whole, then I(U;C) =\n0. MI reaches its maximum value if the term is a perfect indica tor for class\nmembership, that is, if the term is present in a document if an d only if the\ndocument is in the class.\nFigure 13.7 shows terms with high mutual information scores for the six\nclasses in Figure 13.1.6The selected terms (e.g., london ,uk,british for the class\nUK) are of obvious utility for making classi\ufb01cation decisions for their respec-\ntive classes. At the bottom of the list for UKwe \ufb01nd terms like peripherals\nandtonight (not shown in the \ufb01gure) that are clearly not helpful in decid ing\n6. Feature scores were computed on the \ufb01rst 100,000 document s, except for poultry , a rare class,\nfor which 800,000 documents were used. We have omitted numbe rs and other special words\nfrom the top ten lists.\nOnline edition (c)\n2009 Cambridge UP274 13 Text classi\ufb01cation and Naive Bayes\nUK\nlondon 0.1925\nuk 0.0755\nbritish 0.0596\nstg 0.0555\nbritain 0.0469\nplc 0.0357\nengland 0.0238\npence 0.0212\npounds 0.0149\nenglish 0.0126China\nchina 0.0997\nchinese 0.0523\nbeijing 0.0444\nyuan 0.0344\nshanghai 0.0292\nhong 0.0198\nkong 0.0195\nxinhua 0.0155\nprovince 0.0117\ntaiwan 0.0108poultry\npoultry 0.0013\nmeat 0.0008\nchicken 0.0006\nagriculture 0.0005\navian 0.0004\nbroiler 0.0003\nveterinary 0.0003\nbirds 0.0003\ninspection 0.0003\npathogenic 0.0003\ncoffee\ncoffee 0.0111\nbags 0.0042\ngrowers 0.0025\nkg 0.0019\ncolombia 0.0018\nbrazil 0.0016\nexport 0.0014\nexporters 0.0013\nexports 0.0013\ncrop 0.0012elections\nelection 0.0519\nelections 0.0342\npolls 0.0339\nvoters 0.0315\nparty 0.0303\nvote 0.0299\npoll 0.0225\ncandidate 0.0202\ncampaign 0.0202\ndemocratic 0.0198sports\nsoccer 0.0681\ncup 0.0515\nmatch 0.0441\nmatches 0.0408\nplayed 0.0388\nleague 0.0386\nbeat 0.0301\ngame 0.0299\ngames 0.0284\nteam 0.0264\n\u25eeFigure 13.7 Features with high mutual information scores for six Reuter s-RCV1\nclasses.\nwhether the document is in the class. As you might expect, kee ping the in-\nformative terms and eliminating the non-informative ones t ends to reduce\nnoise and improve the classi\ufb01er\u2019s accuracy.\nSuch an accuracy increase can be observed in Figure 13.8, which shows\nF1as a function of vocabulary size after feature selection for Reuters-RCV1.7\nComparing F1at 132,776 features (corresponding to selection of all feat ures)\nand at 10\u2013100 features, we see that MI feature selection incr eases F1by about\n0.1 for the multinomial model and by more than 0.2 for the Bern oulli model.\nFor the Bernoulli model, F1peaks early, at ten features selected. At that point,\nthe Bernoulli model is better than the multinomial model. Wh en basing a\nclassi\ufb01cation decision on only a few features, it is more rob ust to consider bi-\nnary occurrence only. For the multinomial model (MI feature selection), the\npeak occurs later, at 100 features, and its effectiveness re covers somewhat at\n7. We trained the classi\ufb01ers on the \ufb01rst 100,000 documents an d computed F1on the next 100,000.\nThe graphs are averages over \ufb01ve classes.\nOnline edition (c)\n2009 Cambridge UP13.5 Feature selection 275\n## ######\n#\n##### #\n11 0 100 1000 100000.0 0.2 0.4 0.6 0.8\nnumber of features selectedF1 measure\noooooooooo\noooo o\nxx\nxx\nx\nx\nxxxx xxxx x\nbbbbbbb\nb\nb\nb\nbb bb b\n#\no\nx\nbmultinomial, MI\nmultinomial, chisquare\nmultinomial, frequency\nbinomial, MI\n\u25eeFigure 13.8 Effect of feature set size on accuracy for multinomial and Be rnoulli\nmodels.\nthe end when we use all features. The reason is that the multin omial takes\nthe number of occurrences into account in parameter estimat ion and clas-\nsi\ufb01cation and therefore better exploits a larger number of f eatures than the\nBernoulli model. Regardless of the differences between the two methods,\nusing a carefully selected subset of the features results in better effectiveness\nthan using all features.\n13.5.2 \u03c72Feature selection\nAnother popular feature selection method is \u03c72. In statistics, the \u03c72test is \u03c72FEATURE SELECTION\napplied to test the independence of two events, where two eve nts A and B are\nde\ufb01ned to be independent ifP(AB) = P(A)P(B)or, equivalently, P(A|B) = INDEPENDENCE\nP(A)and P(B|A) =P(B). In feature selection, the two events are occurrence\nof the term and occurrence of the class. We then rank terms wit h respect to\nthe following quantity:\nX2(D,t,c) = \u2211\net\u2208{0,1}\u2211\nec\u2208{0,1}(Netec\u2212Eetec)2\nEetec(13.18)\nOnline edition (c)\n2009 Cambridge UP276 13 Text classi\ufb01cation and Naive Bayes\nwhere etand ecare de\ufb01ned as in Equation ( 13.16 ).Nis the observed frequency\ninDand Etheexpected frequency. For example, E11is the expected frequency\noftand coccurring together in a document assuming that term and clas s are\nindependent.\n\u270eExample 13.4: We \ufb01rst compute E11for the data in Example 13.3:\nE11= N\u00d7P(t)\u00d7P(c) =N\u00d7N11+N10\nN\u00d7N11+N01\nN\n= N\u00d749+141\nN\u00d749+27652\nN\u22486.6\nwhere Nis the total number of documents as before.\nWe compute the other Eetecin the same way:\nepoultry =1 epoultry =0\neexport=1 N11=49 E11\u22486.6 N10=27,652 E10\u224827,694.4\neexport=0 N01=141 E01\u2248183.4 N00=774,106 E00\u2248774,063.6\nPlugging these values into Equation ( 13.18 ), we get a X2value of 284:\nX2(D,t,c) = \u2211\net\u2208{0,1}\u2211\nec\u2208{0,1}(Netec\u2212Eetec)2\nEetec\u2248284\nX2is a measure of how much expected counts Eand observed counts N\ndeviate from each other. A high value of X2indicates that the hypothesis of\nindependence, which implies that expected and observed cou nts are similar,\nis incorrect. In our example, X2\u2248284>10.83. Based on Table 13.6, we\ncan reject the hypothesis that poultry andexport are independent with only a\n0.001 chance of being wrong.8Equivalently, we say that the outcome X2\u2248\n284>10.83 is statistically signi\ufb01cant at the 0.001 level. If the two events are STATISTICAL\nSIGNIFICANCE dependent, then the occurrence of the term makes the occurre nce of the class\nmore likely (or less likely), so it should be helpful as a feat ure. This is the\nrationale of \u03c72feature selection.\nAn arithmetically simpler way of computing X2is the following:\nX2(D,t,c) =(N11+N10+N01+N00)\u00d7(N11N00\u2212N10N01)2\n(N11+N01)\u00d7(N11+N10)\u00d7(N10+N00)\u00d7(N01+N00)(13.19)\nThis is equivalent to Equation ( 13.18 ) (Exercise 13.14 ).\n8. We can make this inference because, if the two events are in dependent, then X2\u223c\u03c72, where\n\u03c72is the \u03c72distribution. See, for example, Rice (2006 ).\nOnline edition (c)\n2009 Cambridge UP13.5 Feature selection 277\n\u25eeTable 13.6 Critical values of the \u03c72distribution with one degree of freedom. For\nexample, if the two events are independent, then P(X2>6.63)<0.01. So for X2>\n6.63 the assumption of independence can be rejected with 99% con\ufb01dence.\np \u03c72critical value\n0.1 2.71\n0.05 3.84\n0.01 6.63\n0.005 7.88\n0.001 10.83\n\u2704Assessing \u03c72as a feature selection method\nFrom a statistical point of view, \u03c72feature selection is problematic. For a\ntest with one degree of freedom, the so-called Yates correct ion should be\nused (see Section 13.7), which makes it harder to reach statistical signi\ufb01cance.\nAlso, whenever a statistical test is used multiple times, th en the probability\nof getting at least one error increases. If 1,000 hypotheses are rejected, each\nwith 0.05 error probability, then 0.05 \u00d71000=50 calls of the test will be\nwrong on average. However, in text classi\ufb01cation it rarely m atters whether a\nfew additional terms are added to the feature set or removed f rom it. Rather,\ntherelative importance of features is important. As long as \u03c72feature selec-\ntion only ranks features with respect to their usefulness an d is not used to\nmake statements about statistical dependence or independe nce of variables,\nwe need not be overly concerned that it does not adhere strict ly to statistical\ntheory.\n13.5.3 Frequency-based feature selection\nA third feature selection method is frequency-based feature selection , that is,\nselecting the terms that are most common in the class. Freque ncy can be\neither de\ufb01ned as document frequency (the number of document s in the class\ncthat contain the term t) or as collection frequency (the number of tokens of\ntthat occur in documents in c). Document frequency is more appropriate for\nthe Bernoulli model, collection frequency for the multinom ial model.\nFrequency-based feature selection selects some frequent t erms that have\nno speci\ufb01c information about the class, for example, the day s of the week\n(Monday ,Tuesday , . . . ), which are frequent across classes in newswire text.\nWhen many thousands of features are selected, then frequenc y-based fea-\nture selection often does well. Thus, if somewhat suboptima l accuracy is\nacceptable, then frequency-based feature selection can be a good alternative\nto more complex methods. However, Figure 13.8 is a case where frequency-\nOnline edition (c)\n2009 Cambridge UP278 13 Text classi\ufb01cation and Naive Bayes\nbased feature selection performs a lot worse than MI and \u03c72and should not\nbe used.\n13.5.4 Feature selection for multiple classi\ufb01ers\nIn an operational system with a large number of classi\ufb01ers, i t is desirable\nto select a single set of features instead of a different one f or each classi\ufb01er.\nOne way of doing this is to compute the X2statistic for an n\u00d72 table where\nthe columns are occurrence and nonoccurrence of the term and each row\ncorresponds to one of the classes. We can then select the kterms with the\nhighest X2statistic as before.\nMore commonly, feature selection statistics are \ufb01rst compu ted separately\nfor each class on the two-class classi\ufb01cation task cversus cand then com-\nbined. One combination method computes a single \ufb01gure of mer it for each\nfeature, for example, by averaging the values A(t,c)for feature t, and then\nselects the kfeatures with highest \ufb01gures of merit. Another frequently u sed\ncombination method selects the top k/nfeatures for each of nclassi\ufb01ers and\nthen combines these nsets into one global feature set.\nClassi\ufb01cation accuracy often decreases when selecting kcommon features\nfor a system with nclassi\ufb01ers as opposed to ndifferent sets of size k. But even\nif it does, the gain in ef\ufb01ciency owing to a common document re presentation\nmay be worth the loss in accuracy.\n13.5.5 Comparison of feature selection methods\nMutual information and \u03c72represent rather different feature selection meth-\nods. The independence of term tand class ccan sometimes be rejected with\nhigh con\ufb01dence even if tcarries little information about membership of a\ndocument in c. This is particularly true for rare terms. If a term occurs on ce\nin a large collection and that one occurrence is in the poultry class, then this\nis statistically signi\ufb01cant. But a single occurrence is not very informative\naccording to the information-theoretic de\ufb01nition of infor mation. Because\nits criterion is signi\ufb01cance, \u03c72selects more rare terms (which are often less\nreliable indicators) than mutual information. But the sele ction criterion of\nmutual information also does not necessarily select the ter ms that maximize\nclassi\ufb01cation accuracy.\nDespite the differences between the two methods, the classi \ufb01cation accu-\nracy of feature sets selected with \u03c72and MI does not seem to differ systemat-\nically. In most text classi\ufb01cation problems, there are a few strong indicators\nand many weak indicators. As long as all strong indicators an d a large num-\nber of weak indicators are selected, accuracy is expected to be good. Both\nmethods do this.\nFigure 13.8 compares MI and \u03c72feature selection for the multinomial model.\nOnline edition (c)\n2009 Cambridge UP13.6 Evaluation of text classi\ufb01cation 279\nPeak effectiveness is virtually the same for both methods. \u03c72reaches this\npeak later, at 300 features, probably because the rare, but h ighly signi\ufb01cant\nfeatures it selects initially do not cover all documents in t he class. However,\nfeatures selected later (in the range of 100\u2013300) are of bett er quality than those\nselected by MI.\nAll three methods \u2013 MI, \u03c72and frequency based \u2013 are greedy methods. GREEDY FEATURE\nSELECTION They may select features that contribute no incremental inf ormation over\npreviously selected features. In Figure 13.7,kong is selected as the seventh\nterm even though it is highly correlated with previously sel ectedhong and\ntherefore redundant. Although such redundancy can negativ ely impact ac-\ncuracy, non-greedy methods (see Section 13.7 for references) are rarely used\nin text classi\ufb01cation due to their computational cost.\n?Exercise 13.5\nConsider the following frequencies for the class coffee for four terms in the \ufb01rst 100,000\ndocuments of Reuters-RCV1:\nterm N00 N01 N10 N11\nbrazil 98,012 102 1835 51\ncouncil 96,322 133 3525 20\nproducers 98,524 119 1118 34\nroasted 99,824 143 23 10\nSelect two of these four terms based on (i) \u03c72, (ii) mutual information, (iii) frequency.\n13.6 Evaluation of text classi\ufb01cation\n] Historically, the classic Reuters-21578 collection was t he main benchmark\nfor text classi\ufb01cation evaluation. This is a collection of 2 1,578 newswire ar-\nticles, originally collected and labeled by Carnegie Group , Inc. and Reuters,\nLtd. in the course of developing the CONSTRUE text classi\ufb01ca tion system.\nIt is much smaller than and predates the Reuters-RCV1 collec tion discussed\nin Chapter 4(page 69). The articles are assigned classes from a set of 118\ntopic categories. A document may be assigned several classe s or none, but\nthe commonest case is single assignment (documents with at l east one class\nreceived an average of 1.24 classes). The standard approach to this any-of\nproblem (Chapter 14, page 306) is to learn 118 two-class classi\ufb01ers, one for\neach class, where the two-class classi\ufb01er for class cis the classi\ufb01er for the two TWO -CLASS CLASSIFIER\nclasses cand its complement c.\nFor each of these classi\ufb01ers, we can measure recall, precisi on, and accu-\nracy. In recent work, people almost invariably use the ModApte split , which MODAPTE SPLIT\nincludes only documents that were viewed and assessed by a hu man indexer,\nOnline edition (c)\n2009 Cambridge UP280 13 Text classi\ufb01cation and Naive Bayes\n\u25eeTable 13.7 The ten largest classes in the Reuters-21578 collection wit h number of\ndocuments in training and test sets.\nclass # train # testclass # train # test\nearn 2877 1087 trade 369 119\nacquisitions 1650 179 interest 347 131\nmoney -fx 538 179 ship 197 89\ngrain 433 149 wheat 212 71\ncrude 389 189 corn 182 56\nand comprises 9,603 training documents and 3,299 test docum ents. The dis-\ntribution of documents in classes is very uneven, and some wo rk evaluates\nsystems on only documents in the ten largest classes. They ar e listed in Ta-\nble13.7. A typical document with topics is shown in Figure 13.9.\nIn Section 13.1, we stated as our goal in text classi\ufb01cation the minimizatio n\nof classi\ufb01cation error on test data. Classi\ufb01cation error is 1.0 minus classi\ufb01ca-\ntion accuracy, the proportion of correct decisions, a measu re we introduced\nin Section 8.3(page 155). This measure is appropriate if the percentage of\ndocuments in the class is high, perhaps 10% to 20% and higher. But as we\ndiscussed in Section 8.3, accuracy is not a good measure for \u201csmall\u201d classes\nbecause always saying no, a strategy that defeats the purpos e of building a\nclassi\ufb01er, will achieve high accuracy. The always-no class i\ufb01er is 99% accurate\nfor a class with relative frequency 1%. For small classes, pr ecision, recall and\nF1are better measures.\nWe will use effectiveness as a generic term for measures that evaluate the EFFECTIVENESS\nquality of classi\ufb01cation decisions, including precision, recall, F1, and accu-\nracy. Performance refers to the computational ef\ufb01ciency of classi\ufb01cation and PERFORMANCE\nEFFICIENCY IR systems in this book. However, many researchers mean effe ctiveness, not\nef\ufb01ciency of text classi\ufb01cation when they use the term perfo rmance.\nWhen we process a collection with several two-class classi\ufb01 ers (such as\nReuters-21578 with its 118 classes), we often want to comput e a single ag-\ngregate measure that combines the measures for individual c lassi\ufb01ers. There\nare two methods for doing this. Macroaveraging computes a simple aver- MACROAVERAGING\nage over classes. Microaveraging pools per-document decisions across classes, MICROAVERAGING\nand then computes an effectiveness measure on the pooled con tingency ta-\nble. Table 13.8 gives an example.\nThe differences between the two methods can be large. Macroa veraging\ngives equal weight to each class, whereas microaveraging gi ves equal weight\nto each per-document classi\ufb01cation decision. Because the F1measure ignores\ntrue negatives and its magnitude is mostly determined by the number of\ntrue positives, large classes dominate small classes in mic roaveraging. In the\nexample, microaveraged precision (0.83) is much closer to t he precision of\nOnline edition (c)\n2009 Cambridge UP13.6 Evaluation of text classi\ufb01cation 281\n<REUTERS TOPICS=\u2019\u2019YES\u2019\u2019 LEWISSPLIT=\u2019\u2019TRAIN\u2019\u2019\nCGISPLIT=\u2019\u2019TRAINING-SET\u2019\u2019 OLDID=\u2019\u201912981\u2019\u2019 NEWID=\u2019\u2019798 \u2019\u2019>\n<DATE> 2-MAR-1987 16:51:43.42</DATE>\n<TOPICS><D>livestock</D><D>hog</D></TOPICS>\n<TITLE>AMERICAN PORK CONGRESS KICKS OFF TOMORROW</TITLE>\n<DATELINE> CHICAGO, March 2 - </DATELINE><BODY>The Americ an Pork\nCongress kicks off tomorrow, March 3, in Indianapolis with 1 60\nof the nations pork producers from 44 member states determin ing\nindustry positions on a number of issues, according to the\nNational Pork Producers Council, NPPC.\nDelegates to the three day Congress will be considering 26\nresolutions concerning various issues, including the futu re\ndirection of farm policy and the tax law as it applies to the\nagriculture sector. The delegates will also debate whether to\nendorse concepts of a national PRV (pseudorabies virus) con trol\nand eradication program, the NPPC said. A large\ntrade show, in conjunction with the congress, will feature\nthe latest in technology in all areas of the industry, the NPP C\nadded. Reuter\n\\&\\#3;</BODY></TEXT></REUTERS>\n\u25eeFigure 13.9 A sample document from the Reuters-21578 collection.\nc2(0.9) than to the precision of c1(0.5) because c2is \ufb01ve times larger than\nc1. Microaveraged results are therefore really a measure of ef fectiveness on\nthe large classes in a test collection. To get a sense of effec tiveness on small\nclasses, you should compute macroaveraged results.\nIn one-of classi\ufb01cation (Section 14.5, page 306), microaveraged F1is the\nsame as accuracy (Exercise 13.6).\nTable 13.9 gives microaveraged and macroaveraged effectiveness of Na ive\nBayes for the ModApte split of Reuters-21578. To give a sense of the relative\neffectiveness of NB, we compare it with linear SVMs (rightmo st column; see\nChapter 15), one of the most effective classi\ufb01ers, but also one that is m ore\nexpensive to train than NB. NB has a microaveraged F1of 80%, which is\n9% less than the SVM (89%), a 10% relative decrease (row \u201cmicr o-avg-L (90\nclasses)\u201d). So there is a surprisingly small effectiveness penalty for its sim-\nplicity and ef\ufb01ciency. However, on small classes, some of wh ich only have on\nthe order of ten positive examples in the training set, NB doe s much worse.\nIts macroaveraged F1is 13% below the SVM, a 22% relative decrease (row\n\u201cmacro-avg (90 classes)\u201d).\nThe table also compares NB with the other classi\ufb01ers we cover in this book:\nOnline edition (c)\n2009 Cambridge UP282 13 Text classi\ufb01cation and Naive Bayes\n\u25eeTable 13.8 Macro- and microaveraging. \u201cTruth\u201d is the true class and \u201cca ll\u201d the\ndecision of the classi\ufb01er. In this example, macroaveraged p recision is [10/(10+10) +\n90/(10+90)]/2= (0.5+0.9)/2=0.7. Microaveraged precision is 100/ (100+20)\u2248\n0.83.\nclass 1\ntruth: truth:\nyes no\ncall:\nyes10 10\ncall:\nno10 970class 2\ntruth: truth:\nyes no\ncall:\nyes90 10\ncall:\nno10 890pooled table\ntruth: truth:\nyes no\ncall:\nyes100 20\ncall:\nno20 1860\n\u25eeTable 13.9 Text classi\ufb01cation effectiveness numbers on Reuters-2157 8 for F 1(in\npercent). Results from Li and Yang (2003 ) (a), Joachims (1998 ) (b: kNN) and Dumais\net al. (1998 ) (b: NB, Rocchio, trees, SVM).\n(a) NB Rocchio kNN SVM\nmicro-avg-L (90 classes) 80 85 86 89\nmacro-avg (90 classes) 47 59 60 60\n(b) NB Rocchio kNN trees SVM\nearn 96 93 97 98 98\nacq 88 65 92 90 94\nmoney-fx 57 47 78 66 75\ngrain 79 68 82 85 95\ncrude 80 70 86 85 89\ntrade 64 65 77 73 76\ninterest 65 63 74 67 78\nship 85 49 79 74 86\nwheat 70 69 77 93 92\ncorn 65 48 78 92 90\nmicro-avg (top 10) 82 65 82 88 92\nmicro-avg-D (118 classes) 75 62 n/a n/a 87\nRocchio and kNN. In addition, we give numbers for decision trees , an impor- DECISION TREES\ntant classi\ufb01cation method we do not cover. The bottom part of the table\nshows that there is considerable variation from class to cla ss. For instance,\nNB beats kNN on ship, but is much worse on money-fx .\nComparing parts (a) and (b) of the table, one is struck by the d egree to\nwhich the cited papers\u2019 results differ. This is partly due to the fact that the\nnumbers in (b) are break-even scores (cf. page 161) averaged over 118 classes,\nwhereas the numbers in (a) are true F1scores (computed without any know-\nOnline edition (c)\n2009 Cambridge UP13.6 Evaluation of text classi\ufb01cation 283\nledge of the test set) averaged over ninety classes. This is u nfortunately typ-\nical of what happens when comparing different results in tex t classi\ufb01cation:\nThere are often differences in the experimental setup or the evaluation that\ncomplicate the interpretation of the results.\nThese and other results have shown that the average effectiv eness of NB\nis uncompetitive with classi\ufb01ers like SVMs when trained and tested on inde-\npendent and identically distributed (i.i.d.) data, that is, uniform data with all the\ngood properties of statistical sampling. However, these di fferences may of-\nten be invisible or even reverse themselves when working in t he real world\nwhere, usually, the training sample is drawn from a subset of the data to\nwhich the classi\ufb01er will be applied, the nature of the data dr ifts over time\nrather than being stationary (the problem of concept drift w e mentioned on\npage 269), and there may well be errors in the data (among other proble ms).\nMany practitioners have had the experience of being unable t o build a fancy\nclassi\ufb01er for a certain problem that consistently performs better than NB.\nOur conclusion from the results in Table 13.9 is that, although most re-\nsearchers believe that an SVM is better than kNN and kNN bette r than NB,\nthe ranking of classi\ufb01ers ultimately depends on the class, t he document col-\nlection, and the experimental setup. In text classi\ufb01cation , there is always\nmore to know than simply which machine learning algorithm wa s used, as\nwe further discuss in Section 15.3 (page 334).\nWhen performing evaluations like the one in Table 13.9, it is important to\nmaintain a strict separation between the training set and th e test set. We can\neasily make correct classi\ufb01cation decisions on the test set by using informa-\ntion we have gleaned from the test set, such as the fact that a p articular term\nis a good predictor in the test set (even though this is not the case in the train-\ning set). A more subtle example of using knowledge about the t est set is to\ntry a large number of values of a parameter (e.g., the number o f selected fea-\ntures) and select the value that is best for the test set. As a r ule, accuracy on\nnew data \u2013 the type of data we will encounter when we use the cla ssi\ufb01er in\nan application \u2013 will be much lower than accuracy on a test set that the clas-\nsi\ufb01er has been tuned for. We discussed the same problem in ad h oc retrieval\nin Section 8.1(page 153).\nIn a clean statistical text classi\ufb01cation experiment, you s hould never run\nany program on or even look at the test set while developing a t ext classi\ufb01ca-\ntion system. Instead, set aside a development set for testing while you develop DEVELOPMENT SET\nyour method. When such a set serves the primary purpose of \ufb01nd ing a good\nvalue for a parameter, for example, the number of selected fe atures, then it\nis also called held-out data . Train the classi\ufb01er on the rest of the training set HELD -OUT DATA\nwith different parameter values, and then select the value t hat gives best re-\nsults on the held-out part of the training set. Ideally, at th e very end, when\nall parameters have been set and the method is fully speci\ufb01ed , you run one\n\ufb01nal experiment on the test set and publish the results. Beca use no informa-\nOnline edition (c)\n2009 Cambridge UP284 13 Text classi\ufb01cation and Naive Bayes\n\u25eeTable 13.10 Data for parameter estimation exercise.\ndocID words in document in c=China ?\ntraining set 1 Taipei Taiwan yes\n2 Macao Taiwan Shanghai yes\n3 Japan Sapporo no\n4 Sapporo Osaka Taiwan no\ntest set 5 Taiwan Taiwan Sapporo ?\ntion about the test set was used in developing the classi\ufb01er, the results of this\nexperiment should be indicative of actual performance in pr actice.\nThis ideal often cannot be met; researchers tend to evaluate several sys-\ntems on the same test set over a period of several years. But it is neverthe-\nless highly important to not look at the test data and to run sy stems on it as\nsparingly as possible. Beginners often violate this rule, a nd their results lose\nvalidity because they have implicitly tuned their system to the test data sim-\nply by running many variant systems and keeping the tweaks to the system\nthat worked best on the test set.\n?Exercise 13.6 [\u22c6\u22c6]\nAssume a situation where every document in the test collecti on has been assigned\nexactly one class, and that a classi\ufb01er also assigns exactly one class to each document.\nThis setup is called one-of classi\ufb01cation (Section 14.5, page 306). Show that in one-of\nclassi\ufb01cation (i) the total number of false positive decisi ons equals the total number\nof false negative decisions and (ii) microaveraged F1and accuracy are identical.\nExercise 13.7\nThe class priors in Figure 13.2 are computed as the fraction of documents in the class\nas opposed to the fraction of tokens in the class. Why?\nExercise 13.8\nThe function A PPLY MULTINOMIAL NB in Figure 13.2 has time complexity \u0398(La+\n|C|La). How would you modify the function so that its time complexit y is \u0398(La+\n|C|Ma)?\nExercise 13.9\nBased on the data in Table 13.10 , (i) estimate a multinomial Naive Bayes classi\ufb01er, (ii)\napply the classi\ufb01er to the test document, (iii) estimate a Be rnoulli NB classi\ufb01er, (iv)\napply the classi\ufb01er to the test document. You need not estima te parameters that you\ndon\u2019t need for classifying the test document.\nExercise 13.10\nYour task is to classify words as English or not English. Word s are generated by a\nsource with the following distribution:\nOnline edition (c)\n2009 Cambridge UP13.6 Evaluation of text classi\ufb01cation 285\nevent word English? probability\n1 ozb no 4/9\n2 uzu no 4/9\n3 zoo yes 1/18\n4 bun yes 1/18\n(i) Compute the parameters (priors and conditionals) of a mu ltinomial NB classi-\n\ufb01er that uses the letters b, n, o, u, and z as features. Assume a training set that\nre\ufb02ects the probability distribution of the source perfect ly. Make the same indepen-\ndence assumptions that are usually made for a multinomial cl assi\ufb01er that uses terms\nas features for text classi\ufb01cation. Compute parameters usi ng smoothing, in which\ncomputed-zero probabilities are smoothed into probabilit y 0.01, and computed-nonzero\nprobabilities are untouched. (This simplistic smoothing m ay cause P(A) +P(A)>1.\nSolutions are not required to correct this.) (ii) How does th e classi\ufb01er classify the\nwordzoo? (iii) Classify the word zoousing a multinomial classi\ufb01er as in part (i), but\ndo not make the assumption of positional independence. That is, estimate separate\nparameters for each position in a word. You only need to compu te the parameters\nyou need for classifying zoo.\nExercise 13.11\nWhat are the values of I(Ut;Cc)and X2(D,t,c)if term and class are completely inde-\npendent? What are the values if they are completely dependen t?\nExercise 13.12\nThe feature selection method in Equation ( 13.16 ) is most appropriate for the Bernoulli\nmodel. Why? How could one modify it for the multinomial model ?\nExercise 13.13\nFeatures can also be selected according to information gain (IG), which is de\ufb01ned as: INFORMATION GAIN\nIG(D,t,c) =H(pD)\u2212 \u2211\nx\u2208{Dt+,Dt\u2212}|x|\n|D|H(px)\nwhere His entropy, Dis the training set, and Dt+, and Dt\u2212are the subset of Dwith\nterm t, and the subset of Dwithout term t, respectively. pAis the class distribution\nin (sub)collection A, e.g., pA(c) =0.25, pA(c) =0.75 if a quarter of the documents in\nAare in class c.\nShow that mutual information and information gain are equiv alent.\nExercise 13.14\nShow that the two X2formulas (Equations ( 13.18 ) and ( 13.19 )) are equivalent.\nExercise 13.15\nIn the \u03c72example on page 276we have|N11\u2212E11|=|N10\u2212E10|=|N01\u2212E01|=\n|N00\u2212E00|. Show that this holds in general.\nExercise 13.16\n\u03c72and mutual information do not distinguish between positive ly and negatively cor-\nrelated features. Because most good text classi\ufb01cation fea tures are positively corre-\nlated (i.e., they occur more often in cthan in c), one may want to explicitly rule out\nthe selection of negative indicators. How would you do this?\nOnline edition (c)\n2009 Cambridge UP286 13 Text classi\ufb01cation and Naive Bayes\n13.7 References and further reading\nGeneral introductions to statistical classi\ufb01cation and ma chine learning can be\nfound in ( Hastie et al. 2001 ), (Mitchell 1997 ), and ( Duda et al. 2000 ), including\nmany important methods (e.g., decision trees and boosting) that we do not\ncover. A comprehensive review of text classi\ufb01cation method s and results is\n(Sebastiani 2002 ).Manning and Sch\u00fctze (1999 , Chapter 16) give an accessible\nintroduction to text classi\ufb01cation with coverage of decisi on trees, perceptrons\nand maximum entropy models. More information on the superli near time\ncomplexity of learning methods that are more accurate than N aive Bayes can\nbe found in ( Perkins et al. 2003 ) and ( Joachims 2006a ).\nMaron and Kuhns (1960 ) described one of the \ufb01rst NB text classi\ufb01ers. Lewis\n(1998 ) focuses on the history of NB classi\ufb01cation. Bernoulli and m ultinomial\nmodels and their accuracy for different collections are dis cussed by McCal-\nlum and Nigam (1998 ).Eyheramendy et al. (2003 ) present additional NB\nmodels. Domingos and Pazzani (1997 ),Friedman (1997 ), and Hand and Yu\n(2001 ) analyze why NB performs well although its probability esti mates are\npoor. The \ufb01rst paper also discusses NB\u2019s optimality when the independence\nassumptions are true of the data. Pavlov et al. (2004 ) propose a modi\ufb01ed\ndocument representation that partially addresses the inap propriateness of\nthe independence assumptions. Bennett (2000 ) attributes the tendency of NB\nprobability estimates to be close to either 0 or 1 to the effec t of document\nlength. Ng and Jordan (2001 ) show that NB is sometimes (although rarely)\nsuperior to discriminative methods because it more quickly reaches its opti-\nmal error rate. The basic NB model presented in this chapter c an be tuned for\nbetter effectiveness ( Rennie et al. 2003 ;Ko\u0142cz and Yih 2007 ). The problem of\nconcept drift and other reasons why state-of-the-art class i\ufb01ers do not always\nexcel in practice are discussed by Forman (2006 ) and Hand (2006 ).\nEarly uses of mutual information and \u03c72for feature selection in text clas-\nsi\ufb01cation are Lewis and Ringuette (1994 ) and Sch\u00fctze et al. (1995 ), respec-\ntively. Yang and Pedersen (1997 ) review feature selection methods and their\nimpact on classi\ufb01cation effectiveness. They \ufb01nd that pointwise mutual infor- POINTWISE MUTUAL\nINFORMATION mation is not competitive with other methods. Yang and Pedersen refer to\nexpected mutual information (Equation ( 13.16 )) as information gain (see Ex-\nercise 13.13 , page 285). (Snedecor and Cochran 1989 ) is a good reference for\nthe\u03c72test in statistics, including the Yates\u2019 correction for con tinuity for 2\u00d72\ntables. Dunning (1993 ) discusses problems of the \u03c72test when counts are\nsmall. Nongreedy feature selection techniques are describ ed by Hastie et al.\n(2001 ).Cohen (1995 ) discusses the pitfalls of using multiple signi\ufb01cance test s\nand methods to avoid them. Forman (2004 ) evaluates different methods for\nfeature selection for multiple classi\ufb01ers.\nDavid D. Lewis de\ufb01nes the ModApte split at www.daviddlewis.com/resources/testcollections/reute rs21578/readme\nbased on Apt\u00e9 et al. (1994 ).Lewis (1995 ) describes utility measures for the UTILITY MEASURE\nOnline edition (c)\n2009 Cambridge UP13.7 References and further reading 287\nevaluation of text classi\ufb01cation systems. Yang and Liu (1999 ) employ signif-\nicance tests in the evaluation of text classi\ufb01cation method s.\nLewis et al. (2004 ) \ufb01nd that SVMs (Chapter 15) perform better on Reuters-\nRCV1 than kNN and Rocchio (Chapter 14).\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 289\n14 Vector space classi\ufb01cation\nThe document representation in Naive Bayes is a sequence of t erms or a bi-\nnary vector/an}\u230ara\u230bketle{te1, . . . , e|V|/an}\u230ara\u230bketri}ht\u2208{ 0, 1}|V|. In this chapter we adopt a different\nrepresentation for text classi\ufb01cation, the vector space mo del, developed in\nChapter 6. It represents each document as a vector with one real-value d com-\nponent, usually a tf-idf weight, for each term. Thus, the doc ument space X,\nthe domain of the classi\ufb01cation function \u03b3, isR|V|. This chapter introduces a\nnumber of classi\ufb01cation methods that operate on real-value d vectors.\nThe basic hypothesis in using the vector space model for clas si\ufb01cation is\nthecontiguity hypothesis . CONTIGUITY\nHYPOTHESISContiguity hypothesis. Documents in the same class form a contigu-\nous region and regions of different classes do not overlap.\nThere are many classi\ufb01cation tasks, in particular the type o f text classi\ufb01cation\nthat we encountered in Chapter 13, where classes can be distinguished by\nword patterns. For example, documents in the class China tend to have high\nvalues on dimensions like Chinese ,Beijing , andMao whereas documents in the\nclass UKtend to have high values for London ,British andQueen . Documents\nof the two classes therefore form distinct contiguous regio ns as shown in\nFigure 14.1 and we can draw boundaries that separate them and classify ne w\ndocuments. How exactly this is done is the topic of this chapt er.\nWhether or not a set of documents is mapped into a contiguous r egion de-\npends on the particular choices we make for the document repr esentation:\ntype of weighting, stop list etc. To see that the document rep resentation is\ncrucial, consider the two classes written by a group vs.written by a single per-\nson. Frequent occurrence of the \ufb01rst person pronoun Iis evidence for the\nsingle-person class. But that information is likely delete d from the document\nrepresentation if we use a stop list. If the document represe ntation chosen\nis unfavorable, the contiguity hypothesis will not hold and successful vector\nspace classi\ufb01cation is not possible.\nThe same considerations that led us to prefer weighted repre sentations, in\nparticular length-normalized tf-idf representations, in Chapters 6and 7also\nOnline edition (c)\n2009 Cambridge UP290 14 Vector space classi\ufb01cation\nxx\nx\nx\u22c4\n\u22c4\n\u22c4\u22c4\n\u22c4\n\u22c4\nChina\nKenyaUK\n\u22c6\n\u25eeFigure 14.1 Vector space classi\ufb01cation into three classes.\napply here. For example, a term with 5 occurrences in a docume nt should get\na higher weight than a term with one occurrence, but a weight 5 times larger\nwould give too much emphasis to the term. Unweighted and unno rmalized\ncounts should not be used in vector space classi\ufb01cation.\nWe introduce two vector space classi\ufb01cation methods in this chapter, Roc-\nchio and kNN. Rocchio classi\ufb01cation (Section 14.2) divides the vector space\ninto regions centered on centroids or prototypes , one for each class, computed PROTOTYPE\nas the center of mass of all documents in the class. Rocchio cl assi\ufb01cation is\nsimple and ef\ufb01cient, but inaccurate if classes are not appro ximately spheres\nwith similar radii.\nkNN or knearest neighbor classi\ufb01cation (Section 14.3) assigns the majority\nclass of the knearest neighbors to a test document. kNN requires no explic it\ntraining and can use the unprocessed training set directly i n classi\ufb01cation.\nIt is less ef\ufb01cient than other classi\ufb01cation methods in clas sifying documents.\nIf the training set is large, then kNN can handle non-spheric al and other\ncomplex classes better than Rocchio.\nA large number of text classi\ufb01ers can be viewed as linear clas si\ufb01ers \u2013 clas-\nsi\ufb01ers that classify based on a simple linear combination of the features (Sec-\ntion 14.4). Such classi\ufb01ers partition the space of features into regi ons sepa-\nrated by linear decision hyperplanes , in a manner to be detailed below. Because\nof the bias-variance tradeoff (Section 14.6) more complex nonlinear models\nOnline edition (c)\n2009 Cambridge UP14.1 Document representations and measures of relatedness in vector spaces 291\ndtrue\ndprojectedx1x2x3x4\nx5\nx\u2032\n1x\u2032\n2x\u2032\n3x\u2032\n4x\u2032\n5x\u2032\n1x\u2032\n2x\u2032\n3x\u2032\n4x\u2032\n5\n\u25eeFigure 14.2 Projections of small areas of the unit sphere preserve dista nces. Left:\nA projection of the 2D semicircle to 1D. For the points x1,x2,x3,x4,x5at x coordinates\n\u22120.9,\u22120.2, 0, 0.2, 0.9 the distance |x2x3|\u2248 0.201 only differs by 0.5% from |x\u2032\n2x\u2032\n3|=\n0.2; but|x1x3|/|x\u2032\n1x\u2032\n3|=dtrue/dprojected\u22481.06/0.9\u22481.18 is an example of a large\ndistortion (18%) when projecting a large area. Right: The co rresponding projection of\nthe 3D hemisphere to 2D.\nare not systematically better than linear models. Nonlinea r models have\nmore parameters to \ufb01t on a limited amount of training data and are more\nlikely to make mistakes for small and noisy data sets.\nWhen applying two-class classi\ufb01ers to problems with more th an two classes,\nthere are one-of tasks \u2013 a document must be assigned to exactly one of several\nmutually exclusive classes \u2013 and any-of tasks \u2013 a document can be assigned to\nany number of classes as we will explain in Section 14.5. Two-class classi\ufb01ers\nsolve any-of problems and can be combined to solve one-of pro blems.\n14.1 Document representations and measures of relatedness in vec-\ntor spaces\nAs in Chapter 6, we represent documents as vectors in R|V|in this chapter.\nTo illustrate properties of document vectors in vector clas si\ufb01cation, we will\nrender these vectors as points in a plane as in the example in F igure 14.1.\nIn reality, document vectors are length-normalized unit ve ctors that point\nto the surface of a hypersphere. We can view the 2D planes in ou r \ufb01gures\nas projections onto a plane of the surface of a (hyper-)spher e as shown in\nFigure 14.2. Distances on the surface of the sphere and on the projection\nplane are approximately the same as long as we restrict ourse lves to small\nareas of the surface and choose an appropriate projection (E xercise 14.1).\nOnline edition (c)\n2009 Cambridge UP292 14 Vector space classi\ufb01cation\nDecisions of many vector space classi\ufb01ers are based on a noti on of dis-\ntance, e.g., when computing the nearest neighbors in kNN cla ssi\ufb01cation.\nWe will use Euclidean distance in this chapter as the underly ing distance\nmeasure. We observed earlier (Exercise 6.18, page 131) that there is a direct\ncorrespondence between cosine similarity and Euclidean di stance for length-\nnormalized vectors. In vector space classi\ufb01cation, it rare ly matters whether\nthe relatedness of two documents is expressed in terms of sim ilarity or dis-\ntance.\nHowever, in addition to documents, centroids or averages of vectors also\nplay an important role in vector space classi\ufb01cation. Centr oids are not length-\nnormalized. For unnormalized vectors, dot product, cosine similarity and\nEuclidean distance all have different behavior in general ( Exercise 14.6). We\nwill be mostly concerned with small local regions when compu ting the sim-\nilarity between a document and a centroid, and the smaller th e region the\nmore similar the behavior of the three measures is.\n?Exercise 14.1\nFor small areas, distances on the surface of the hypersphere are approximated well\nby distances on its projection (Figure 14.2) because \u03b1\u2248sin\u03b1for small angles. For\nwhat size angle is the distortion \u03b1/ sin(\u03b1)(i) 1.01, (ii) 1.05 and (iii) 1.1?\n14.2 Rocchio classi\ufb01cation\nFigure 14.1 shows three classes, China ,UKand Kenya , in a two-dimensional\n(2D) space. Documents are shown as circles, diamonds and X\u2019s . The bound-\naries in the \ufb01gure, which we call decision boundaries , are chosen to separate DECISION BOUNDARY\nthe three classes, but are otherwise arbitrary. To classify a new document,\ndepicted as a star in the \ufb01gure, we determine the region it occ urs in and as-\nsign it the class of that region \u2013 China in this case. Our task in vector space\nclassi\ufb01cation is to devise algorithms that compute good bou ndaries where\n\u201cgood\u201d means high classi\ufb01cation accuracy on data unseen dur ing training.\nPerhaps the best-known way of computing good class boundari es is Roc- ROCCHIO\nCLASSIFICATION chio classi\ufb01cation , which uses centroids to de\ufb01ne the boundaries. The centroid\nCENTROIDof a class cis computed as the vector average or center of mass of its mem-\nbers:\n/vector\u00b5(c) =1\n|Dc|\u2211\nd\u2208Dc/vectorv(d) (14.1)\nwhere Dcis the set of documents in Dwhose class is c:Dc={d:/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht\u2208D}.\nWe denote the normalized vector of dby/vectorv(d)(Equation ( 6.11), page 122).\nThree example centroids are shown as solid circles in Figure 14.3.\nThe boundary between two classes in Rocchio classi\ufb01cation i s the set of\npoints with equal distance from the two centroids. For examp le,|a1|=|a2|,\nOnline edition (c)\n2009 Cambridge UP14.2 Rocchio classi\ufb01cation 293\nxx\nx\nx\u22c4\n\u22c4\n\u22c4\u22c4\n\u22c4\n\u22c4\nChina\nKenyaUK\n\u22c6a1\na2b1\nb2c1\nc2\n\u25eeFigure 14.3 Rocchio classi\ufb01cation.\n|b1|=|b2|, and|c1|=|c2|in the \ufb01gure. This set of points is always a line.\nThe generalization of a line in M-dimensional space is a hyperplane, which\nwe de\ufb01ne as the set of points /vectorxthat satisfy:\n/vectorwT/vectorx=b (14.2)\nwhere /vectorwis the M-dimensional normal vector1of the hyperplane and bis a NORMAL VECTOR\nconstant. This de\ufb01nition of hyperplanes includes lines (an y line in 2D can\nbe de\ufb01ned by w1x1+w2x2=b) and 2-dimensional planes (any plane in 3D\ncan be de\ufb01ned by w1x1+w2x2+w3x3=b). A line divides a plane in two,\na plane divides 3-dimensional space in two, and hyperplanes divide higher-\ndimensional spaces in two.\nThus, the boundaries of class regions in Rocchio classi\ufb01cat ion are hyper-\nplanes. The classi\ufb01cation rule in Rocchio is to classify a po int in accordance\nwith the region it falls into. Equivalently, we determine th e centroid /vector\u00b5(c)that\nthe point is closest to and then assign it to c. As an example, consider the star\nin Figure 14.3. It is located in the China region of the space and Rocchio\ntherefore assigns it to China . We show the Rocchio algorithm in pseudocode\nin Figure 14.4.\n1. Recall from basic linear algebra that /vectorv\u00b7/vectorw=/vectorvT/vectorw, i.e., the dot product of /vectorvand/vectorwequals the\nproduct by matrix multiplication of the transpose of /vectorvand/vectorw.\nOnline edition (c)\n2009 Cambridge UP294 14 Vector space classi\ufb01cation\nterm weights\nvector Chinese Japan Tokyo Macao Beijing Shanghai\n/vectord1 0 0 0 0 1.0 0\n/vectord2 0 0 0 0 0 1.0\n/vectord3 0 0 0 1.0 0 0\n/vectord4 0 0.71 0.71 0 0 0\n/vectord5 0 0.71 0.71 0 0 0\n/vector\u00b5c 0 0 0 0.33 0.33 0.33\n/vector\u00b5c 0 0.71 0.71 0 0 0\n\u25eeTable 14.1 Vectors and class centroids for the data in Table 13.1.\n\u270eExample 14.1: Table 14.1 shows the tf-idf vector representations of the \ufb01ve docu-\nments in Table 13.1 (page 261), using the formula (1+log10tft,d)log10(4/df t)if tf t,d>\n0 (Equation ( 6.14), page 127). The two class centroids are \u00b5c=1/3\u00b7(/vectord1+/vectord2+/vectord3)\nand \u00b5c=1/1\u00b7(/vectord4). The distances of the test document from the centroids are\n|\u00b5c\u2212/vectord5|\u22481.15 and|\u00b5c\u2212/vectord5|=0.0. Thus, Rocchio assigns d5toc.\nThe separating hyperplane in this case has the following par ameters:\n/vectorw\u2248(0\u22120.71\u22120.71 1/3 1/3 1/3 )T\nb=\u22121/3\nSee Exercise 14.15 for how to compute /vectorwand b. We can easily verify that this hy-\nperplane separates the documents as desired: /vectorwT/vectord1\u22480\u00b70+\u22120.71\u00b70+\u22120.71\u00b70+\n1/3\u00b70+1/3\u00b71.0+1/3\u00b70=1/3>b(and, similarly, /vectorwT/vectordi>bfori=2 and i=3)\nand/vectorwT/vectord4=\u22121<b. Thus, documents in care above the hyperplane ( /vectorwT/vectord>b) and\ndocuments in care below the hyperplane ( /vectorwT/vectord<b).\nThe assignment criterion in Figure 14.4 is Euclidean distance (A PPLY ROC-\nCHIO , line 1). An alternative is cosine similarity:\nAssign dto class c=arg max\nc\u2032cos(/vector\u00b5(c\u2032),/vectorv(d))\nAs discussed in Section 14.1, the two assignment criteria will sometimes\nmake different classi\ufb01cation decisions. We present the Euc lidean distance\nvariant of Rocchio classi\ufb01cation here because it emphasize s Rocchio\u2019s close\ncorrespondence to K-means clustering (Section 16.4, page 360).\nRocchio classi\ufb01cation is a form of Rocchio relevance feedba ck (Section 9.1.1 ,\npage 178). The average of the relevant documents, corresponding to t he most\nimportant component of the Rocchio vector in relevance feed back (Equa-\ntion ( 9.3), page 182), is the centroid of the \u201cclass\u201d of relevant documents.\nWe omit the query component of the Rocchio formula in Rocchio classi\ufb01ca-\ntion since there is no query in text classi\ufb01cation. Rocchio c lassi\ufb01cation can be\nOnline edition (c)\n2009 Cambridge UP14.2 Rocchio classi\ufb01cation 295\nTRAIN ROCCHIO (C,D)\n1for each cj\u2208C\n2doDj\u2190{ d:/an}\u230ara\u230bketle{td,cj/an}\u230ara\u230bketri}ht\u2208D}\n3 /vector\u00b5j\u21901\n|Dj|\u2211d\u2208Dj/vectorv(d)\n4return{/vector\u00b51, . . . ,/vector\u00b5J}\nAPPLY ROCCHIO ({/vector\u00b51, . . . ,/vector\u00b5J},d)\n1return arg minj|/vector\u00b5j\u2212/vectorv(d)|\n\u25eeFigure 14.4 Rocchio classi\ufb01cation: Training and testing.\na\naa\naa\naaa\na\naaa\naaa a\na aa\naa\na\naa\naa\naa\naaa\naaaa\na\naaa\na\nbb\nb\nbbbbbb\nbb\nb\nbb\nbb\nb\nbbX X A\nBo\n\u25eeFigure 14.5 The multimodal class \u201ca\u201d consists of two different clusters (small\nupper circles centered on X\u2019s). Rocchio classi\ufb01cation will misclassify \u201co\u201d as \u201ca\u201d\nbecause it is closer to the centroid A of the \u201ca\u201d class than to t he centroid B of the \u201cb\u201d\nclass.\napplied to J>2 classes whereas Rocchio relevance feedback is designed to\ndistinguish only two classes, relevant and nonrelevant.\nIn addition to respecting contiguity, the classes in Rocchi o classi\ufb01cation\nmust be approximate spheres with similar radii. In Figure 14.3, the solid\nsquare just below the boundary between UKand Kenya is a better \ufb01t for the\nclass UKsince UKis more scattered than Kenya . But Rocchio assigns it to\nKenya because it ignores details of the distribution of points in a class and\nonly uses distance from the centroid for classi\ufb01cation.\nThe assumption of sphericity also does not hold in Figure 14.5. We can-\nOnline edition (c)\n2009 Cambridge UP296 14 Vector space classi\ufb01cation\nmode time complexity\ntraining \u0398(|D|Lave+|C||V|)\ntesting \u0398(La+|C|Ma) =\u0398(|C|Ma)\n\u25eeTable 14.2 Training and test times for Rocchio classi\ufb01cation. Laveis the average\nnumber of tokens per document. Laand Maare the numbers of tokens and types,\nrespectively, in the test document. Computing Euclidean di stance between the class\ncentroids and a document is \u0398(|C|Ma).\nnot represent the \u201ca\u201d class well with a single prototype beca use it has two\nclusters. Rocchio often misclassi\ufb01es this type of multimodal class . A text clas- MULTIMODAL CLASS\nsi\ufb01cation example for multimodality is a country like Burma , which changed\nits name to Myanmar in 1989. The two clusters before and after the name\nchange need not be close to each other in space. We also encoun tered the\nproblem of multimodality in relevance feedback (Section 9.1.2 , page 184).\nTwo-class classi\ufb01cation is another case where classes are r arely distributed\nlike spheres with similar radii. Most two-class classi\ufb01ers distinguish between\na class like China that occupies a small region of the space and its widely\nscattered complement. Assuming equal radii will result in a large number\nof false positives. Most two-class classi\ufb01cation problems therefore require a\nmodi\ufb01ed decision rule of the form:\nAssign dto class ciff|/vector\u00b5(c)\u2212/vectorv(d)|<|/vector\u00b5(c)\u2212/vectorv(d)|\u2212b\nfor a positive constant b. As in Rocchio relevance feedback, the centroid of\nthe negative documents is often not used at all, so that the de cision criterion\nsimpli\ufb01es to|/vector\u00b5(c)\u2212/vectorv(d)|<b\u2032for a positive constant b\u2032.\nTable 14.2 gives the time complexity of Rocchio classi\ufb01cation.2Adding all\ndocuments to their respective (unnormalized) centroid is \u0398(|D|Lave)(as op-\nposed to \u0398(|D||V|)) since we need only consider non-zero entries. Dividing\neach vector sum by the size of its class to compute the centroi d is \u0398(|V|).\nOverall, training time is linear in the size of the collectio n (cf. Exercise 13.1).\nThus, Rocchio classi\ufb01cation and Naive Bayes have the same li near training\ntime complexity.\nIn the next section, we will introduce another vector space c lassi\ufb01cation\nmethod, kNN, that deals better with classes that have non-sp herical, discon-\nnected or other irregular shapes.\n?Exercise 14.2 [\u22c6]\nShow that Rocchio classi\ufb01cation can assign a label to a docum ent that is different from\nits training set label.\n2. We write \u0398(|D|Lave)for\u0398(T)and assume that the length of test documents is bounded as\nwe did on page 262.\nOnline edition (c)\n2009 Cambridge UP14.3 k nearest neighbor 297\nxx\nxxxxx\nxxx\nx\u22c4\n\u22c4\n\u22c4\n\u22c4\u22c4\n\u22c4\n\u22c4\n\u22c4\u22c4\n\u22c4\u22c4\u22c6\n\u25eeFigure 14.6 Voronoi tessellation and decision boundaries (double line s) in 1NN\nclassi\ufb01cation. The three classes are: X, circle and diamond .\n14.3 knearest neighbor\nUnlike Rocchio, k nearest neighbor orkNN classi\ufb01cation determines the deci- kNEAREST NEIGHBOR\nCLASSIFICATION sion boundary locally. For 1NN we assign each document to the class of its\nclosest neighbor. For kNN we assign each document to the majo rity class of\nitskclosest neighbors where kis a parameter. The rationale of kNN classi\ufb01-\ncation is that, based on the contiguity hypothesis, we expec t a test document\ndto have the same label as the training documents located in th e local region\nsurrounding d.\nDecision boundaries in 1NN are concatenated segments of the Voronoi tes- VORONOI\nTESSELLATION sellation as shown in Figure 14.6. The Voronoi tessellation of a set of objects\ndecomposes space into Voronoi cells, where each object\u2019s ce ll consists of all\npoints that are closer to the object than to other objects. In our case, the ob-\njects are documents. The Voronoi tessellation then partiti ons the plane into\n|D|convex polygons, each containing its corresponding docume nt (and no\nother) as shown in Figure 14.6, where a convex polygon is a convex region in\n2-dimensional space bounded by lines.\nFor general k\u2208Nin kNN, consider the region in the space for which the\nset of knearest neighbors is the same. This again is a convex polygon and the\nspace is partitioned into convex polygons, within each of wh ich the set of k\nOnline edition (c)\n2009 Cambridge UP298 14 Vector space classi\ufb01cation\nTRAIN -KNN(C,D)\n1D\u2032\u2190PREPROCESS (D)\n2k\u2190SELECT -K(C,D\u2032)\n3return D\u2032,k\nAPPLY -KNN(C,D\u2032,k,d)\n1Sk\u2190COMPUTE NEAREST NEIGHBORS (D\u2032,k,d)\n2for each cj\u2208C\n3dopj\u2190|Sk\u2229cj|/k\n4return arg maxjpj\n\u25eeFigure 14.7 kNN training (with preprocessing) and testing. pjis an estimate for\nP(cj|Sk) =P(cj|d).cjdenotes the set of all documents in the class cj.\nnearest neighbors is invariant (Exercise 14.11 ).3\n1NN is not very robust. The classi\ufb01cation decision of each te st document\nrelies on the class of a single training document, which may b e incorrectly\nlabeled or atypical. kNN for k>1 is more robust. It assigns documents to\nthe majority class of their kclosest neighbors, with ties broken randomly.\nThere is a probabilistic version of this kNN classi\ufb01cation a lgorithm. We\ncan estimate the probability of membership in class cas the proportion of the\nknearest neighbors in c. Figure 14.6 gives an example for k=3. Probabil-\nity estimates for class membership of the star are \u02c6P(circle class|star) = 1/3,\n\u02c6P(X class|star) = 2/3, and \u02c6P(diamond class|star) = 0. The 3nn estimate\n(\u02c6P1(circle class|star) = 1/3) and the 1nn estimate ( \u02c6P1(circle class|star) = 1)\ndiffer with 3nn preferring the X class and 1nn preferring the circle class .\nThe parameter kin kNN is often chosen based on experience or knowledge\nabout the classi\ufb01cation problem at hand. It is desirable for kto be odd to\nmake ties less likely. k=3 and k=5 are common choices, but much larger\nvalues between 50 and 100 are also used. An alternative way of setting the\nparameter is to select the kthat gives best results on a held-out portion of the\ntraining set.\nWe can also weight the \u201cvotes\u201d of the knearest neighbors by their cosine\n3. The generalization of a polygon to higher dimensions is a p olytope. A polytope is a region\ninM-dimensional space bounded by (M\u22121)-dimensional hyperplanes. In Mdimensions, the\ndecision boundaries for kNN consist of segments of (M\u22121)-dimensional hyperplanes that form\nthe Voronoi tessellation into convex polytopes for the trai ning set of documents. The decision\ncriterion of assigning a document to the majority class of it sknearest neighbors applies equally\ntoM=2 (tessellation into polygons) and M>2 (tessellation into polytopes).\nOnline edition (c)\n2009 Cambridge UP14.3 k nearest neighbor 299\nkNN with preprocessing of training set\ntraining \u0398(|D|Lave)\ntesting \u0398(La+|D|MaveMa) =\u0398(|D|MaveMa)\nkNN without preprocessing of training set\ntraining \u0398(1)\ntesting \u0398(La+|D|LaveMa) =\u0398(|D|LaveMa)\n\u25eeTable 14.3 Training and test times for kNN classi\ufb01cation. Maveis the average size\nof the vocabulary of documents in the collection.\nsimilarity. In this scheme, a class\u2019s score is computed as:\nscore(c,d) = \u2211\nd\u2032\u2208Sk(d)Ic(d\u2032)cos(/vectorv(d\u2032),/vectorv(d))\nwhere Sk(d)is the set of d\u2019sknearest neighbors and Ic(d\u2032) =1 iff d\u2032is in class\ncand 0 otherwise. We then assign the document to the class with the highest\nscore. Weighting by similarities is often more accurate tha n simple voting.\nFor example, if two classes have the same number of neighbors in the top k,\nthe class with the more similar neighbors wins.\nFigure 14.7 summarizes the kNN algorithm.\n\u270eExample 14.2: The distances of the test document from the four training doc u-\nments in Table 14.1 are|/vectord1\u2212/vectord5|=|/vectord2\u2212/vectord5|=|/vectord3\u2212/vectord5|\u22481.41 and|/vectord4\u2212/vectord5|=0.0.\nd5\u2019s nearest neighbor is therefore d4and 1NN assigns d5tod4\u2019s class, c.\n\u270414.3.1 Time complexity and optimality of kNN\nTable 14.3 gives the time complexity of kNN. kNN has properties that are\nquite different from most other classi\ufb01cation algorithms. Training a kNN\nclassi\ufb01er simply consists of determining kand preprocessing documents. In\nfact, if we preselect a value for kand do not preprocess, then kNN requires\nno training at all. In practice, we have to perform preproces sing steps like\ntokenization. It makes more sense to preprocess training do cuments once\nas part of the training phase rather than repeatedly every ti me we classify a\nnew test document.\nTest time is \u0398(|D|MaveMa)for kNN. It is linear in the size of the training\nset as we need to compute the distance of each training docume nt from the\ntest document. Test time is independent of the number of clas sesJ. kNN\ntherefore has a potential advantage for problems with large J.\nIn kNN classi\ufb01cation, we do not perform any estimation of par ameters as\nwe do in Rocchio classi\ufb01cation (centroids) or in Naive Bayes (priors and con-\nditional probabilities). kNN simply memorizes all example s in the training\nOnline edition (c)\n2009 Cambridge UP300 14 Vector space classi\ufb01cation\nset and then compares the test document to them. For this reas on, kNN is\nalso called memory-based learning orinstance-based learning . It is usually desir- MEMORY -BASED\nLEARNING able to have as much training data as possible in machine lear ning. But in\nkNN large training sets come with a severe ef\ufb01ciency penalty in classi\ufb01ca-\ntion.\nCan kNN testing be made more ef\ufb01cient than \u0398(|D|MaveMa)or, ignoring\nthe length of documents, more ef\ufb01cient than \u0398(|D|)? There are fast kNN\nalgorithms for small dimensionality M(Exercise 14.12 ). There are also ap-\nproximations for large Mthat give error bounds for speci\ufb01c ef\ufb01ciency gains\n(see Section 14.7). These approximations have not been extensively tested\nfor text classi\ufb01cation applications, so it is not clear whet her they can achieve\nmuch better ef\ufb01ciency than \u0398(|D|)without a signi\ufb01cant loss of accuracy.\nThe reader may have noticed the similarity between the probl em of \ufb01nding\nnearest neighbors of a test document and ad hoc retrieval, wh ere we search\nfor the documents with the highest similarity to the query (S ection 6.3.2 ,\npage 123). In fact, the two problems are both knearest neighbor problems\nand only differ in the relative density of (the vector of) the test document\nin kNN (10s or 100s of non-zero entries) versus the sparsenes s of (the vec-\ntor of) the query in ad hoc retrieval (usually fewer than 10 no n-zero entries).\nWe introduced the inverted index for ef\ufb01cient ad hoc retriev al in Section 1.1\n(page 6). Is the inverted index also the solution for ef\ufb01cient kNN?\nAn inverted index restricts a search to those documents that have at least\none term in common with the query. Thus in the context of kNN, t he in-\nverted index will be ef\ufb01cient if the test document has no term overlap with a\nlarge number of training documents. Whether this is the case depends on the\nclassi\ufb01cation problem. If documents are long and no stop lis t is used, then\nless time will be saved. But with short documents and a large s top list, an\ninverted index may well cut the average test time by a factor o f 10 or more.\nThe search time in an inverted index is a function of the lengt h of the post-\nings lists of the terms in the query. Postings lists grow subl inearly with the\nlength of the collection since the vocabulary increases acc ording to Heaps\u2019\nlaw \u2013 if the probability of occurrence of some terms increase s, then the prob-\nability of occurrence of others must decrease. However, mos t new terms are\ninfrequent. We therefore take the complexity of inverted in dex search to be\n\u0398(T)(as discussed in Section 2.4.2 , page 41) and, assuming average docu-\nment length does not change over time, \u0398(T) =\u0398(|D|).\nAs we will see in the next chapter, kNN\u2019s effectiveness is clo se to that of the\nmost accurate learning methods in text classi\ufb01cation (Tabl e15.2, page 334). A\nmeasure of the quality of a learning method is its Bayes error rate , the average BAYES ERROR RATE\nerror rate of classi\ufb01ers learned by it for a particular probl em. kNN is not\noptimal for problems with a non-zero Bayes error rate \u2013 that i s, for problems\nwhere even the best possible classi\ufb01er has a non-zero classi \ufb01cation error. The\nerror of 1NN is asymptotically (as the training set increase s) bounded by\nOnline edition (c)\n2009 Cambridge UP14.4 Linear versus nonlinear classi\ufb01ers 301\n\u25eeFigure 14.8 There are an in\ufb01nite number of hyperplanes that separate two linearly\nseparable classes.\ntwice the Bayes error rate. That is, if the optimal classi\ufb01er has an error rate\nofx, then 1NN has an asymptotic error rate of less than 2 x. This is due to the\neffect of noise \u2013 we already saw one example of noise in the for m of noisy\nfeatures in Section 13.5 (page 271), but noise can also take other forms as we\nwill discuss in the next section. Noise affects two componen ts of kNN: the\ntest document and the closest training document. The two sou rces of noise\nare additive, so the overall error of 1NN is twice the optimal error rate. For\nproblems with Bayes error rate 0, the error rate of 1NN will ap proach 0 as\nthe size of the training set increases.\n?Exercise 14.3\nExplain why kNN handles multimodal classes better than Rocc hio.\n14.4 Linear versus nonlinear classi\ufb01ers\nIn this section, we show that the two learning methods Naive B ayes and\nRocchio are instances of linear classi\ufb01ers, the perhaps mos t important group\nof text classi\ufb01ers, and contrast them with nonlinear classi \ufb01ers. To simplify\nthe discussion, we will only consider two-class classi\ufb01ers in this section and\nde\ufb01ne a linear classi\ufb01er as a two-class classi\ufb01er that decides class membership LINEAR CLASSIFIER\nby comparing a linear combination of the features to a thresh old.\nIn two dimensions, a linear classi\ufb01er is a line. Five example s are shown\nin Figure 14.8. These lines have the functional form w1x1+w2x2=b. The\nclassi\ufb01cation rule of a linear classi\ufb01er is to assign a docum ent to cifw1x1+\nw2x2>band to cifw1x1+w2x2\u2264b. Here, (x1,x2)Tis the two-dimensional\nvector representation of the document and (w1,w2)Tis the parameter vector\nOnline edition (c)\n2009 Cambridge UP302 14 Vector space classi\ufb01cation\nAPPLY LINEAR CLASSIFIER (/vectorw,b,/vectorx)\n1score\u2190\u2211M\ni=1wixi\n2ifscore>b\n3 then return 1\n4 else return 0\n\u25eeFigure 14.9 Linear classi\ufb01cation algorithm.\nthat de\ufb01nes (together with b) the decision boundary. An alternative geomet-\nric interpretation of a linear classi\ufb01er is provided in Figu re15.7 (page 343).\nWe can generalize this 2D linear classi\ufb01er to higher dimensi ons by de\ufb01ning\na hyperplane as we did in Equation ( 14.2), repeated here as Equation ( 14.3):\n/vectorwT/vectorx=b (14.3)\nThe assignment criterion then is: assign to cif/vectorwT/vectorx>band to cif/vectorwT/vectorx\u2264b.\nWe call a hyperplane that we use as a linear classi\ufb01er a decision hyperplane . DECISION HYPERPLANE\nThe corresponding algorithm for linear classi\ufb01cation in Mdimensions is\nshown in Figure 14.9. Linear classi\ufb01cation at \ufb01rst seems trivial given the\nsimplicity of this algorithm. However, the dif\ufb01culty is in t raining the lin-\near classi\ufb01er, that is, in determining the parameters /vectorwand bbased on the\ntraining set. In general, some learning methods compute muc h better param-\neters than others where our criterion for evaluating the qua lity of a learning\nmethod is the effectiveness of the learned linear classi\ufb01er on new data.\nWe now show that Rocchio and Naive Bayes are linear classi\ufb01er s. To see\nthis for Rocchio, observe that a vector /vectorxis on the decision boundary if it has\nequal distance to the two class centroids:\n|/vector\u00b5(c1)\u2212/vectorx|=|/vector\u00b5(c2)\u2212/vectorx| (14.4)\nSome basic arithmetic shows that this corresponds to a linea r classi\ufb01er with\nnormal vector /vectorw=/vector\u00b5(c1)\u2212/vector\u00b5(c2)and b=0.5\u2217(|/vector\u00b5(c1)|2\u2212|/vector\u00b5(c2)|2)(Exer-\ncise 14.15 ).\nWe can derive the linearity of Naive Bayes from its decision r ule, which\nchooses the category cwith the largest \u02c6P(c|d)(Figure 13.2, page 260) where:\n\u02c6P(c|d)\u221d\u02c6P(c)\u220f\n1\u2264k\u2264nd\u02c6P(tk|c)\nand ndis the number of tokens in the document that are part of the voc abu-\nlary. Denoting the complement category as \u00afc, we obtain for the log odds:\nlog\u02c6P(c|d)\n\u02c6P(\u00afc|d)=log\u02c6P(c)\n\u02c6P(\u00afc)+\u2211\n1\u2264k\u2264ndlog\u02c6P(tk|c)\n\u02c6P(tk|\u00afc)(14.5)\nOnline edition (c)\n2009 Cambridge UP14.4 Linear versus nonlinear classi\ufb01ers 303\nti wi d1id2iti wi d1id2i\nprime 0.70 0 1 dlrs -0.71 1 1\nrate 0.67 1 0 world -0.35 1 0\ninterest 0.63 0 0 sees -0.33 0 0\nrates 0.60 0 0 year -0.25 0 0\ndiscount 0.46 1 0 group -0.24 0 0\nbundesbank 0.43 0 0 dlr -0.24 0 0\n\u25eeTable 14.4 A linear classi\ufb01er. The dimensions tiand parameters wiof a linear\nclassi\ufb01er for the class interest (as ininterest rate ) in Reuters-21578. The threshold is\nb=0. Terms like dlrandworld have negative weights because they are indicators for\nthe competing class currency .\nWe choose class cif the odds are greater than 1 or, equivalently, if the log\nodds are greater than 0. It is easy to see that Equation ( 14.5) is an instance\nof Equation ( 14.3) for wi=log[\u02c6P(ti|c)/\u02c6P(ti|\u00afc)],xi=number of occurrences\noftiind, and b=\u2212log[\u02c6P(c)/\u02c6P(\u00afc)]. Here, the index i, 1\u2264i\u2264M, refers\nto terms of the vocabulary (not to positions in daskdoes; cf. Section 13.4.1 ,\npage 270) and /vectorxand/vectorwareM-dimensional vectors. So in log space, Naive\nBayes is a linear classi\ufb01er.\n\u270eExample 14.3: Table 14.4 de\ufb01nes a linear classi\ufb01er for the category interest in\nReuters-21578 (see Section 13.6, page 279). We assign document /vectord1\u201crate discount\ndlrs world\u201d to interest since/vectorwT/vectord1=0.67\u00b71+0.46\u00b71+ (\u22120.71)\u00b71+ (\u22120.35)\u00b71=\n0.07>0=b. We assign /vectord2\u201cprime dlrs\u201d to the complement class (not in interest ) since\n/vectorwT/vectord2=\u22120.01\u2264b. For simplicity, we assume a simple binary vector represent ation\nin this example: 1 for occurring terms, 0 for non-occurring t erms.\nFigure 14.10 is a graphical example of a linear problem , which we de\ufb01ne to\nmean that the underlying distributions P(d|c)and P(d|c)of the two classes\nare separated by a line. We call this separating line the class boundary . It is CLASS BOUNDARY\nthe \u201ctrue\u201d boundary of the two classes and we distinguish it f rom the deci-\nsion boundary that the learning method computes to approxim ate the class\nboundary.\nAs is typical in text classi\ufb01cation, there are some noise documents in Fig- NOISE DOCUMENT\nure14.10 (marked with arrows) that do not \ufb01t well into the overall dist ri-\nbution of the classes. In Section 13.5 (page 271), we de\ufb01ned a noise feature\nas a misleading feature that, when included in the document r epresentation,\non average increases the classi\ufb01cation error. Analogously , a noise document\nis a document that, when included in the training set, mislea ds the learn-\ning method and increases classi\ufb01cation error. Intuitively , the underlying\ndistribution partitions the representation space into are as with mostly ho-\nOnline edition (c)\n2009 Cambridge UP304 14 Vector space classi\ufb01cation\n\u25eeFigure 14.10 A linear problem with noise. In this hypothetical web page cl assi\ufb01-\ncation scenario, Chinese-only web pages are solid circles a nd mixed Chinese-English\nweb pages are squares. The two classes are separated by a line ar class boundary\n(dashed line, short dashes), except for three noise documen ts (marked with arrows).\nmogeneous class assignments. A document that does not confo rm with the\ndominant class in its area is a noise document.\nNoise documents are one reason why training a linear classi\ufb01 er is hard. If\nwe pay too much attention to noise documents when choosing th e decision\nhyperplane of the classi\ufb01er, then it will be inaccurate on ne w data. More\nfundamentally, it is usually dif\ufb01cult to determine which do cuments are noise\ndocuments and therefore potentially misleading.\nIf there exists a hyperplane that perfectly separates the tw o classes, then\nwe call the two classes linearly separable . In fact, if linear separability holds, LINEAR SEPARABILITY\nthen there is an in\ufb01nite number of linear separators (Exerci se14.4) as illus-\ntrated by Figure 14.8, where the number of possible separating hyperplanes\nis in\ufb01nite.\nFigure 14.8 illustrates another challenge in training a linear classi\ufb01 er. If we\nare dealing with a linearly separable problem, then we need a criterion for\nselecting among all decision hyperplanes that perfectly se parate the training\ndata. In general, some of these hyperplanes will do well on ne w data, some\nOnline edition (c)\n2009 Cambridge UP14.4 Linear versus nonlinear classi\ufb01ers 305\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\n\u25eeFigure 14.11 A nonlinear problem.\nwill not.\nAn example of a nonlinear classi\ufb01er is kNN. The nonlinearity of kNN is NONLINEAR\nCLASSIFIER intuitively clear when looking at examples like Figure 14.6. The decision\nboundaries of kNN (the double lines in Figure 14.6) are locally linear seg-\nments, but in general have a complex shape that is not equival ent to a line in\n2D or a hyperplane in higher dimensions.\nFigure 14.11 is another example of a nonlinear problem: there is no good\nlinear separator between the distributions P(d|c)and P(d|c)because of the\ncircular \u201cenclave\u201d in the upper left part of the graph. Linea r classi\ufb01ers mis-\nclassify the enclave, whereas a nonlinear classi\ufb01er like kN N will be highly\naccurate for this type of problem if the training set is large enough.\nIf a problem is nonlinear and its class boundaries cannot be a pproximated\nwell with linear hyperplanes, then nonlinear classi\ufb01ers ar e often more accu-\nrate than linear classi\ufb01ers. If a problem is linear, it is bes t to use a simpler\nlinear classi\ufb01er.\n?Exercise 14.4\nProve that the number of linear separators of two classes is e ither in\ufb01nite or zero.\nOnline edition (c)\n2009 Cambridge UP306 14 Vector space classi\ufb01cation\n14.5 Classi\ufb01cation with more than two classes\nWe can extend two-class linear classi\ufb01ers to J>2 classes. The method to use\ndepends on whether the classes are mutually exclusive or not .\nClassi\ufb01cation for classes that are not mutually exclusive i s called any-of , ANY -OF\nCLASSIFICATION multilabel , ormultivalue classi\ufb01cation . In this case, a document can belong to\nseveral classes simultaneously, or to a single class, or to n one of the classes.\nA decision on one class leaves all options open for the others . It is some-\ntimes said that the classes are independent of each other, but this is misleading\nsince the classes are rarely statistically independent in t he sense de\ufb01ned on\npage 275. In terms of the formal de\ufb01nition of the classi\ufb01cation probl em in\nEquation ( 13.1) (page 256), we learn Jdifferent classi\ufb01ers \u03b3jin any-of classi-\n\ufb01cation, each returning either cjorcj:\u03b3j(d)\u2208{cj,cj}.\nSolving an any-of classi\ufb01cation task with linear classi\ufb01er s is straightfor-\nward:\n1.Build a classi\ufb01er for each class, where the training set cons ists of the set\nof documents in the class (positive labels) and its compleme nt (negative\nlabels).\n2.Given the test document, apply each classi\ufb01er separately. T he decision of\none classi\ufb01er has no in\ufb02uence on the decisions of the other cl assi\ufb01ers.\nThe second type of classi\ufb01cation with more than two classes i sone-of clas- ONE -OF\nCLASSIFICATION si\ufb01cation . Here, the classes are mutually exclusive. Each document mu st\nbelong to exactly one of the classes. One-of classi\ufb01cation i s also called multi-\nnomial ,polytomous4,multiclass , orsingle-label classi\ufb01cation . Formally, there is a\nsingle classi\ufb01cation function \u03b3in one-of classi\ufb01cation whose range is C, i.e.,\n\u03b3(d)\u2208{c1, . . . , cJ}. kNN is a (nonlinear) one-of classi\ufb01er.\nTrue one-of problems are less common in text classi\ufb01cation t han any-of\nproblems. With classes like UK,China ,poultry , orcoffee , a document can be\nrelevant to many topics simultaneously \u2013 as when the prime mi nister of the\nUK visits China to talk about the coffee and poultry trade.\nNevertheless, we will often make a one-of assumption, as we d id in Fig-\nure14.1, even if classes are not really mutually exclusive. For the c lassi\ufb01ca-\ntion problem of identifying the language of a document, the o ne-of assump-\ntion is a good approximation as most text is written in only on e language.\nIn such cases, imposing a one-of constraint can increase the classi\ufb01er\u2019s ef-\nfectiveness because errors that are due to the fact that the a ny-of classi\ufb01ers\nassigned a document to either no class or more than one class a re eliminated.\nJhyperplanes do not divide R|V|into Jdistinct regions as illustrated in\nFigure 14.12 . Thus, we must use a combination method when using two-\nclass linear classi\ufb01ers for one-of classi\ufb01cation. The simp lest method is to\n4. A synonym of polytomous ispolychotomous .\nOnline edition (c)\n2009 Cambridge UP14.5 Classi\ufb01cation with more than two classes 307\n?\n\u25eeFigure 14.12 Jhyperplanes do not divide space into Jdisjoint regions.\nrank classes and then select the top-ranked class. Geometri cally, the ranking\ncan be with respect to the distances from the Jlinear separators. Documents\nclose to a class\u2019s separator are more likely to be misclassi\ufb01 ed, so the greater\nthe distance from the separator, the more plausible it is tha t a positive clas-\nsi\ufb01cation decision is correct. Alternatively, we can use a d irect measure of\ncon\ufb01dence to rank classes, e.g., probability of class membe rship. We can\nstate this algorithm for one-of classi\ufb01cation with linear c lassi\ufb01ers as follows:\n1.Build a classi\ufb01er for each class, where the training set cons ists of the set\nof documents in the class (positive labels) and its compleme nt (negative\nlabels).\n2.Given the test document, apply each classi\ufb01er separately.\n3.Assign the document to the class with\n\u2022the maximum score,\n\u2022the maximum con\ufb01dence value,\n\u2022or the maximum probability.\nAn important tool for analyzing the performance of a classi\ufb01 er for J>2\nclasses is the confusion matrix . The confusion matrix shows for each pair of CONFUSION MATRIX\nclasses/an}\u230ara\u230bketle{tc1,c2/an}\u230ara\u230bketri}ht, how many documents from c1were incorrectly assigned to c2.\nIn Table 14.5, the classi\ufb01er manages to distinguish the three \ufb01nancial cl asses\nmoney-fx ,trade , and interest from the three agricultural classes wheat ,corn,\nand grain , but makes many errors within these two groups. The confusio n\nmatrix can help pinpoint opportunities for improving the ac curacy of the\nOnline edition (c)\n2009 Cambridge UP308 14 Vector space classi\ufb01cation\nassigned class money-fx trade interest wheat corn grain\ntrue class\nmoney-fx 95 0 10 0 0 0\ntrade 1 1 90 0 1 0\ninterest 13 0 0 0 0 0\nwheat 0 0 1 34 3 7\ncorn 1 0 2 13 26 5\ngrain 0 0 2 14 5 10\n\u25eeTable 14.5 A confusion matrix for Reuters-21578. For example, 14 docum ents\nfrom grain were incorrectly assigned to wheat . Adapted from Picca et al. (2006 ).\nsystem. For example, to address the second largest error in T able 14.5 (14 in\nthe row grain ), one could attempt to introduce features that distinguish wheat\ndocuments from grain documents.\n?Exercise 14.5\nCreate a training set of 300 documents, 100 each from three di fferent languages (e.g.,\nEnglish, French, Spanish). Create a test set by the same proc edure, but also add 100\ndocuments from a fourth language. Train (i) a one-of classi\ufb01 er (ii) an any-of classi-\n\ufb01er on this training set and evaluate it on the test set. (iii) Are there any interesting\ndifferences in how the two classi\ufb01ers behave on this task?\n\u270414.6 The bias-variance tradeoff\nNonlinear classi\ufb01ers are more powerful than linear classi\ufb01 ers. For some\nproblems, there exists a nonlinear classi\ufb01er with zero clas si\ufb01cation error, but\nno such linear classi\ufb01er. Does that mean that we should alway s use nonlinear\nclassi\ufb01ers for optimal effectiveness in statistical text c lassi\ufb01cation?\nTo answer this question, we introduce the bias-variance tra deoff in this sec-\ntion, one of the most important concepts in machine learning . The tradeoff\nhelps explain why there is no universally optimal learning m ethod. Selecting\nan appropriate learning method is therefore an unavoidable part of solving\na text classi\ufb01cation problem.\nThroughout this section, we use linear and nonlinear classi \ufb01ers as proto-\ntypical examples of \u201cless powerful\u201d and \u201cmore powerful\u201d lea rning, respec-\ntively. This is a simpli\ufb01cation for a number of reasons. Firs t, many nonlinear\nmodels subsume linear models as a special case. For instance , a nonlinear\nlearning method like kNN will in some cases produce a linear c lassi\ufb01er. Sec-\nond, there are nonlinear models that are less complex than li near models.\nFor instance, a quadratic polynomial with two parameters is less powerful\nthan a 10,000-dimensional linear classi\ufb01er. Third, the com plexity of learn-\ning is not really a property of the classi\ufb01er because there ar e many aspects\nOnline edition (c)\n2009 Cambridge UP14.6 The bias-variance tradeoff 309\nof learning (such as feature selection, cf. (Section 13.5, page 271), regulariza-\ntion, and constraints such as margin maximization in Chapte r15) that make\na learning method either more powerful or less powerful with out affecting\nthe type of classi\ufb01er that is the \ufb01nal result of learning \u2013 reg ardless of whether\nthat classi\ufb01er is linear or nonlinear. We refer the reader to the publications\nlisted in Section 14.7 for a treatment of the bias-variance tradeoff that takes\ninto account these complexities. In this section, linear an d nonlinear classi-\n\ufb01ers will simply serve as proxies for weaker and stronger lea rning methods\nin text classi\ufb01cation.\nWe \ufb01rst need to state our objective in text classi\ufb01cation mor e precisely. In\nSection 13.1 (page 256), we said that we want to minimize classi\ufb01cation er-\nror on the test set. The implicit assumption was that trainin g documents\nand test documents are generated according to the same under lying distri-\nbution. We will denote this distribution P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht)where dis the document\nand cits label or class. Figures 13.4 and 13.5 were examples of generative\nmodels that decompose P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht)into the product of P(c)and P(d|c). Fig-\nures 14.10 and 14.11 depict generative models for /an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}htwith d\u2208R2and\nc\u2208{square, solid circle }.\nIn this section, instead of using the number of correctly cla ssi\ufb01ed test doc-\numents (or, equivalently, the error rate on test documents) as evaluation\nmeasure, we adopt an evaluation measure that addresses the i nherent un-\ncertainty of labeling. In many text classi\ufb01cation problems , a given document\nrepresentation can arise from documents belonging to diffe rent classes. This\nis because documents from different classes can be mapped to the same doc-\nument representation. For example, the one-sentence docum ents China sues\nFrance and France sues China are mapped to the same document representa-\ntion d\u2032={China ,France ,sues}in a bag of words model. But only the latter\ndocument is relevant to the class c\u2032=legal actions brought by France (which\nmight be de\ufb01ned, for example, as a standing query by an intern ational trade\nlawyer).\nTo simplify the calculations in this section, we do not count the number\nof errors on the test set when evaluating a classi\ufb01er, but ins tead look at how\nwell the classi\ufb01er estimates the conditional probability P(c|d)of a document\nbeing in a class. In the above example, we might have P(c\u2032|d\u2032) =0.5.\nOur goal in text classi\ufb01cation then is to \ufb01nd a classi\ufb01er \u03b3such that, aver-\naged over documents d,\u03b3(d)is as close as possible to the true probability\nP(c|d). We measure this using mean squared error:\nMSE(\u03b3) =Ed[\u03b3(d)\u2212P(c|d)]2(14.6)\nwhere Edis the expectation with respect to P(d). The mean squared error\nterm gives partial credit for decisions by \u03b3that are close if not completely\nright.\nOnline edition (c)\n2009 Cambridge UP310 14 Vector space classi\ufb01cation\nE[x\u2212\u03b1]2=Ex2\u22122Ex\u03b1+\u03b12(14.8)\n= ( Ex)2\u22122Ex\u03b1+\u03b12\n+Ex2\u22122(Ex)2+ (Ex)2\n= [ Ex\u2212\u03b1]2\n+Ex2\u2212E2x(Ex) +E(Ex)2\n= [ Ex\u2212\u03b1]2+E[x\u2212Ex]2\nEDEd[\u0393D(d)\u2212P(c|d)]2=EdED[\u0393D(d)\u2212P(c|d)]2(14.9)\n=Ed[ [ED\u0393D(d)\u2212P(c|d)]2\n+ED[\u0393D(d)\u2212ED\u0393D(d)]2]\n\u25eeFigure 14.13 Arithmetic transformations for the bias-variance decompo sition.\nFor the derivation of Equation ( 14.9), we set \u03b1=P(c|d)and x=\u0393D(d)in Equa-\ntion ( 14.8).\nWe de\ufb01ne a classi\ufb01er \u03b3to be optimal for a distribution P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht)if it mini- OPTIMAL CLASSIFIER\nmizes MSE (\u03b3).\nMinimizing MSE is a desideratum for classi\ufb01ers . We also need a criterion\nforlearning methods . Recall that we de\ufb01ned a learning method \u0393as a function\nthat takes a labeled training set Das input and returns a classi\ufb01er \u03b3.\nFor learning methods, we adopt as our goal to \ufb01nd a \u0393that, averaged over\ntraining sets, learns classi\ufb01ers \u03b3with minimal MSE. We can formalize this as\nminimizing learning error : LEARNING ERROR\nlearning-error (\u0393) =ED[MSE(\u0393(D))] (14.7)\nwhere EDis the expectation over labeled training sets. To keep thing s simple,\nwe can assume that training sets have a \ufb01xed size \u2013 the distrib ution P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht)\nthen de\ufb01nes a distribution P(D)over training sets.\nWe can use learning error as a criterion for selecting a learn ing method in\nstatistical text classi\ufb01cation. A learning method \u0393isoptimal for a distribution OPTIMAL LEARNING\nMETHOD P(D)if it minimizes the learning error.\nWriting \u0393Dfor\u0393(D)for better readability, we can transform Equation ( 14.7)\nas follows:\nlearning-error (\u0393) = ED[MSE(\u0393D)]\n=EDEd[\u0393D(d)\u2212P(c|d)]2(14.10)\n=Ed[bias(\u0393,d) +variance (\u0393,d)] (14.11)\nOnline edition (c)\n2009 Cambridge UP14.6 The bias-variance tradeoff 311\nbias(\u0393,d) = [ P(c|d)\u2212ED\u0393D(d)]2(14.12)\nvariance (\u0393,d) = ED[\u0393D(d)\u2212ED\u0393D(d)]2(14.13)\nwhere the equivalence between Equations ( 14.10 ) and ( 14.11 ) is shown in\nEquation ( 14.9) in Figure 14.13 . Note that dand Dare independent of each\nother. In general, for a random document dand a random training set D,D\ndoes not contain a labeled instance of d.\nBias is the squared difference between P(c|d), the true conditional prob- BIAS\nability of dbeing in c, and \u0393D(d), the prediction of the learned classi\ufb01er,\naveraged over training sets. Bias is large if the learning me thod produces\nclassi\ufb01ers that are consistently wrong. Bias is small if (i) the classi\ufb01ers are\nconsistently right or (ii) different training sets cause er rors on different docu-\nments or (iii) different training sets cause positive and ne gative errors on the\nsame documents, but that average out to close to 0. If one of th ese three con-\nditions holds, then ED\u0393D(d), the expectation over all training sets, is close to\nP(c|d).\nLinear methods like Rocchio and Naive Bayes have a high bias f or non-\nlinear problems because they can only model one type of class boundary, a\nlinear hyperplane. If the generative model P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht)has a complex nonlinear\nclass boundary, the bias term in Equation ( 14.11 ) will be high because a large\nnumber of points will be consistently misclassi\ufb01ed. For exa mple, the circular\nenclave in Figure 14.11 does not \ufb01t a linear model and will be misclassi\ufb01ed\nconsistently by linear classi\ufb01ers.\nWe can think of bias as resulting from our domain knowledge (o r lack\nthereof) that we build into the classi\ufb01er. If we know that the true boundary\nbetween the two classes is linear, then a learning method tha t produces linear\nclassi\ufb01ers is more likely to succeed than a nonlinear method . But if the true\nclass boundary is not linear and we incorrectly bias the clas si\ufb01er to be linear,\nthen classi\ufb01cation accuracy will be low on average.\nNonlinear methods like kNN have low bias. We can see in Figure 14.6 that\nthe decision boundaries of kNN are variable \u2013 depending on th e distribu-\ntion of documents in the training set, learned decision boun daries can vary\ngreatly. As a result, each document has a chance of being clas si\ufb01ed correctly\nfor some training sets. The average prediction ED\u0393D(d)is therefore closer to\nP(c|d)and bias is smaller than for a linear learning method.\nVariance is the variation of the prediction of learned classi\ufb01ers: th e aver- VARIANCE\nage squared difference between \u0393D(d)and its average ED\u0393D(d). Variance is\nlarge if different training sets Dgive rise to very different classi\ufb01ers \u0393D. It is\nsmall if the training set has a minor effect on the classi\ufb01cat ion decisions \u0393D\nmakes, be they correct or incorrect. Variance measures how i nconsistent the\ndecisions are, not whether they are correct or incorrect.\nLinear learning methods have low variance because most rand omly drawn\ntraining sets produce similar decision hyperplanes. The de cision lines pro-\nOnline edition (c)\n2009 Cambridge UP312 14 Vector space classi\ufb01cation\nduced by linear learning methods in Figures 14.10 and 14.11 will deviate\nslightly from the main class boundaries, depending on the tr aining set, but\nthe class assignment for the vast majority of documents (wit h the exception\nof those close to the main boundary) will not be affected. The circular enclave\nin Figure 14.11 will be consistently misclassi\ufb01ed.\nNonlinear methods like kNN have high variance. It is apparen t from Fig-\nure14.6 that kNN can model very complex boundaries between two class es.\nIt is therefore sensitive to noise documents of the sort depi cted in Figure 14.10 .\nAs a result the variance term in Equation ( 14.11 ) is large for kNN: Test doc-\numents are sometimes misclassi\ufb01ed \u2013 if they happen to be clos e to a noise\ndocument in the training set \u2013 and sometimes correctly class i\ufb01ed \u2013 if there\nare no noise documents in the training set near them. This res ults in high\nvariation from training set to training set.\nHigh-variance learning methods are prone to over\ufb01tting the training data. OVERFITTING\nThe goal in classi\ufb01cation is to \ufb01t the training data to the ext ent that we cap-\nture true properties of the underlying distribution P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht). In over\ufb01tting,\nthe learning method also learns from noise. Over\ufb01tting incr eases MSE and\nfrequently is a problem for high-variance learning methods .\nWe can also think of variance as the model complexity or, equivalently, mem- MEMORY CAPACITY\nory capacity of the learning method \u2013 how detailed a characterization of t he\ntraining set it can remember and then apply to new data. This c apacity corre-\nsponds to the number of independent parameters available to \ufb01t the training\nset. Each kNN neighborhood Skmakes an independent classi\ufb01cation deci-\nsion. The parameter in this case is the estimate \u02c6P(c|Sk)from Figure 14.7.\nThus, kNN\u2019s capacity is only limited by the size of the traini ng set. It can\nmemorize arbitrarily large training sets. In contrast, the number of parame-\nters of Rocchio is \ufb01xed \u2013 Jparameters per dimension, one for each centroid\n\u2013 and independent of the size of the training set. The Rocchio classi\ufb01er (in\nform of the centroids de\ufb01ning it) cannot \u201cremember\u201d \ufb01ne-gra ined details of\nthe distribution of the documents in the training set.\nAccording to Equation ( 14.7), our goal in selecting a learning method is to\nminimize learning error. The fundamental insight captured by Equation ( 14.11 ),\nwhich we can succinctly state as: learning-error = bias + var iance, is that the\nlearning error has two components, bias and variance, which in general can-\nnot be minimized simultaneously. When comparing two learni ng methods\n\u03931and \u03932, in most cases the comparison comes down to one method having\nhigher bias and lower variance and the other lower bias and hi gher variance.\nThe decision for one learning method vs. another is then not s imply a mat-\nter of selecting the one that reliably produces good classi\ufb01 ers across training\nsets (small variance) or the one that can learn classi\ufb01catio n problems with\nvery dif\ufb01cult decision boundaries (small bias). Instead, w e have to weigh\nthe respective merits of bias and variance in our applicatio n and choose ac-\ncordingly. This tradeoff is called the bias-variance tradeoff . BIAS -VARIANCE\nTRADEOFF\nOnline edition (c)\n2009 Cambridge UP14.6 The bias-variance tradeoff 313\nFigure 14.10 provides an illustration, which is somewhat contrived, but\nwill be useful as an example for the tradeoff. Some Chinese te xt contains\nEnglish words written in the Roman alphabet like CPU,ONLINE , andGPS.\nConsider the task of distinguishing Chinese-only web pages from mixed\nChinese-English web pages. A search engine might offer Chin ese users with-\nout knowledge of English (but who understand loanwords like CPU) the op-\ntion of \ufb01ltering out mixed pages. We use two features for this classi\ufb01cation\ntask: number of Roman alphabet characters and number of Chin ese char-\nacters on the web page. As stated earlier, the distribution P(/an}\u230ara\u230bketle{td,c/an}\u230ara\u230bketri}ht) of the\ngenerative model generates most mixed (respectively, Chin ese) documents\nabove (respectively, below) the short-dashed line, but the re are a few noise\ndocuments.\nIn Figure 14.10 , we see three classi\ufb01ers:\n\u2022One-feature classi\ufb01er. Shown as a dotted horizontal line. This classi\ufb01er\nuses only one feature, the number of Roman alphabet characte rs. Assum-\ning a learning method that minimizes the number of misclassi \ufb01cations\nin the training set, the position of the horizontal decision boundary is\nnot greatly affected by differences in the training set (e.g ., noise docu-\nments). So a learning method producing this type of classi\ufb01e r has low\nvariance. But its bias is high since it will consistently mis classify squares\nin the lower left corner and \u201csolid circle\u201d documents with mo re than 50\nRoman characters.\n\u2022Linear classi\ufb01er. Shown as a dashed line with long dashes. Learning lin-\near classi\ufb01ers has less bias since only noise documents and p ossibly a few\ndocuments close to the boundary between the two classes are m isclassi-\n\ufb01ed. The variance is higher than for the one-feature classi\ufb01 ers, but still\nsmall: The dashed line with long dashes deviates only slight ly from the\ntrue boundary between the two classes, and so will almost all linear de-\ncision boundaries learned from training sets. Thus, very fe w documents\n(documents close to the class boundary) will be inconsisten tly classi\ufb01ed.\n\u2022\u201cFit-training-set-perfectly\u201d classi\ufb01er. Shown as a solid line. Here, the\nlearning method constructs a decision boundary that perfec tly separates\nthe classes in the training set. This method has the lowest bi as because\nthere is no document that is consistently misclassi\ufb01ed \u2013 the classi\ufb01ers\nsometimes even get noise documents in the test set right. But the variance\nof this learning method is high. Because noise documents can move the\ndecision boundary arbitrarily, test documents close to noi se documents\nin the training set will be misclassi\ufb01ed \u2013 something that a li near learning\nmethod is unlikely to do.\nIt is perhaps surprising that so many of the best-known text c lassi\ufb01cation\nalgorithms are linear. Some of these methods, in particular linear SVMs, reg-\nOnline edition (c)\n2009 Cambridge UP314 14 Vector space classi\ufb01cation\nularized logistic regression and regularized linear regre ssion, are among the\nmost effective known methods. The bias-variance tradeoff p rovides insight\ninto their success. Typical classes in text classi\ufb01cation a re complex and seem\nunlikely to be modeled well linearly. However, this intuiti on is misleading\nfor the high-dimensional spaces that we typically encounte r in text appli-\ncations. With increased dimensionality, the likelihood of linear separability\nincreases rapidly (Exercise 14.17 ). Thus, linear models in high-dimensional\nspaces are quite powerful despite their linearity. Even mor e powerful nonlin-\near learning methods can model decision boundaries that are more complex\nthan a hyperplane, but they are also more sensitive to noise i n the training\ndata. Nonlinear learning methods sometimes perform better if the training\nset is large, but by no means in all cases.\n14.7 References and further reading\nAs discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio\n(1971 ).Joachims (1997 ) presents a probabilistic analysis of the method. Roc-\nchio classi\ufb01cation was widely used as a classi\ufb01cation metho d in TREC in the\n1990s ( Buckley et al. 1994a ;b,Voorhees and Harman 2005 ). Initially, it was\nused as a form of routing . Routing merely ranks documents according to rel- ROUTING\nevance to a class without assigning them. Early work on \ufb01ltering , a true clas- FILTERING\nsi\ufb01cation approach that makes an assignment decision on eac h document,\nwas published by Ittner et al. (1995 ) and Schapire et al. (1998 ). The de\ufb01nition\nof routing we use here should not be confused with another sen se. Routing\ncan also refer to the electronic distribution of documents t o subscribers, the\nso-called push model of document distribution. In a pull model , each transfer PUSH MODEL\nPULL MODEL of a document to the user is initiated by the user \u2013 for example , by means\nof search or by selecting it from a list of documents on a news a ggregation\nwebsite.\nSome authors restrict the name Roccchio classi\ufb01cation to two-class problems\nand use the terms cluster-based (Iwayama and Tokunaga 1995 ) and centroid- CENTROID -BASED\nCLASSIFICATION based classi\ufb01cation (Han and Karypis 2000 ,Tan and Cheng 2007 ) for Rocchio\nclassi\ufb01cation with J>2.\nA more detailed treatment of kNN can be found in ( Hastie et al. 2001 ), in-\ncluding methods for tuning the parameter k. An example of an approximate\nfast kNN algorithm is locality-based hashing ( Andoni et al. 2006 ).Klein-\nberg (1997 ) presents an approximate \u0398((Mlog2M)(M+logN))kNN algo-\nrithm (where Mis the dimensionality of the space and Nthe number of data\npoints), but at the cost of exponential storage requirement s:\u0398((NlogM)2M).\nIndyk (2004 ) surveys nearest neighbor methods in high-dimensional spa ces.\nEarly work on kNN in text classi\ufb01cation was motivated by the a vailability\nof massively parallel hardware architectures ( Creecy et al. 1992 ).Yang (1994 )\nOnline edition (c)\n2009 Cambridge UP14.8 Exercises 315\nuses an inverted index to speed up kNN classi\ufb01cation. The opt imality result\nfor 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart\n(1967 ).\nThe effectiveness of Rocchio classi\ufb01cation and kNN is highl y dependent\non careful parameter tuning (in particular, the parameters b\u2032for Rocchio on\npage 296and kfor kNN), feature engineering (Section 15.3, page 334) and\nfeature selection (Section 13.5, page 271).Buckley and Salton (1995 ),Schapire\net al. (1998 ),Yang and Kisiel (2003 ) and Moschitti (2003 ) address these issues\nfor Rocchio and Yang (2001 ) and Ault and Yang (2002 ) for kNN. Zavrel et al.\n(2000 ) compare feature selection methods for kNN.\nThe bias-variance tradeoff was introduced by Geman et al. (1992 ). The\nderivation in Section 14.6 is for MSE (\u03b3), but the tradeoff applies to many\nloss functions (cf. Friedman (1997 ),Domingos (2000 )).Sch\u00fctze et al. (1995 )\nand Lewis et al. (1996 ) discuss linear classi\ufb01ers for text and Hastie et al. (2001 )\nlinear classi\ufb01ers in general. Readers interested in the alg orithms mentioned,\nbut not described in this chapter may wish to consult Bishop (2006 ) for neu-\nral networks, Hastie et al. (2001 ) for linear and logistic regression, and Min-\nsky and Papert (1988 ) for the perceptron algorithm. Anagnostopoulos et al.\n(2006 ) show that an inverted index can be used for highly ef\ufb01cient d ocument\nclassi\ufb01cation with any linear classi\ufb01er, provided that the classi\ufb01er is still ef-\nfective when trained on a modest number of features via featu re selection.\nWe have only presented the simplest method for combining two -class clas-\nsi\ufb01ers into a one-of classi\ufb01er. Another important method is the use of error-\ncorrecting codes, where a vector of decisions of different t wo-class classi\ufb01ers\nis constructed for each document. A test document\u2019s decisio n vector is then\n\u201ccorrected\u201d based on the distribution of decision vectors i n the training set,\na procedure that incorporates information from all two-cla ss classi\ufb01ers and\ntheir correlations into the \ufb01nal classi\ufb01cation decision ( Dietterich and Bakiri\n1995 ).Ghamrawi and McCallum (2005 ) also exploit dependencies between\nclasses in any-of classi\ufb01cation. Allwein et al. (2000 ) propose a general frame-\nwork for combining two-class classi\ufb01ers.\n14.8 Exercises\n?Exercise 14.6\nIn Figure 14.14 , which of the three vectors /vectora,/vectorb, and/vectorcis (i) most similar to /vectorxaccording\nto dot product similarity, (ii) most similar to /vectorxaccording to cosine similarity, (iii)\nclosest to /vectorxaccording to Euclidean distance?\nExercise 14.7\nDownload Reuters-21578 and train and test Rocchio and kNN cl assi\ufb01ers for the classes\nacquisitions ,corn,crude ,earn,grain ,interest ,money-fx ,ship,trade , and wheat . Use the\nModApte split. You may want to use one of a number of software p ackages that im-\nOnline edition (c)\n2009 Cambridge UP316 14 Vector space classi\ufb01cation\n0 1 2 3 4 5 6 7 8012345678\naxbc\n\u25eeFigure 14.14 Example for differences between Euclidean distance, dot pr oduct\nsimilarity and cosine similarity. The vectors are /vectora= (0.5 1.5 )T,/vectorx= (2 2)T,/vectorb=\n(4 4)T, and/vectorc= (8 6)T.\nplement Rocchio classi\ufb01cation and kNN classi\ufb01cation, for e xample, the Bow toolkit\n(McCallum 1996 ).\nExercise 14.8\nDownload 20 Newgroups (page 154) and train and test Rocchio and kNN classi\ufb01ers\nfor its 20 classes.\nExercise 14.9\nShow that the decision boundaries in Rocchio classi\ufb01cation are, as in kNN, given by\nthe Voronoi tessellation.\nExercise 14.10 [\u22c6]\nComputing the distance between a dense centroid and a sparse vector is \u0398(M)for\na naive implementation that iterates over all Mdimensions. Based on the equality\n\u2211(xi\u2212\u00b5i)2=1.0+\u2211\u00b52\ni\u22122\u2211xi\u00b5iand assuming that \u2211\u00b52\nihas been precomputed,\nwrite down an algorithm that is \u0398(Ma)instead, where Mais the number of distinct\nterms in the test document.\nExercise 14.11 [\u22c6 \u22c6 \u22c6 ]\nProve that the region of the plane consisting of all points wi th the same knearest\nneighbors is a convex polygon.\nExercise 14.12\nDesign an algorithm that performs an ef\ufb01cient 1NN search in 1 dimension (where\nef\ufb01ciency is with respect to the number of documents N). What is the time complexity\nof the algorithm?\nExercise 14.13 [\u22c6 \u22c6 \u22c6 ]\nDesign an algorithm that performs an ef\ufb01cient 1NN search in 2 dimensions with at\nmost polynomial (in N) preprocessing time.\nOnline edition (c)\n2009 Cambridge UP14.8 Exercises 317\n/Bullet\n/Bullet\n\u25eeFigure 14.15 A simple non-separable set of points.\nExercise 14.14 [\u22c6 \u22c6 \u22c6 ]\nCan one design an exact ef\ufb01cient algorithm for 1NN for very la rgeMalong the ideas\nyou used to solve the last exercise?\nExercise 14.15\nShow that Equation ( 14.4) de\ufb01nes a hyperplane with /vectorw=/vector\u00b5(c1)\u2212/vector\u00b5(c2)and b=\n0.5\u2217(|/vector\u00b5(c1)|2\u2212|/vector\u00b5(c2)|2).\nExercise 14.16\nWe can easily construct non-separable data sets in high dime nsions by embedding\na non-separable set like the one shown in Figure 14.15 . Consider embedding Fig-\nure14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a small\ndistance in a random direction). Why would you expect the res ulting con\ufb01guration\nto be linearly separable? How likely is then a non-separable set of m\u226aMpoints in\nM-dimensional space?\nExercise 14.17\nAssuming two classes, show that the percentage of non-separ able assignments of the\nvertices of a hypercube decreases with dimensionality MforM>1. For example,\nforM=1 the proportion of non-separable assignments is 0, for M=2, it is 2/16.\nOne of the two non-separable cases for M=2 is shown in Figure 14.15 , the other is\nits mirror image. Solve the exercise either analytically or by simulation.\nExercise 14.18\nAlthough we point out the similarities of Naive Bayes with li near vector space classi-\n\ufb01ers, it does not make sense to represent count vectors (the d ocument representations\nin NB) in a continuous vector space. There is however a formal ization of NB that is\nanalogous to Rocchio. Show that NB assigns a document to the c lass (represented as\na parameter vector) whose Kullback-Leibler (KL) divergenc e (Section 12.4, page 251)\nto the document (represented as a count vector as in Section 13.4.1 (page 270), nor-\nmalized to sum to 1) is smallest.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 319\n15Support vector machines and\nmachine learning on documents\nImproving classi\ufb01er effectiveness has been an area of inten sive machine-\nlearning research over the last two decades, and this work ha s led to a new\ngeneration of state-of-the-art classi\ufb01ers, such as suppor t vector machines,\nboosted decision trees, regularized logistic regression, neural networks, and\nrandom forests. Many of these methods, including support ve ctor machines\n(SVMs), the main topic of this chapter, have been applied wit h success to\ninformation retrieval problems, particularly text classi \ufb01cation. An SVM is a\nkind of large-margin classi\ufb01er: it is a vector space based ma chine learning\nmethod where the goal is to \ufb01nd a decision boundary between tw o classes\nthat is maximally far from any point in the training data (pos sibly discount-\ning some points as outliers or noise).\nWe will initially motivate and develop SVMs for the case of tw o-class data\nsets that are separable by a linear classi\ufb01er (Section 15.1), and then extend the\nmodel in Section 15.2 to non-separable data, multi-class problems, and non-\nlinear models, and also present some additional discussion of SVM perfor-\nmance. The chapter then moves to consider the practical depl oyment of text\nclassi\ufb01ers in Section 15.3: what sorts of classi\ufb01ers are appropriate when, and\nhow can you exploit domain-speci\ufb01c text features in classi\ufb01 cation? Finally,\nwe will consider how the machine learning technology that we have been\nbuilding for text classi\ufb01cation can be applied back to the pr oblem of learning\nhow to rank documents in ad hoc retrieval (Section 15.4). While several ma-\nchine learning methods have been applied to this task, use of SVMs has been\nprominent. Support vector machines are not necessarily bet ter than other\nmachine learning methods (except perhaps in situations wit h little training\ndata), but they perform at the state-of-the-art level and ha ve much current\ntheoretical and empirical appeal.\nOnline edition (c)\n2009 Cambridge UP320 15 Support vector machines and machine learning on document s\n/Bullet\n/Bullet/Bullet\n/Bullet\n/Bullet/Bullet\n/Bullet/Bullet/Bullet/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle\n/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/TriangleSupport vectors Maximum\nmargin\ndecision\nhyperplane\nMargin is\nmaximized\n\u25eeFigure 15.1 The support vectors are the 5 points right up against the marg in of\nthe classi\ufb01er.\n15.1 Support vector machines: The linearly separable case\nFor two-class, separable training data sets, such as the one in Figure 14.8\n(page 301), there are lots of possible linear separators. Intuitivel y, a decision\nboundary drawn in the middle of the void between data items of the two\nclasses seems better than one which approaches very close to examples of\none or both classes. While some learning methods such as the p erceptron\nalgorithm (see references in Section 14.7, page 314) \ufb01nd just any linear sepa-\nrator, others, like Naive Bayes, search for the best linear s eparator according\nto some criterion. The SVM in particular de\ufb01nes the criterio n to be looking\nfor a decision surface that is maximally far away from any dat a point. This\ndistance from the decision surface to the closest data point determines the\nmargin of the classi\ufb01er. This method of construction necessarily m eans that MARGIN\nthe decision function for an SVM is fully speci\ufb01ed by a (usual ly small) sub-\nset of the data which de\ufb01nes the position of the separator. Th ese points are\nreferred to as the support vectors (in a vector space, a point can be thought of SUPPORT VECTOR\nas a vector between the origin and that point). Figure 15.1 shows the margin\nand support vectors for a sample problem. Other data points p lay no part in\ndetermining the decision surface that is chosen.\nOnline edition (c)\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 321\n\u25eeFigure 15.2 An intuition for large-margin classi\ufb01cation. Insisting on a large mar-\ngin reduces the capacity of the model: the range of angles at w hich the fat deci-\nsion surface can be placed is smaller than for a decision hype rplane (cf. Figure 14.8,\npage 301).\nMaximizing the margin seems good because points near the dec ision sur-\nface represent very uncertain classi\ufb01cation decisions: th ere is almost a 50%\nchance of the classi\ufb01er deciding either way. A classi\ufb01er wit h a large margin\nmakes no low certainty classi\ufb01cation decisions. This gives you a classi\ufb01ca-\ntion safety margin: a slight error in measurement or a slight document vari-\nation will not cause a misclassi\ufb01cation. Another intuition motivating SVMs\nis shown in Figure 15.2. By construction, an SVM classi\ufb01er insists on a large\nmargin around the decision boundary. Compared to a decision hyperplane,\nif you have to place a fat separator between classes, you have fewer choices\nof where it can be put. As a result of this, the memory capacity of the model\nhas been decreased, and hence we expect that its ability to co rrectly general-\nize to test data is increased (cf. the discussion of the bias- variance tradeoff in\nChapter 14, page 312).\nLet us formalize an SVM with algebra. A decision hyperplane ( page 302)\ncan be de\ufb01ned by an intercept term band a decision hyperplane normal vec-\ntor/vectorwwhich is perpendicular to the hyperplane. This vector is com monly\nOnline edition (c)\n2009 Cambridge UP322 15 Support vector machines and machine learning on document s\nreferred to in the machine learning literature as the weight vector . To choose WEIGHT VECTOR\namong all the hyperplanes that are perpendicular to the norm al vector, we\nspecify the intercept term b. Because the hyperplane is perpendicular to the\nnormal vector, all points /vectorxon the hyperplane satisfy /vectorwT/vectorx=\u2212b. Now sup-\npose that we have a set of training data points D={(/vectorxi,yi)}, where each\nmember is a pair of a point /vectorxiand a class label yicorresponding to it.1For\nSVMs, the two data classes are always named +1 and\u22121 (rather than 1 and\n0), and the intercept term is always explicitly represented asb(rather than\nbeing folded into the weight vector /vectorwby adding an extra always-on feature).\nThe math works out much more cleanly if you do things this way, as we will\nsee almost immediately in the de\ufb01nition of functional margi n. The linear\nclassi\ufb01er is then:\nf(/vectorx) =sign(/vectorwT/vectorx+b) (15.1)\nA value of\u22121 indicates one class, and a value of +1 the other class.\nWe are con\ufb01dent in the classi\ufb01cation of a point if it is far awa y from the\ndecision boundary. For a given data set and decision hyperpl ane, we de\ufb01ne\nthefunctional margin of the ithexample /vectorxiwith respect to a hyperplane /an}\u230ara\u230bketle{t/vectorw,b/an}\u230ara\u230bketri}ht FUNCTIONAL MARGIN\nas the quantity yi(/vectorwT/vectorxi+b). The functional margin of a data set with re-\nspect to a decision surface is then twice the functional marg in of any of the\npoints in the data set with minimal functional margin (the fa ctor of 2 comes\nfrom measuring across the whole width of the margin, as in Fig ure15.3).\nHowever, there is a problem with using this de\ufb01nition as is: t he value is un-\nderconstrained, because we can always make the functional m argin as big\nas we wish by simply scaling up /vectorwand b. For example, if we replace /vectorwby\n5/vectorwand bby 5 bthen the functional margin yi(5/vectorwT/vectorxi+5b)is \ufb01ve times as\nlarge. This suggests that we need to place some constraint on the size of the\n/vectorwvector. To get a sense of how to do that, let us look at the actua l geometry.\nWhat is the Euclidean distance from a point /vectorxto the decision boundary? In\nFigure 15.3, we denote by rthis distance. We know that the shortest distance\nbetween a point and a hyperplane is perpendicular to the plan e, and hence,\nparallel to /vectorw. A unit vector in this direction is /vectorw/|/vectorw|. The dotted line in the\ndiagram is then a translation of the vector r/vectorw/|/vectorw|. Let us label the point on\nthe hyperplane closest to /vectorxas/vectorx\u2032. Then:\n/vectorx\u2032=/vectorx\u2212yr/vectorw\n|/vectorw|(15.2)\nwhere multiplying by yjust changes the sign for the two cases of /vectorxbeing on\neither side of the decision surface. Moreover, /vectorx\u2032lies on the decision boundary\n1. As discussed in Section 14.1 (page 291), we present the general case of points in a vector\nspace, but if the points are length normalized document vect ors, then all the action is taking\nplace on the surface of a unit sphere, and the decision surfac e intersects the sphere\u2019s surface.\nOnline edition (c)\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 323\n0 1 2 3 4 5 6 7 801234567\n/Bullet\n/Bullet/Bullet\n/Bullet\n/Bullet/Bullet\n/Bullet/Bullet/Bullet/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle/vectorx\n+\n/vectorx\u2032r/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/Triangle\n\u03c1\n/vectorw\n\u25eeFigure 15.3 The geometric margin of a point ( r) and a decision boundary ( \u03c1).\nand so satis\ufb01es /vectorwT/vectorx\u2032+b=0. Hence:\n/vectorwT/parenleftbig/vectorx\u2212yr/vectorw\n|/vectorw|/parenrightbig+b=0 (15.3)\nSolving for rgives:2\nr=y/vectorwT/vectorx+b\n|/vectorw|(15.4)\nAgain, the points closest to the separating hyperplane are s upport vectors.\nThe geometric margin of the classi\ufb01er is the maximum width of the band that GEOMETRIC MARGIN\ncan be drawn separating the support vectors of the two classe s. That is, it is\ntwice the minimum value over data points for rgiven in Equation ( 15.4), or,\nequivalently, the maximal width of one of the fat separators shown in Fig-\nure15.2. The geometric margin is clearly invariant to scaling of par ameters:\nif we replace /vectorwby 5/vectorwand bby 5b, then the geometric margin is the same, be-\ncause it is inherently normalized by the length of /vectorw. This means that we can\nimpose any scaling constraint we wish on /vectorwwithout affecting the geometric\nmargin. Among other choices, we could use unit vectors, as in Chapter 6, by\n2. Recall that|/vectorw|=\u221a\n/vectorwT/vectorw.\nOnline edition (c)\n2009 Cambridge UP324 15 Support vector machines and machine learning on document s\nrequiring that|/vectorw|=1. This would have the effect of making the geometric\nmargin the same as the functional margin.\nSince we can scale the functional margin as we please, for con venience in\nsolving large SVMs, let us choose to require that the functio nal margin of all\ndata points is at least 1 and that it is equal to 1 for at least on e data vector.\nThat is, for all items in the data:\nyi(/vectorwT/vectorxi+b)\u22651 (15.5)\nand there exist support vectors for which the inequality is a n equality. Since\neach example\u2019s distance from the hyperplane is ri=yi(/vectorwT/vectorxi+b)/|/vectorw|, the\ngeometric margin is \u03c1=2/|/vectorw|. Our desire is still to maximize this geometric\nmargin. That is, we want to \ufb01nd /vectorwand bsuch that:\n\u2022\u03c1=2/|/vectorw|is maximized\n\u2022For all (/vectorxi,yi)\u2208D,yi(/vectorwT/vectorxi+b)\u22651\nMaximizing 2/|/vectorw|is the same as minimizing |/vectorw|/2. This gives the \ufb01nal stan-\ndard formulation of an SVM as a minimization problem:\n(15.6) Find/vectorwand bsuch that:\n\u20221\n2/vectorwT/vectorwis minimized, and\n\u2022for all{(/vectorxi,yi)},yi(/vectorwT/vectorxi+b)\u22651\nWe are now optimizing a quadratic function subject to linear constraints.\nQuadratic optimization problems are a standard, well-known class of mathe- QUADRATIC\nPROGRAMMING matical optimization problems, and many algorithms exist f or solving them.\nWe could in principle build our SVM using standard quadratic programming\n(QP) libraries, but there has been much recent research in th is area aiming to\nexploit the structure of the kind of QP that emerges from an SV M. As a result,\nthere are more intricate but much faster and more scalable li braries available\nespecially for building SVMs, which almost everyone uses to build models.\nWe will not present the details of such algorithms here.\nHowever, it will be helpful to what follows to understand the shape of the\nsolution of such an optimization problem. The solution invo lves construct-\ning a dual problem where a Lagrange multiplier \u03b1iis associated with each\nconstraint yi(/vectorwT/vectorxi+b)\u22651 in the primal problem:\n(15.7) Find \u03b11, . . .\u03b1Nsuch that \u2211\u03b1i\u22121\n2\u2211i\u2211j\u03b1i\u03b1jyiyj/vectorxiT/vectorxjis maximized, and\n\u2022\u2211i\u03b1iyi=0\n\u2022\u03b1i\u22650 for all 1\u2264i\u2264N\nThe solution is then of the form:\nOnline edition (c)\n2009 Cambridge UP15.1 Support vector machines: The linearly separable case 325\n0 1 2 30123\n/Bullet\n/Bullet/SolidTriangle/Triangle\n\u25eeFigure 15.4 A tiny 3 data point training set for an SVM.\n(15.8) /vectorw=\u2211\u03b1iyi/vectorxi\nb=yk\u2212/vectorwT/vectorxkfor any /vectorxksuch that \u03b1k/ne}ationslash=0\nIn the solution, most of the \u03b1iare zero. Each non-zero \u03b1iindicates that the\ncorresponding /vectorxiis a support vector. The classi\ufb01cation function is then:\nf(/vectorx) =sign(\u2211i\u03b1iyi/vectorxiT/vectorx+b) (15.9)\nBoth the term to be maximized in the dual problem and the class ifying func-\ntion involve a dot product between pairs of points ( /vectorxand/vectorxior/vectorxiand/vectorxj), and\nthat is the only way the data are used \u2013 we will return to the sig ni\ufb01cance of\nthis later.\nTo recap, we start with a training data set. The data set uniqu ely de\ufb01nes\nthe best separating hyperplane, and we feed the data through a quadratic\noptimization procedure to \ufb01nd this plane. Given a new point /vectorxto classify,\nthe classi\ufb01cation function f(/vectorx)in either Equation ( 15.1) or Equation ( 15.9) is\ncomputing the projection of the point onto the hyperplane no rmal. The sign\nof this function determines the class to assign to the point. If the point is\nwithin the margin of the classi\ufb01er (or another con\ufb01dence thr eshold tthat we\nmight have determined to minimize classi\ufb01cation mistakes) then the classi-\n\ufb01er can return \u201cdon\u2019t know\u201d rather than one of the two classes . The value\noff(/vectorx)may also be transformed into a probability of classi\ufb01cation ; \ufb01tting\na sigmoid to transform the values is standard ( Platt 2000 ). Also, since the\nmargin is constant, if the model includes dimensions from va rious sources,\ncareful rescaling of some dimensions may be required. Howev er, this is not\na problem if our documents (points) are on the unit hypersphe re.\n\u270eExample 15.1: Consider building an SVM over the (very little) data set show n in\nFigure 15.4. Working geometrically, for an example like this, the maxim um margin\nweight vector will be parallel to the shortest line connecti ng points of the two classes,\nthat is, the line between (1, 1)and(2, 3), giving a weight vector of (1, 2). The opti-\nmal decision surface is orthogonal to that line and intersec ts it at the halfway point.\nOnline edition (c)\n2009 Cambridge UP326 15 Support vector machines and machine learning on document s\nTherefore, it passes through (1.5, 2). So, the SVM decision boundary is:\ny=x1+2x2\u22125.5\nWorking algebraically, with the standard constraint that s ign(yi(/vectorwT/vectorxi+b))\u22651,\nwe seek to minimize |/vectorw|. This happens when this constraint is satis\ufb01ed with equalit y\nby the two support vectors. Further we know that the solution is/vectorw= (a, 2a)for some\na. So we have that:\na+2a+b=\u22121\n2a+6a+b= 1\nTherefore, a=2/5 and b=\u221211/5. So the optimal hyperplane is given by /vectorw=\n(2/5, 4/5 )and b=\u221211/5.\nThe margin \u03c1is 2/|/vectorw|=2/\u221a\n4/25+16/25 =2/(2\u221a\n5/5) =\u221a\n5. This answer can\nbe con\ufb01rmed geometrically by examining Figure 15.4.\n?Exercise 15.1 [\u22c6]\nWhat is the minimum number of support vectors that there can b e for a data set\n(which contains instances of each class)?\nExercise 15.2 [\u22c6\u22c6]\nThe basis of being able to use kernels in SVMs (see Section 15.2.3 ) is that the classi\ufb01ca-\ntion function can be written in the form of Equation ( 15.9) (where, for large problems,\nmost \u03b1iare 0). Show explicitly how the classi\ufb01cation function coul d be written in this\nform for the data set from Example 15.1. That is, write fas a function where the data\npoints appear and the only variable is /vectorx.\nExercise 15.3 [\u22c6\u22c6]\nInstall an SVM package such as SVMlight ( http://svmlight.joachims.org/ ), and build an\nSVM for the data set discussed in Example 15.1. Con\ufb01rm that the program gives the\nsame solution as the text. For SVMlight, or another package t hat accepts the same\ntraining data format, the training \ufb01le would be:\n+11:2 2:3\n\u221211:2 2:0\n\u221211:1 2:1\nThe training command for SVMlight is then:\nsvm_learn -c 1 -aalphas.dat train.dat model.dat\nThe-c 1 option is needed to turn off use of the slack variables that we discuss in\nSection 15.2.1 . Check that the norm of the weight vector agrees with what we f ound\nin Example 15.1. Examine the \ufb01le alphas.dat which contains the \u03b1ivalues, and check\nthat they agree with your answers in Exercise 15.2.\nOnline edition (c)\n2009 Cambridge UP15.2 Extensions to the SVM model 327\n/Bullet\n/Bullet/Bullet\n/Bullet\n/Bullet/Bullet\n/Bullet/Bullet/Bullet\n/Bullet/Bullet/vectorxi\n\u03bei/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle\n/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle/SolidTriangle/Triangle\n/SolidTriangle/Triangle/SolidTriangle/Triangle\n/SolidTriangle/Triangle\n/vectorxj\u03bej\n\u25eeFigure 15.5 Large margin classi\ufb01cation with slack variables.\n15.2 Extensions to the SVM model\n15.2.1 Soft margin classi\ufb01cation\nFor the very high dimensional problems common in text classi \ufb01cation, some-\ntimes the data are linearly separable. But in the general cas e they are not, and\neven if they are, we might prefer a solution that better separ ates the bulk of\nthe data while ignoring a few weird noise documents.\nIf the training set Dis not linearly separable, the standard approach is to\nallow the fat decision margin to make a few mistakes (some poi nts \u2013 outliers\nor noisy examples \u2013 are inside or on the wrong side of the margi n). We then\npay a cost for each misclassi\ufb01ed example, which depends on ho w far it is\nfrom meeting the margin requirement given in Equation ( 15.5). To imple-\nment this, we introduce slack variables \u03bei. A non-zero value for \u03beiallows /vectorxito SLACK VARIABLES\nnot meet the margin requirement at a cost proportional to the value of \u03bei. See\nFigure 15.5.\nThe formulation of the SVM optimization problem with slack v ariables is:\n(15.10) Find/vectorw,b, and \u03bei\u22650 such that:\n\u20221\n2/vectorwT/vectorw+C\u2211i\u03beiis minimized\n\u2022and for all{(/vectorxi,yi)},yi(/vectorwT/vectorxi+b)\u22651\u2212\u03bei\nOnline edition (c)\n2009 Cambridge UP328 15 Support vector machines and machine learning on document s\nThe optimization problem is then trading off how fat it can ma ke the margin\nversus how many points have to be moved around to allow this ma rgin.\nThe margin can be less than 1 for a point /vectorxiby setting \u03bei>0, but then one\npays a penalty of C\u03beiin the minimization for having done that. The sum of\nthe\u03beigives an upper bound on the number of training errors. Soft-m argin\nSVMs minimize training error traded off against margin. The parameter C\nis a regularization term, which provides a way to control over\ufb01tting: as C REGULARIZATION\nbecomes large, it is unattractive to not respect the data at t he cost of reducing\nthe geometric margin; when it is small, it is easy to account f or some data\npoints with the use of slack variables and to have a fat margin placed so it\nmodels the bulk of the data.\nThe dual problem for soft margin classi\ufb01cation becomes:\n(15.11) Find \u03b11, . . .\u03b1Nsuch that \u2211\u03b1i\u22121\n2\u2211i\u2211j\u03b1i\u03b1jyiyj/vectorxiT/vectorxjis maximized, and\n\u2022\u2211i\u03b1iyi=0\n\u20220\u2264\u03b1i\u2264Cfor all 1\u2264i\u2264N\nNeither the slack variables \u03beinor Lagrange multipliers for them appear in the\ndual problem. All we are left with is the constant Cbounding the possible\nsize of the Lagrange multipliers for the support vector data points. As before,\nthe/vectorxiwith non-zero \u03b1iwill be the support vectors. The solution of the dual\nproblem is of the form:\n(15.12) /vectorw=\u2211\u03b1yi/vectorxi\nb=yk(1\u2212\u03bek)\u2212/vectorwT/vectorxkfork=arg maxk\u03b1k\nAgain /vectorwis not needed explicitly for classi\ufb01cation, which can be don e in terms\nof dot products with data points, as in Equation ( 15.9).\nTypically, the support vectors will be a small proportion of the training\ndata. However, if the problem is non-separable or with small margin, then\nevery data point which is misclassi\ufb01ed or within the margin w ill have a non-\nzero \u03b1i. If this set of points becomes large, then, for the nonlinear case which\nwe turn to in Section 15.2.3 , this can be a major slowdown for using SVMs at\ntest time.\nThe complexity of training and testing with linear SVMs is sh own in Ta-\nble15.1.3The time for training an SVM is dominated by the time for solvi ng\nthe underlying QP , and so the theoretical and empirical comp lexity varies de-\npending on the method used to solve it. The standard result fo r solving QPs\nis that it takes time cubic in the size of the data set ( Kozlov et al. 1979 ). All the\nrecent work on SVM training has worked to reduce that complex ity, often by\n3. We write \u0398(|D|Lave)for\u0398(T)(page 262) and assume that the length of test documents is\nbounded as we did on page 262.\nOnline edition (c)\n2009 Cambridge UP15.2 Extensions to the SVM model 329\nClassi\ufb01er Mode Method Time complexity\nNB training \u0398(|D|Lave+|C||V|)\nNB testing \u0398(|C|Ma)\nRocchio training \u0398(|D|Lave+|C||V|)\nRocchio testing \u0398(|C|Ma)\nkNN training preprocessing \u0398(|D|Lave)\nkNN testing preprocessing \u0398(|D|MaveMa)\nkNN training no preprocessing \u0398(1)\nkNN testing no preprocessing \u0398(|D|LaveMa)\nSVM training conventional O(|C||D|3Mave);\n\u2248O(|C||D|1.7Mave), empirically\nSVM training cutting planes O(|C||D|Mave)\nSVM testing O(|C|Ma)\n\u25eeTable 15.1 Training and testing complexity of various classi\ufb01ers incl uding SVMs.\nTraining is the time the learning method takes to learn a clas si\ufb01er over D, while test-\ning is the time it takes a classi\ufb01er to classify one document. For SVMs, multiclass\nclassi\ufb01cation is assumed to be done by a set of |C|one-versus-rest classi\ufb01ers. Laveis\nthe average number of tokens per document, while Maveis the average vocabulary\n(number of non-zero features) of a document. Laand Maare the numbers of tokens\nand types, respectively, in the test document.\nbeing satis\ufb01ed with approximate solutions. Standardly, em pirical complex-\nity is about O(|D|1.7)(Joachims 2006a ). Nevertheless, the super-linear train-\ning time of traditional SVM algorithms makes them dif\ufb01cult o r impossible\nto use on very large training data sets. Alternative traditi onal SVM solu-\ntion algorithms which are linear in the number of training ex amples scale\nbadly with a large number of features, which is another stand ard attribute\nof text problems. However, a new training algorithm based on cutting plane\ntechniques gives a promising answer to this issue by having r unning time\nlinear in the number of training examples and the number of no n-zero fea-\ntures in examples ( Joachims 2006a ). Nevertheless, the actual speed of doing\nquadratic optimization remains much slower than simply cou nting terms as\nis done in a Naive Bayes model. Extending SVM algorithms to no nlinear\nSVMs, as in the next section, standardly increases training complexity by a\nfactor of|D|(since dot products between examples need to be calculated) ,\nmaking them impractical. In practice it can often be cheaper to materialize\nOnline edition (c)\n2009 Cambridge UP330 15 Support vector machines and machine learning on document s\nthe higher-order features and to train a linear SVM.4\n15.2.2 Multiclass SVMs\nSVMs are inherently two-class classi\ufb01ers. The traditional way to do mul-\nticlass classi\ufb01cation with SVMs is to use one of the methods d iscussed in\nSection 14.5 (page 306). In particular, the most common technique in prac-\ntice has been to build |C|one-versus-rest classi\ufb01ers (commonly referred to as\n\u201cone-versus-all\u201d or OVA classi\ufb01cation), and to choose the c lass which classi-\n\ufb01es the test datum with greatest margin. Another strategy is to build a set\nof one-versus-one classi\ufb01ers, and to choose the class that i s selected by the\nmost classi\ufb01ers. While this involves building |C|(|C|\u22121)/2 classi\ufb01ers, the\ntime for training classi\ufb01ers may actually decrease, since t he training data set\nfor each classi\ufb01er is much smaller.\nHowever, these are not very elegant approaches to solving mu lticlass prob-\nlems. A better alternative is provided by the construction o f multiclass SVMs,\nwhere we build a two-class classi\ufb01er over a feature vector \u03a6(/vectorx,y)derived\nfrom the pair consisting of the input features and the class o f the datum. At\ntest time, the classi\ufb01er chooses the class y=arg maxy\u2032/vectorwT\u03a6(/vectorx,y\u2032). The mar-\ngin during training is the gap between this value for the corr ect class and\nfor the nearest other class, and so the quadratic program for mulation will\nrequire that\u2200i\u2200y/ne}ationslash=yi/vectorwT\u03a6(/vectorxi,yi)\u2212/vectorwT\u03a6(/vectorxi,y)\u22651\u2212\u03bei. This general\nmethod can be extended to give a multiclass formulation of va rious kinds of\nlinear classi\ufb01ers. It is also a simple instance of a generali zation of classi\ufb01ca-\ntion where the classes are not just a set of independent, cate gorical labels, but\nmay be arbitrary structured objects with relationships de\ufb01 ned between them.\nIn the SVM world, such work comes under the label of structural SVMs . We STRUCTURAL SVM S\nmention them again in Section 15.4.2 .\n15.2.3 Nonlinear SVMs\nWith what we have presented so far, data sets that are linearl y separable (per-\nhaps with a few exceptions or some noise) are well-handled. B ut what are\nwe going to do if the data set just doesn\u2019t allow classi\ufb01catio n by a linear clas-\nsi\ufb01er? Let us look at a one-dimensional case. The top data set in Figure 15.6\nis straightforwardly classi\ufb01ed by a linear classi\ufb01er but th e middle data set is\nnot. We instead need to be able to pick out an interval. One way to solve this\nproblem is to map the data on to a higher dimensional space and then to use\na linear classi\ufb01er in the higher dimensional space. For exam ple, the bottom\npart of the \ufb01gure shows that a linear separator can easily cla ssify the data\n4. Materializing the features refers to directly calculati ng higher order and interaction terms\nand then putting them into a linear model.\nOnline edition (c)\n2009 Cambridge UP15.2 Extensions to the SVM model 331\n\u25eeFigure 15.6 Projecting data that is not linearly separable into a higher dimensional\nspace can make it linearly separable.\nif we use a quadratic function to map the data into two dimensi ons (a po-\nlar coordinates projection would be another possibility). The general idea is\nto map the original feature space to some higher-dimensiona l feature space\nwhere the training set is separable. Of course, we would want to do so in\nways that preserve relevant dimensions of relatedness betw een data points,\nso that the resultant classi\ufb01er should still generalize wel l.\nSVMs, and also a number of other linear classi\ufb01ers, provide a n easy and\nef\ufb01cient way of doing this mapping to a higher dimensional sp ace, which is\nreferred to as \u201cthe kernel trick \u201d. It\u2019s not really a trick: it just exploits the math KERNEL TRICK\nthat we have seen. The SVM linear classi\ufb01er relies on a dot pro duct between\ndata point vectors. Let K(/vectorxi,/vectorxj) =/vectorxiT/vectorxj. Then the classi\ufb01er we have seen so\nOnline edition (c)\n2009 Cambridge UP332 15 Support vector machines and machine learning on document s\nfar is:\nf(/vectorx) =sign(\u2211\ni\u03b1iyiK(/vectorxi,/vectorx) +b) (15.13)\nNow suppose we decide to map every data point into a higher dim ensional\nspace via some transformation \u03a6:/vectorx/ma\u221asto\u2192\u03c6(/vectorx). Then the dot product becomes\n\u03c6(/vectorxi)T\u03c6(/vectorxj). If it turned out that this dot product (which is just a real nu m-\nber) could be computed simply and ef\ufb01ciently in terms of the o riginal data\npoints, then we wouldn\u2019t have to actually map from /vectorx/ma\u221asto\u2192\u03c6(/vectorx). Rather, we\ncould simply compute the quantity K(/vectorxi,/vectorxj) =\u03c6(/vectorxi)T\u03c6(/vectorxj), and then use the\nfunction\u2019s value in Equation ( 15.13 ). A kernel function K is such a function KERNEL FUNCTION\nthat corresponds to a dot product in some expanded feature sp ace.\n\u270eExample 15.2: The quadratic kernel in two dimensions. For 2-dimensional\nvectors /vectoru= (u1u2),/vectorv= (v1v2), consider K(/vectoru,/vectorv) = ( 1+/vectoruT/vectorv)2. We wish to\nshow that this is a kernel, i.e., that K(/vectoru,/vectorv) =\u03c6(/vectoru)T\u03c6(/vectorv)for some \u03c6. Consider \u03c6(/vectoru) =\n(1u2\n1\u221a\n2u1u2u2\n2\u221a\n2u1\u221a\n2u2). Then:\nK(/vectoru,/vectorv) = ( 1+/vectoruT/vectorv)2(15.14)\n= 1+u2\n1v2\n1+2u1v1u2v2+u2\n2v2\n2+2u1v1+2u2v2\n= ( 1u2\n1\u221a\n2u1u2u2\n2\u221a\n2u1\u221a\n2u2)T(1v2\n1\u221a\n2v1v2v2\n2\u221a\n2v1\u221a\n2v2)\n= \u03c6(/vectoru)T\u03c6(/vectorv)\nIn the language of functional analysis, what kinds of functi ons are valid\nkernel functions ? Kernel functions are sometimes more precisely referred to KERNEL\nasMercer kernels , because they must satisfy Mercer\u2019s condition: for any g(/vectorx) MERCER KERNEL\nsuch that/integraltext\ng(/vectorx)2d/vectorxis \ufb01nite, we must have that:\n/integraldisplay\nK(/vectorx,/vectorz)g(/vectorx)g(/vectorz)d/vectorxd/vectorz\u22650 . (15.15)\nA kernel function Kmust be continuous, symmetric, and have a positive def-\ninite gram matrix. Such a Kmeans that there exists a mapping to a reproduc-\ning kernel Hilbert space (a Hilbert space is a vector space cl osed under dot\nproducts) such that the dot product there gives the same valu e as the function\nK. If a kernel does not satisfy Mercer\u2019s condition, then the co rresponding QP\nmay have no solution. If you would like to better understand t hese issues,\nyou should consult the books on SVMs mentioned in Section 15.5. Other-\nwise, you can content yourself with knowing that 90% of work w ith kernels\nuses one of two straightforward families of functions of two vectors, which\nwe de\ufb01ne below, and which de\ufb01ne valid kernels.\nThe two commonly used families of kernels are polynomial ker nels and\nradial basis functions. Polynomial kernels are of the form K(/vectorx,/vectorz) = ( 1+\nOnline edition (c)\n2009 Cambridge UP15.2 Extensions to the SVM model 333\n/vectorxT/vectorz)d. The case of d=1 is a linear kernel, which is what we had before the\nstart of this section (the constant 1 just changing the thres hold). The case of\nd=2 gives a quadratic kernel, and is very commonly used. We illu strated\nthe quadratic kernel in Example 15.2.\nThe most common form of radial basis function is a Gaussian di stribution,\ncalculated as:\nK(/vectorx,/vectorz) =e\u2212(/vectorx\u2212/vectorz)2/(2\u03c32)(15.16)\nA radial basis function (rbf) is equivalent to mapping the da ta into an in\ufb01-\nnite dimensional Hilbert space, and so we cannot illustrate the radial basis\nfunction concretely, as we did a quadratic kernel. Beyond th ese two families,\nthere has been interesting work developing other kernels, s ome of which is\npromising for text applications. In particular, there has b een investigation of\nstring kernels (see Section 15.5).\nThe world of SVMs comes with its own language, which is rather different\nfrom the language otherwise used in machine learning. The te rminology\ndoes have deep roots in mathematics, but it\u2019s important not t o be too awed\nby that terminology. Really, we are talking about some quite simple things. A\npolynomial kernel allows us to model feature conjunctions ( up to the order of\nthe polynomial). That is, if we want to be able to model occurr ences of pairs\nof words, which give distinctive information about topic cl assi\ufb01cation, not\ngiven by the individual words alone, like perhaps operating AND system or\nethnic AND cleansing , then we need to use a quadratic kernel. If occurrences\nof triples of words give distinctive information, then we ne ed to use a cubic\nkernel. Simultaneously you also get the powers of the basic f eatures \u2013 for\nmost text applications, that probably isn\u2019t useful, but jus t comes along with\nthe math and hopefully doesn\u2019t do harm. A radial basis functi on allows you\nto have features that pick out circles (hyperspheres) \u2013 alth ough the decision\nboundaries become much more complex as multiple such featur es interact. A\nstring kernel lets you have features that are character subs equences of terms.\nAll of these are straightforward notions which have also bee n used in many\nother places under different names.\n15.2.4 Experimental results\nWe presented results in Section 13.6 showing that an SVM is a very effec-\ntive text classi\ufb01er. The results of Dumais et al. (1998 ) given in Table 13.9\nshow SVMs clearly performing the best. This was one of severa l pieces of\nwork from this time that established the strong reputation o f SVMs for text\nclassi\ufb01cation. Another pioneering work on scaling and eval uating SVMs\nfor text classi\ufb01cation was ( Joachims 1998 ). We present some of his results\nOnline edition (c)\n2009 Cambridge UP334 15 Support vector machines and machine learning on document s\nRoc- Dec. linear SVM rbf-SVM\nNB chio Trees kNN C=0.5 C=1.0 \u03c3\u22487\nearn 96.0 96.1 96.1 97.8 98.0 98.2 98.1\nacq 90.7 92.1 85.3 91.8 95.5 95.6 94.7\nmoney-fx 59.6 67.6 69.4 75.4 78.8 78.5 74.3\ngrain 69.8 79.5 89.1 82.6 91.9 93.1 93.4\ncrude 81.2 81.5 75.5 85.8 89.4 89.4 88.7\ntrade 52.2 77.4 59.2 77.9 79.2 79.2 76.6\ninterest 57.6 72.5 49.1 76.7 75.6 74.8 69.1\nship 80.9 83.1 80.9 79.8 87.4 86.5 85.8\nwheat 63.4 79.4 85.5 72.9 86.6 86.8 82.4\ncorn 45.2 62.2 87.7 71.4 87.5 87.8 84.6\nmicroavg. 72.3 79.9 79.4 82.6 86.7 87.5 86.4\n\u25eeTable 15.2 SVM classi\ufb01er break-even F 1from ( Joachims 2002a , p. 114). Results\nare shown for the 10 largest categories and for microaverage d performance over all\n90 categories on the Reuters-21578 data set.\nfrom ( Joachims 2002a ) in Table 15.2.5Joachims used a large number of term\nfeatures in contrast to Dumais et al. (1998 ), who used MI feature selection\n(Section 13.5.1 , page 272) to build classi\ufb01ers with a much more limited num-\nber of features. The success of the linear SVM mirrors the res ults discussed\nin Section 14.6 (page 308) on other linear approaches like Naive Bayes. It\nseems that working with simple term features can get one a lon g way. It is\nagain noticeable the extent to which different papers\u2019 resu lts for the same ma-\nchine learning methods differ. In particular, based on repl ications by other\nresearchers, the Naive Bayes results of ( Joachims 1998 ) appear too weak, and\nthe results in Table 13.9 should be taken as representative.\n15.3 Issues in the classi\ufb01cation of text documents\nThere are lots of applications of text classi\ufb01cation in the c ommercial world;\nemail spam \ufb01ltering is perhaps now the most ubiquitous. Jackson and Mou-\nlinier (2002 ) write: \u201cThere is no question concerning the commercial val ue of\nbeing able to classify documents automatically by content. There are myriad\n5. These results are in terms of the break-even F1(see Section 8.4). Many researchers disprefer\nthis measure for text classi\ufb01cation evaluation, since its c alculation may involve interpolation\nrather than an actual parameter setting of the system and it i s not clear why this value should\nbe reported rather than maximal F1or another point on the precision/recall curve motivated by\nthe task at hand. While earlier results in ( Joachims 1998 ) suggested notable gains on this task\nfrom the use of higher order polynomial or rbf kernels, this w as with hard-margin SVMs. With\nsoft-margin SVMs, a simple linear SVM with the default C=1 performs best.\nOnline edition (c)\n2009 Cambridge UP15.3 Issues in the classi\ufb01cation of text documents 335\npotential applications of such a capability for corporate I ntranets, govern-\nment departments, and Internet publishers.\u201d\nMost of our discussion of classi\ufb01cation has focused on intro ducing various\nmachine learning methods rather than discussing particula r features of text\ndocuments relevant to classi\ufb01cation. This bias is appropri ate for a textbook,\nbut is misplaced for an application developer. It is frequen tly the case that\ngreater performance gains can be achieved from exploiting d omain-speci\ufb01c\ntext features than from changing from one machine learning m ethod to an-\nother. Jackson and Moulinier (2002 ) suggest that \u201cUnderstanding the data\nis one of the keys to successful categorization, yet this is a n area in which\nmost categorization tool vendors are extremely weak. Many o f the \u2018one size\n\ufb01ts all\u2019 tools on the market have not been tested on a wide rang e of content\ntypes.\u201d In this section we wish to step back a little and consi der the applica-\ntions of text classi\ufb01cation, the space of possible solution s, and the utility of\napplication-speci\ufb01c heuristics.\n15.3.1 Choosing what kind of classi\ufb01er to use\nWhen confronted with a need to build a text classi\ufb01er, the \ufb01rs t question to\nask is how much training data is there currently available? N one? Very little?\nQuite a lot? Or a huge amount, growing every day? Often one of t he biggest\npractical challenges in \ufb01elding a machine learning classi\ufb01 er in real applica-\ntions is creating or obtaining enough training data. For man y problems and\nalgorithms, hundreds or thousands of examples from each cla ss are required\nto produce a high performance classi\ufb01er and many real world c ontexts in-\nvolve large sets of categories. We will initially assume tha t the classi\ufb01er is\nneeded as soon as possible; if a lot of time is available for im plementation,\nmuch of it might be spent on assembling data resources.\nIf you have no labeled training data, and especially if there are existing\nstaff knowledgeable about the domain of the data, then you sh ould never\nforget the solution of using hand-written rules. That is, yo u write standing\nqueries, as we touched on at the beginning of Chapter 13. For example:\nIF(wheatORgrain)AND NOT(wholeORbread)THEN c=grain\nIn practice, rules get a lot bigger than this, and can be phras ed using more\nsophisticated query languages than just Boolean expressio ns, including the\nuse of numeric scores. With careful crafting (that is, by hum ans tuning the\nrules on development data), the accuracy of such rules can be come very high.\nJacobs and Rau (1990 ) report identifying articles about takeovers with 92%\nprecision and 88.5% recall, and Hayes and Weinstein (1990 ) report 94% re-\ncall and 84% precision over 675 categories on Reuters newswi re documents.\nNevertheless the amount of work to create such well-tuned ru les is very\nlarge. A reasonable estimate is 2 days per class, and extra ti me has to go\nOnline edition (c)\n2009 Cambridge UP336 15 Support vector machines and machine learning on document s\ninto maintenance of rules, as the content of documents in cla sses drifts over\ntime (cf. page 269).\nIf you have fairly little data and you are going to train a supe rvised clas-\nsi\ufb01er, then machine learning theory says you should stick to a classi\ufb01er with\nhigh bias, as we discussed in Section 14.6 (page 308). For example, there\nare theoretical and empirical results that Naive Bayes does well in such cir-\ncumstances ( Ng and Jordan 2001 ,Forman and Cohen 2004 ), although this\neffect is not necessarily observed in practice with regular ized models over\ntextual data ( Klein and Manning 2002 ). At any rate, a very low bias model\nlike a nearest neighbor model is probably counterindicated . Regardless, the\nquality of the model will be adversely affected by the limite d training data.\nHere, the theoretically interesting answer is to try to appl ysemi-supervised SEMI -SUPERVISED\nLEARNING training methods . This includes methods such as bootstrapping or the EM\nalgorithm, which we will introduce in Section 16.5 (page 368). In these meth-\nods, the system gets some labeled documents, and a further la rge supply\nof unlabeled documents over which it can attempt to learn. On e of the big\nadvantages of Naive Bayes is that it can be straightforwardl y extended to\nbe a semi-supervised learning algorithm, but for SVMs, ther e is also semi-\nsupervised learning work which goes under the title of transductive SVMs . TRANSDUCTIVE SVM S\nSee the references for pointers.\nOften, the practical answer is to work out how to get more labe led data as\nquickly as you can. The best way to do this is to insert yoursel f into a process\nwhere humans will be willing to label data for you as part of th eir natural\ntasks. For example, in many cases humans will sort or route em ail for their\nown purposes, and these actions give information about clas ses. The alter-\nnative of getting human labelers expressly for the task of tr aining classi\ufb01ers\nis often dif\ufb01cult to organize, and the labeling is often of lo wer quality, be-\ncause the labels are not embedded in a realistic task context . Rather than\ngetting people to label all or a random sample of documents, t here has also\nbeen considerable research on active learning , where a system is built which ACTIVE LEARNING\ndecides which documents a human should label. Usually these are the ones\non which a classi\ufb01er is uncertain of the correct classi\ufb01cati on. This can be ef-\nfective in reducing annotation costs by a factor of 2\u20134, but h as the problem\nthat the good documents to label to train one type of classi\ufb01e r often are not\nthe good documents to label to train a different type of class i\ufb01er.\nIf there is a reasonable amount of labeled data, then you are i n the per-\nfect position to use everything that we have presented about text classi\ufb01-\ncation. For instance, you may wish to use an SVM. However, if y ou are\ndeploying a linear classi\ufb01er such as an SVM, you should proba bly design\nan application that overlays a Boolean rule-based classi\ufb01e r over the machine\nlearning classi\ufb01er. Users frequently like to adjust things that do not come\nout quite right, and if management gets on the phone and wants the classi-\n\ufb01cation of a particular document \ufb01xed right now, then this is much easier to\nOnline edition (c)\n2009 Cambridge UP15.3 Issues in the classi\ufb01cation of text documents 337\ndo by hand-writing a rule than by working out how to adjust the weights\nof an SVM without destroying the overall classi\ufb01cation accu racy. This is one\nreason why machine learning models like decision trees whic h produce user-\ninterpretable Boolean-like models retain considerable po pularity.\nIf a huge amount of data are available, then the choice of clas si\ufb01er probably\nhas little effect on your results and the best choice may be un clear (cf. Banko\nand Brill 2001 ). It may be best to choose a classi\ufb01er based on the scalabilit y\nof training or even runtime ef\ufb01ciency. To get to this point, y ou need to have\nhuge amounts of data. The general rule of thumb is that each do ubling of\nthe training data size produces a linear increase in classi\ufb01 er performance,\nbut with very large amounts of data, the improvement becomes sub-linear.\n15.3.2 Improving classi\ufb01er performance\nFor any particular application, there is usually signi\ufb01can t room for improv-\ning classi\ufb01er effectiveness through exploiting features s peci\ufb01c to the domain\nor document collection. Often documents will contain zones which are espe-\ncially useful for classi\ufb01cation. Often there will be partic ular subvocabularies\nwhich demand special treatment for optimal classi\ufb01cation e ffectiveness.\nLarge and dif\ufb01cult category taxonomies\nIf a text classi\ufb01cation problem consists of a small number of well-separated\ncategories, then many classi\ufb01cation algorithms are likely to work well. But\nmany real classi\ufb01cation problems consist of a very large num ber of often\nvery similar categories. The reader might think of examples like web direc-\ntories (the Yahoo! Directory or the Open Directory Project) , library classi-\n\ufb01cation schemes (Dewey Decimal or Library of Congress) or th e classi\ufb01ca-\ntion schemes used in legal or medical applications. For inst ance, the Yahoo!\nDirectory consists of over 200,000 categories in a deep hier archy. Accurate\nclassi\ufb01cation over large sets of closely related classes is inherently dif\ufb01cult.\nMost large sets of categories have a hierarchical structure , and attempting\nto exploit the hierarchy by doing hierarchical classi\ufb01cation is a promising ap- HIERARCHICAL\nCLASSIFICATION proach. However, at present the effectiveness gains from do ing this rather\nthan just working with the classes that are the leaves of the h ierarchy re-\nmain modest.6But the technique can be very useful simply to improve the\nscalability of building classi\ufb01ers over large hierarchies . Another simple way\nto improve the scalability of classi\ufb01ers over large hierarc hies is the use of\naggressive feature selection. We provide references to som e work on hierar-\nchical classi\ufb01cation in Section 15.5.\n6. Using the small hierarchy in Figure 13.1 (page 257) as an example, the leaf classes are ones\nlikepoultry and coffee , as opposed to higher-up classes like industries .\nOnline edition (c)\n2009 Cambridge UP338 15 Support vector machines and machine learning on document s\nA general result in machine learning is that you can always ge t a small\nboost in classi\ufb01cation accuracy by combining multiple clas si\ufb01ers, provided\nonly that the mistakes that they make are at least somewhat in dependent.\nThere is now a large literature on techniques such as voting, bagging, and\nboosting multiple classi\ufb01ers. Again, there are some pointe rs in the refer-\nences. Nevertheless, ultimately a hybrid automatic/manua l solution may be\nneeded to achieve suf\ufb01cient classi\ufb01cation accuracy. A comm on approach in\nsuch situations is to run a classi\ufb01er \ufb01rst, and to accept all i ts high con\ufb01dence\ndecisions, but to put low con\ufb01dence decisions in a queue for m anual review.\nSuch a process also automatically leads to the production of new training\ndata which can be used in future versions of the machine learn ing classi\ufb01er.\nHowever, note that this is a case in point where the resulting training data is\nclearly notrandomly sampled from the space of documents.\nFeatures for text\nThe default in both ad hoc retrieval and text classi\ufb01cation i s to use terms\nas features. However, for text classi\ufb01cation, a great deal o f mileage can be\nachieved by designing additional features which are suited to a speci\ufb01c prob-\nlem. Unlike the case of IR query languages, since these featu res are internal\nto the classi\ufb01er, there is no problem of communicating these features to an\nend user. This process is generally referred to as feature engineering . At pre- FEATURE ENGINEERING\nsent, feature engineering remains a human craft, rather tha n something done\nby machine learning. Good feature engineering can often mar kedly improve\nthe performance of a text classi\ufb01er. It is especially bene\ufb01c ial in some of the\nmost important applications of text classi\ufb01cation, like sp am and porn \ufb01lter-\ning.\nClassi\ufb01cation problems will often contain large numbers of terms which\ncan be conveniently grouped, and which have a similar vote in text classi-\n\ufb01cation problems. Typical examples might be year mentions o r strings of\nexclamation marks. Or they may be more specialized tokens li ke ISBNs or\nchemical formulas. Often, using them directly in a classi\ufb01e r would greatly in-\ncrease the vocabulary without providing classi\ufb01catory pow er beyond know-\ning that, say, a chemical formula is present. In such cases, t he number of\nfeatures and feature sparseness can be reduced by matching s uch items with\nregular expressions and converting them into distinguishe d tokens. Con-\nsequently, effectiveness and classi\ufb01er speed are normally enhanced. Some-\ntimes all numbers are converted into a single feature, but of ten some value\ncan be had by distinguishing different kinds of numbers, suc h as four digit\nnumbers (which are usually years) versus other cardinal num bers versus real\nnumbers with a decimal point. Similar techniques can be appl ied to dates,\nISBN numbers, sports game scores, and so on.\nGoing in the other direction, it is often useful to increase t he number of fea-\nOnline edition (c)\n2009 Cambridge UP15.3 Issues in the classi\ufb01cation of text documents 339\ntures by matching parts of words, and by matching selected mu ltiword pat-\nterns that are particularly discriminative. Parts of words are often matched\nby character k-gram features. Such features can be particularly good at pr o-\nviding classi\ufb01cation clues for otherwise unknown words whe n the classi\ufb01er\nis deployed. For instance, an unknown word ending in -rase is likely to be an\nenzyme, even if it wasn\u2019t seen in the training data. Good mult iword patterns\nare often found by looking for distinctively common word pai rs (perhaps\nusing a mutual information criterion between words, in a sim ilar way to\nits use in Section 13.5.1 (page 272) for feature selection) and then using fea-\nture selection methods evaluated against classes. They are useful when the\ncomponents of a compound would themselves be misleading as c lassi\ufb01ca-\ntion cues. For instance, this would be the case if the keyword ethnic was\nmost indicative of the categories food and arts, the keyword cleansing was\nmost indicative of the category home , but the collocation ethnic cleansing in-\nstead indicates the category world news . Some text classi\ufb01ers also make use\nof features from named entity recognizers (cf. page 195).\nDo techniques like stemming and lowercasing (Section 2.2, page 22) help\nfor text classi\ufb01cation? As always, the ultimate test is empi rical evaluations\nconducted on an appropriate test collection. But it is never theless useful to\nnote that such techniques have a more restricted chance of be ing useful for\nclassi\ufb01cation. For IR, you often need to collapse forms of a w ord like oxy-\ngenate and oxygenation , because the appearance of either in a document is a\ngood clue that the document will be relevant to a query about oxygenation .\nGiven copious training data, stemming necessarily deliver s no value for text\nclassi\ufb01cation. If several forms that stem together have a si milar signal, the\nparameters estimated for all of them will have similar weigh ts. Techniques\nlike stemming help only in compensating for data sparseness . This can be\na useful role (as noted at the start of this section), but ofte n different forms\nof a word can convey signi\ufb01cantly different cues about the co rrect document\nclassi\ufb01cation. Overly aggressive stemming can easily degr ade classi\ufb01cation\nperformance.\nDocument zones in text classi\ufb01cation\nAs already discussed in Section 6.1, documents usually have zones, such as\nmail message headers like the subject and author, or the titl e and keywords\nof a research article. Text classi\ufb01ers can usually gain from making use of\nthese zones during training and classi\ufb01cation.\nUpweighting document zones. In text classi\ufb01cation problems, you can fre-\nquently get a nice boost to effectiveness by differentially weighting contri-\nbutions from different document zones. Often, upweighting title words is\nparticularly effective ( Cohen and Singer 1999 , p. 163). As a rule of thumb,\nOnline edition (c)\n2009 Cambridge UP340 15 Support vector machines and machine learning on document s\nit is often effective to double the weight of title words in te xt classi\ufb01cation\nproblems. You can also get value from upweighting words from pieces of\ntext that are not so much clearly de\ufb01ned zones, but where neve rtheless evi-\ndence from document structure or content suggests that they are important.\nMurata et al. (2000 ) suggest that you can also get value (in an ad hoc retrieval\ncontext) from upweighting the \ufb01rst sentence of a (newswire) document.\nSeparate feature spaces for document zones. There are two strategies that\ncan be used for document zones. Above we upweighted words tha t appear\nin certain zones. This means that we are using the same featur es (that is, pa-\nrameters are \u201ctied\u201d across different zones), but we pay more attention to the PARAMETER TYING\noccurrence of terms in particular zones. An alternative str ategy is to have a\ncompletely separate set of features and corresponding para meters for words\noccurring in different zones. This is in principle more powe rful: a word\ncould usually indicate the topic Middle East when in the title but Commodities\nwhen in the body of a document. But, in practice, tying parame ters is usu-\nally more successful. Having separate feature sets means ha ving two or more\ntimes as many parameters, many of which will be much more spar sely seen\nin the training data, and hence with worse estimates, wherea s upweighting\nhas no bad effects of this sort. Moreover, it is quite uncommo n for words to\nhave different preferences when appearing in different zon es; it is mainly the\nstrength of their vote that should be adjusted. Nevertheles s, ultimately this\nis a contingent result, depending on the nature and quantity of the training\ndata.\nConnections to text summarization. In Section 8.7, we mentioned the \ufb01eld\nof text summarization, and how most work in that \ufb01eld has adop ted the\nlimited goal of extracting and assembling pieces of the orig inal text that are\njudged to be central based on features of sentences that cons ider the sen-\ntence\u2019s position and content. Much of this work can be used to suggest zones\nthat may be distinctively useful for text classi\ufb01cation. Fo r example Ko\u0142cz\net al. (2000 ) consider a form of feature selection where you classify doc u-\nments based only on words in certain zones. Based on text summ arization\nresearch, they consider using (i) only the title, (ii) only t he \ufb01rst paragraph,\n(iii) only the paragraph with the most title words or keyword s, (iv) the \ufb01rst\ntwo paragraphs or the \ufb01rst and last paragraph, or (v) all sent ences with a\nminimum number of title words or keywords. In general, these positional\nfeature selection methods produced as good results as mutua l information\n(Section 13.5.1 ), and resulted in quite competitive classi\ufb01ers. Ko et al. (2004 )\nalso took inspiration from text summarization research to u pweight sen-\ntences with either words from the title or words that are cent ral to the doc-\nument\u2019s content, leading to classi\ufb01cation accuracy gains o f almost 1%. This\nOnline edition (c)\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 341\npresumably works because most such sentences are somehow mo re central\nto the concerns of the document.\n?Exercise 15.4 [\u22c6\u22c6]\nSpam email often makes use of various cloaking techniques to try to get through. One\nmethod is to pad or substitute characters so as to defeat word -based text classi\ufb01ers.\nFor example, you see terms like the following in spam email:\nRep1icaRolex bonmus Viiiaaaagra pi11z\nPHARlbdMACY [LEV]i[IT]l[RA] se \u2227xual ClAfLlS\nDiscuss how you could engineer features that would largely d efeat this strategy.\nExercise 15.5 [\u22c6\u22c6]\nAnother strategy often used by purveyors of email spam is to f ollow the message\nthey wish to send (such as buying a cheap stock or whatever) wi th a paragraph of\ntext from another innocuous source (such as a news article). Why might this strategy\nbe effective? How might it be addressed by a text classi\ufb01er?\nExercise 15.6 [\u22c6]\nWhat other kinds of features appear as if they would be useful in an email spam\nclassi\ufb01er?\n15.4 Machine learning methods in ad hoc information retriev al\nRather than coming up with term and document weighting funct ions by\nhand, as we primarily did in Chapter 6, we can view different sources of rele-\nvance signal (cosine score, title match, etc.) as features i n a learning problem.\nA classi\ufb01er that has been fed examples of relevant and nonrel evant docu-\nments for each of a set of queries can then \ufb01gure out the relati ve weights\nof these signals. If we con\ufb01gure the problem so that there are pairs of a\ndocument and a query which are assigned a relevance judgment ofrelevant\nornonrelevant , then we can think of this problem too as a text classi\ufb01cation\nproblem. Taking such a classi\ufb01cation approach is not necess arily best, and\nwe present an alternative in Section 15.4.2 . Nevertheless, given the material\nwe have covered, the simplest place to start is to approach th is problem as\na classi\ufb01cation problem, by ordering the documents accordi ng to the con\ufb01-\ndence of a two-class classi\ufb01er in its relevance decision. An d this move is not\npurely pedagogical; exactly this approach is sometimes use d in practice.\n15.4.1 A simple example of machine-learned scoring\nIn this section we generalize the methodology of Section 6.1.2 (page 113) to\nmachine learning of the scoring function. In Section 6.1.2 we considered a\ncase where we had to combine Boolean indicators of relevance ; here we con-\nsider more general factors to further develop the notion of m achine-learned\nOnline edition (c)\n2009 Cambridge UP342 15 Support vector machines and machine learning on document s\nExample DocID Query Cosine score \u03c9 Judgment\n\u03a61 37linux operating system 0.032 3 relevant\n\u03a62 37penguin logo 0.02 4 nonrelevant\n\u03a63 238operatingsystem 0.043 2 relevant\n\u03a64 238runtimeenvironment 0.004 2 nonrelevant\n\u03a65 1741kernellayer 0.022 3 relevant\n\u03a66 2094devicedriver 0.03 2 relevant\n\u03a67 3191devicedriver 0.027 5 nonrelevant\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u25eeTable 15.3 Training examples for machine-learned scoring.\nrelevance. In particular, the factors we now consider go bey ond Boolean\nfunctions of query term presence in document zones, as in Sec tion 6.1.2 .\nWe develop the ideas in a setting where the scoring function i s a linear\ncombination of two factors: (1) the vector space cosine simi larity between\nquery and document and (2) the minimum window width \u03c9within which\nthe query terms lie. As we noted in Section 7.2.2 (page 144), query term\nproximity is often very indicative of a document being on top ic, especially\nwith longer documents and on the web. Among other things, thi s quantity\ngives us an implementation of implicit phrases. Thus we have one factor that\ndepends on the statistics of query terms in the document as a b ag of words,\nand another that depends on proximity weighting. We conside r only two\nfeatures in the development of the ideas because a two-featu re exposition\nremains simple enough to visualize. The technique can be gen eralized to\nmany more features.\nAs in Section 6.1.2 , we are provided with a set of training examples , each\nof which is a pair consisting of a query and a document, togeth er with a\nrelevance judgment for that document on that query that is ei ther relevant or\nnonrelevant . For each such example we can compute the vector space cosine\nsimilarity, as well as the window width \u03c9. The result is a training set as\nshown in Table 15.3, which resembles Figure 6.5(page 115) from Section 6.1.2 .\nHere, the two features (cosine score denoted \u03b1and window width \u03c9) are\nreal-valued predictors. If we once again quantify the judgm entrelevant as 1\nand nonrelevant as 0, we seek a scoring function that combines the values of\nthe features to generate a value that is (close to) 0 or 1. We wi sh this func-\ntion to be in agreement with our set of training examples as fa r as possible.\nWithout loss of generality, a linear classi\ufb01er will use a lin ear combination of\nfeatures of the form\nScore(d,q) =Score(\u03b1,\u03c9) =a\u03b1+b\u03c9+c, (15.17)\nwith the coef\ufb01cients a,b,cto be learned from the training data. While it is\nOnline edition (c)\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 34302 3 4 5\n0 . 0 50 . 0 2 5\ncos\ni n\nescore\n\rT e r m p r o x i m i t y \u0018\nRRR\nRR\nRR\nRRRRNNNN\nN\nNNN\nNN\u25eeFigure 15.7 A collection of training examples. Each R denotes a training example\nlabeled relevant , while each N is a training example labeled nonrelevant .\npossible to formulate this as an error minimization problem as we did in\nSection 6.1.2 , it is instructive to visualize the geometry of Equation ( 15.17 ).\nThe examples in Table 15.3 can be plotted on a two-dimensional plane with\naxes corresponding to the cosine score \u03b1and the window width \u03c9. This is\ndepicted in Figure 15.7.\nIn this setting, the function Score(\u03b1,\u03c9)from Equation ( 15.17 ) represents\na plane \u201changing above\u201d Figure 15.7. Ideally this plane (in the direction\nperpendicular to the page containing Figure 15.7) assumes values close to\n1 above the points marked R, and values close to 0 above the poi nts marked\nN. Since a plane is unlikely to assume only values close to 0 or 1 above the\ntraining sample points, we make use of thresholding : given any query and\ndocument for which we wish to determine relevance, we pick a v alue \u03b8and\nifScore(\u03b1,\u03c9)>\u03b8we declare the document to be relevant , else we declare\nthe document to be nonrelevant . As we know from Figure 14.8 (page 301),\nall points that satisfy Score(\u03b1,\u03c9) = \u03b8form a line (shown as a dashed line\nin Figure 15.7) and we thus have a linear classi\ufb01er that separates relevant\nOnline edition (c)\n2009 Cambridge UP344 15 Support vector machines and machine learning on document s\nfrom nonrelevant instances. Geometrically, we can \ufb01nd the s eparating line\nas follows. Consider the line passing through the plane Score(\u03b1,\u03c9)whose\nheight is \u03b8above the page containing Figure 15.7. Project this line down onto\nFigure 15.7; this will be the dashed line in Figure 15.7. Then, any subse-\nquent query/document pair that falls below the dashed line i n Figure 15.7 is\ndeemed nonrelevant ; above the dashed line, relevant .\nThus, the problem of making a binary relevant /nonrelevant judgment given\ntraining examples as above turns into one of learning the das hed line in Fig-\nure15.7 separating relevant training examples from the nonrelevant ones. Be-\ning in the \u03b1-\u03c9plane, this line can be written as a linear equation involvin g\n\u03b1and \u03c9, with two parameters (slope and intercept). The methods of l in-\near classi\ufb01cation that we have already looked at in Chapters 13\u201315provide\nmethods for choosing this line. Provided we can build a suf\ufb01c iently rich col-\nlection of training samples, we can thus altogether avoid ha nd-tuning score\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\nto maintain a suitably representative set of training examp les, whose rele-\nvance assessments must be made by experts.\n15.4.2 Result ranking by machine learning\nThe above ideas can be readily generalized to functions of ma ny more than\ntwo variables. There are lots of other scores that are indica tive of the rel-\nevance of a document to a query, including static quality (Pa geRank-style\nmeasures, discussed in Chapter 21), document age, zone contributions, doc-\nument length, and so on. Providing that these measures can be calculated\nfor a training document collection with relevance judgment s, any number\nof such measures can be used to train a machine learning class i\ufb01er. For in-\nstance, we could train an SVM over binary relevance judgment s, and order\ndocuments based on their probability of relevance, which is monotonic with\nthe documents\u2019 signed distance from the decision boundary.\nHowever, approaching IR result ranking like this is not nece ssarily the\nright way to think about the problem. Statisticians normall y \ufb01rst divide\nproblems into classi\ufb01cation problems (where a categorical variable is pre-\ndicted) versus regression problems (where a real number is predicted). In REGRESSION\nbetween is the specialized \ufb01eld of ordinal regression where a ranking is pre- ORDINAL REGRESSION\ndicted. Machine learning for ad hoc retrieval is most proper ly thought of as\nan ordinal regression problem, where the goal is to rank a set of documents\nfor a query, given training data of the same sort. This formul ation gives\nsome additional power, since documents can be evaluated rel ative to other\ncandidate documents for the same query, rather than having t o be mapped\nto a global scale of goodness, while also weakening the probl em space, since\njust a ranking is required rather than an absolute measure of relevance. Is-\nsues of ranking are especially germane in web search, where t he ranking at\nOnline edition (c)\n2009 Cambridge UP15.4 Machine learning methods in ad hoc information retriev al 345\nthe very top of the results list is exceedingly important, wh ereas decisions\nof relevance of a document to a query may be much less importan t. Such\nwork can and has been pursued using the structural SVM framew ork which\nwe mentioned in Section 15.2.2 , where the class being predicted is a ranking\nof results for a query, but here we will present the slightly s impler ranking\nSVM.\nThe construction of a ranking SVM proceeds as follows. We begin with a RANKING SVM\nset of judged queries. For each training query q, we have a set of documents\nreturned in response to the query, which have been totally or dered by a per-\nson for relevance to the query. We construct a vector of featu res\u03c8j=\u03c8(dj,q)\nfor each document/query pair, using features such as those d iscussed in Sec-\ntion 15.4.1 , and many more. For two documents diand dj, we then form the\nvector of feature differences:\n\u03a6(di,dj,q) =\u03c8(di,q)\u2212\u03c8(dj,q) (15.18)\nBy hypothesis, one of diand djhas been judged more relevant. If diis\njudged more relevant than dj, denoted di\u227adj(dishould precede djin the\nresults ordering), then we will assign the vector \u03a6(di,dj,q)the class yijq=\n+1; otherwise\u22121. The goal then is to build a classi\ufb01er which will return\n/vectorwT\u03a6(di,dj,q)>0 iff di\u227adj (15.19)\nThis SVM learning task is formalized in a manner much like the other exam-\nples that we saw before:\n(15.20) Find/vectorw, and \u03bei,j\u22650 such that:\n\u20221\n2/vectorwT/vectorw+C\u2211i,j\u03bei,jis minimized\n\u2022and for all{\u03a6(di,dj,q):di\u227adj},/vectorwT\u03a6(di,dj,q)\u22651\u2212\u03bei,j\nWe can leave out yijqin the statement of the constraint, since we only need\nto consider the constraint for document pairs ordered in one direction, since\n\u227ais antisymmetric. These constraints are then solved, as bef ore, to give\na linear classi\ufb01er which can rank pairs of documents. This ap proach has\nbeen used to build ranking functions which outperform stand ard hand-built\nranking functions in IR evaluations on standard data sets; s ee the references\nfor papers that present such results.\nBoth of the methods that we have just looked at use a linear wei ghting\nof document features that are indicators of relevance, as ha s most work in\nthis area. It is therefore perhaps interesting to note that m uch of traditional\nIR weighting involves nonlinear scaling of basic measurements (such as log-\nweighting of term frequency, or idf). At the present time, ma chine learning is\nvery good at producing optimal weights for features in a line ar combination\nOnline edition (c)\n2009 Cambridge UP346 15 Support vector machines and machine learning on document s\n(or other similar restricted model classes), but it is not go od at coming up\nwith good nonlinear scalings of basic measurements. This ar ea remains the\ndomain of human feature engineering.\nThe idea of learning ranking functions has been around for a n umber of\nyears, but it is only very recently that suf\ufb01cient machine le arning knowledge,\ntraining document collections, and computational power ha ve come together\nto make this method practical and exciting. It is thus too ear ly to write some-\nthing de\ufb01nitive on machine learning approaches to ranking i n information\nretrieval, but there is every reason to expect the use and imp ortance of ma-\nchine learned ranking approaches to grow over time. While sk illed humans\ncan do a very good job at de\ufb01ning ranking functions by hand, ha nd tuning\nis dif\ufb01cult, and it has to be done again for each new document c ollection and\nclass of users.\n?Exercise 15.7\nPlot the \ufb01rst 7 rows of Table 15.3 in the \u03b1-\u03c9plane to produce a \ufb01gure like that in\nFigure 15.7.\nExercise 15.8\nWrite down the equation of a line in the \u03b1-\u03c9plane separating the Rs from the Ns.\nExercise 15.9\nGive a training example (consisting of values for \u03b1,\u03c9and the relevance judgment)\nthat when added to the training set makes it impossible to sep arate the R\u2019s from the\nN\u2019s using a line in the \u03b1-\u03c9plane.\n15.5 References and further reading\nThe somewhat quirky name support vector machine originates in the neu-\nral networks literature, where learning algorithms were th ought of as ar-\nchitectures, and often referred to as \u201cmachines\u201d. The disti nctive element of\nthis model is that the decision boundary to use is completely decided (\u201csup-\nported\u201d) by a few training data points, the support vectors.\nFor a more detailed presentation of SVMs, a good, well-known article-\nlength introduction is ( Burges 1998 ).Chen et al. (2005 ) introduce the more\nrecent \u03bd-SVM, which provides an alternative parameterization for d ealing\nwith inseparable problems, whereby rather than specifying a penalty C, you\nspecify a parameter \u03bdwhich bounds the number of examples which can ap-\npear on the wrong side of the decision surface. There are now a lso several\nbooks dedicated to SVMs, large margin learning, and kernels : (Cristianini\nand Shawe-Taylor 2000 ) and ( Sch\u00f6lkopf and Smola 2001 ) are more math-\nematically oriented, while ( Shawe-Taylor and Cristianini 2004 ) aims to be\nmore practical. For the foundations by their originator, se e (Vapnik 1998 ).\nOnline edition (c)\n2009 Cambridge UP15.5 References and further reading 347\nSome recent, more general books on statistical learning, su ch as ( Hastie et al.\n2001 ) also give thorough coverage of SVMs.\nThe construction of multiclass SVMs is discussed in ( Weston and Watkins\n1999 ), (Crammer and Singer 2001 ), and ( Tsochantaridis et al. 2005 ). The last\nreference provides an introduction to the general framewor k of structural\nSVMs.\nThe kernel trick was \ufb01rst presented in ( Aizerman et al. 1964 ). For more\nabout string kernels and other kernels for structured data, see ( Lodhi et al.\n2002 ) and ( Gaertner et al. 2002 ). The Advances in Neural Information Pro-\ncessing (NIPS) conferences have become the premier venue fo r theoretical\nmachine learning work, such as on SVMs. Other venues such as S IGIR are\nmuch stronger on experimental methodology and using text-s peci\ufb01c features\nto improve classi\ufb01er effectiveness.\nA recent comparison of most current machine learning classi \ufb01ers (though\non problems rather different from typical text problems) ca n be found in\n(Caruana and Niculescu-Mizil 2006 ). (Li and Yang 2003 ), discussed in Sec-\ntion 13.6, is the most recent comparative evaluation of machine learn ing clas-\nsi\ufb01ers on text classi\ufb01cation. Older examinations of classi \ufb01ers on text prob-\nlems can be found in ( Yang 1999 ,Yang and Liu 1999 ,Dumais et al. 1998 ).\nJoachims (2002a ) presents his work on SVMs applied to text problems in de-\ntail. Zhang and Oles (2001 ) present an insightful comparison of Naive Bayes,\nregularized logistic regression and SVM classi\ufb01ers.\nJoachims (1999 ) discusses methods of making SVM learning practical over\nlarge text data sets. Joachims (2006a ) improves on this work.\nA number of approaches to hierarchical classi\ufb01cation have b een developed\nin order to deal with the common situation where the classes t o be assigned\nhave a natural hierarchical organization ( Koller and Sahami 1997 ,McCal-\nlum et al. 1998 ,Weigend et al. 1999 ,Dumais and Chen 2000 ). In a recent\nlarge study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005 )\nconclude that hierarchical classi\ufb01cation noticeably if st ill modestly outper-\nforms \ufb02at classi\ufb01cation. Classi\ufb01er effectiveness remains limited by the very\nsmall number of training documents for many classes. For a mo re general\napproach that can be applied to modeling relations between c lasses, which\nmay be arbitrary rather than simply the case of a hierarchy, s eeTsochan-\ntaridis et al. (2005 ).\nMoschitti and Basili (2004 ) investigate the use of complex nominals, proper\nnouns and word senses as features in text classi\ufb01cation.\nDietterich (2002 ) overviews ensemble methods for classi\ufb01er combination,\nwhile Schapire (2003 ) focuses particularly on boosting, which is applied to\ntext classi\ufb01cation in ( Schapire and Singer 2000 ).\nChapelle et al. (2006 ) present an introduction to work in semi-supervised\nmethods, including in particular chapters on using EM for se mi-supervised\ntext classi\ufb01cation ( Nigam et al. 2006 ) and on transductive SVMs ( Joachims\nOnline edition (c)\n2009 Cambridge UP348 15 Support vector machines and machine learning on document s\n2006b ).Sindhwani and Keerthi (2006 ) present a more ef\ufb01cient implementa-\ntion of a transductive SVM for large data sets.\nTong and Koller (2001 ) explore active learning with SVMs for text classi-\n\ufb01cation; Baldridge and Osborne (2004 ) point out that examples selected for\nannotation with one classi\ufb01er in an active learning context may be no better\nthan random examples when used with another classi\ufb01er.\nMachine learning approaches to ranking for ad hoc retrieval were pio-\nneered in ( Wong et al. 1988 ), (Fuhr 1992 ), and ( Gey 1994 ). But limited training\ndata and poor machine learning techniques meant that these p ieces of work\nachieved only middling results, and hence they only had limi ted impact at\nthe time.\nTaylor et al. (2006 ) study using machine learning to tune the parameters\nof the BM25 family of ranking functions (Section 11.4.3 , page 232) so as to\nmaximize NDCG (Section 8.4, page 163). Machine learning approaches to\nordinal regression appear in ( Herbrich et al. 2000 ) and ( Burges et al. 2005 ),\nand are applied to clickstream data in ( Joachims 2002b ).Cao et al. (2006 )\nstudy how to make this approach effective in IR, and Qin et al. (2007 ) suggest\nan extension involving using multiple hyperplanes. Yue et al. (2007 ) study\nhow to do ranking with a structural SVM approach, and in parti cular show\nhow this construction can be effectively used to directly op timize for MAP\n(Section 8.4, page 158), rather than using surrogate measures like accuracy or\narea under the ROC curve. Geng et al. (2007 ) study feature selection for the\nranking problem.\nOther approaches to learning to rank have also been shown to b e effective\nfor web search, such as ( Burges et al. 2005 ,Richardson et al. 2006 ).\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 349\n16 Flat clustering\nClustering algorithms group a set of documents into subsets orclusters . The CLUSTER\nalgorithms\u2019 goal is to create clusters that are coherent int ernally, but clearly\ndifferent from each other. In other words, documents within a cluster should\nbe as similar as possible; and documents in one cluster shoul d be as dissimi-\nlar as possible from documents in other clusters.\n0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0 2.5\n\u25eeFigure 16.1 An example of a data set with a clear cluster structure.\nClustering is the most common form of unsupervised learning . No super- UNSUPERVISED\nLEARNING vision means that there is no human expert who has assigned do cuments\nto classes. In clustering, it is the distribution and makeup of the data that\nwill determine cluster membership. A simple example is Figu re16.1. It is\nvisually clear that there are three distinct clusters of poi nts. This chapter and\nChapter 17introduce algorithms that \ufb01nd such clusters in an unsupervi sed\nfashion.\nThe difference between clustering and classi\ufb01cation may no t seem great\nat \ufb01rst. After all, in both cases we have a partition of a set of documents\ninto groups. But as we will see the two problems are fundament ally differ-\nent. Classi\ufb01cation is a form of supervised learning (Chapte r13, page 256):\nour goal is to replicate a categorical distinction that a hum an supervisor im-\nOnline edition (c)\n2009 Cambridge UP350 16 Flat clustering\nposes on the data. In unsupervised learning, of which cluste ring is the most\nimportant example, we have no such teacher to guide us.\nThe key input to a clustering algorithm is the distance measu re. In Fig-\nure16.1, the distance measure is distance in the 2D plane. This measu re sug-\ngests three different clusters in the \ufb01gure. In document clu stering, the dis-\ntance measure is often also Euclidean distance. Different d istance measures\ngive rise to different clusterings. Thus, the distance meas ure is an important\nmeans by which we can in\ufb02uence the outcome of clustering.\nFlat clustering creates a \ufb02at set of clusters without any explicit structure that FLAT CLUSTERING\nwould relate clusters to each other. Hierarchical clustering creates a hierarchy\nof clusters and will be covered in Chapter 17. Chapter 17also addresses the\ndif\ufb01cult problem of labeling clusters automatically.\nA second important distinction can be made between hard and s oft cluster-\ning algorithms. Hard clustering computes a hard assignment \u2013 each document HARD CLUSTERING\nis a member of exactly one cluster. The assignment of soft clustering algo- SOFT CLUSTERING\nrithms issoft\u2013 a document\u2019s assignment is a distribution over all cluster s.\nIn a soft assignment, a document has fractional membership i n several clus-\nters. Latent semantic indexing, a form of dimensionality re duction, is a soft\nclustering algorithm (Chapter 18, page 417).\nThis chapter motivates the use of clustering in information retrieval by\nintroducing a number of applications (Section 16.1), de\ufb01nes the problem\nwe are trying to solve in clustering (Section 16.2) and discusses measures\nfor evaluating cluster quality (Section 16.3). It then describes two \ufb02at clus-\ntering algorithms, K-means (Section 16.4), a hard clustering algorithm, and\nthe Expectation-Maximization (or EM) algorithm (Section 16.5), a soft clus-\ntering algorithm. K-means is perhaps the most widely used \ufb02at clustering\nalgorithm due to its simplicity and ef\ufb01ciency. The EM algori thm is a gen-\neralization of K-means and can be applied to a large variety of document\nrepresentations and distributions.\n16.1 Clustering in information retrieval\nThe cluster hypothesis states the fundamental assumption we make when us- CLUSTER HYPOTHESIS\ning clustering in information retrieval.\nCluster hypothesis. Documents in the same cluster behave similarly\nwith respect to relevance to information needs.\nThe hypothesis states that if there is a document from a clust er that is rele-\nvant to a search request, then it is likely that other documen ts from the same\ncluster are also relevant. This is because clustering puts t ogether documents\nthat share many terms. The cluster hypothesis essentially i s the contiguity\nOnline edition (c)\n2009 Cambridge UP16.1 Clustering in information retrieval 351\nApplication What is Bene\ufb01t Example\nclustered?\nSearch result clustering search\nresultsmore effective information\npresentation to userFigure 16.2\nScatter-Gather (subsets of)\ncollectionalternative user interface:\n\u201csearch without typing\u201dFigure 16.3\nCollection clustering collection effective information pre-\nsentation for exploratory\nbrowsingMcKeown et al. (2002 ),\nhttp://news.google.com\nLanguage modeling collection increased precision and/or\nrecallLiu and Croft (2004 )\nCluster-based retrieval collection higher ef\ufb01ciency: faster\nsearchSalton (1971a )\n\u25eeTable 16.1 Some applications of clustering in information retrieval.\nhypothesis in Chapter 14(page 289). In both cases, we posit that similar\ndocuments behave similarly with respect to relevance.\nTable 16.1 shows some of the main applications of clustering in informa -\ntion retrieval. They differ in the set of documents that they cluster \u2013 search\nresults, collection or subsets of the collection \u2013 and the as pect of an informa-\ntion retrieval system they try to improve \u2013 user experience, user interface,\neffectiveness or ef\ufb01ciency of the search system. But they ar e all based on the\nbasic assumption stated by the cluster hypothesis.\nThe \ufb01rst application mentioned in Table 16.1 issearch result clustering where SEARCH RESULT\nCLUSTERING by search results we mean the documents that were returned in response to\na query. The default presentation of search results in infor mation retrieval is\na simple list. Users scan the list from top to bottom until the y have found\nthe information they are looking for. Instead, search resul t clustering clus-\nters the search results, so that similar documents appear to gether. It is often\neasier to scan a few coherent groups than many individual doc uments. This\nis particularly useful if a search term has different word se nses. The example\nin Figure 16.2 isjaguar . Three frequent senses on the web refer to the car, the\nanimal and an Apple operating system. The Clustered Results panel returned\nby the Viv\u00edsimo search engine ( http://vivisimo.com ) can be a more effective user\ninterface for understanding what is in the search results th an a simple list of\ndocuments.\nA better user interface is also the goal of Scatter-Gather , the second ap- SCATTER -GATHER\nplication in Table 16.1. Scatter-Gather clusters the whole collection to get\ngroups of documents that the user can select or gather . The selected groups\nare merged and the resulting set is again clustered. This pro cess is repeated\nuntil a cluster of interest is found. An example is shown in Fi gure 16.3.\nOnline edition (c)\n2009 Cambridge UP352 16 Flat clustering\n\u25eeFigure 16.2 Clustering of search results to improve recall. None of the t op hits\ncover the animal sense of jaguar , but users can easily access it by clicking on the cat\ncluster in the Clustered Results panel on the left (third arrow from the top).\nAutomatically generated clusters like those in Figure 16.3 are not as neatly\norganized as a manually constructed hierarchical tree like the Open Direc-\ntory athttp://dmoz.org . Also, \ufb01nding descriptive labels for clusters automati-\ncally is a dif\ufb01cult problem (Section 17.7, page 396). But cluster-based navi-\ngation is an interesting alternative to keyword searching, the standard infor-\nmation retrieval paradigm. This is especially true in scena rios where users\nprefer browsing over searching because they are unsure abou t which search\nterms to use.\nAs an alternative to the user-mediated iterative clusterin g in Scatter-Gather,\nwe can also compute a static hierarchical clustering of a col lection that is\nnot in\ufb02uenced by user interactions (\u201cCollection clusterin g\u201d in Table 16.1).\nGoogle News and its precursor, the Columbia NewsBlaster sys tem, are ex-\namples of this approach. In the case of news, we need to freque ntly recom-\npute the clustering to make sure that users can access the lat est breaking\nstories. Clustering is well suited for access to a collectio n of news stories\nsince news reading is not really search, but rather a process of selecting a\nsubset of stories about recent events.\nOnline edition (c)\n2009 Cambridge UP16.1 Clustering in information retrieval 353\n\u25eeFigure 16.3 An example of a user session in Scatter-Gather. A collection of New\nYork Times news stories is clustered (\u201cscattered\u201d) into eig ht clusters (top row). The\nuser manually gathers three of these into a smaller collection International Stories and\nperforms another scattering operation. This process repea ts until a small cluster with\nrelevant documents is found (e.g., Trinidad ).\nThe fourth application of clustering exploits the cluster h ypothesis directly\nfor improving search results, based on a clustering of the en tire collection.\nWe use a standard inverted index to identify an initial set of documents that\nmatch the query, but we then add other documents from the same clusters\neven if they have low similarity to the query. For example, if the query is car\nand several car documents are taken from a cluster of automob ile documents,\nthen we can add documents from this cluster that use terms oth er thancar\n(automobile ,vehicle etc). This can increase recall since a group of documents\nwith high mutual similarity is often relevant as a whole.\nMore recently this idea has been used for language modeling. Equation ( 12.10 ),\npage 245, showed that to avoid sparse data problems in the language mo d-\neling approach to IR, the model of document dcan be interpolated with a\nOnline edition (c)\n2009 Cambridge UP354 16 Flat clustering\ncollection model. But the collection contains many documen ts with terms\nuntypical of d. By replacing the collection model with a model derived from\nd\u2019s cluster, we get more accurate estimates of the occurrence probabilities of\nterms in d.\nClustering can also speed up search. As we saw in Section 6.3.2 (page 123)\nsearch in the vector space model amounts to \ufb01nding the neares t neighbors\nto the query. The inverted index supports fast nearest-neig hbor search for\nthe standard IR setting. However, sometimes we may not be abl e to use an\ninverted index ef\ufb01ciently, e.g., in latent semantic indexi ng (Chapter 18). In\nsuch cases, we could compute the similarity of the query to ev ery document,\nbut this is slow. The cluster hypothesis offers an alternati ve: Find the clus-\nters that are closest to the query and only consider document s from these\nclusters. Within this much smaller set, we can compute simil arities exhaus-\ntively and rank documents in the usual way. Since there are ma ny fewer\nclusters than documents, \ufb01nding the closest cluster is fast ; and since the doc-\numents matching a query are all similar to each other, they te nd to be in\nthe same clusters. While this algorithm is inexact, the expe cted decrease in\nsearch quality is small. This is essentially the applicatio n of clustering that\nwas covered in Section 7.1.6 (page 141).\n?Exercise 16.1\nDe\ufb01ne two documents as similar if they have at least two prope r names like Clinton\norSarkozy in common. Give an example of an information need and two docu ments,\nfor which the cluster hypothesis does nothold for this notion of similarity.\nExercise 16.2\nMake up a simple one-dimensional example (i.e. points on a li ne) with two clusters\nwhere the inexactness of cluster-based retrieval shows up. In your example, retriev-\ning clusters close to the query should do worse than direct ne arest neighbor search.\n16.2 Problem statement\nWe can de\ufb01ne the goal in hard \ufb02at clustering as follows. Given (i) a set of\ndocuments D={d1, . . . , dN}, (ii) a desired number of clusters K, and (iii)\nanobjective function that evaluates the quality of a clustering, we want to OBJECTIVE FUNCTION\ncompute an assignment \u03b3:D\u2192 { 1, . . . , K}that minimizes (or, in other\ncases, maximizes) the objective function. In most cases, we also demand that\n\u03b3is surjective, i.e., that none of the Kclusters is empty.\nThe objective function is often de\ufb01ned in terms of similarit y or distance\nbetween documents. Below, we will see that the objective in K-means clus-\ntering is to minimize the average distance between document s and their cen-\ntroids or, equivalently, to maximize the similarity betwee n documents and\ntheir centroids. The discussion of similarity measures and distance metrics\nOnline edition (c)\n2009 Cambridge UP16.2 Problem statement 355\nin Chapter 14(page 291) also applies to this chapter. As in Chapter 14, we use\nboth similarity and distance to talk about relatedness betw een documents.\nFor documents, the type of similarity we want is usually topi c similarity\nor high values on the same dimensions in the vector space mode l. For exam-\nple, documents about China have high values on dimensions li keChinese ,\nBeijing , andMao whereas documents about the UK tend to have high values\nforLondon ,Britain andQueen . We approximate topic similarity with cosine\nsimilarity or Euclidean distance in vector space (Chapter 6). If we intend to\ncapture similarity of a type other than topic, for example, s imilarity of lan-\nguage, then a different representation may be appropriate. When computing\ntopic similarity, stop words can be safely ignored, but they are important\ncues for separating clusters of English (in which theoccurs frequently and la\ninfrequently) and French documents (in which theoccurs infrequently and la\nfrequently).\nA note on terminology. An alternative de\ufb01nition of hard clustering is that\na document can be a full member of more than one cluster. Partitional clus- PARTITIONAL\nCLUSTERING tering always refers to a clustering where each document belongs to exactly\none cluster. (But in a partitional hierarchical clustering (Chapter 17) all mem-\nbers of a cluster are of course also members of its parent.) On the de\ufb01nition\nof hard clustering that permits multiple membership, the di fference between\nsoft clustering and hard clustering is that membership valu es in hard clus-\ntering are either 0 or 1, whereas they can take on any non-nega tive value in\nsoft clustering.\nSome researchers distinguish between exhaustive clusterings that assign EXHAUSTIVE\neach document to a cluster and non-exhaustive clusterings, in which some\ndocuments will be assigned to no cluster. Non-exhaustive cl usterings in\nwhich each document is a member of either no cluster or one clu ster are\ncalled exclusive . We de\ufb01ne clustering to be exhaustive in this book. EXCLUSIVE\n16.2.1 Cardinality \u2013 the number of clusters\nA dif\ufb01cult issue in clustering is determining the number of c lusters or cardi- CARDINALITY\nnality of a clustering, which we denote by K. Often Kis nothing more than\na good guess based on experience or domain knowledge. But for K-means,\nwe will also introduce a heuristic method for choosing Kand an attempt to\nincorporate the selection of Kinto the objective function. Sometimes the ap-\nplication puts constraints on the range of K. For example, the Scatter-Gather\ninterface in Figure 16.3 could not display more than about K=10 clusters\nper layer because of the size and resolution of computer moni tors in the early\n1990s.\nSince our goal is to optimize an objective function, cluster ing is essentially\nOnline edition (c)\n2009 Cambridge UP356 16 Flat clustering\na search problem. The brute force solution would be to enumer ate all pos-\nsible clusterings and pick the best. However, there are expo nentially many\npartitions, so this approach is not feasible.1For this reason, most \ufb02at clus-\ntering algorithms re\ufb01ne an initial partitioning iterative ly. If the search starts\nat an unfavorable initial point, we may miss the global optim um. Finding a\ngood starting point is therefore another important problem we have to solve\nin \ufb02at clustering.\n16.3 Evaluation of clustering\nTypical objective functions in clustering formalize the go al of attaining high\nintra-cluster similarity (documents within a cluster are s imilar) and low inter-\ncluster similarity (documents from different clusters are dissimilar). This is\naninternal criterion for the quality of a clustering. But good scores on an INTERNAL CRITERION\nOF QUALITY internal criterion do not necessarily translate into good e ffectiveness in an\napplication. An alternative to internal criteria is direct evaluation in the ap-\nplication of interest. For search result clustering, we may want to measure\nthe time it takes users to \ufb01nd an answer with different cluste ring algorithms.\nThis is the most direct evaluation, but it is expensive, espe cially if large user\nstudies are necessary.\nAs a surrogate for user judgments, we can use a set of classes i n an evalua-\ntion benchmark or gold standard (see Section 8.5, page 164, and Section 13.6,\npage 279). The gold standard is ideally produced by human judges with a\ngood level of inter-judge agreement (see Chapter 8, page 152). We can then\ncompute an external criterion that evaluates how well the clustering matches EXTERNAL CRITERION\nOF QUALITY the gold standard classes. For example, we may want to say tha t the opti-\nmal clustering of the search results for jaguar in Figure 16.2 consists of three\nclasses corresponding to the three senses car,animal , and operating system .\nIn this type of evaluation, we only use the partition provide d by the gold\nstandard, not the class labels.\nThis section introduces four external criteria of clusteri ng quality. Purity is\na simple and transparent evaluation measure. Normalized mutual information\ncan be information-theoretically interpreted. The Rand index penalizes both\nfalse positive and false negative decisions during cluster ing. The F measure\nin addition supports differential weighting of these two ty pes of errors.\nTo compute purity , each cluster is assigned to the class which is most fre- PURITY\nquent in the cluster, and then the accuracy of this assignmen t is measured\nby counting the number of correctly assigned documents and d ividing by N.\n1. An upper bound on the number of clusterings is KN/K!. The exact number of different\npartitions of Ndocuments into Kclusters is the Stirling number of the second kind. See\nhttp://mathworld.wolfram.com/StirlingNumberoftheSec ondKind.html orComtet (1974 ).\nOnline edition (c)\n2009 Cambridge UP16.3 Evaluation of clustering 357\nx\no\nx xxx\nox\noo\u22c4o x\n\u22c4\u22c4\u22c4\nxcluster 1 cluster 2 cluster 3\n\u25eeFigure 16.4 Purity as an external evaluation criterion for cluster qual ity. Majority\nclass and number of members of the majority class for the thre e clusters are: x, 5\n(cluster 1); o, 4 (cluster 2); and \u22c4, 3 (cluster 3). Purity is (1/17)\u00d7(5+4+3)\u22480.71.\npurity NMI RI F5\nlower bound 0.0 0.0 0.0 0.0\nmaximum 1 1 1 1\nvalue for Figure 16.4 0.71 0.36 0.68 0.46\n\u25eeTable 16.2 The four external evaluation measures applied to the cluste ring in\nFigure 16.4.\nFormally:\npurity (\u2126,C) =1\nN\u2211\nkmax\nj|\u03c9k\u2229cj| (16.1)\nwhere \u2126={\u03c91,\u03c92, . . . , \u03c9K}is the set of clusters and C={c1,c2, . . . , cJ}is\nthe set of classes. We interpret \u03c9kas the set of documents in \u03c9kand cjas the\nset of documents in cjin Equation ( 16.1).\nWe present an example of how to compute purity in Figure 16.4.2Bad\nclusterings have purity values close to 0, a perfect cluster ing has a purity of\n1. Purity is compared with the other three measures discusse d in this chapter\nin Table 16.2.\nHigh purity is easy to achieve when the number of clusters is l arge \u2013 in\nparticular, purity is 1 if each document gets its own cluster . Thus, we cannot\nuse purity to trade off the quality of the clustering against the number of\nclusters.\nA measure that allows us to make this tradeoff is normalized mutual infor- NORMALIZED MUTUAL\nINFORMATION\n2. Recall our note of caution from Figure 14.2 (page 291) when looking at this and other 2D\n\ufb01gures in this and the following chapter: these illustratio ns can be misleading because 2D pro-\njections of length-normalized vectors distort similariti es and distances between points.\nOnline edition (c)\n2009 Cambridge UP358 16 Flat clustering\nmation orNMI :\nNMI(\u2126,C) =I(\u2126;C)\n[H(\u2126) +H(C)]/2(16.2)\nIis mutual information (cf. Chapter 13, page 272):\nI(\u2126;C) = \u2211\nk\u2211\njP(\u03c9k\u2229cj)logP(\u03c9k\u2229cj)\nP(\u03c9k)P(cj)(16.3)\n=\u2211\nk\u2211\nj|\u03c9k\u2229cj|\nNlogN|\u03c9k\u2229cj|\n|\u03c9k||cj|(16.4)\nwhere P(\u03c9k),P(cj), and P(\u03c9k\u2229cj)are the probabilities of a document being\nin cluster \u03c9k, class cj, and in the intersection of \u03c9kand cj, respectively. Equa-\ntion ( 16.4) is equivalent to Equation ( 16.3) for maximum likelihood estimates\nof the probabilities (i.e., the estimate of each probabilit y is the corresponding\nrelative frequency).\nHis entropy as de\ufb01ned in Chapter 5(page 99):\nH(\u2126) =\u2212\u2211\nkP(\u03c9k)logP(\u03c9k) (16.5)\n=\u2212\u2211\nk|\u03c9k|\nNlog|\u03c9k|\nN(16.6)\nwhere, again, the second equation is based on maximum likeli hood estimates\nof the probabilities.\nI(\u2126;C)in Equation ( 16.3) measures the amount of information by which\nour knowledge about the classes increases when we are told wh at the clusters\nare. The minimum of I(\u2126;C)is 0 if the clustering is random with respect to\nclass membership. In that case, knowing that a document is in a particular\ncluster does not give us any new information about what its cl ass might be.\nMaximum mutual information is reached for a clustering \u2126exactthat perfectly\nrecreates the classes \u2013 but also if clusters in \u2126exactare further subdivided into\nsmaller clusters (Exercise 16.7). In particular, a clustering with K=None-\ndocument clusters has maximum MI. So MI has the same problem a s purity:\nit does not penalize large cardinalities and thus does not fo rmalize our bias\nthat, other things being equal, fewer clusters are better.\nThe normalization by the denominator [H(\u2126)+H(C)]/2 in Equation ( 16.2)\n\ufb01xes this problem since entropy tends to increase with the nu mber of clus-\nters. For example, H(\u2126)reaches its maximum log NforK=N, which en-\nsures that NMI is low for K=N. Because NMI is normalized, we can use\nit to compare clusterings with different numbers of cluster s. The particular\nform of the denominator is chosen because [H(\u2126) +H(C)]/2 is a tight upper\nbound on I(\u2126;C)(Exercise 16.8). Thus, NMI is always a number between 0\nand 1.\nOnline edition (c)\n2009 Cambridge UP16.3 Evaluation of clustering 359\nAn alternative to this information-theoretic interpretat ion of clustering is\nto view it as a series of decisions, one for each of the N(N\u22121)/2 pairs of\ndocuments in the collection. We want to assign two documents to the same\ncluster if and only if they are similar. A true positive (TP) d ecision assigns\ntwo similar documents to the same cluster, a true negative (T N) decision as-\nsigns two dissimilar documents to different clusters. Ther e are two types\nof errors we can commit. A false positive (FP) decision assig ns two dissim-\nilar documents to the same cluster. A false negative (FN) dec ision assigns\ntwo similar documents to different clusters. The Rand index (RI) measures RAND INDEX\nRI the percentage of decisions that are correct. That is, it is s imply accuracy\n(Section 8.3, page 155).\nRI=TP+TN\nTP+FP+FN+TN\nAs an example, we compute RI for Figure 16.4. We \ufb01rst compute TP +FP.\nThe three clusters contain 6, 6, and 5 points, respectively, so the total number\nof \u201cpositives\u201d or pairs of documents that are in the same clus ter is:\nTP+FP=/parenleftbigg\n6\n2/parenrightbigg\n+/parenleftbigg\n6\n2/parenrightbigg\n+/parenleftbigg\n5\n2/parenrightbigg\n=40\nOf these, the x pairs in cluster 1, the o pairs in cluster 2, the \u22c4pairs in cluster 3,\nand the x pair in cluster 3 are true positives:\nTP=/parenleftbigg\n5\n2/parenrightbigg\n+/parenleftbigg\n4\n2/parenrightbigg\n+/parenleftbigg\n3\n2/parenrightbigg\n+/parenleftbigg\n2\n2/parenrightbigg\n=20\nThus, FP =40\u221220=20.\nFN and TN are computed similarly, resulting in the following contingency\ntable:\nSame cluster Different clusters\nSame class TP=20 FN =24\nDifferent classes FP=20 TN =72\nRI is then (20+72)/(20+20+24+72)\u22480.68.\nThe Rand index gives equal weight to false positives and fals e negatives.\nSeparating similar documents is sometimes worse than putti ng pairs of dis-\nsimilar documents in the same cluster. We can use the F measure (Section 8.3, FMEASURE\npage 154) to penalize false negatives more strongly than false posit ives by\nselecting a value \u03b2>1, thus giving more weight to recall.\nP=TP\nTP+FPR=TP\nTP+FNF\u03b2=(\u03b22+1)PR\n\u03b22P+R\nOnline edition (c)\n2009 Cambridge UP360 16 Flat clustering\nBased on the numbers in the contingency table, P=20/40 =0.5 and R=\n20/44\u22480.455. This gives us F1\u22480.48 for \u03b2=1 and F5\u22480.456 for \u03b2=5.\nIn information retrieval, evaluating clustering with Fhas the advantage that\nthe measure is already familiar to the research community.\n?Exercise 16.3\nReplace every point din Figure 16.4 with two identical copies of din the same class.\n(i) Is it less dif\ufb01cult, equally dif\ufb01cult or more dif\ufb01cult to cluster this set of 34 points\nas opposed to the 17 points in Figure 16.4? (ii) Compute purity, NMI, RI, and F5for\nthe clustering with 34 points. Which measures increase and w hich stay the same after\ndoubling the number of points? (iii) Given your assessment i n (i) and the results in\n(ii), which measures are best suited to compare the quality o f the two clusterings?\n16.4 K-means\nK-means is the most important \ufb02at clustering algorithm. Its o bjective is to\nminimize the average squared Euclidean distance (Chapter 6, page 131) of\ndocuments from their cluster centers where a cluster center is de\ufb01ned as the\nmean or centroid /vector\u00b5of the documents in a cluster \u03c9: CENTROID\n/vector\u00b5(\u03c9) =1\n|\u03c9|\u2211\n/vectorx\u2208\u03c9/vectorx\nThe de\ufb01nition assumes that documents are represented as len gth-normalized\nvectors in a real-valued space in the familiar way. We used ce ntroids for Roc-\nchio classi\ufb01cation in Chapter 14(page 292). They play a similar role here.\nThe ideal cluster in K-means is a sphere with the centroid as its center of\ngravity. Ideally, the clusters should not overlap. Our desi derata for classes\nin Rocchio classi\ufb01cation were the same. The difference is th at we have no la-\nbeled training set in clustering for which we know which docu ments should\nbe in the same cluster.\nA measure of how well the centroids represent the members of t heir clus-\nters is the residual sum of squares orRSS, the squared distance of each vector RESIDUAL SUM OF\nSQUARES from its centroid summed over all vectors:\nRSS k=\u2211\n/vectorx\u2208\u03c9k|/vectorx\u2212/vector\u00b5(\u03c9k)|2\nRSS=K\n\u2211\nk=1RSS k (16.7)\nRSS is the objective function in K-means and our goal is to minimize it. Since\nNis \ufb01xed, minimizing RSS is equivalent to minimizing the aver age squared\ndistance, a measure of how well centroids represent their do cuments.\nOnline edition (c)\n2009 Cambridge UP16.4 K-means 361\nK-MEANS ({/vectorx1, . . . ,/vectorxN},K)\n1(/vectors1,/vectors2, . . . ,/vectorsK)\u2190SELECT RANDOM SEEDS({/vectorx1, . . . ,/vectorxN},K)\n2fork\u21901toK\n3do/vector\u00b5k\u2190/vectorsk\n4while stopping criterion has not been met\n5do for k\u21901toK\n6 do\u03c9k\u2190{}\n7 forn\u21901toN\n8 doj\u2190arg minj\u2032|/vector\u00b5j\u2032\u2212/vectorxn|\n9 \u03c9j\u2190\u03c9j\u222a{/vectorxn}(reassignment of vectors)\n10 fork\u21901toK\n11 do/vector\u00b5k\u21901\n|\u03c9k|\u2211/vectorx\u2208\u03c9k/vectorx (recomputation of centroids)\n12 return{/vector\u00b51, . . . ,/vector\u00b5K}\n\u25eeFigure 16.5 The K-means algorithm. For most IR applications, the vectors\n/vectorxn\u2208RMshould be length-normalized. Alternative methods of seed s election and\ninitialization are discussed on page 364.\nThe \ufb01rst step of K-means is to select as initial cluster centers Krandomly\nselected documents, the seeds . The algorithm then moves the cluster centers SEED\naround in space in order to minimize RSS. As shown in Figure 16.5, this is\ndone iteratively by repeating two steps until a stopping cri terion is met: reas-\nsigning documents to the cluster with the closest centroid; and recomputing\neach centroid based on the current members of its cluster. Fi gure 16.6 shows\nsnapshots from nine iterations of the K-means algorithm for a set of points.\nThe \u201ccentroid\u201d column of Table 17.2 (page 397) shows examples of centroids.\nWe can apply one of the following termination conditions.\n\u2022A \ufb01xed number of iterations Ihas been completed. This condition limits\nthe runtime of the clustering algorithm, but in some cases th e quality of\nthe clustering will be poor because of an insuf\ufb01cient number of iterations.\n\u2022Assignment of documents to clusters (the partitioning func tion \u03b3) does\nnot change between iterations. Except for cases with a bad lo cal mini-\nmum, this produces a good clustering, but runtimes may be una cceptably\nlong.\n\u2022Centroids /vector\u00b5kdo not change between iterations. This is equivalent to \u03b3not\nchanging (Exercise 16.5).\n\u2022Terminate when RSS falls below a threshold. This criterion e nsures that\nthe clustering is of a desired quality after termination. In practice, we\nOnline edition (c)\n2009 Cambridge UP362 16 Flat clustering\n0 1 2 3 4 5 601234\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet\n/Bullet/Bullet /Bullet\n/Bullet/Bullet/Bullet\n/Bullet/Bullet/Bullet/Bullet/Bullet\n/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet /Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet\u00d7\u00d7\nselection of seeds0 1 2 3 4 5 601234\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet\n/Bullet/Bullet /Bullet\n/Bullet/Bullet/Bullet\n/Bullet/Bullet/Bullet/Bullet/Bullet\n/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet /Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet/Bullet\n/Bullet\u00d7\u00d7\nassignment of documents (iter. 1)\n0 1 2 3 4 5 601234\n++\n++\n++\n++\n+++o\no\n+o +\n+++\n++ + +o\n+\n+o\n++++ oo\n+o\n++o+\no\n\u00d7\u00d7\u00d7\u00d7\nrecomputation/movement of /vector\u00b5\u2019s (iter. 1)0 1 2 3 4 5 601234\n++\n++\n++\n++\n++++\n+\n+o +\n+++\n+o + oo\no\no+\no+o+ o+\noo\no+o+\no\u00d7\u00d7\n/vector\u00b5\u2019s after convergence (iter. 9)\n0 1 2 3 4 5 601234\n..\n..\n..\n..\n....\n.\n.. .\n...\n.. . ..\n.\n..\n.... ..\n..\n....\n.\nmovement of /vector\u00b5\u2019s in 9 iterations\n\u25eeFigure 16.6 AK-means example for K=2 in R2. The position of the two cen-\ntroids ( /vector\u00b5\u2019s shown as X\u2019s in the top four panels) converges after nine it erations.\nOnline edition (c)\n2009 Cambridge UP16.4 K-means 363\nneed to combine it with a bound on the number of iterations to g uarantee\ntermination.\n\u2022Terminate when the decrease in RSS falls below a threshold \u03b8. For small \u03b8,\nthis indicates that we are close to convergence. Again, we ne ed to combine\nit with a bound on the number of iterations to prevent very lon g runtimes.\nWe now show that K-means converges by proving that RSS monotonically\ndecreases in each iteration. We will use decrease in the meaning decrease or does\nnot change in this section. First, RSS decreases in the reassignment st ep since\neach vector is assigned to the closest centroid, so the dista nce it contributes\nto RSS decreases. Second, it decreases in the recomputation step because the\nnew centroid is the vector /vectorvfor which RSS kreaches its minimum.\nRSS k(/vectorv) = \u2211\n/vectorx\u2208\u03c9k|/vectorv\u2212/vectorx|2=\u2211\n/vectorx\u2208\u03c9kM\n\u2211\nm=1(vm\u2212xm)2(16.8)\n\u2202RSS k(/vectorv)\n\u2202vm=\u2211\n/vectorx\u2208\u03c9k2(vm\u2212xm) (16.9)\nwhere xmand vmare the mthcomponents of their respective vectors. Setting\nthe partial derivative to zero, we get:\nvm=1\n|\u03c9k|\u2211\n/vectorx\u2208\u03c9kxm (16.10)\nwhich is the componentwise de\ufb01nition of the centroid. Thus, we minimize\nRSS kwhen the old centroid is replaced with the new centroid. RSS, the sum\nof the RSS k, must then also decrease during recomputation.\nSince there is only a \ufb01nite set of possible clusterings, a mon otonically de-\ncreasing algorithm will eventually arrive at a (local) mini mum. Take care,\nhowever, to break ties consistently, e.g., by assigning a do cument to the clus-\nter with the lowest index if there are several equidistant ce ntroids. Other-\nwise, the algorithm can cycle forever in a loop of clustering s that have the\nsame cost.\nWhile this proves the convergence of K-means, there is unfortunately no\nguarantee that a global minimum in the objective function will be reached.\nThis is a particular problem if a document set contains many outliers , doc- OUTLIER\numents that are far from any other documents and therefore do not \ufb01t well\ninto any cluster. Frequently, if an outlier is chosen as an in itial seed, then no\nother vector is assigned to it during subsequent iterations . Thus, we end up\nwith a singleton cluster (a cluster with only one document) even though there SINGLETON CLUSTER\nis probably a clustering with lower RSS. Figure 16.7 shows an example of a\nsuboptimal clustering resulting from a bad choice of initia l seeds.\nOnline edition (c)\n2009 Cambridge UP364 16 Flat clustering\n0 1 2 3 40123\n\u00d7\u00d7\n\u00d7\u00d7\n\u00d7\u00d7d1d2 d3\nd4d5 d6\n\u25eeFigure 16.7 The outcome of clustering in K-means depends on the initial seeds.\nFor seeds d2and d5,K-means converges to {{d1,d2,d3},{d4,d5,d6}}, a suboptimal\nclustering. For seeds d2and d3, it converges to{{d1,d2,d4,d5},{d3,d6}}, the global\noptimum for K=2.\nAnother type of suboptimal clustering that frequently occu rs is one with\nempty clusters (Exercise 16.11 ).\nEffective heuristics for seed selection include (i) exclud ing outliers from\nthe seed set; (ii) trying out multiple starting points and ch oosing the cluster-\ning with lowest cost; and (iii) obtaining seeds from another method such as\nhierarchical clustering. Since deterministic hierarchic al clustering methods\nare more predictable than K-means, a hierarchical clustering of a small ran-\ndom sample of size iK(e.g., for i=5 or i=10) often provides good seeds\n(see the description of the Buckshot algorithm, Chapter 17, page 399).\nOther initialization methods compute seeds that are not sel ected from the\nvectors to be clustered. A robust method that works well for a large variety\nof document distributions is to select i(e.g., i=10) random vectors for each\ncluster and use their centroid as the seed for this cluster. S ee Section 16.6 for\nmore sophisticated initializations.\nWhat is the time complexity of K-means? Most of the time is spent on com-\nputing vector distances. One such operation costs \u0398(M). The reassignment\nstep computes KN distances, so its overall complexity is \u0398(KNM ). In the\nrecomputation step, each vector gets added to a centroid onc e, so the com-\nplexity of this step is \u0398(NM). For a \ufb01xed number of iterations I, the overall\ncomplexity is therefore \u0398(IKNM ). Thus, K-means is linear in all relevant\nfactors: iterations, number of clusters, number of vectors and dimensionality\nof the space. This means that K-means is more ef\ufb01cient than the hierarchical\nalgorithms in Chapter 17. We had to \ufb01x the number of iterations I, which can\nbe tricky in practice. But in most cases, K-means quickly reaches either com-\nplete convergence or a clustering that is close to convergen ce. In the latter\ncase, a few documents would switch membership if further ite rations were\ncomputed, but this has a small effect on the overall quality o f the clustering.\nOnline edition (c)\n2009 Cambridge UP16.4 K-means 365\nThere is one subtlety in the preceding argument. Even a linea r algorithm\ncan be quite slow if one of the arguments of \u0398(. . .)is large, and Musually is\nlarge. High dimensionality is not a problem for computing th e distance be-\ntween two documents. Their vectors are sparse, so that only a small fraction\nof the theoretically possible Mcomponentwise differences need to be com-\nputed. Centroids, however, are dense since they pool all ter ms that occur in\nany of the documents of their clusters. As a result, distance computations are\ntime consuming in a naive implementation of K-means. However, there are\nsimple and effective heuristics for making centroid-docum ent similarities as\nfast to compute as document-document similarities. Trunca ting centroids to\nthe most signi\ufb01cant kterms (e.g., k=1000) hardly decreases cluster quality\nwhile achieving a signi\ufb01cant speedup of the reassignment st ep (see refer-\nences in Section 16.6).\nThe same ef\ufb01ciency problem is addressed by K-medoids , a variant of K- K-MEDOIDS\nmeans that computes medoids instead of centroids as cluster centers. We\nde\ufb01ne the medoid of a cluster as the document vector that is closest to the MEDOID\ncentroid. Since medoids are sparse document vectors, dista nce computations\nare fast.\n\u270416.4.1 Cluster cardinality in K-means\nWe stated in Section 16.2 that the number of clusters Kis an input to most \ufb02at\nclustering algorithms. What do we do if we cannot come up with a plausible\nguess for K?\nA naive approach would be to select the optimal value of Kaccording to\nthe objective function, namely the value of Kthat minimizes RSS. De\ufb01ning\nRSS min(K)as the minimal RSS of all clusterings with Kclusters, we observe\nthat RSS min(K)is a monotonically decreasing function in K(Exercise 16.13 ),\nwhich reaches its minimum 0 for K=Nwhere Nis the number of doc-\numents. We would end up with each document being in its own clu ster.\nClearly, this is not an optimal clustering.\nA heuristic method that gets around this problem is to estima te RSS min(K)\nas follows. We \ufb01rst perform i(e.g., i=10) clusterings with Kclusters (each\nwith a different initialization) and compute the RSS of each . Then we take the\nminimum of the iRSS values. We denote this minimum by /hatwidestRSS min(K). Now\nwe can inspect the values /hatwidestRSS min(K)asKincreases and \ufb01nd the \u201cknee\u201d in the\ncurve \u2013 the point where successive decreases in /hatwidestRSS minbecome noticeably\nsmaller. There are two such points in Figure 16.8, one at K=4, where the\ngradient \ufb02attens slightly, and a clearer \ufb02attening at K=9. This is typical:\nthere is seldom a single best number of clusters. We still nee d to employ an\nexternal constraint to choose from a number of possible valu es of K(4 and 9\nin this case).\nOnline edition (c)\n2009 Cambridge UP366 16 Flat clustering\n2 4 6 8 101750 1800 1850 1900 1950\nnumber of clustersresidual sum of squares\n\u25eeFigure 16.8 Estimated minimal residual sum of squares as a function of th e num-\nber of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there\nare two points where the /hatwidestRSS mincurve \ufb02attens: at 4 clusters and at 9 clusters. The\ndocuments were selected from the categories China ,Germany ,Russia and Sports , so\ntheK=4 clustering is closest to the Reuters classi\ufb01cation.\nA second type of criterion for cluster cardinality imposes a penalty for each\nnew cluster \u2013 where conceptually we start with a single clust er containing all\ndocuments and then search for the optimal number of clusters Kby succes-\nsively incrementing Kby one. To determine the cluster cardinality in this\nway, we create a generalized objective function that combin es two elements:\ndistortion , a measure of how much documents deviate from the prototype o f DISTORTION\ntheir clusters (e.g., RSS for K-means); and a measure of model complexity . We MODEL COMPLEXITY\ninterpret a clustering here as a model of the data. Model comp lexity in clus-\ntering is usually the number of clusters or a function thereo f. For K-means,\nwe then get this selection criterion for K:\nK=arg min\nK[RSS min(K) +\u03bbK] (16.11)\nwhere \u03bbis a weighting factor. A large value of \u03bbfavors solutions with few\nclusters. For \u03bb=0, there is no penalty for more clusters and K=Nis the\nbest solution.\nOnline edition (c)\n2009 Cambridge UP16.4 K-means 367\nThe obvious dif\ufb01culty with Equation ( 16.11 ) is that we need to determine\n\u03bb. Unless this is easier than determining Kdirectly, then we are back to\nsquare one. In some cases, we can choose values of \u03bbthat have worked well\nfor similar data sets in the past. For example, if we periodic ally cluster news\nstories from a newswire, there is likely to be a \ufb01xed value of \u03bbthat gives us\nthe right Kin each successive clustering. In this application, we woul d not\nbe able to determine Kbased on past experience since Kchanges.\nA theoretical justi\ufb01cation for Equation ( 16.11 ) is the Akaike Information Cri- AKAIKE INFORMATION\nCRITERION terion or AIC, an information-theoretic measure that trades off di stortion\nagainst model complexity. The general form of AIC is:\nAIC: K=arg min\nK[\u22122L(K) +2q(K)] (16.12)\nwhere\u2212L(K), the negative maximum log-likelihood of the data for Kclus-\nters, is a measure of distortion and q(K), the number of parameters of a\nmodel with Kclusters, is a measure of model complexity. We will not at-\ntempt to derive the AIC here, but it is easy to understand intu itively. The\n\ufb01rst property of a good model of the data is that each data poin t is modeled\nwell by the model. This is the goal of low distortion. But mode ls should\nalso be small (i.e., have low model complexity) since a model that merely\ndescribes the data (and therefore has zero distortion) is wo rthless. AIC pro-\nvides a theoretical justi\ufb01cation for one particular way of w eighting these two\nfactors, distortion and model complexity, when selecting a model.\nForK-means, the AIC can be stated as follows:\nAIC: K=arg min\nK[RSS min(K) +2MK] (16.13)\nEquation ( 16.13 ) is a special case of Equation ( 16.11 ) for \u03bb=2M.\nTo derive Equation ( 16.13 ) from Equation ( 16.12 ) observe that q(K) =KM\ninK-means since each element of the Kcentroids is a parameter that can be\nvaried independently; and that L(K) =\u2212(1/2)RSS min(K)(modulo a con-\nstant) if we view the model underlying K-means as a Gaussian mixture with\nhard assignment, uniform cluster priors and identical sphe rical covariance\nmatrices (see Exercise 16.19 ).\nThe derivation of AIC is based on a number of assumptions, e.g ., that the\ndata are independent and identically distributed. These as sumptions are\nonly approximately true for data sets in information retrie val. As a conse-\nquence, the AIC can rarely be applied without modi\ufb01cation in text clustering.\nIn Figure 16.8, the dimensionality of the vector space is M\u224850,000. Thus,\n2MK>50,000 dominates the smaller RSS-based term ( /hatwidestRSS min(1)<5000,\nnot shown in the \ufb01gure) and the minimum of the expression is re ached for\nK=1. But as we know, K=4 (corresponding to the four classes China ,\nOnline edition (c)\n2009 Cambridge UP368 16 Flat clustering\nGermany ,Russia and Sports ) is a better choice than K=1. In practice, Equa-\ntion ( 16.11 ) is often more useful than Equation ( 16.13 ) \u2013 with the caveat that\nwe need to come up with an estimate for \u03bb.\n?Exercise 16.4\nWhy are documents that do not use the same term for the concept carlikely to end\nup in the same cluster in K-means clustering?\nExercise 16.5\nTwo of the possible termination conditions for K-means were (1) assignment does not\nchange, (2) centroids do not change (page 361). Do these two conditions imply each\nother?\n\u270416.5 Model-based clustering\nIn this section, we describe a generalization of K-means, the EM algorithm.\nIt can be applied to a larger variety of document representat ions and distri-\nbutions than K-means.\nInK-means, we attempt to \ufb01nd centroids that are good representa tives. We\ncan view the set of Kcentroids as a model that generates the data. Generating\na document in this model consists of \ufb01rst picking a centroid a t random and\nthen adding some noise. If the noise is normally distributed , this procedure\nwill result in clusters of spherical shape. Model-based clustering assumes that MODEL -BASED\nCLUSTERING the data were generated by a model and tries to recover the ori ginal model\nfrom the data. The model that we recover from the data then de\ufb01 nes clusters\nand an assignment of documents to clusters.\nA commonly used criterion for estimating the model paramete rs is maxi-\nmum likelihood. In K-means, the quantity exp (\u2212RSS)is proportional to the\nlikelihood that a particular model (i.e., a set of centroids ) generated the data.\nForK-means, maximum likelihood and minimal RSS are equivalent c riteria.\nWe denote the model parameters by \u0398. InK-means, \u0398={/vector\u00b51, . . . ,/vector\u00b5K}.\nMore generally, the maximum likelihood criterion is to sele ct the parame-\nters \u0398that maximize the log-likelihood of generating the data D:\n\u0398=arg max\n\u0398L(D|\u0398) =arg max\n\u0398logN\n\u220f\nn=1P(dn|\u0398) =arg max\n\u0398N\n\u2211\nn=1logP(dn|\u0398)\nL(D|\u0398)is the objective function that measures the goodness of the c luster-\ning. Given two clusterings with the same number of clusters, we prefer the\none with higher L(D|\u0398).\nThis is the same approach we took in Chapter 12(page 237) for language\nmodeling and in Section 13.1 (page 265) for text classi\ufb01cation. In text clas-\nsi\ufb01cation, we chose the class that maximizes the likelihood of generating a\nparticular document. Here, we choose the clustering \u0398that maximizes the\nOnline edition (c)\n2009 Cambridge UP16.5 Model-based clustering 369\nlikelihood of generating a given set of documents. Once we ha ve\u0398, we can\ncompute an assignment probability P(d|\u03c9k;\u0398)for each document-cluster\npair. This set of assignment probabilities de\ufb01nes a soft clu stering.\nAn example of a soft assignment is that a document about Chine se cars\nmay have a fractional membership of 0.5 in each of the two clus ters China\nand automobiles , re\ufb02ecting the fact that both topics are pertinent. A hard cl us-\ntering like K-means cannot model this simultaneous relevance to two topi cs.\nModel-based clustering provides a framework for incorpora ting our know-\nledge about a domain. K-means and the hierarchical algorithms in Chap-\nter17make fairly rigid assumptions about the data. For example, c lusters\ninK-means are assumed to be spheres. Model-based clustering of fers more\n\ufb02exibility. The clustering model can be adapted to what we kn ow about\nthe underlying distribution of the data, be it Bernoulli (as in the example\nin Table 16.3), Gaussian with non-spherical variance (another model tha t is\nimportant in document clustering) or a member of a different family.\nA commonly used algorithm for model-based clustering is the Expectation- EXPECTATION -\nMAXIMIZATION\nALGORITHMMaximization algorithm orEM algorithm . EM clustering is an iterative algo-\nrithm that maximizes L(D|\u0398). EM can be applied to many different types of\nprobabilistic modeling. We will work with a mixture of multi variate Bernoulli\ndistributions here, the distribution we know from Section 11.3 (page 222) and\nSection 13.3 (page 263):\nP(d|\u03c9k;\u0398) =/parenleft\uf8ecigg\n\u220f\ntm\u2208dqmk/parenright\uf8ecigg/parenleft\uf8ecigg\n\u220f\ntm/\u2208d(1\u2212qmk)/parenright\uf8ecigg\n(16.14)\nwhere \u0398={\u03981, . . . , \u0398K},\u0398k= (\u03b1k,q1k, . . . , qMk), and qmk=P(Um=1|\u03c9k)\nare the parameters of the model.3P(Um=1|\u03c9k)is the probability that a\ndocument from cluster \u03c9kcontains term tm. The probability \u03b1kis the prior of\ncluster \u03c9k: the probability that a document dis in \u03c9kif we have no informa-\ntion about d.\nThe mixture model then is:\nP(d|\u0398) =K\n\u2211\nk=1\u03b1k/parenleft\uf8ecigg\n\u220f\ntm\u2208dqmk/parenright\uf8ecigg/parenleft\uf8ecigg\n\u220f\ntm/\u2208d(1\u2212qmk)/parenright\uf8ecigg\n(16.15)\nIn this model, we generate a document by \ufb01rst picking a cluste rkwith prob-\nability \u03b1kand then generating the terms of the document according to th e\nparameters qmk. Recall that the document representation of the multivaria te\nBernoulli is a vector of MBoolean values (and not a real-valued vector).\n3.Umis the random variable we de\ufb01ned in Section 13.3 (page 266) for the Bernoulli Naive Bayes\nmodel. It takes the values 1 (term tmis present in the document) and 0 (term tmis absent in the\ndocument).\nOnline edition (c)\n2009 Cambridge UP370 16 Flat clustering\nHow do we use EM to infer the parameters of the clustering from the data?\nThat is, how do we choose parameters \u0398that maximize L(D|\u0398)? EM is simi-\nlar to K-means in that it alternates between an expectation step , corresponding EXPECTATION STEP\nto reassignment, and a maximization step , corresponding to recomputation of MAXIMIZATION STEP\nthe parameters of the model. The parameters of K-means are the centroids,\nthe parameters of the instance of EM in this section are the \u03b1kand qmk.\nThe maximization step recomputes the conditional paramete rsqmkand the\npriors \u03b1kas follows:\nMaximization step: qmk=\u2211N\nn=1rnkI(tm\u2208dn)\n\u2211N\nn=1rnk\u03b1k=\u2211N\nn=1rnk\nN(16.16)\nwhere I(tm\u2208dn) = 1 if tm\u2208dnand 0 otherwise and rnkis the soft as-\nsignment of document dnto cluster kas computed in the preceding iteration.\n(We\u2019ll address the issue of initialization in a moment.) The se are the max-\nimum likelihood estimates for the parameters of the multiva riate Bernoulli\nfrom Table 13.3 (page 268) except that documents are assigned fractionally to\nclusters here. These maximum likelihood estimates maximiz e the likelihood\nof the data given the model.\nThe expectation step computes the soft assignment of docume nts to clus-\nters given the current parameters qmkand \u03b1k:\nExpectation step :rnk=\u03b1k(\u220ftm\u2208dnqmk)(\u220ftm/\u2208dn(1\u2212qmk))\n\u2211K\nk=1\u03b1k(\u220ftm\u2208dnqmk)(\u220ftm/\u2208dn(1\u2212qmk))(16.17)\nThis expectation step applies Equations ( 16.14 ) and ( 16.15 ) to computing the\nlikelihood that \u03c9kgenerated document dn. It is the classi\ufb01cation procedure\nfor the multivariate Bernoulli in Table 13.3. Thus, the expectation step is\nnothing else but Bernoulli Naive Bayes classi\ufb01cation (incl uding normaliza-\ntion, i.e. dividing by the denominator, to get a probability distribution over\nclusters).\nWe clustered a set of 11 documents into two clusters using EM i n Ta-\nble16.3. After convergence in iteration 25, the \ufb01rst 5 documents are assigned\nto cluster 1 ( ri,1=1.00) and the last 6 to cluster 2 ( ri,1=0.00). Somewhat\natypically, the \ufb01nal assignment is a hard assignment here. E M usually con-\nverges to a soft assignment. In iteration 25, the prior \u03b11for cluster 1 is\n5/11\u22480.45 because 5 of the 11 documents are in cluster 1. Some terms\nare quickly associated with one cluster because the initial assignment can\n\u201cspread\u201d to them unambiguously. For example, membership in cluster 2\nspreads from document 7 to document 8 in the \ufb01rst iteration be cause they\nsharesugar (r8,1=0 in iteration 1). For parameters of terms occurring\nin ambiguous contexts, convergence takes longer. Seed docu ments 6 and 7\nOnline edition (c)\n2009 Cambridge UP16.5 Model-based clustering 371\n(a) docID document text docID document text\n1 hot chocolate cocoa beans 7 sweet sugar\n2 cocoa ghana africa 8 sugar cane brazil\n3 beans harvest ghana 9 sweet sugar beet\n4 cocoa butter 10 sweet cake icing\n5 butter truf\ufb02es 11 cake black forest\n6 sweet chocolate\n(b) Parameter Iteration of clustering\n0 1 2 3 4 5 15 25\n\u03b11 0.50 0.45 0.53 0.57 0.58 0.54 0.45\nr1,1 1.00 1.00 1.00 1.00 1.00 1.00 1.00\nr2,1 0.50 0.79 0.99 1.00 1.00 1.00 1.00\nr3,1 0.50 0.84 1.00 1.00 1.00 1.00 1.00\nr4,1 0.50 0.75 0.94 1.00 1.00 1.00 1.00\nr5,1 0.50 0.52 0.66 0.91 1.00 1.00 1.00\nr6,1 1.00 1.00 1.00 1.00 1.00 1.00 0.83 0.00\nr7,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nr8,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nr9,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nr10,1 0.50 0.40 0.14 0.01 0.00 0.00 0.00\nr11,1 0.50 0.57 0.58 0.41 0.07 0.00 0.00\nqafrica,1 0.000 0.100 0.134 0.158 0.158 0.169 0.200\nqafrica,2 0.000 0.083 0.042 0.001 0.000 0.000 0.000\nqbrazil,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nqbrazil,2 0.000 0.167 0.195 0.213 0.214 0.196 0.167\nqcocoa,1 0.000 0.400 0.432 0.465 0.474 0.508 0.600\nqcocoa,2 0.000 0.167 0.090 0.014 0.001 0.000 0.000\nqsugar,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nqsugar,2 1.000 0.500 0.585 0.640 0.642 0.589 0.500\nqsweet,1 1.000 0.300 0.238 0.180 0.159 0.153 0.000\nqsweet,2 1.000 0.417 0.507 0.610 0.640 0.608 0.667\n\u25eeTable 16.3 The EM clustering algorithm. The table shows a set of documen ts\n(a) and parameter values for selected iterations during EM c lustering (b). Parameters\nshown are prior \u03b11, soft assignment scores rn,1(both omitted for cluster 2), and lexical\nparameters qm,kfor a few terms. The authors initially assigned document 6 to clus-\nter 1 and document 7 to cluster 2 (iteration 0). EM converges a fter 25 iterations. For\nsmoothing, the rnkin Equation ( 16.16 ) were replaced with rnk+\u01ebwhere \u01eb=0.0001.\nOnline edition (c)\n2009 Cambridge UP372 16 Flat clustering\nboth contain sweet . As a result, it takes 25 iterations for the term to be unam-\nbiguously associated with cluster 2. ( qsweet,1 =0 in iteration 25.)\nFinding good seeds is even more critical for EM than for K-means. EM is\nprone to get stuck in local optima if the seeds are not chosen w ell. This is a\ngeneral problem that also occurs in other applications of EM .4Therefore, as\nwith K-means, the initial assignment of documents to clusters is o ften com-\nputed by a different algorithm. For example, a hard K-means clustering may\nprovide the initial assignment, which EM can then \u201csoften up .\u201d\n?Exercise 16.6\nWe saw above that the time complexity of K-means is \u0398(IKNM ). What is the time\ncomplexity of EM?\n16.6 References and further reading\nBerkhin (2006b ) gives a general up-to-date survey of clustering methods wi th\nspecial attention to scalability. The classic reference fo r clustering in pat-\ntern recognition, covering both K-means and EM, is ( Duda et al. 2000 ).Ras-\nmussen (1992 ) introduces clustering from an information retrieval pers pec-\ntive. Anderberg (1973 ) provides a general introduction to clustering for ap-\nplications. In addition to Euclidean distance and cosine si milarity, Kullback-\nLeibler divergence is often used in clustering as a measure o f how (dis)similar\ndocuments and clusters are ( Xu and Croft 1999 ,Muresan and Harper 2004 ,\nKurland and Lee 2004 ).\nThe cluster hypothesis is due to Jardine and van Rijsbergen (1971 ) who\nstate it as follows: Associations between documents convey information about t he\nrelevance of documents to requests. Salton (1971a ;1975 ),Croft (1978 ),Voorhees\n(1985a ),Can and Ozkarahan (1990 ),Cacheda et al. (2003 ),Can et al. (2004 ),\nSingitham et al. (2004 ) and Alting\u00f6vde et al. (2008 ) investigate the ef\ufb01ciency\nand effectiveness of cluster-based retrieval. While some o f these studies\nshow improvements in effectiveness, ef\ufb01ciency or both, the re is no consensus\nthat cluster-based retrieval works well consistently acro ss scenarios. Cluster-\nbased language modeling was pioneered by Liu and Croft (2004 ).\nThere is good evidence that clustering of search results imp roves user ex-\nperience and search result quality ( Hearst and Pedersen 1996 ,Zamir and Et-\nzioni 1999 ,Tombros et al. 2002 ,K\u00e4ki 2005 ,Toda and Kataoka 2005 ), although\nnot as much as search result structuring based on carefully e dited category\nhierarchies ( Hearst 2006 ). The Scatter-Gather interface for browsing collec-\ntions was presented by Cutting et al. (1992 ). A theoretical framework for an-\n4. For example, this problem is common when EM is used to estim ate parameters of hidden\nMarkov models, probabilistic grammars, and machine transl ation models in natural language\nprocessing ( Manning and Sch\u00fctze 1999 ).\nOnline edition (c)\n2009 Cambridge UP16.6 References and further reading 373\nalyzing the properties of Scatter/Gather and other informa tion seeking user\ninterfaces is presented by Pirolli (2007 ).Sch\u00fctze and Silverstein (1997 ) eval-\nuate LSI (Chapter 18) and truncated representations of centroids for ef\ufb01cient\nK-means clustering.\nThe Columbia NewsBlaster system ( McKeown et al. 2002 ), a forerunner to\nthe now much more famous and re\ufb01ned Google News ( http://news.google.com ),\nused hierarchical clustering (Chapter 17) to give two levels of news topic\ngranularity. See Hatzivassiloglou et al. (2000 ) for details, and Chen and Lin\n(2000 ) and Radev et al. (2001 ) for related systems. Other applications of\nclustering in information retrieval are duplicate detecti on (Yang and Callan\n(2006 ), Section 19.6, page 438), novelty detection (see references in Section 17.9,\npage 399) and metadata discovery on the semantic web ( Alonso et al. 2006 ).\nThe discussion of external evaluation measures is partiall y based on Strehl\n(2002 ).Dom (2002 ) proposes a measure Q0that is better motivated theoret-\nically than NMI. Q0is the number of bits needed to transmit class member-\nships assuming cluster memberships are known. The Rand inde x is due to\nRand (1971 ).Hubert and Arabie (1985 ) propose an adjusted Rand index that ADJUSTED RAND INDEX\nranges between\u22121 and 1 and is 0 if there is only chance agreement between\nclusters and classes (similar to \u03bain Chapter 8, page 165).Basu et al. (2004 ) ar-\ngue that the three evaluation measures NMI, Rand index and F m easure give\nvery similar results. Stein et al. (2003 ) propose expected edge density as an in-\nternal measure and give evidence that it is a good predictor o f the quality of a\nclustering. Kleinberg (2002 ) and Meil\u02d8 a (2005 ) present axiomatic frameworks\nfor comparing clusterings.\nAuthors that are often credited with the invention of the K-means algo-\nrithm include Lloyd (1982 ) (\ufb01rst distributed in 1957), Ball (1965 ),MacQueen\n(1967 ), and Hartigan and Wong (1979 ).Arthur and Vassilvitskii (2006 ) in-\nvestigate the worst-case complexity of K-means. Bradley and Fayyad (1998 ),\nPelleg and Moore (1999 ) and Davidson and Satyanarayana (2003 ) investi-\ngate the convergence properties of K-means empirically and how it depends\non initial seed selection. Dhillon and Modha (2001 ) compare K-means clus-\nters with SVD-based clusters (Chapter 18). The K-medoid algorithm was\npresented by Kaufman and Rousseeuw (1990 ). The EM algorithm was orig-\ninally introduced by Dempster et al. (1977 ). An in-depth treatment of EM is\n(McLachlan and Krishnan 1996 ). See Section 18.5 (page 417) for publications\non latent analysis, which can also be viewed as soft clusteri ng.\nAIC is due to Akaike (1974 ) (see also Burnham and Anderson (2002 )). An\nalternative to AIC is BIC, which can be motivated as a Bayesia n model se-\nlection procedure ( Schwarz 1978 ).Fraley and Raftery (1998 ) show how to\nchoose an optimal number of clusters based on BIC. An applica tion of BIC to\nK-means is ( Pelleg and Moore 2000 ).Hamerly and Elkan (2003 ) propose an\nalternative to BIC that performs better in their experiment s. Another in\ufb02u-\nential Bayesian approach for determining the number of clus ters (simultane-\nOnline edition (c)\n2009 Cambridge UP374 16 Flat clustering\nously with cluster assignment) is described by Cheeseman and Stutz (1996 ).\nTwo methods for determining cardinality without external c riteria are pre-\nsented by Tibshirani et al. (2001 ).\nWe only have space here for classical completely unsupervis ed clustering.\nAn important current topic of research is how to use prior kno wledge to\nguide clustering (e.g., Ji and Xu (2006 )) and how to incorporate interactive\nfeedback during clustering (e.g., Huang and Mitchell (2006 )).Fayyad et al.\n(1998 ) propose an initialization for EM clustering. For algorith ms that can\ncluster very large data sets in one scan through the data see Bradley et al.\n(1998 ).\nThe applications in Table 16.1 all cluster documents. Other information re-\ntrieval applications cluster words (e.g., Crouch 1988 ), contexts of words (e.g.,\nSch\u00fctze and Pedersen 1995 ) or words and documents simultaneously (e.g.,\nTishby and Slonim 2000 ,Dhillon 2001 ,Zha et al. 2001 ). Simultaneous clus-\ntering of words and documents is an example of co-clustering orbiclustering . CO-CLUSTERING\n16.7 Exercises\n?Exercise 16.7\nLet\u2126be a clustering that exactly reproduces a class structure Cand \u2126\u2032a clustering\nthat further subdivides some clusters in \u2126. Show that I(\u2126;C) =I(\u2126\u2032;C).\nExercise 16.8\nShow that I(\u2126;C)\u2264[H(\u2126) +H(C)]/2.\nExercise 16.9\nMutual information is symmetric in the sense that its value d oes not change if the\nroles of clusters and classes are switched: I(\u2126;C) = I(C;\u2126). Which of the other\nthree evaluation measures are symmetric in this sense?\nExercise 16.10\nCompute RSS for the two clusterings in Figure 16.7.\nExercise 16.11\n(i) Give an example of a set of points and three initial centro ids (which need not be\nmembers of the set of points) for which 3-means converges to a clustering with an\nempty cluster. (ii) Can a clustering with an empty cluster be the global optimum with\nrespect to RSS?\nExercise 16.12\nDownload Reuters-21578. Discard documents that do not occu r in one of the 10\nclasses acquisitions ,corn,crude ,earn,grain ,interest ,money-fx ,ship,trade , and wheat .\nDiscard documents that occur in two of these 10 classes. (i) C ompute a K-means clus-\ntering of this subset into 10 clusters. There are a number of s oftware packages that\nimplement K-means, such as WEKA ( Witten and Frank 2005 ) and R ( R Development\nCore Team 2005 ). (ii) Compute purity, normalized mutual information, F1and RI for\nOnline edition (c)\n2009 Cambridge UP16.7 Exercises 375\nthe clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Ta-\nble14.5, page 308) for the 10 classes and 10 clusters. Identify classes that gi ve rise to\nfalse positives and false negatives.\nExercise 16.13\nProve that RSS min(K)is monotonically decreasing in K.\nExercise 16.14\nThere is a soft version of K-means that computes the fractional membership of a doc-\nument in a cluster as a monotonically decreasing function of the distance \u2206from its\ncentroid, e.g., as e\u2212\u2206. Modify reassignment and recomputation steps of hard K-means\nfor this soft version.\nExercise 16.15\nIn the last iteration in Table 16.3, document 6 is in cluster 2 even though it was the\ninitial seed for cluster 1. Why does the document change memb ership?\nExercise 16.16\nThe values of the parameters qmkin iteration 25 in Table 16.3 are rounded. What are\nthe exact values that EM will converge to?\nExercise 16.17\nPerform a K-means clustering for the documents in Table 16.3. After how many\niterations does K-means converge? Compare the result with the EM clustering i n\nTable 16.3 and discuss the differences.\nExercise 16.18 [\u22c6 \u22c6 \u22c6 ]\nModify the expectation and maximization steps of EM for a Gau ssian mixture. The\nmaximization step computes the maximum likelihood paramet er estimates \u03b1k,/vector\u00b5k,\nand \u03a3kfor each of the clusters. The expectation step computes for e ach vector a soft\nassignment to clusters (Gaussians) based on their current p arameters. Write down\nthe equations for Gaussian mixtures corresponding to Equat ions ( 16.16 ) and ( 16.17 ).\nExercise 16.19 [\u22c6 \u22c6 \u22c6 ]\nShow that K-means can be viewed as the limiting case of EM for Gaussian mi xtures\nif variance is very small and all covariances are 0.\nExercise 16.20 [\u22c6 \u22c6 \u22c6 ]\nThewithin-point scatter of a clustering is de\ufb01ned as \u2211k1\n2\u2211/vectorxi\u2208\u03c9k\u2211/vectorxj\u2208\u03c9k|/vectorxi\u2212/vectorxj|2. Show WITHIN -POINT\nSCATTERthat minimizing RSS and minimizing within-point scatter ar e equivalent.\nExercise 16.21 [\u22c6 \u22c6 \u22c6 ]\nDerive an AIC criterion for the multivariate Bernoulli mixt ure model from Equa-\ntion ( 16.12 ).\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 377\n17 Hierarchical clustering\nFlat clustering is ef\ufb01cient and conceptually simple, but as we saw in Chap-\nter16it has a number of drawbacks. The algorithms introduced in Ch ap-\nter16return a \ufb02at unstructured set of clusters, require a prespec i\ufb01ed num-\nber of clusters as input and are nondeterministic. Hierarchical clustering (or HIERARCHICAL\nCLUSTERING hierarchic clustering ) outputs a hierarchy, a structure that is more informative\nthan the unstructured set of clusters returned by \ufb02at cluste ring.1Hierarchical\nclustering does not require us to prespecify the number of cl usters and most\nhierarchical algorithms that have been used in IR are determ inistic. These ad-\nvantages of hierarchical clustering come at the cost of lowe r ef\ufb01ciency. The\nmost common hierarchical clustering algorithms have a comp lexity that is at\nleast quadratic in the number of documents compared to the li near complex-\nity of K-means and EM (cf. Section 16.4, page 364).\nThis chapter \ufb01rst introduces agglomerative hierarchical clustering (Section 17.1)\nand presents four different agglomerative algorithms, in S ections 17.2\u201317.4,\nwhich differ in the similarity measures they employ: single -link, complete-\nlink, group-average, and centroid similarity. We then disc uss the optimality\nconditions of hierarchical clustering in Section 17.5. Section 17.6 introduces\ntop-down (or divisive ) hierarchical clustering. Section 17.7 looks at labeling\nclusters automatically, a problem that must be solved whene ver humans in-\nteract with the output of clustering. We discuss implementa tion issues in\nSection 17.8. Section 17.9 provides pointers to further reading, including ref-\nerences to soft hierarchical clustering, which we do not cov er in this book.\nThere are few differences between the applications of \ufb02at an d hierarchi-\ncal clustering in information retrieval. In particular, hi erarchical clustering\nis appropriate for any of the applications shown in Table 16.1 (page 351; see\nalso Section 16.6, page 372). In fact, the example we gave for collection clus-\ntering is hierarchical. In general, we select \ufb02at clusterin g when ef\ufb01ciency\nis important and hierarchical clustering when one of the pot ential problems\n1. In this chapter, we only consider hierarchies that are bin ary trees like the one shown in Fig-\nure17.1 \u2013 but hierarchical clustering can be easily extended to othe r types of trees.\nOnline edition (c)\n2009 Cambridge UP378 17 Hierarchical clustering\nof \ufb02at clustering (not enough structure, predetermined num ber of clusters,\nnon-determinism) is a concern. In addition, many researche rs believe that hi-\nerarchical clustering produces better clusters than \ufb02at cl ustering. However,\nthere is no consensus on this issue (see references in Sectio n17.9).\n17.1 Hierarchical agglomerative clustering\nHierarchical clustering algorithms are either top-down or bottom-up. Bottom-\nup algorithms treat each document as a singleton cluster at t he outset and\nthen successively merge (or agglomerate ) pairs of clusters until all clusters\nhave been merged into a single cluster that contains all docu ments. Bottom-\nup hierarchical clustering is therefore called hierarchical agglomerative cluster- HIERARCHICAL\nAGGLOMERATIVE\nCLUSTERINGingorHAC . Top-down clustering requires a method for splitting a clus ter.\nHACIt proceeds by splitting clusters recursively until indivi dual documents are\nreached. See Section 17.6. HAC is more frequently used in IR than top-down\nclustering and is the main subject of this chapter.\nBefore looking at speci\ufb01c similarity measures used in HAC in Sections\n17.2\u201317.4, we \ufb01rst introduce a method for depicting hierarchical clus terings\ngraphically, discuss a few key properties of HACs and presen t a simple algo-\nrithm for computing an HAC.\nAn HAC clustering is typically visualized as a dendrogram as shown in DENDROGRAM\nFigure 17.1. Each merge is represented by a horizontal line. The y-coord inate\nof the horizontal line is the similarity of the two clusters t hat were merged,\nwhere documents are viewed as singleton clusters. We call th is similarity the\ncombination similarity of the merged cluster. For example, the combination COMBINATION\nSIMILARITY similarity of the cluster consisting of Lloyd\u2019s CEO questioned and Lloyd\u2019s chief\n/ U.S. grilling in Figure 17.1 is\u22480.56. We de\ufb01ne the combination similarity\nof a singleton cluster as its document\u2019s self-similarity (w hich is 1.0 for cosine\nsimilarity).\nBy moving up from the bottom layer to the top node, a dendrogra m al-\nlows us to reconstruct the history of merges that resulted in the depicted\nclustering. For example, we see that the two documents entit ledWar hero\nColin Powell were merged \ufb01rst in Figure 17.1 and that the last merge added\nAg trade reform to a cluster consisting of the other 29 documents.\nA fundamental assumption in HAC is that the merge operation i smono- MONOTONICITY\ntonic . Monotonic means that if s1,s2, . . . , sK\u22121are the combination similarities\nof the successive merges of an HAC, then s1\u2265s2\u2265. . .\u2265sK\u22121holds. A non-\nmonotonic hierarchical clustering contains at least one inversion s i<si+1INVERSION\nand contradicts the fundamental assumption that we chose th e best merge\navailable at each step. We will see an example of an inversion in Figure 17.12 .\nHierarchical clustering does not require a prespeci\ufb01ed num ber of clusters.\nHowever, in some applications we want a partition of disjoin t clusters just as\nOnline edition (c)\n2009 Cambridge UP17.1 Hierarchical agglomerative clustering 3791.0 0.8 0.6 0.4 0.2 0.0\nAg trade reform.\nBack\u2212to\u2212school spending is up\nLloyd\u2019s CEO questioned\nLloyd\u2019s chief / U.S. grilling\nViag stays positive\nChrysler / Latin America\nOhio Blue Cross\nJapanese prime minister / Mexico\nCompuServe reports loss\nSprint / Internet access service\nPlanet Hollywood\nTrocadero: tripling of revenues\nGerman unions split\nWar hero Colin Powell\nWar hero Colin Powell\nOil prices slip\nChains may raise prices\nClinton signs law\nLawsuit against tobacco companies\nsuits against tobacco firms\nIndiana tobacco lawsuit\nMost active stocks\nMexican markets\nHog prices tumble\nNYSE closing averages\nBritish FTSE index\nFed holds interest rates steady\nFed to keep interest rates steady\nFed keeps interest rates steady\nFed keeps interest rates steady\n\u25eeFigure 17.1 A dendrogram of a single-link clustering of 30 documents fro m\nReuters-RCV1. Two possible cuts of the dendrogram are shown : at 0.4 into 24 clusters\nand at 0.1 into 12 clusters.\nOnline edition (c)\n2009 Cambridge UP380 17 Hierarchical clustering\nin \ufb02at clustering. In those cases, the hierarchy needs to be c ut at some point.\nA number of criteria can be used to determine the cutting poin t:\n\u2022Cut at a prespeci\ufb01ed level of similarity. For example, we cut the dendro-\ngram at 0.4 if we want clusters with a minimum combination sim ilarity\nof 0.4. In Figure 17.1, cutting the diagram at y=0.4 yields 24 clusters\n(grouping only documents with high similarity together) an d cutting it at\ny=0.1 yields 12 clusters (one large \ufb01nancial news cluster and 1 1 smaller\nclusters).\n\u2022Cut the dendrogram where the gap between two successive comb ination\nsimilarities is largest. Such large gaps arguably indicate \u201cnatural\u201d clus-\nterings. Adding one more cluster decreases the quality of th e clustering\nsigni\ufb01cantly, so cutting before this steep decrease occurs is desirable. This\nstrategy is analogous to looking for the knee in the K-means graph in Fig-\nure16.8 (page 366).\n\u2022Apply Equation ( 16.11 ) (page 366):\nK=arg min\nK\u2032[RSS(K\u2032) +\u03bbK\u2032]\nwhere K\u2032refers to the cut of the hierarchy that results in K\u2032clusters, RSS is\nthe residual sum of squares and \u03bbis a penalty for each additional cluster.\nInstead of RSS, another measure of distortion can be used.\n\u2022As in \ufb02at clustering, we can also prespecify the number of clu sters Kand\nselect the cutting point that produces Kclusters.\nA simple, naive HAC algorithm is shown in Figure 17.2. We \ufb01rst compute\ntheN\u00d7Nsimilarity matrix C. The algorithm then executes N\u22121 steps\nof merging the currently most similar clusters. In each iter ation, the two\nmost similar clusters are merged and the rows and columns of t he merged\ncluster iinCare updated.2The clustering is stored as a list of merges in\nA.Iindicates which clusters are still available to be merged. T he function\nSIM(i,m,j)computes the similarity of cluster jwith the merge of clusters i\nand m. For some HAC algorithms, SIM(i,m,j)is simply a function of C[j][i]\nand C[j][m], for example, the maximum of these two values for single-lin k.\nWe will now re\ufb01ne this algorithm for the different similarit y measures\nof single-link and complete-link clustering (Section 17.2) and group-average\nand centroid clustering (Sections 17.3 and 17.4). The merge criteria of these\nfour variants of HAC are shown in Figure 17.3.\n2. We assume that we use a deterministic method for breaking t ies, such as always choose the\nmerge that is the \ufb01rst cluster with respect to a total orderin g of the subsets of the document set\nD.\nOnline edition (c)\n2009 Cambridge UP17.1 Hierarchical agglomerative clustering 381\nSIMPLE HAC(d1, . . . , dN)\n1forn\u21901toN\n2do for i\u21901toN\n3 doC[n][i]\u2190SIM(dn,di)\n4 I[n]\u21901(keeps track of active clusters)\n5A\u2190[](assembles clustering as a sequence of merges)\n6fork\u21901toN\u22121\n7do/an}\u230ara\u230bketle{ti,m/an}\u230ara\u230bketri}ht\u2190 arg max{/an}\u230ara\u230bketle{ti,m/an}\u230ara\u230bketri}ht:i/ne}ationslash=m\u2227I[i]=1\u2227I[m]=1}C[i][m]\n8 A.APPEND (/an}\u230ara\u230bketle{ti,m/an}\u230ara\u230bketri}ht)(store merge)\n9 forj\u21901toN\n10 doC[i][j]\u2190SIM(i,m,j)\n11 C[j][i]\u2190SIM(i,m,j)\n12 I[m]\u21900(deactivate cluster)\n13 return A\n\u25eeFigure 17.2 A simple, but inef\ufb01cient HAC algorithm.\n/Bullet\n/Bullet\n/Bullet/Bullet\n(a) single-link: maximum similarity/Bullet\n/Bullet\n/Bullet/Bullet\n(b) complete-link: minimum similarity\n/Bullet\n/Bullet\n/Bullet/Bullet\n(c) centroid: average inter-similarity/Bullet\n/Bullet\n/Bullet/Bullet\n(d) group-average: average of all similarities\n\u25eeFigure 17.3 The different notions of cluster similarity used by the four HAC al-\ngorithms. An inter-similarity is a similarity between two documents from different\nclusters.\nOnline edition (c)\n2009 Cambridge UP382 17 Hierarchical clustering\n0 1 2 3 40123\n\u00d7d5\n\u00d7d6\n\u00d7d7\n\u00d7d8\u00d7d1\n\u00d7d2\n\u00d7d3\n\u00d7d4\n0 1 2 3 40123\n\u00d7d5\n\u00d7d6\n\u00d7d7\n\u00d7d8\u00d7d1\n\u00d7d2\n\u00d7d3\n\u00d7d4\n\u25eeFigure 17.4 A single-link (left) and complete-link (right) clustering of eight doc-\numents. The ellipses correspond to successive clustering s tages. Left: The single-link\nsimilarity of the two upper two-point clusters is the simila rity of d2and d3(solid\nline), which is greater than the single-link similarity of t he two left two-point clusters\n(dashed line). Right: The complete-link similarity of the t wo upper two-point clusters\nis the similarity of d1and d4(dashed line), which is smaller than the complete-link\nsimilarity of the two left two-point clusters (solid line).\n17.2 Single-link and complete-link clustering\nInsingle-link clustering orsingle-linkage clustering , the similarity of two clus- SINGLE -LINK\nCLUSTERING ters is the similarity of their most similar members (see Figure 17.3, (a))3. This\nsingle-link merge criterion is local. We pay attention solely to the area where\nthe two clusters come closest to each other. Other, more dist ant parts of the\ncluster and the clusters\u2019 overall structure are not taken in to account.\nIncomplete-link clustering orcomplete-linkage clustering , the similarity of two COMPLETE -LINK\nCLUSTERING clusters is the similarity of their most dissimilar members (see Figure 17.3, (b)).\nThis is equivalent to choosing the cluster pair whose merge h as the smallest\ndiameter. This complete-link merge criterion is non-local ; the entire structure\nof the clustering can in\ufb02uence merge decisions. This result s in a preference\nfor compact clusters with small diameters over long, stragg ly clusters, but\nalso causes sensitivity to outliers. A single document far f rom the center can\nincrease diameters of candidate merge clusters dramatical ly and completely\nchange the \ufb01nal clustering.\nFigure 17.4 depicts a single-link and a complete-link clustering of eig ht\ndocuments. The \ufb01rst four steps, each producing a cluster con sisting of a pair\nof two documents, are identical. Then single-link clusteri ng joins the up-\nper two pairs (and after that the lower two pairs) because on t he maximum-\nsimilarity de\ufb01nition of cluster similarity, those two clus ters are closest. Complete-\n3. Throughout this chapter, we equate similarity with proxi mity in 2D depictions of clustering.\nOnline edition (c)\n2009 Cambridge UP17.2 Single-link and complete-link clustering 3831.0 0.8 0.6 0.4 0.2 0.0\nNYSE closing averages\nHog prices tumble\nOil prices slip\nAg trade reform.\nChrysler / Latin America\nJapanese prime minister / Mexico\nFed holds interest rates steady\nFed to keep interest rates steady\nFed keeps interest rates steady\nFed keeps interest rates steady\nMexican markets\nBritish FTSE index\nWar hero Colin Powell\nWar hero Colin Powell\nLloyd\u2019s CEO questioned\nLloyd\u2019s chief / U.S. grilling\nOhio Blue Cross\nLawsuit against tobacco companies\nsuits against tobacco firms\nIndiana tobacco lawsuit\nViag stays positive\nMost active stocks\nCompuServe reports loss\nSprint / Internet access service\nPlanet Hollywood\nTrocadero: tripling of revenues\nBack\u2212to\u2212school spending is up\nGerman unions split\nChains may raise prices\nClinton signs law\n\u25eeFigure 17.5 A dendrogram of a complete-link clustering. The same 30 docu ments\nwere clustered with single-link clustering in Figure 17.1.\nOnline edition (c)\n2009 Cambridge UP384 17 Hierarchical clustering\n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7\n\u25eeFigure 17.6 Chaining in single-link clustering. The local criterion in single-link\nclustering can cause undesirable elongated clusters.\nlink clustering joins the left two pairs (and then the right t wo pairs) because\nthose are the closest pairs according to the minimum-simila rity de\ufb01nition of\ncluster similarity.4\nFigure 17.1 is an example of a single-link clustering of a set of document s\nand Figure 17.5 is the complete-link clustering of the same set. When cuttin g\nthe last merge in Figure 17.5, we obtain two clusters of similar size (doc-\numents 1\u201316, from NYSE closing averages toLloyd\u2019s chief / U.S. grilling , and\ndocuments 17\u201330, from Ohio Blue Cross toClinton signs law ). There is no cut\nof the dendrogram in Figure 17.1 that would give us an equally balanced\nclustering.\nBoth single-link and complete-link clustering have graph- theoretic inter-\npretations. De\ufb01ne skto be the combination similarity of the two clusters\nmerged in step k, and G(sk)the graph that links all data points with a similar-\nity of at least sk. Then the clusters after step kin single-link clustering are the\nconnected components of G(sk)and the clusters after step kin complete-link\nclustering are maximal cliques of G(sk). Aconnected component is a maximal CONNECTED\nCOMPONENT set of connected points such that there is a path connecting e ach pair. A clique\nCLIQUEis a set of points that are completely linked with each other.\nThese graph-theoretic interpretations motivate the terms single-link and\ncomplete-link clustering. Single-link clusters at step kare maximal sets of\npoints that are linked via at least one link (a single link) of similarity s\u2265sk;\ncomplete-link clusters at step kare maximal sets of points that are completely\nlinked with each other via links of similarity s\u2265sk.\nSingle-link and complete-link clustering reduce the asses sment of cluster\nquality to a single similarity between a pair of documents: t he two most sim-\nilar documents in single-link clustering and the two most di ssimilar docu-\nments in complete-link clustering. A measurement based on o ne pair cannot\nfully re\ufb02ect the distribution of documents in a cluster. It i s therefore not sur-\nprising that both algorithms often produce undesirable clu sters. Single-link\nclustering can produce straggling clusters as shown in Figu re17.6. Since the\nmerge criterion is strictly local, a chain of points can be ex tended for long\n4. If you are bothered by the possibility of ties, assume that d1has coordinates (1+\u01eb, 3\u2212\u01eb)and\nthat all other points have integer coordinates.\nOnline edition (c)\n2009 Cambridge UP17.2 Single-link and complete-link clustering 385\n0 1 2 3 4 5 6 701\u00d7d1\n\u00d7d2\n\u00d7d3\n\u00d7d4\n\u00d7d5\n\u25eeFigure 17.7 Outliers in complete-link clustering. The \ufb01ve documents ha ve\nthe x-coordinates 1 +2\u01eb, 4, 5+2\u01eb, 6 and 7\u2212\u01eb. Complete-link clustering cre-\nates the two clusters shown as ellipses. The most intuitive t wo-cluster cluster-\ning is{{d1},{d2,d3,d4,d5}}, but in complete-link clustering, the outlier d1splits\n{d2,d3,d4,d5}as shown.\ndistances without regard to the overall shape of the emergin g cluster. This\neffect is called chaining . CHAINING\nThe chaining effect is also apparent in Figure 17.1. The last eleven merges\nof the single-link clustering (those above the 0.1 line) add on single docu-\nments or pairs of documents, corresponding to a chain. The co mplete-link\nclustering in Figure 17.5 avoids this problem. Documents are split into two\ngroups of roughly equal size when we cut the dendrogram at the last merge.\nIn general, this is a more useful organization of the data tha n a clustering\nwith chains.\nHowever, complete-link clustering suffers from a differen t problem. It\npays too much attention to outliers, points that do not \ufb01t wel l into the global\nstructure of the cluster. In the example in Figure 17.7 the four documents\nd2,d3,d4,d5are split because of the outlier d1at the left edge (Exercise 17.1).\nComplete-link clustering does not \ufb01nd the most intuitive cl uster structure in\nthis example.\n17.2.1 Time complexity of HAC\nThe complexity of the naive HAC algorithm in Figure 17.2 is\u0398(N3)because\nwe exhaustively scan the N\u00d7Nmatrix Cfor the largest similarity in each of\nN\u22121 iterations.\nFor the four HAC methods discussed in this chapter a more ef\ufb01c ient algo-\nrithm is the priority-queue algorithm shown in Figure 17.8. Its time complex-\nity is \u0398(N2logN). The rows C[k]of the N\u00d7Nsimilarity matrix Care sorted\nin decreasing order of similarity in the priority queues P.P[k].MAX () then\nreturns the cluster in P[k]that currently has the highest similarity with \u03c9k,\nwhere we use \u03c9kto denote the kthcluster as in Chapter 16. After creating the\nmerged cluster of \u03c9k1and \u03c9k2,\u03c9k1is used as its representative. The function\nSIMcomputes the similarity function for potential merge pairs : largest simi-\nlarity for single-link, smallest similarity for complete- link, average similarity\nfor GAAC (Section 17.3), and centroid similarity for centroid clustering (Sec-\nOnline edition (c)\n2009 Cambridge UP386 17 Hierarchical clustering\nEFFICIENT HAC(/vectord1, . . . ,/vectordN)\n1forn\u21901toN\n2do for i\u21901toN\n3 doC[n][i].sim\u2190/vectordn\u00b7/vectordi\n4 C[n][i].index\u2190i\n5 I[n]\u21901\n6 P[n]\u2190priority queue for C[n]sorted on sim\n7 P[n].DELETE (C[n][n])(don\u2019t want self-similarities)\n8A\u2190[]\n9fork\u21901toN\u22121\n10 dok1\u2190arg max{k:I[k]=1}P[k].MAX().sim\n11 k2\u2190P[k1].MAX().index\n12 A.APPEND (/an}\u230ara\u230bketle{tk1,k2/an}\u230ara\u230bketri}ht)\n13 I[k2]\u21900\n14 P[k1]\u2190[]\n15 for each iwith I[i] =1\u2227i/ne}ationslash=k1\n16 doP[i].DELETE (C[i][k1])\n17 P[i].DELETE (C[i][k2])\n18 C[i][k1].sim\u2190SIM(i,k1,k2)\n19 P[i].INSERT (C[i][k1])\n20 C[k1][i].sim\u2190SIM(i,k1,k2)\n21 P[k1].INSERT (C[k1][i])\n22 return A\nclustering algorithm SIM(i,k1,k2)\nsingle-link max(SIM(i,k1),SIM(i,k2))\ncomplete-link min(SIM(i,k1),SIM(i,k2))\ncentroid (1\nNm/vectorvm)\u00b7(1\nNi/vectorvi)\ngroup-average1\n(Nm+Ni)(Nm+Ni\u22121)[(/vectorvm+/vectorvi)2\u2212(Nm+Ni)]\ncompute C[5]1 2 3 4 5\n0.2 0.8 0.6 0.4 1.0\ncreate P[5](by sorting)2 3 4 1\n0.8 0.6 0.4 0.2\nmerge 2 and 3, update\nsimilarity of 2, delete 32 4 1\n0.3 0.4 0.2\ndelete and reinsert 24 2 1\n0.4 0.3 0.2\n\u25eeFigure 17.8 The priority-queue algorithm for HAC. Top: The algorithm. C enter:\nFour different similarity measures. Bottom: An example for processing steps 6 and\n16\u201319. This is a made up example showing P[5]for a 5\u00d75 matrix C.\nOnline edition (c)\n2009 Cambridge UP17.2 Single-link and complete-link clustering 387\nSINGLE LINKCLUSTERING (d1, . . . , dN)\n1forn\u21901toN\n2do for i\u21901toN\n3 doC[n][i].sim\u2190SIM(dn,di)\n4 C[n][i].index\u2190i\n5 I[n]\u2190n\n6 NBM[n]\u2190arg maxX\u2208{C[n][i]:n/ne}ationslash=i}X.sim\n7A\u2190[]\n8forn\u21901toN\u22121\n9doi1\u2190arg max{i:I[i]=i}NBM[i].sim\n10 i2\u2190I[NBM[i1].index ]\n11 A.APPEND (/an}\u230ara\u230bketle{ti1,i2/an}\u230ara\u230bketri}ht)\n12 fori\u21901toN\n13 do if I[i] =i\u2227i/ne}ationslash=i1\u2227i/ne}ationslash=i2\n14 then C[i1][i].sim\u2190C[i][i1].sim\u2190max(C[i1][i].sim, C[i2][i].sim)\n15 ifI[i] =i2\n16 then I[i]\u2190i1\n17 NBM[i1]\u2190arg maxX\u2208{C[i1][i]:I[i]=i\u2227i/ne}ationslash=i1}X.sim\n18 return A\n\u25eeFigure 17.9 Single-link clustering algorithm using an NBM array. After merging\ntwo clusters i1and i2, the \ufb01rst one ( i1) represents the merged cluster. If I[i] =i, then i\nis the representative of its current cluster. If I[i]/ne}ationslash=i, then ihas been merged into the\ncluster represented by I[i]and will therefore be ignored when updating NBM[i1].\ntion 17.4). We give an example of how a row of Cis processed (Figure 17.8,\nbottom panel). The loop in lines 1\u20137 is \u0398(N2)and the loop in lines 9\u201321 is\n\u0398(N2logN)for an implementation of priority queues that supports dele tion\nand insertion in \u0398(logN). The overall complexity of the algorithm is there-\nfore \u0398(N2logN). In the de\ufb01nition of the function SIM,/vectorvmand/vectorviare the\nvector sums of \u03c9k1\u222a\u03c9k2and \u03c9i, respectively, and Nmand Niare the number\nof documents in \u03c9k1\u222a\u03c9k2and \u03c9i, respectively.\nThe argument of E FFICIENT HAC in Figure 17.8 is a set of vectors (as op-\nposed to a set of generic documents) because GAAC and centroi d clustering\n(Sections 17.3 and 17.4) require vectors as input. The complete-link version\nof E FFICIENT HAC can also be applied to documents that are not represented\nas vectors.\nFor single-link, we can introduce a next-best-merge array ( NBM) as a fur-\nther optimization as shown in Figure 17.9. NBM keeps track of what the best\nmerge is for each cluster. Each of the two top level for-loops in Figure 17.9\nare\u0398(N2), thus the overall complexity of single-link clustering is \u0398(N2).\nOnline edition (c)\n2009 Cambridge UP388 17 Hierarchical clustering\n0 1 2 3 4 5 6 7 8 9 1001\u00d7d1\n\u00d7d2\n\u00d7d3\n\u00d7d4\n\u25eeFigure 17.10 Complete-link clustering is not best-merge persistent. At \ufb01rst, d2is\nthe best-merge cluster for d3. But after merging d1and d2,d4becomes d3\u2019s best-merge\ncandidate. In a best-merge persistent algorithm like singl e-link, d3\u2019s best-merge clus-\nter would be{d1,d2}.\nCan we also speed up the other three HAC algorithms with an NBM ar-\nray? We cannot because only single-link clustering is best-merge persistent . BEST -MERGE\nPERSISTENCE Suppose that the best merge cluster for \u03c9kis\u03c9jin single-link clustering.\nThen after merging \u03c9jwith a third cluster \u03c9i/ne}ationslash=\u03c9k, the merge of \u03c9iand \u03c9j\nwill be \u03c9k\u2019s best merge cluster (Exercise 17.6). In other words, the best-merge\ncandidate for the merged cluster is one of the two best-merge candidates of\nits components in single-link clustering. This means that Ccan be updated\nin\u0398(N)in each iteration \u2013 by taking a simple max of two values on line 14\nin Figure 17.9 for each of the remaining \u2264Nclusters.\nFigure 17.10 demonstrates that best-merge persistence does not hold for\ncomplete-link clustering, which means that we cannot use an NBM array to\nspeed up clustering. After merging d3\u2019s best merge candidate d2with cluster\nd1, an unrelated cluster d4becomes the best merge candidate for d3. This is\nbecause the complete-link merge criterion is non-local and can be affected by\npoints at a great distance from the area where two merge candi dates meet.\nIn practice, the ef\ufb01ciency penalty of the \u0398(N2logN)algorithm is small\ncompared with the \u0398(N2)single-link algorithm since computing the similar-\nity between two documents (e.g., as a dot product) is an order of magnitude\nslower than comparing two scalars in sorting. All four HAC al gorithms in\nthis chapter are \u0398(N2)with respect to similarity computations. So the differ-\nence in complexity is rarely a concern in practice when choos ing one of the\nalgorithms.\n?Exercise 17.1\nShow that complete-link clustering creates the two-cluste r clustering depicted in Fig-\nure17.7.\n17.3 Group-average agglomerative clustering\nGroup-average agglomerative clustering orGAAC (see Figure 17.3, (d)) evaluates GROUP -AVERAGE\nAGGLOMERATIVE\nCLUSTERINGcluster quality based on allsimilarities between documents, thus avoiding\nthe pitfalls of the single-link and complete-link criteria , which equate cluster\nOnline edition (c)\n2009 Cambridge UP17.3 Group-average agglomerative clustering 389\nsimilarity with the similarity of a single pair of documents . GAAC is also\ncalled group-average clustering and average-link clustering . GAAC computes\nthe average similarity SIM-GAof all pairs of documents, including pairs from\nthe same cluster. But self-similarities are not included in the average:\nSIM-GA(\u03c9i,\u03c9j) =1\n(Ni+Nj)(Ni+Nj\u22121)\u2211\ndm\u2208\u03c9i\u222a\u03c9j\u2211\ndn\u2208\u03c9i\u222a\u03c9j,dn/ne}ationslash=dm/vectordm\u00b7/vectordn (17.1)\nwhere /vectordis the length-normalized vector of document d,\u00b7denotes the dot\nproduct, and Niand Njare the number of documents in \u03c9iand \u03c9j, respec-\ntively.\nThe motivation for GAAC is that our goal in selecting two clus ters \u03c9i\nand \u03c9jas the next merge in HAC is that the resulting merge cluster \u03c9k=\n\u03c9i\u222a\u03c9jshould be coherent. To judge the coherence of \u03c9k, we need to look\nat all document-document similarities within \u03c9k, including those that occur\nwithin \u03c9iand those that occur within \u03c9j.\nWe can compute the measure SIM-GAef\ufb01ciently because the sum of indi-\nvidual vector similarities is equal to the similarities of t heir sums:\n\u2211\ndm\u2208\u03c9i\u2211\ndn\u2208\u03c9j(/vectordm\u00b7/vectordn) = ( \u2211\ndm\u2208\u03c9i/vectordm)\u00b7(\u2211\ndn\u2208\u03c9j/vectordn) (17.2)\nWith ( 17.2), we have:\nSIM-GA(\u03c9i,\u03c9j) =1\n(Ni+Nj)(Ni+Nj\u22121)[(\u2211\ndm\u2208\u03c9i\u222a\u03c9j/vectordm)2\u2212(Ni+Nj)] (17.3)\nThe term (Ni+Nj)on the right is the sum of Ni+Njself-similarities of value\n1.0. With this trick we can compute cluster similarity in con stant time (as-\nsuming we have available the two vector sums \u2211dm\u2208\u03c9i/vectordmand \u2211dm\u2208\u03c9j/vectordm)\ninstead of in \u0398(NiNj). This is important because we need to be able to com-\npute the function SIM on lines 18 and 20 in E FFICIENT HAC (Figure 17.8)\nin constant time for ef\ufb01cient implementations of GAAC. Note that for two\nsingleton clusters, Equation ( 17.3) is equivalent to the dot product.\nEquation ( 17.2) relies on the distributivity of the dot product with respec t\nto vector addition. Since this is crucial for the ef\ufb01cient co mputation of a\nGAAC clustering, the method cannot be easily applied to repr esentations of\ndocuments that are not real-valued vectors. Also, Equation (17.2) only holds\nfor the dot product. While many algorithms introduced in thi s book have\nnear-equivalent descriptions in terms of dot product, cosi ne similarity and\nEuclidean distance (cf. Section 14.1, page 291), Equation ( 17.2) can only be\nexpressed using the dot product. This is a fundamental diffe rence between\nsingle-link/complete-link clustering and GAAC. The \ufb01rst t wo only require a\nOnline edition (c)\n2009 Cambridge UP390 17 Hierarchical clustering\nsquare matrix of similarities as input and do not care how the se similarities\nwere computed.\nTo summarize, GAAC requires (i) documents represented as ve ctors, (ii)\nlength normalization of vectors, so that self-similaritie s are 1.0, and (iii) the\ndot product as the measure of similarity between vectors and sums of vec-\ntors.\nThe merge algorithms for GAAC and complete-link clustering are the same\nexcept that we use Equation ( 17.3) as similarity function in Figure 17.8. There-\nfore, the overall time complexity of GAAC is the same as for co mplete-link\nclustering: \u0398(N2logN). Like complete-link clustering, GAAC is not best-\nmerge persistent (Exercise 17.6). This means that there is no \u0398(N2)algorithm\nfor GAAC that would be analogous to the \u0398(N2)algorithm for single-link in\nFigure 17.9.\nWe can also de\ufb01ne group-average similarity as including sel f-similarities:\nSIM-GA\u2032(\u03c9i,\u03c9j) =1\n(Ni+Nj)2(\u2211\ndm\u2208\u03c9i\u222a\u03c9j/vectordm)2=1\nNi+Nj\u2211\ndm\u2208\u03c9i\u222a\u03c9j[/vectordm\u00b7/vector\u00b5(\u03c9i\u222a\u03c9j)] (17.4)\nwhere the centroid /vector\u00b5(\u03c9)is de\ufb01ned as in Equation ( 14.1) (page 292). This\nde\ufb01nition is equivalent to the intuitive de\ufb01nition of clust er quality as average\nsimilarity of documents /vectordmto the cluster\u2019s centroid /vector\u00b5.\nSelf-similarities are always equal to 1.0, the maximum poss ible value for\nlength-normalized vectors. The proportion of self-simila rities in Equation ( 17.4)\nisi/i2=1/ifor a cluster of size i. This gives an unfair advantage to small\nclusters since they will have proportionally more self-sim ilarities. For two\ndocuments d1,d2with a similarity s, we have SIM-GA\u2032(d1,d2) = ( 1+s)/2.\nIn contrast, SIM-GA(d1,d2) = s\u2264(1+s)/2. This similarity SIM-GA(d1,d2)\nof two documents is the same as in single-link, complete-lin k and centroid\nclustering. We prefer the de\ufb01nition in Equation ( 17.3), which excludes self-\nsimilarities from the average, because we do not want to pena lize large clus-\nters for their smaller proportion of self-similarities and because we want a\nconsistent similarity value sfor document pairs in all four HAC algorithms.\n?Exercise 17.2\nApply group-average clustering to the points in Figures 17.6 and 17.7. Map them onto\nthe surface of the unit sphere in a three-dimensional space t o get length-normalized\nvectors. Is the group-average clustering different from th e single-link and complete-\nlink clusterings?\nOnline edition (c)\n2009 Cambridge UP17.4 Centroid clustering 391\n0 1 2 3 4 5 6 7012345\u00d7d1\n\u00d7d2\u00d7d3\n\u00d7d4\n\u00d7d5\u00d7d6/Bullet/Circle\n\u00b51/Bullet/Circle\u00b53/Bullet/Circle\u00b52\n\u25eeFigure 17.11 Three iterations of centroid clustering. Each iteration me rges the\ntwo clusters whose centroids are closest.\n17.4 Centroid clustering\nIn centroid clustering, the similarity of two clusters is de \ufb01ned as the similar-\nity of their centroids:\nSIM-CENT(\u03c9i,\u03c9j) = /vector\u00b5(\u03c9i)\u00b7/vector\u00b5(\u03c9j) (17.5)\n= (1\nNi\u2211\ndm\u2208\u03c9i/vectordm)\u00b7(1\nNj\u2211\ndn\u2208\u03c9j/vectordn)\n=1\nNiNj\u2211\ndm\u2208\u03c9i\u2211\ndn\u2208\u03c9j/vectordm\u00b7/vectordn (17.6)\nEquation ( 17.5) is centroid similarity. Equation ( 17.6) shows that centroid\nsimilarity is equivalent to average similarity of all pairs of documents from\ndifferent clusters. Thus, the difference between GAAC and centroid cl ustering\nis that GAAC considers all pairs of documents in computing av erage pair-\nwise similarity (Figure 17.3, (d)) whereas centroid clustering excludes pairs\nfrom the same cluster (Figure 17.3, (c)).\nFigure 17.11 shows the \ufb01rst three steps of a centroid clustering. The \ufb01rst\ntwo iterations form the clusters {d5,d6}with centroid \u00b51and{d1,d2}with\ncentroid \u00b52because the pairs /an}\u230ara\u230bketle{td5,d6/an}\u230ara\u230bketri}htand/an}\u230ara\u230bketle{td1,d2/an}\u230ara\u230bketri}hthave the highest centroid\nsimilarities. In the third iteration, the highest centroid similarity is between\n\u00b51and d4producing the cluster {d4,d5,d6}with centroid \u00b53.\nLike GAAC, centroid clustering is not best-merge persisten t and therefore\n\u0398(N2logN)(Exercise 17.6).\nIn contrast to the other three HAC algorithms, centroid clus tering is not\nmonotonic. So-called inversions can occur: Similarity can increase during INVERSION\nOnline edition (c)\n2009 Cambridge UP392 17 Hierarchical clustering\n0 1 2 3 4 5012345\n\u00d7 \u00d7\u00d7\n/Bullet/Circled1 d2d3\n\u22124\n\u22123\n\u22122\n\u22121\n0\nd1 d2 d3\n\u25eeFigure 17.12 Centroid clustering is not monotonic. The documents d1at(1+\u01eb, 1),\nd2at(5, 1), and d3at(3, 1+2\u221a\n3)are almost equidistant, with d1and d2closer to\neach other than to d3. The non-monotonic inversion in the hierarchical clusteri ng\nof the three points appears as an intersecting merge line in t he dendrogram. The\nintersection is circled.\nclustering as in the example in Figure 17.12 , where we de\ufb01ne similarity as\nnegative distance. In the \ufb01rst merge, the similarity of d1and d2is\u2212(4\u2212\u01eb). In\nthe second merge, the similarity of the centroid of d1and d2(the circle) and d3\nis\u2248\u2212 cos(\u03c0/6)\u00d74=\u2212\u221a\n3/2\u00d74\u2248\u2212 3.46>\u2212(4\u2212\u01eb). This is an example\nof an inversion: similarity increases in this sequence of two clustering steps.\nIn a monotonic HAC algorithm, similarity is monotonically decreasing from\niteration to iteration.\nIncreasing similarity in a series of HAC clustering steps co ntradicts the\nfundamental assumption that small clusters are more cohere nt than large\nclusters. An inversion in a dendrogram shows up as a horizont al merge line\nthat is lower than the previous merge line. All merge lines in Figures 17.1\nand 17.5 are higher than their predecessors because single-link and complete-\nlink clustering are monotonic clustering algorithms.\nDespite its non-monotonicity, centroid clustering is ofte n used because its\nsimilarity measure \u2013 the similarity of two centroids \u2013 is con ceptually simpler\nthan the average of all pairwise similarities in GAAC. Figur e17.11 is all one\nneeds to understand centroid clustering. There is no equall y simple graph\nthat would explain how GAAC works.\n?Exercise 17.3\nFor a \ufb01xed set of Ndocuments there are up to N2distinct similarities between clusters\nin single-link and complete-link clustering. How many dist inct cluster similarities are\nthere in GAAC and centroid clustering?\nOnline edition (c)\n2009 Cambridge UP17.5 Optimality of HAC 393\n\u270417.5 Optimality of HAC\nTo state the optimality conditions of hierarchical cluster ing precisely, we \ufb01rst\nde\ufb01ne the combination similarity COMB -SIMof a clustering \u2126={\u03c91, . . . , \u03c9K}\nas the smallest combination similarity of any of its Kclusters:\nCOMB -SIM({\u03c91, . . . , \u03c9K}) =min\nkCOMB -SIM(\u03c9k)\nRecall that the combination similarity of a cluster \u03c9that was created as the\nmerge of \u03c91and \u03c92is the similarity of \u03c91and \u03c92(page 378).\nWe then de\ufb01ne \u2126={\u03c91, . . . , \u03c9K}to be optimal if all clusterings \u2126\u2032with k OPTIMAL CLUSTERING\nclusters, k\u2264K, have lower combination similarities:\n|\u2126\u2032|\u2264| \u2126|\u21d2 COMB -SIM(\u2126\u2032)\u2264COMB -SIM(\u2126)\nFigure 17.12 shows that centroid clustering is not optimal. The cluster-\ning{{d1,d2},{d3}}(for K=2) has combination similarity \u2212(4\u2212\u01eb)and\n{{d1,d2,d3}}(for K=1) has combination similarity -3.46. So the cluster-\ning{{d1,d2},{d3}}produced in the \ufb01rst merge is not optimal since there is\na clustering with fewer clusters ( {{d1,d2,d3}}) that has higher combination\nsimilarity. Centroid clustering is not optimal because inv ersions can occur.\nThe above de\ufb01nition of optimality would be of limited use if i t was only\napplicable to a clustering together with its merge history. However, we can\nshow (Exercise 17.4) that combination similarity for the three non-inversion COMBINATION\nSIMILARITY algorithms can be read off from the cluster without knowing i ts history. These\ndirect de\ufb01nitions of combination similarity are as follows .\nsingle-link The combination similarity of a cluster \u03c9is the smallest similar-\nity of any bipartition of the cluster, where the similarity o f a bipartition is\nthe largest similarity between any two documents from the tw o parts:\nCOMB -SIM(\u03c9) = min\n{\u03c9\u2032:\u03c9\u2032\u2282\u03c9}max\ndi\u2208\u03c9\u2032max\ndj\u2208\u03c9\u2212\u03c9\u2032SIM(di,dj)\nwhere each/an}\u230ara\u230bketle{t\u03c9\u2032,\u03c9\u2212\u03c9\u2032/an}\u230ara\u230bketri}htis a bipartition of \u03c9.\ncomplete-link The combination similarity of a cluster \u03c9is the smallest sim-\nilarity of any two points in \u03c9: min di\u2208\u03c9min dj\u2208\u03c9SIM(di,dj).\nGAAC The combination similarity of a cluster \u03c9is the average of all pair-\nwise similarities in \u03c9(where self-similarities are not included in the aver-\nage): Equation ( 17.3).\nIf we use these de\ufb01nitions of combination similarity, then o ptimality is a\nproperty of a set of clusters and not of a process that produce s a set of clus-\nters.\nOnline edition (c)\n2009 Cambridge UP394 17 Hierarchical clustering\nWe can now prove the optimality of single-link clustering by induction\nover the number of clusters K. We will give a proof for the case where no two\npairs of documents have the same similarity, but it can easil y be extended to\nthe case with ties.\nThe inductive basis of the proof is that a clustering with K=Nclusters has\ncombination similarity 1.0, which is the largest value poss ible. The induc-\ntion hypothesis is that a single-link clustering \u2126Kwith Kclusters is optimal:\nCOMB -SIM(\u2126K)\u2265COMB -SIM(\u2126\u2032\nK)for all \u2126\u2032\nK. Assume for contradiction that\nthe clustering \u2126K\u22121we obtain by merging the two most similar clusters in\n\u2126Kis not optimal and that instead a different sequence of merge s\u2126\u2032\nK,\u2126\u2032\nK\u22121\nleads to the optimal clustering with K\u22121 clusters. We can write the as-\nsumption that \u2126\u2032\nK\u22121is optimal and that \u2126K\u22121is not as COMB -SIM(\u2126\u2032\nK\u22121)>\nCOMB -SIM(\u2126K\u22121).\nCase 1: The two documents linked by s=COMB -SIM(\u2126\u2032\nK\u22121)are in the\nsame cluster in \u2126K. They can only be in the same cluster if a merge with sim-\nilarity smaller than shas occurred in the merge sequence producing \u2126K. This\nimplies s>COMB -SIM(\u2126K). Thus, COMB -SIM(\u2126\u2032\nK\u22121) =s>COMB -SIM(\u2126K)>\nCOMB -SIM(\u2126\u2032\nK)>COMB -SIM(\u2126\u2032\nK\u22121). Contradiction.\nCase 2: The two documents linked by s=COMB -SIM(\u2126\u2032\nK\u22121)are not in\nthe same cluster in \u2126K. But s=COMB -SIM(\u2126\u2032\nK\u22121)>COMB -SIM(\u2126K\u22121), so\nthe single-link merging rule should have merged these two cl usters when\nprocessing \u2126K. Contradiction.\nThus, \u2126K\u22121is optimal.\nIn contrast to single-link clustering, complete-link clus tering and GAAC\nare not optimal as this example shows:\n\u00d7 \u00d7 \u00d7 \u00d7 1 3 3\nd1 d2 d3 d4\nBoth algorithms merge the two points with distance 1 ( d2and d3) \ufb01rst and\nthus cannot \ufb01nd the two-cluster clustering {{d1,d2},{d3,d4}}. But{{d1,d2},{d3,d4}}\nis optimal on the optimality criteria of complete-link clus tering and GAAC.\nHowever, the merge criteria of complete-link clustering an d GAAC ap-\nproximate the desideratum of approximate sphericity bette r than the merge\ncriterion of single-link clustering. In many applications , we want spheri-\ncal clusters. Thus, even though single-link clustering may seem preferable at\n\ufb01rst because of its optimality, it is optimal with respect to the wrong criterion\nin many document clustering applications.\nTable 17.1 summarizes the properties of the four HAC algorithms intro-\nduced in this chapter. We recommend GAAC for document cluste ring be-\ncause it is generally the method that produces the clusterin g with the best\nOnline edition (c)\n2009 Cambridge UP17.6 Divisive clustering 395\nmethod combination similarity time compl. optimal? comment\nsingle-link max inter-similarity of any 2 docs \u0398(N2) yes chaining effect\ncomplete-link min inter-similarity of any 2 docs \u0398(N2logN)no sensitive to outliers\ngroup-average average of all sims \u0398(N2logN)nobest choice for\nmost applications\ncentroid average inter-similarity \u0398(N2logN)no inversions can occur\n\u25eeTable 17.1 Comparison of HAC algorithms.\nproperties for applications. It does not suffer from chaini ng, from sensitivity\nto outliers and from inversions.\nThere are two exceptions to this recommendation. First, for non-vector\nrepresentations, GAAC is not applicable and clustering sho uld typically be\nperformed with the complete-link method.\nSecond, in some applications the purpose of clustering is no t to create a\ncomplete hierarchy or exhaustive partition of the entire do cument set. For\ninstance, \ufb01rst story detection ornovelty detection is the task of detecting the \ufb01rst FIRST STORY\nDETECTION occurrence of an event in a stream of news stories. One approa ch to this task\nis to \ufb01nd a tight cluster within the documents that were sent a cross the wire\nin a short period of time and are dissimilar from all previous documents. For\nexample, the documents sent over the wire in the minutes afte r the World\nTrade Center attack on September 11, 2001 form such a cluster . Variations of\nsingle-link clustering can do well on this task since it is th e structure of small\nparts of the vector space \u2013 and not global structure \u2013 that is i mportant in this\ncase.\nSimilarly, we will describe an approach to duplicate detect ion on the web\nin Section 19.6 (page 440) where single-link clustering is used in the guise of\nthe union-\ufb01nd algorithm. Again, the decision whether a grou p of documents\nare duplicates of each other is not in\ufb02uenced by documents th at are located\nfar away and single-link clustering is a good choice for dupl icate detection.\n?Exercise 17.4\nShow the equivalence of the two de\ufb01nitions of combination si milarity: the process\nde\ufb01nition on page 378and the static de\ufb01nition on page 393.\n17.6 Divisive clustering\nSo far we have only looked at agglomerative clustering, but a cluster hierar-\nchy can also be generated top-down. This variant of hierarch ical clustering\nis called top-down clustering ordivisive clustering . We start at the top with all TOP-DOWN\nCLUSTERING documents in one cluster. The cluster is split using a \ufb02at clu stering algo-\nOnline edition (c)\n2009 Cambridge UP396 17 Hierarchical clustering\nrithm. This procedure is applied recursively until each doc ument is in its\nown singleton cluster.\nTop-down clustering is conceptually more complex than bott om-up clus-\ntering since we need a second, \ufb02at clustering algorithm as a \u201c subroutine\u201d. It\nhas the advantage of being more ef\ufb01cient if we do not generate a complete\nhierarchy all the way down to individual document leaves. Fo r a \ufb01xed num-\nber of top levels, using an ef\ufb01cient \ufb02at algorithm like K-means, top-down\nalgorithms are linear in the number of documents and cluster s. So they run\nmuch faster than HAC algorithms, which are at least quadrati c.\nThere is evidence that divisive algorithms produce more acc urate hierar-\nchies than bottom-up algorithms in some circumstances. See the references\non bisecting K-means in Section 17.9. Bottom-up methods make cluster-\ning decisions based on local patterns without initially tak ing into account\nthe global distribution. These early decisions cannot be un done. Top-down\nclustering bene\ufb01ts from complete information about the glo bal distribution\nwhen making top-level partitioning decisions.\n17.7 Cluster labeling\nIn many applications of \ufb02at clustering and hierarchical clu stering, particu-\nlarly in analysis tasks and in user interfaces (see applicat ions in Table 16.1,\npage 351), human users interact with clusters. In such settings, we m ust label\nclusters, so that users can see what a cluster is about.\nDifferential cluster labeling selects cluster labels by comparing the distribu- DIFFERENTIAL CLUSTER\nLABELING tion of terms in one cluster with that of other clusters. The f eature selection\nmethods we introduced in Section 13.5 (page 271) can all be used for differen-\ntial cluster labeling.5In particular, mutual information (MI) (Section 13.5.1 ,\npage 272) or, equivalently, information gain and the \u03c72-test (Section 13.5.2 ,\npage 275) will identify cluster labels that characterize one cluste r in contrast\nto other clusters. A combination of a differential test with a penalty for rare\nterms often gives the best labeling results because rare ter ms are not neces-\nsarily representative of the cluster as a whole.\nWe apply three labeling methods to a K-means clustering in Table 17.2. In\nthis example, there is almost no difference between MI and \u03c72. We therefore\nomit the latter.\nCluster-internal labeling computes a label that solely depends on the cluster CLUSTER -INTERNAL\nLABELING itself, not on other clusters. Labeling a cluster with the ti tle of the document\nclosest to the centroid is one cluster-internal method. Tit les are easier to read\nthan a list of terms. A full title can also contain important c ontext that didn\u2019t\nmake it into the top 10 terms selected by MI. On the web, anchor text can\n5. Selecting the most frequent terms is a non-differential f eature selection technique we dis-\ncussed in Section 13.5. It can also be used for labeling clusters.\nOnline edition (c)\n2009 Cambridge UP17.7 Cluster labeling 397\nlabeling method\n# docs centroid mutual information title\n4 622oil plant mexico pro-\nduction crude power\n000 re\ufb01nery gas bpdplant oil production\nbarrels crude bpd\nmexico dolly capacity\npetroleumMEXICO: Hurri-\ncane Dolly heads for\nMexico coast\n9 1017police security russian\npeople military peace\nkilled told grozny\ncourtpolice killed military\nsecurity peace told\ntroops forces rebels\npeopleRUSSIA: Russia\u2019s\nLebed meets rebel\nchief in Chechnya\n10 125900 000 tonnes traders\nfutures wheat prices\ncents september\ntonnedelivery traders\nfutures tonne tonnes\ndesk wheat prices 000\n00USA: Export Business\n- Grain/oilseeds com-\nplex\n\u25eeTable 17.2 Automatically computed cluster labels. This is for three of ten clusters\n(4, 9, and 10) in a K-means clustering of the \ufb01rst 10,000 documents in Reuters-R CV1.\nThe last three columns show cluster summaries computed by th ree labeling methods:\nmost highly weighted terms in centroid (centroid), mutual i nformation, and the title\nof the document closest to the centroid of the cluster (title ). Terms selected by only\none of the \ufb01rst two methods are in bold.\nplay a role similar to a title since the anchor text pointing t o a page can serve\nas a concise summary of its contents.\nIn Table 17.2, the title for cluster 9 suggests that many of its documents a re\nabout the Chechnya con\ufb02ict, a fact the MI terms do not reveal. However, a\nsingle document is unlikely to be representative of all docu ments in a cluster.\nAn example is cluster 4, whose selected title is misleading. The main topic of\nthe cluster is oil. Articles about hurricane Dolly only ende d up in this cluster\nbecause of its effect on oil prices.\nWe can also use a list of terms with high weights in the centroi d of the clus-\nter as a label. Such highly weighted terms (or, even better, p hrases, especially\nnoun phrases) are often more representative of the cluster t han a few titles\ncan be, even if they are not \ufb01ltered for distinctiveness as in the differential\nmethods. However, a list of phrases takes more time to digest for users than\na well crafted title.\nCluster-internal methods are ef\ufb01cient, but they fail to dis tinguish terms\nthat are frequent in the collection as a whole from those that are frequent only\nin the cluster. Terms like year orTuesday may be among the most frequent in\na cluster, but they are not helpful in understanding the cont ents of a cluster\nwith a speci\ufb01c topic like oil.\nIn Table 17.2, the centroid method selects a few more uninformative terms\n(000,court ,cents ,september ) than MI ( forces ,desk ), but most of the terms se-\nOnline edition (c)\n2009 Cambridge UP398 17 Hierarchical clustering\nlected by either method are good descriptors. We get a good se nse of the\ndocuments in a cluster from scanning the selected terms.\nFor hierarchical clustering, additional complications ar ise in cluster label-\ning. Not only do we need to distinguish an internal node in the tree from\nits siblings, but also from its parent and its children. Docu ments in child\nnodes are by de\ufb01nition also members of their parent node, so w e cannot use\na naive differential method to \ufb01nd labels that distinguish t he parent from\nits children. However, more complex criteria, based on a com bination of\noverall collection frequency and prevalence in a given clus ter, can determine\nwhether a term is a more informative label for a child node or a parent node\n(see Section 17.9).\n17.8 Implementation notes\nMost problems that require the computation of a large number of dot prod-\nucts bene\ufb01t from an inverted index. This is also the case for H AC clustering.\nComputational savings due to the inverted index are large if there are many\nzero similarities \u2013 either because many documents do not sha re any terms or\nbecause an aggressive stop list is used.\nIn low dimensions, more aggressive optimizations are possi ble that make\nthe computation of most pairwise similarities unnecessary (Exercise 17.10 ).\nHowever, no such algorithms are known in higher dimensions. We encoun-\ntered the same problem in kNN classi\ufb01cation (see Section 14.7, page 314).\nWhen using GAAC on a large document set in high dimensions, we have\nto take care to avoid dense centroids. For dense centroids, c lustering can\ntake time \u0398(MN2logN)where Mis the size of the vocabulary, whereas\ncomplete-link clustering is \u0398(MaveN2logN)where Maveis the average size\nof the vocabulary of a document. So for large vocabularies co mplete-link\nclustering can be more ef\ufb01cient than an unoptimized impleme ntation of GAAC.\nWe discussed this problem in the context of K-means clustering in Chap-\nter16(page 365) and suggested two solutions: truncating centroids (keepi ng\nonly highly weighted terms) and representing clusters by me ans of sparse\nmedoids instead of dense centroids. These optimizations ca n also be applied\nto GAAC and centroid clustering.\nEven with these optimizations, HAC algorithms are all \u0398(N2)or\u0398(N2logN)\nand therefore infeasible for large sets of 1,000,000 or more documents. For\nsuch large sets, HAC can only be used in combination with a \ufb02at clustering\nalgorithm like K-means. Recall that K-means requires a set of seeds as initial-\nization (Figure 16.5, page 361). If these seeds are badly chosen, then the re-\nsulting clustering will be of poor quality. We can employ an H AC algorithm\nto compute seeds of high quality. If the HAC algorithm is appl ied to a docu-\nment subset of size\u221a\nN, then the overall runtime of K-means cum HAC seed\nOnline edition (c)\n2009 Cambridge UP17.9 References and further reading 399\ngeneration is \u0398(N). This is because the application of a quadratic algorithm\nto a sample of size\u221a\nNhas an overall complexity of \u0398(N). An appropriate\nadjustment can be made for an \u0398(N2logN)algorithm to guarantee linear-\nity. This algorithm is referred to as the Buckshot algorithm . It combines the BUCKSHOT\nALGORITHM determinism and higher reliability of HAC with the ef\ufb01cienc y of K-means.\n17.9 References and further reading\nAn excellent general review of clustering is ( Jain et al. 1999 ). Early references\nfor speci\ufb01c HAC algorithms are ( King 1967 ) (single-link), ( Sneath and Sokal\n1973 ) (complete-link, GAAC) and ( Lance and Williams 1967 ) (discussing a\nlarge variety of hierarchical clustering algorithms). The single-link algorithm\nin Figure 17.9 is similar to Kruskal\u2019s algorithm for constructing a minimum KRUSKAL \u2019S\nALGORITHM spanning tree. A graph-theoretical proof of the correctnes s of Kruskal\u2019s al-\ngorithm (which is analogous to the proof in Section 17.5) is provided by Cor-\nmen et al. (1990 , Theorem 23.1). See Exercise 17.5 for the connection between\nminimum spanning trees and single-link clusterings.\nIt is often claimed that hierarchical clustering algorithm s produce better\nclusterings than \ufb02at algorithms ( Jain and Dubes (1988 , p. 140), Cutting et al.\n(1992 ),Larsen and Aone (1999 )) although more recently there have been ex-\nperimental results suggesting the opposite ( Zhao and Karypis 2002 ). Even\nwithout a consensus on average behavior, there is no doubt th at results of\nEM and K-means are highly variable since they will often converge to a local\noptimum of poor quality. The HAC algorithms we have presente d here are\ndeterministic and thus more predictable.\nThe complexity of complete-link, group-average and centro id clustering\nis sometimes given as \u0398(N2)(Day and Edelsbrunner 1984 ,Voorhees 1985b ,\nMurtagh 1983 ) because a document similarity computation is an order of\nmagnitude more expensive than a simple comparison, the main operation\nexecuted in the merging steps after the N\u00d7Nsimilarity matrix has been\ncomputed.\nThe centroid algorithm described here is due to Voorhees (1985b ). Voorhees\nrecommends complete-link and centroid clustering over sin gle-link for a re-\ntrieval application. The Buckshot algorithm was originall y published by Cut-\nting et al. (1993 ).Allan et al. (1998 ) apply single-link clustering to \ufb01rst story\ndetection.\nAn important HAC technique not discussed here is Ward\u2019s method (Ward WARD \u2019S METHOD\nJr. 1963 ,El-Hamdouchi and Willett 1986 ), also called minimum variance clus-\ntering . In each step, it selects the merge with the smallest RSS (Cha pter 16,\npage 360). The merge criterion in Ward\u2019s method (a function of all ind ividual\ndistances from the centroid) is closely related to the merge criterion in GAAC\n(a function of all individual similarities to the centroid) .\nOnline edition (c)\n2009 Cambridge UP400 17 Hierarchical clustering\nDespite its importance for making the results of clustering useful, compar-\natively little work has been done on labeling clusters. Popescul and Ungar\n(2000 ) obtain good results with a combination of \u03c72and collection frequency\nof a term. Glover et al. (2002b ) use information gain for labeling clusters of\nweb pages. Stein and zu Eissen \u2019s approach is ontology-based ( 2004 ). The\nmore complex problem of labeling nodes in a hierarchy (which requires dis-\ntinguishing more general labels for parents from more speci \ufb01c labels for chil-\ndren) is tackled by Glover et al. (2002a ) and Treeratpituk and Callan (2006 ).\nSome clustering algorithms attempt to \ufb01nd a set of labels \ufb01rs t and then build\n(often overlapping) clusters around the labels, thereby av oiding the problem\nof labeling altogether ( Zamir and Etzioni 1999 ,K\u00e4ki 2005 ,Osi\u00b4 nski and Weiss\n2005 ). We know of no comprehensive study that compares the qualit y of\nsuch \u201clabel-based\u201d clustering to the clustering algorithm s discussed in this\nchapter and in Chapter 16. In principle, work on multi-document summa-\nrization ( McKeown and Radev 1995 ) is also applicable to cluster labeling, but\nmulti-document summaries are usually longer than the short text fragments\nneeded when labeling clusters (cf. Section 8.7, page 170). Presenting clusters\nin a way that users can understand is a UI problem. We recommen d read-\ning ( Baeza-Yates and Ribeiro-Neto 1999 , ch. 10) for an introduction to user\ninterfaces in IR.\nAn example of an ef\ufb01cient divisive algorithm is bisecting K-means ( Stein-\nbach et al. 2000 ).Spectral clustering algorithms ( Kannan et al. 2000 ,Dhillon SPECTRAL CLUSTERING\n2001 ,Zha et al. 2001 ,Ng et al. 2001a ), including principal direction divisive\npartitioning (PDDP) (whose bisecting decisions are based on SVD, see Chap -\nter18) (Boley 1998 ,Savaresi and Boley 2004 ), are computationally more ex-\npensive than bisecting K-means, but have the advantage of being determin-\nistic.\nUnlike K-means and EM, most hierarchical clustering algorithms do n ot\nhave a probabilistic interpretation. Model-based hierarc hical clustering ( Vaithyanathan\nand Dom 2000 ,Kamvar et al. 2002 ,Castro et al. 2004 ) is an exception.\nThe evaluation methodology described in Section 16.3 (page 356) is also\napplicable to hierarchical clustering. Specialized evalu ation measures for hi-\nerarchies are discussed by Fowlkes and Mallows (1983 ),Larsen and Aone\n(1999 ) and Sahoo et al. (2006 ).\nThe R environment ( R Development Core Team 2005 ) offers good support\nfor hierarchical clustering. The R function hclust implements single-link,\ncomplete-link, group-average, and centroid clustering; a nd Ward\u2019s method.\nAnother option provided is median clustering which represents each cluster\nby its medoid (cf. k-medoids in Chapter 16, page 365). Support for cluster-\ning vectors in high-dimensional spaces is provided by the so ftware package\nCLUTO ( http://glaros.dtc.umn.edu/gkhome/views/cluto ).\nOnline edition (c)\n2009 Cambridge UP17.10 Exercises 401\n17.10 Exercises\n?Exercise 17.5\nA single-link clustering can also be computed from the minimum spanning tree of a MINIMUM SPANNING\nTREE graph. The minimum spanning tree connects the vertices of a g raph at the smallest\npossible cost, where cost is de\ufb01ned as the sum over all edges o f the graph. In our\ncase the cost of an edge is the distance between two documents . Show that if \u2206k\u22121>\n\u2206k>. . .>\u22061are the costs of the edges of a minimum spanning tree, then the se\nedges correspond to the k\u22121 merges in constructing a single-link clustering.\nExercise 17.6\nShow that single-link clustering is best-merge persistent and that GAAC and centroid\nclustering are not best-merge persistent.\nExercise 17.7\na.Consider running 2-means clustering on a collection with do cuments from two\ndifferent languages. What result would you expect?\nb.Would you expect the same result when running an HAC algorith m?\nExercise 17.8\nDownload Reuters-21578. Keep only documents that are in the classes crude ,inter-\nest, and grain . Discard documents that are members of more than one of these three\nclasses. Compute a (i) single-link, (ii) complete-link, (i ii) GAAC, (iv) centroid cluster-\ning of the documents. (v) Cut each dendrogram at the second br anch from the top to\nobtain K=3 clusters. Compute the Rand index for each of the 4 clusterin gs. Which\nclustering method performs best?\nExercise 17.9\nSuppose a run of HAC \ufb01nds the clustering with K=7 to have the highest value on\nsome prechosen goodness measure of clustering. Have we foun d the highest-value\nclustering among all clusterings with K=7?\nExercise 17.10\nConsider the task of producing a single-link clustering of Npoints on a line:\n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7\nShow that we only need to compute a total of about Nsimilarities. What is the overall\ncomplexity of single-link clustering for a set of points on a line?\nExercise 17.11\nProve that single-link, complete-link, and group-average clustering are monotonic in\nthe sense de\ufb01ned on page 378.\nExercise 17.12\nForNpoints, there are\u2264NKdifferent \ufb02at clusterings into Kclusters (Section 16.2,\npage 356). What is the number of different hierarchical clusterings (or dendrograms)\nofNdocuments? Are there more \ufb02at clusterings or more hierarchi cal clusterings for\ngiven Kand N?\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 403\n18Matrix decompositions and latent\nsemantic indexing\nOn page 123we introduced the notion of a term-document matrix: anM\u00d7N\nmatrix C, each of whose rows represents a term and each of whose column s\nrepresents a document in the collection. Even for a collecti on of modest size,\nthe term-document matrix Cis likely to have several tens of thousands of\nrows and columns. In Section 18.1.1 we \ufb01rst develop a class of operations\nfrom linear algebra, known as matrix decomposition . In Section 18.2 we use a\nspecial form of matrix decomposition to construct a low-rank approximation\nto the term-document matrix. In Section 18.3 we examine the application\nof such low-rank approximations to indexing and retrieving documents, a\ntechnique referred to as latent semantic indexing . While latent semantic in-\ndexing has not been established as a signi\ufb01cant force in scor ing and ranking\nfor information retrieval, it remains an intriguing approa ch to clustering in a\nnumber of domains including for collections of text documen ts (Section 16.6,\npage 372). Understanding its full potential remains an area of activ e research.\nReaders who do not require a refresher on linear algebra may s kip Sec-\ntion 18.1, although Example 18.1 is especially recommended as it highlights\na property of eigenvalues that we exploit later in the chapte r.\n18.1 Linear algebra review\nWe brie\ufb02y review some necessary background in linear algebr a. Let Cbe\nanM\u00d7Nmatrix with real-valued entries; for a term-document matri x, all\nentries are in fact non-negative. The rank of a matrix is the number of linearly RANK\nindependent rows (or columns) in it; thus, r ank(C)\u2264min{M,N}. A square\nr\u00d7rmatrix all of whose off-diagonal entries are zero is called a diagonal\nmatrix ; its rank is equal to the number of non-zero diagonal entries . If all\nrdiagonal entries of such a diagonal matrix are 1, it is called the identity\nmatrix of dimension rand represented by Ir.\nFor a square M\u00d7Mmatrix Cand a vector /vectorxthat is not all zeros, the values\nOnline edition (c)\n2009 Cambridge UP404 18 Matrix decompositions and latent semantic indexing\nof\u03bbsatisfying\nC/vectorx=\u03bb/vectorx (18.1)\nare called the eigenvalues ofC. The N-vector /vectorxsatisfying Equation ( 18.1) EIGENVALUE\nfor an eigenvalue \u03bbis the corresponding right eigenvector . The eigenvector\ncorresponding to the eigenvalue of largest magnitude is cal led the principal\neigenvector. In a similar fashion, the left eigenvectors ofCare the M-vectors y\nsuch that\n/vectoryTC=\u03bb/vectoryT. (18.2)\nThe number of non-zero eigenvalues of Cis at most rank (C).\nThe eigenvalues of a matrix are found by solving the characteristic equation ,\nwhich is obtained by rewriting Equation ( 18.1) in the form (C\u2212\u03bbIM)/vectorx=0.\nThe eigenvalues of Care then the solutions of |(C\u2212\u03bbIM)|=0, where|S|\ndenotes the determinant of a square matrix S. The equation|(C\u2212\u03bbIM)|=0\nis an Mth order polynomial equation in \u03bband can have at most Mroots,\nwhich are the eigenvalues of C. These eigenvalues can in general be complex,\neven if all entries of Care real.\nWe now examine some further properties of eigenvalues and ei genvectors,\nto set up the central idea of singular value decompositions i n Section 18.2 be-\nlow. First, we look at the relationship between matrix-vect or multiplication\nand eigenvalues.\n\u270eExample 18.1: Consider the matrix\nS=\uf8eb\n\uf8ed30 0 0\n0 20 0\n0 0 1\uf8f6\n\uf8f8.\nClearly the matrix has rank 3, and has 3 non-zero eigenvalues \u03bb1=30,\u03bb2=20 and\n\u03bb3=1, with the three corresponding eigenvectors\n/vectorx1=\uf8eb\n\uf8ed1\n0\n0\uf8f6\n\uf8f8,/vectorx2=\uf8eb\n\uf8ed0\n1\n0\uf8f6\n\uf8f8and/vectorx3=\uf8eb\n\uf8ed0\n0\n1\uf8f6\n\uf8f8.\nFor each of the eigenvectors, multiplication by Sacts as if we were multiplying the\neigenvector by a multiple of the identity matrix; the multip le is different for each\neigenvector. Now, consider an arbitrary vector, such as /vectorv=\uf8eb\n\uf8ed2\n4\n6\uf8f6\n\uf8f8. We can always\nexpress /vectorvas a linear combination of the three eigenvectors of S; in the current example\nwe have\n/vectorv=\uf8eb\n\uf8ed2\n4\n6\uf8f6\n\uf8f8=2/vectorx1+4/vectorx2+6/vectorx3.\nOnline edition (c)\n2009 Cambridge UP18.1 Linear algebra review 405\nSuppose we multiply /vectorvbyS:\nS/vectorv= S(2/vectorx1+4/vectorx2+6/vectorx3)\n= 2S/vectorx1+4S/vectorx2+6S/vectorx3\n= 2\u03bb1/vectorx1+4\u03bb2/vectorx2+6\u03bb3/vectorx3\n= 60/vectorx1+80/vectorx2+6/vectorx3. (18.3)\nExample 18.1 shows that even though /vectorvis an arbitrary vector, the effect of\nmultiplication by Sis determined by the eigenvalues and eigenvectors of S.\nFurthermore, it is intuitively apparent from Equation ( 18.3) that the product\nS/vectorvis relatively unaffected by terms arising from the small eig envalues of S;\nin our example, since \u03bb3=1, the contribution of the third term on the right\nhand side of Equation ( 18.3) is small. In fact, if we were to completely ignore\nthe contribution in Equation ( 18.3) from the third eigenvector corresponding\nto\u03bb3=1, then the product S/vectorvwould be computed to be\uf8eb\n\uf8ed60\n80\n0\uf8f6\n\uf8f8rather than\nthe correct product which is\uf8eb\n\uf8ed60\n80\n6\uf8f6\n\uf8f8; these two vectors are relatively close\nto each other by any of various metrics one could apply (such a s the length\nof their vector difference).\nThis suggests that the effect of small eigenvalues (and thei r eigenvectors)\non a matrix-vector product is small. We will carry forward th is intuition\nwhen studying matrix decompositions and low-rank approxim ations in Sec-\ntion 18.2. Before doing so, we examine the eigenvectors and eigenvalu es of\nspecial forms of matrices that will be of particular interes t to us.\nFor a symmetric matrix S, the eigenvectors corresponding to distinct eigen-\nvalues are orthogonal . Further, if Sis both real and symmetric, the eigenvalues\nare all real.\n\u270eExample 18.2: Consider the real, symmetric matrix\nS=/parenleftbigg2 1\n1 2/parenrightbigg\n. (18.4)\nFrom the characteristic equation |S\u2212\u03bbI|=0, we have the quadratic (2\u2212\u03bb)2\u22121=\n0, whose solutions yield the eigenvalues 3 and 1. The corresp onding eigenvectors/parenleftbigg1\n\u22121/parenrightbigg\nand/parenleftbigg1\n1/parenrightbigg\nare orthogonal.\nOnline edition (c)\n2009 Cambridge UP406 18 Matrix decompositions and latent semantic indexing\n18.1.1 Matrix decompositions\nIn this section we examine ways in which a square matrix can be factored\ninto the product of matrices derived from its eigenvectors; we refer to this\nprocess as matrix decomposition . Matrix decompositions similar to the ones MATRIX\nDECOMPOSITION in this section will form the basis of our principal text-ana lysis technique\nin Section 18.3, where we will look at decompositions of non-square term-\ndocument matrices. The square decompositions in this secti on are simpler\nand can be treated with suf\ufb01cient mathematical rigor to help the reader un-\nderstand how such decompositions work. The detailed mathem atical deriva-\ntion of the more complex decompositions in Section 18.2 are beyond the\nscope of this book.\nWe begin by giving two theorems on the decomposition of a squa re ma-\ntrix into the product of three matrices of a special form. The \ufb01rst of these,\nTheorem 18.1, gives the basic factorization of a square real-valued matr ix\ninto three factors. The second, Theorem 18.2, applies to square symmetric\nmatrices and is the basis of the singular value decompositio n described in\nTheorem 18.3.\nTheorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valued\nM\u00d7M matrix with M linearly independent eigenvectors. Then the re exists an\neigen decomposition EIGEN DECOMPOSITION\nS=U\u039bU\u22121, (18.5)\nwhere the columns of U are the eigenvectors of S and \u039bis a diagonal matrix whose\ndiagonal entries are the eigenvalues of S in decreasing orde r\n\uf8eb\n\uf8ec\uf8ec\uf8ed\u03bb1\n\u03bb2\n\u00b7\u00b7\u00b7\n\u03bbM\uf8f6\n\uf8f7\uf8f7\uf8f8,\u03bbi\u2265\u03bbi+1. (18.6)\nIf the eigenvalues are distinct, then this decomposition is unique.\nTo understand how Theorem 18.1 works, we note that Uhas the eigenvec-\ntors of Sas columns\nU=(/vectoru1/vectoru2\u00b7\u00b7\u00b7/vectoruM). (18.7)\nThen we have\nSU=S(/vectoru1/vectoru2\u00b7\u00b7\u00b7/vectoruM)\n=(\u03bb1/vectoru1\u03bb2/vectoru2\u00b7\u00b7\u00b7\u03bbM/vectoruM)\n=(/vectoru1/vectoru2\u00b7\u00b7\u00b7/vectoruM)\uf8eb\n\uf8ec\uf8ec\uf8ed\u03bb1\n\u03bb2\n\u00b7\u00b7\u00b7\n\u03bbM\uf8f6\n\uf8f7\uf8f7\uf8f8.\nOnline edition (c)\n2009 Cambridge UP18.2 Term-document matrices and singular value decomposit ions 407\nThus, we have SU=U\u039b, orS=U\u039bU\u22121.\nWe next state a closely related decomposition of a symmetric square matrix\ninto the product of matrices derived from its eigenvectors. This will pave the\nway for the development of our main tool for text analysis, th e singular value\ndecomposition (Section 18.2).\nTheorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-\nmetric real-valued M \u00d7M matrix with M linearly independent eigenvectors. Then\nthere exists a symmetric diagonal decomposition SYMMETRIC DIAGONAL\nDECOMPOSITION\nS=Q\u039bQT, (18.8)\nwhere the columns of Q are the orthogonal and normalized (uni t length, real) eigen-\nvectors of S, and \u039bis the diagonal matrix whose entries are the eigenvalues of S .\nFurther, all entries of Q are real and we have Q\u22121=QT.\nWe will build on this symmetric diagonal decomposition to bu ild low-rank\napproximations to term-document matrices.\n?Exercise 18.1\nWhat is the rank of the 3 \u00d73 diagonal matrix below?\n\uf8eb\n\uf8ed1 1 0\n0 1 1\n1 2 1\uf8f6\n\uf8f8\nExercise 18.2\nShow that \u03bb=2 is an eigenvalue of\nC=/parenleftbigg6\u22122\n4 0/parenrightbigg\n.\nFind the corresponding eigenvector.\nExercise 18.3\nCompute the unique eigen decomposition of the 2 \u00d72 matrix in ( 18.4).\n18.2 Term-document matrices and singular value decomposit ions\nThe decompositions we have been studying thus far apply to sq uare matri-\nces. However, the matrix we are interested in is the M\u00d7Nterm-document\nmatrix Cwhere (barring a rare coincidence) M/ne}ationslash=N; furthermore, Cis very\nunlikely to be symmetric. To this end we \ufb01rst describe an exte nsion of the\nsymmetric diagonal decomposition known as the singular value decomposi- SINGULAR VALUE\nDECOMPOSITION tion. We then show in Section 18.3 how this can be used to construct an ap-\nproximate version of C. It is beyond the scope of this book to develop a full\nOnline edition (c)\n2009 Cambridge UP408 18 Matrix decompositions and latent semantic indexing\ntreatment of the mathematics underlying singular value dec ompositions; fol-\nlowing the statement of Theorem 18.3 we relate the singular value decompo-\nsition to the symmetric diagonal decompositions from Secti on18.1.1 . Given SYMMETRIC DIAGONAL\nDECOMPOSITION C, let Ube the M\u00d7Mmatrix whose columns are the orthogonal eigenvec-\ntors of CCT, and Vbe the N\u00d7Nmatrix whose columns are the orthogonal\neigenvectors of CTC. Denote by CTthe transpose of a matrix C.\nTheorem 18.3. Let r be the rank of the M \u00d7N matrix C. Then, there is a singular-\nvalue decomposition (SVD for short) of C of the form SVD\nC=U\u03a3VT, (18.9)\nwhere\n1.The eigenvalues \u03bb1, . . . , \u03bbrof CCTare the same as the eigenvalues of CTC;\n2.For1\u2264i\u2264r, let \u03c3i=\u221a\u03bbi, with \u03bbi\u2265\u03bbi+1. Then the M\u00d7N matrix \u03a3is\ncomposed by setting \u03a3ii=\u03c3ifor1\u2264i\u2264r, and zero otherwise.\nThe values \u03c3iare referred to as the singular values ofC. It is instructive to\nexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this rather\nthan derive the general proof of Theorem 18.3, which is beyond the scope of\nthis book.\nBy multiplying Equation ( 18.9) by its transposed version, we have\nCCT=U\u03a3VTV\u03a3UT=U\u03a32UT. (18.10)\nNote now that in Equation ( 18.10 ), the left-hand side is a square symmetric\nmatrix real-valued matrix, and the right-hand side represe nts its symmetric\ndiagonal decomposition as in Theorem 18.2. What does the left-hand side\nCCTrepresent? It is a square matrix with a row and a column corres pond-\ning to each of the Mterms. The entry (i,j)in the matrix is a measure of the\noverlap between the ith and jth terms, based on their co-occurrence in docu-\nments. The precise mathematical meaning depends on the mann er in which\nCis constructed based on term weighting. Consider the case wh ereCis the\nterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then the\nentry(i,j)inCCTis the number of documents in which both term iand term\njoccur.\nWhen writing down the numerical values of the SVD, it is conve ntional\nto represent \u03a3as an r\u00d7rmatrix with the singular values on the diagonals,\nsince all its entries outside this sub-matrix are zeros. Acc ordingly, it is con-\nventional to omit the rightmost M\u2212rcolumns of Ucorresponding to these\nomitted rows of \u03a3; likewise the rightmost N\u2212rcolumns of Vare omitted\nsince they correspond in VTto the rows that will be multiplied by the N\u2212r\ncolumns of zeros in \u03a3. This written form of the SVD is sometimes known\nOnline edition (c)\n2009 Cambridge UP18.2 Term-document matrices and singular value decomposit ions 409\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr r\nr\nr\nrrr\nrrr\nrrr\nC = U \u03a3 VT\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr r\nr\nr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\n\u25eeFigure 18.1 Illustration of the singular-value decomposition. In this schematic\nillustration of ( 18.9), we see two cases illustrated. In the top half of the \ufb01gure, w e\nhave a matrix Cfor which M>N. The lower half illustrates the case M<N.\nas the reduced SVD ortruncated SVD and we will encounter it again in Ex- REDUCED SVD\nTRUNCATED SVD ercise 18.9. Henceforth, our numerical examples and exercises will use this\nreduced form.\n\u270eExample 18.3: We now illustrate the singular-value decomposition of a 4 \u00d72 ma-\ntrix of rank 2; the singular values are \u03a311=2.236 and \u03a322=1.\nC=\uf8eb\n\uf8ec\uf8ec\uf8ed1\u22121\n0 1\n1 0\n\u22121 1\uf8f6\n\uf8f7\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8ec\uf8ed\u22120.632 0.000\n0.316\u22120.707\n\u22120.316\u22120.707\n0.632 0.000\uf8f6\n\uf8f7\uf8f7\uf8f8/parenleftbigg\n2.236 0.000\n0.000 1.000/parenrightbigg/parenleftbigg\u22120.707 0.707\n\u22120.707\u22120.707/parenrightbigg\n. (18.11)\nAs with the matrix decompositions de\ufb01ned in Section 18.1.1 , the singu-\nlar value decomposition of a matrix can be computed by a varie ty of algo-\nrithms, many of which have been publicly available software implementa-\ntions; pointers to these are given in Section 18.5.\n?Exercise 18.4\nLet\nC=\uf8eb\n\uf8ed1 1\n0 1\n1 0\uf8f6\n\uf8f8 (18.12)\nbe the term-document incidence matrix for a collection. Com pute the co-occurrence\nmatrix CCT. What is the interpretation of the diagonal entries of CCTwhen Cis a\nterm-document incidence matrix?\nOnline edition (c)\n2009 Cambridge UP410 18 Matrix decompositions and latent semantic indexing\nExercise 18.5\nVerify that the SVD of the matrix in Equation ( 18.12 ) is\nU=\uf8eb\n\uf8ed\u22120.816 0.000\n\u22120.408\u22120.707\n\u22120.408 0.707\uf8f6\n\uf8f8,\u03a3=/parenleftbigg1.732 0.000\n0.000 1.000/parenrightbigg\nand VT=/parenleftbigg\u22120.707\u22120.707\n0.707\u22120.707/parenrightbigg\n, (18.13)\nby verifying all of the properties in the statement of Theore m18.3.\nExercise 18.6\nSuppose that Cis a binary term-document incidence matrix. What do the entr ies of\nCTCrepresent?\nExercise 18.7\nLet\nC=\uf8eb\n\uf8ed0 2 1\n0 3 0\n2 1 0\uf8f6\n\uf8f8 (18.14)\nbe a term-document matrix whose entries are term frequencie s; thus term 1 occurs 2\ntimes in document 2 and once in document 3. Compute CCT; observe that its entries\nare largest where two terms have their most frequent occurre nces together in the same\ndocument.\n18.3 Low-rank approximations\nWe next state a matrix approximation problem that at \ufb01rst see ms to have\nlittle to do with information retrieval. We describe a solut ion to this matrix\nproblem using singular-value decompositions, then develo p its application\nto information retrieval.\nGiven an M\u00d7Nmatrix Cand a positive integer k, we wish to \ufb01nd an\nM\u00d7Nmatrix Ckof rank at most k, so as to minimize the Frobenius norm of FROBENIUS NORM\nthe matrix difference X=C\u2212Ck, de\ufb01ned to be\n/\u230aard\u230alX/\u230aard\u230alF=/radicaltp/radicalvertex/radicalvertex/radicalbtM\n\u2211\ni=1N\n\u2211\nj=1X2\nij. (18.15)\nThus, the Frobenius norm of Xmeasures the discrepancy between Ckand C;\nour goal is to \ufb01nd a matrix Ckthat minimizes this discrepancy, while con-\nstraining Ckto have rank at most k. If ris the rank of C, clearly Cr=C\nand the Frobenius norm of the discrepancy is zero in this case . When kis far\nsmaller than r, we refer to Ckas a low-rank approximation . LOW -RANK\nAPPROXIMATION The singular value decomposition can be used to solve the low -rank ma-\ntrix approximation problem. We then derive from it an applic ation to ap-\nproximating term-document matrices. We invoke the followi ng three-step\nprocedure to this end:\nOnline edition (c)\n2009 Cambridge UP18.3 Low-rank approximations 411\nCk = U \u03a3k VT\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr\nrrr r\nr\nr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\nrrrrr\n\u25eeFigure 18.2 Illustration of low rank approximation using the singular- value de-\ncomposition. The dashed boxes indicate the matrix entries a ffected by \u201czeroing out\u201d\nthe smallest singular values.\n1.Given C, construct its SVD in the form shown in ( 18.9); thus, C=U\u03a3VT.\n2.Derive from \u03a3the matrix \u03a3kformed by replacing by zeros the r\u2212ksmall-\nest singular values on the diagonal of \u03a3.\n3.Compute and output Ck=U\u03a3kVTas the rank- kapproximation to C.\nThe rank of Ckis at most k: this follows from the fact that \u03a3khas at most\nknon-zero values. Next, we recall the intuition of Example 18.1: the effect\nof small eigenvalues on matrix products is small. Thus, it se ems plausible\nthat replacing these small eigenvalues by zero will not subs tantially alter the\nproduct, leaving it \u201cclose\u201d to C. The following theorem due to Eckart and\nYoung tells us that, in fact, this procedure yields the matri x of rank kwith\nthe lowest possible Frobenius error.\nTheorem 18.4.\nmin\nZ|rank(Z)=k/\u230aard\u230alC\u2212Z/\u230aard\u230alF=/\u230aard\u230alC\u2212Ck/\u230aard\u230alF=\u03c3k+1. (18.16)\nRecalling that the singular values are in decreasing order \u03c31\u2265\u03c32\u2265\u00b7\u00b7\u00b7 ,\nwe learn from Theorem 18.4 that Ckis the best rank- kapproximation to C,\nincurring an error (measured by the Frobenius norm of C\u2212Ck) equal to \u03c3k+1.\nThus the larger kis, the smaller this error (and in particular, for k=r, the\nerror is zero since \u03a3r=\u03a3; provided r<M,N, then \u03c3r+1=0 and thus\nCr=C).\nTo derive further insight into why the process of truncating the smallest\nr\u2212ksingular values in \u03a3helps generate a rank- kapproximation of low error,\nwe examine the form of Ck:\nCk=U\u03a3kVT(18.17)\nOnline edition (c)\n2009 Cambridge UP412 18 Matrix decompositions and latent semantic indexing\n=U\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03c310 0 0 0\n0\u00b7\u00b7\u00b7 0 0 0\n0 0 \u03c3k0 0\n0 0 0 0 0\n0 0 0 0\u00b7\u00b7\u00b7\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8VT(18.18)\n=k\n\u2211\ni=1\u03c3i/vectorui/vectorvT\ni, (18.19)\nwhere /vectoruiand/vectorviare the ith columns of Uand V, respectively. Thus, /vectorui/vectorvT\niis\na rank-1 matrix, so that we have just expressed Ckas the sum of krank-1\nmatrices each weighted by a singular value. As iincreases, the contribution\nof the rank-1 matrix /vectorui/vectorvT\niis weighted by a sequence of shrinking singular\nvalues \u03c3i.\n?Exercise 18.8\nCompute a rank 1 approximation C1to the matrix Cin Example 18.12 , using the SVD\nas in Exercise 18.13 . What is the Frobenius norm of the error of this approximatio n?\nExercise 18.9\nConsider now the computation in Exercise 18.8. Following the schematic in Fig-\nure18.2, notice that for a rank 1 approximation we have \u03c31being a scalar. Denote\nbyU1the \ufb01rst column of Uand by V1the \ufb01rst column of V. Show that the rank-1\napproximation to Ccan then be written as U1\u03c31VT\n1=\u03c31U1VT\n1.\nExercise 18.10\nExercise 18.9 can be generalized to rank kapproximations: we let U\u2032\nkand V\u2032\nkdenote\nthe \u201creduced\u201d matrices formed by retaining only the \ufb01rst kcolumns of Uand V,\nrespectively. Thus U\u2032\nkis an M\u00d7kmatrix while V\u2032T\nkis ak\u00d7Nmatrix. Then, we have\nCk=U\u2032\nk\u03a3\u2032\nkV\u2032T\nk, (18.20)\nwhere \u03a3\u2032\nkis the square k\u00d7ksubmatrix of \u03a3kwith the singular values \u03c31, . . . , \u03c3kon\nthe diagonal. The primary advantage of using ( 18.20 ) is to eliminate a lot of redun-\ndant columns of zeros in Uand V, thereby explicitly eliminating multiplication by\ncolumns that do not affect the low-rank approximation; this version of the SVD is\nsometimes known as the reduced SVD or truncated SVD and is a co mputationally\nsimpler representation from which to compute the low rank ap proximation.\nFor the matrix Cin Example 18.3, write down both \u03a32and \u03a3\u2032\n2.\n18.4 Latent semantic indexing\nWe now discuss the approximation of a term-document matrix Cby one of\nlower rank using the SVD. The low-rank approximation to Cyields a new\nrepresentation for each document in the collection. We will cast queries\nOnline edition (c)\n2009 Cambridge UP18.4 Latent semantic indexing 413\ninto this low-rank representation as well, enabling us to co mpute query-\ndocument similarity scores in this low-rank representatio n. This process is\nknown as latent semantic indexing (generally abbreviated LSI). LATENT SEMANTIC\nINDEXING But \ufb01rst, we motivate such an approximation. Recall the vect or space rep-\nresentation of documents and queries introduced in Section 6.3(page 120).\nThis vector space representation enjoys a number of advanta ges including\nthe uniform treatment of queries and documents as vectors, t he induced\nscore computation based on cosine similarity, the ability t o weight differ-\nent terms differently, and its extension beyond document re trieval to such\napplications as clustering and classi\ufb01cation. The vector s pace representa-\ntion suffers, however, from its inability to cope with two cl assic problems\narising in natural languages: synonymy and polysemy . Synonymy refers to a\ncase where two different words (say carandautomobile ) have the same mean-\ning. Because the vector space representation fails to captu re the relationship\nbetween synonymous terms such as carandautomobile \u2013 according each a\nseparate dimension in the vector space. Consequently the co mputed simi-\nlarity/vectorq\u00b7/vectordbetween a query /vectorq(say,car) and a document /vectordcontaining both car\nandautomobile underestimates the true similarity that a user would percei ve.\nPolysemy on the other hand refers to the case where a term such ascharge\nhas multiple meanings, so that the computed similarity /vectorq\u00b7/vectordoverestimates\nthe similarity that a user would perceive. Could we use the co -occurrences\nof terms (whether, for instance, charge occurs in a document containing steed\nversus in a document containing electron ) to capture the latent semantic as-\nsociations of terms and alleviate these problems?\nEven for a collection of modest size, the term-document matr ixCis likely\nto have several tens of thousand of rows and columns, and a ran k in the\ntens of thousands as well. In latent semantic indexing (some times referred\nto as latent semantic analysis (LSA) ), we use the SVD to construct a low-rank LSA\napproximation Ckto the term-document matrix, for a value of kthat is far\nsmaller than the original rank of C. In the experimental work cited later\nin this section, kis generally chosen to be in the low hundreds. We thus\nmap each row/column (respectively corresponding to a term/ document) to\nak-dimensional space; this space is de\ufb01ned by the kprincipal eigenvectors\n(corresponding to the largest eigenvalues) of CCTand CTC. Note that the\nmatrix Ckis itself still an M\u00d7Nmatrix, irrespective of k.\nNext, we use the new k-dimensional LSI representation as we did the orig-\ninal representation \u2013 to compute similarities between vect ors. A query vector\n/vectorqis mapped into its representation in the LSI space by the tran sformation\n/vectorqk=\u03a3\u22121\nkUT\nk/vectorq. (18.21)\nNow, we may use cosine similarities as in Section 6.3.1 (page 120) to com-\npute the similarity between a query and a document, between t wo docu-\nOnline edition (c)\n2009 Cambridge UP414 18 Matrix decompositions and latent semantic indexing\nments, or between two terms. Note especially that Equation ( 18.21 ) does not\nin any way depend on /vectorqbeing a query; it is simply a vector in the space of\nterms. This means that if we have an LSI representation of a co llection of\ndocuments, a new document not in the collection can be \u201cfolde d in\u201d to this\nrepresentation using Equation ( 18.21 ). This allows us to incrementally add\ndocuments to an LSI representation. Of course, such increme ntal addition\nfails to capture the co-occurrences of the newly added docum ents (and even\nignores any new terms they contain). As such, the quality of t he LSI rep-\nresentation will degrade as more documents are added and wil l eventually\nrequire a recomputation of the LSI representation.\nThe \ufb01delity of the approximation of CktoCleads us to hope that the rel-\native values of cosine similarities are preserved: if a quer y is close to a doc-\nument in the original space, it remains relatively close in t hek-dimensional\nspace. But this in itself is not suf\ufb01ciently interesting, es pecially given that\nthe sparse query vector /vectorqturns into a dense query vector /vectorqkin the low-\ndimensional space. This has a signi\ufb01cant computational cos t, when com-\npared with the cost of processing /vectorqin its native form.\n\u270eExample 18.4: Consider the term-document matrix C=\nd1d2d3d4d5d6\nship 1 0 1 0 0 0\nboat 0 1 0 0 0 0\nocean 1 1 0 0 0 0\nvoyage 1 0 0 1 1 0\ntrip 0 0 0 1 0 1\nIts singular value decomposition is the product of three mat rices as below. First we\nhave Uwhich in this example is:\n1 2 3 4 5\nship\u22120.44\u22120.30 0.57 0.58 0.25\nboat\u22120.13\u22120.33\u22120.59 0.00 0.73\nocean\u22120.48\u22120.51\u22120.37 0.00\u22120.61\nvoyage\u22120.70 0.35 0.15 \u22120.58 0.16\ntrip\u22120.26 0.65\u22120.41 0.58\u22120.09\nWhen applying the SVD to a term-document matrix, Uis known as the SVD term\nmatrix . The singular values are \u03a3=\n2.16 0.00 0.00 0.00 0.00\n0.00 1.59 0.00 0.00 0.00\n0.00 0.00 1.28 0.00 0.00\n0.00 0.00 0.00 1.00 0.00\n0.00 0.00 0.00 0.00 0.39\nFinally we have VT, which in the context of a term-document matrix is known as\ntheSVD document matrix :\nOnline edition (c)\n2009 Cambridge UP18.4 Latent semantic indexing 415\nd1 d2 d3 d4 d5 d6\n1\u22120.75\u22120.28\u22120.20\u22120.45\u22120.33\u22120.12\n2\u22120.29\u22120.53\u22120.19 0.63 0.22 0.41\n3 0.28\u22120.75 0.45\u22120.20 0.12\u22120.33\n4 0.00 0.00 0.58 0.00 \u22120.58 0.58\n5\u22120.53 0.29 0.63 0.19 0.41 \u22120.22\nBy \u201czeroing out\u201d all but the two largest singular values of \u03a3, we obtain \u03a32=\n2.16 0.00 0.00 0.00 0.00\n0.00 1.59 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00\nFrom this, we compute C2=\nd1 d2 d3 d4 d5 d6\n1\u22121.62\u22120.60\u22120.44\u22120.97\u22120.70\u22120.26\n2\u22120.46\u22120.84\u22120.30 1.00 0.35 0.65\n3 0.00 0.00 0.00 0.00 0.00 0.00\n4 0.00 0.00 0.00 0.00 0.00 0.00\n5 0.00 0.00 0.00 0.00 0.00 0.00\nNotice that the low-rank approximation, unlike the origina l matrix C, can have\nnegative entries.\nExamination of C2and \u03a32in Example 18.4 shows that the last 3 rows of\neach of these matrices are populated entirely by zeros. This suggests that\nthe SVD product U\u03a3VTin Equation ( 18.18 ) can be carried out with only two\nrows in the representations of \u03a32and VT; we may then replace these matrices\nby their truncated versions \u03a3\u2032\n2and(V\u2032)T. For instance, the truncated SVD\ndocument matrix (V\u2032)Tin this example is:\nd1 d2 d3 d4 d5 d6\n1\u22121.62\u22120.60\u22120.44\u22120.97\u22120.70\u22120.26\n2\u22120.46\u22120.84\u22120.30 1.00 0.35 0.65\nFigure 18.3 illustrates the documents in (V\u2032)Tin two dimensions. Note\nalso that C2is dense relative to C.\nWe may in general view the low-rank approximation of CbyCkas a con-\nstrained optimization problem: subject to the constraint that Ckhave rank at\nmost k, we seek a representation of the terms and documents compris ingC\nwith low Frobenius norm for the error C\u2212Ck. When forced to squeeze the\nterms/documents down to a k-dimensional space, the SVD should bring to-\ngether terms with similar co-occurrences. This intuition s uggests, then, that\nnot only should retrieval quality not suffer too much from th e dimension\nreduction, but in fact may improve .\nOnline edition (c)\n2009 Cambridge UP416 18 Matrix decompositions and latent semantic indexing\n\u22120.5\u22121.0\u22121.50.51.0\n\u22120.5\n\u22121.0dim 2\ndim 1\n\u00d7\nd1\n\u00d7\nd2\u00d7d3\u00d7\nd4\n\u00d7\nd5\u00d7d6\n\u25eeFigure 18.3 The documents of Example 18.4 reduced to two dimensions in (V\u2032)T.\nDumais (1993 ) and Dumais (1995 ) conducted experiments with LSI on\nTREC documents and tasks, using the commonly-used Lanczos a lgorithm\nto compute the SVD. At the time of their work in the early 1990\u2019 s, the LSI\ncomputation on tens of thousands of documents took approxim ately a day\non one machine. On these experiments, they achieved precisi on at or above\nthat of the median TREC participant. On about 20% of TREC topi cs their\nsystem was the top scorer, and reportedly slightly better on average than\nstandard vector spaces for LSI at about 350 dimensions. Here are some con-\nclusions on LSI \ufb01rst suggested by their work, and subsequent ly veri\ufb01ed by\nmany other experiments.\n\u2022The computational cost of the SVD is signi\ufb01cant; at the time o f this writ-\ning, we know of no successful experiment with over one millio n docu-\nments. This has been the biggest obstacle to the widespread a doption to\nLSI. One approach to this obstacle is to build the LSI represe ntation on a\nrandomly sampled subset of the documents in the collection, following\nwhich the remaining documents are \u201cfolded in\u201d as detailed wi th Equa-\ntion ( 18.21 ).\nOnline edition (c)\n2009 Cambridge UP18.5 References and further reading 417\n\u2022As we reduce k, recall tends to increase, as expected.\n\u2022Most surprisingly, a value of kin the low hundreds can actually increase\nprecision on some query benchmarks. This appears to suggest that for a\nsuitable value of k, LSI addresses some of the challenges of synonymy.\n\u2022LSI works best in applications where there is little overlap between queries\nand documents.\nThe experiments also documented some modes where LSI failed to match\nthe effectiveness of more traditional indexes and score com putations. Most\nnotably (and perhaps obviously), LSI shares two basic drawb acks of vector\nspace retrieval: there is no good way of expressing negation s (\ufb01nd docu-\nments that contain german but notshepherd ), and no way of enforcing Boolean\nconditions.\nLSI can be viewed as soft clustering by interpreting each dimension of the SOFT CLUSTERING\nreduced space as a cluster and the value that a document has on that dimen-\nsion as its fractional membership in that cluster.\n18.5 References and further reading\nStrang (1986 ) provides an excellent introductory overview of matrix dec om-\npositions including the singular value decomposition. The orem 18.4 is due\ntoEckart and Young (1936 ). The connection between information retrieval\nand low-rank approximations of the term-document matrix wa s introduced\ninDeerwester et al. (1990 ), with a subsequent survey of results in Berry\net al. (1995 ).Dumais (1993 ) and Dumais (1995 ) describe experiments on\nTREC benchmarks giving evidence that at least on some benchm arks, LSI\ncan produce better precision and recall than standard vecto r-space retrieval.\nhttp://www.cs.utk.edu/ \u02dcberry/lsi++/ andhttp://lsi.argreenhouse.com/lsi/LSIpapers.html\noffer comprehensive pointers to the literature and softwar e of LSI. Sch\u00fctze\nand Silverstein (1997 ) evaluate LSI and truncated representations of cen-\ntroids for ef\ufb01cient K-means clustering (Section 16.4).Bast and Majumdar\n(2005 ) detail the role of the reduced dimension kin LSI and how different\npairs of terms get coalesced together at differing values of k. Applications of\nLSI to cross-language information retrieval (where documents in two or more CROSS -LANGUAGE\nINFORMATION\nRETRIEVALdifferent languages are indexed, and a query posed in one lan guage is ex-\npected to retrieve documents in other languages) are develo ped in Berry and\nYoung (1995 ) and Littman et al. (1998 ). LSI (referred to as LSA in more gen-\neral settings) has been applied to host of other problems in c omputer science\nranging from memory modeling to computer vision.\nHofmann (1999a ;b) provides an initial probabilistic extension of the basic\nlatent semantic indexing technique. A more satisfactory fo rmal basis for a\nOnline edition (c)\n2009 Cambridge UP418 18 Matrix decompositions and latent semantic indexing\nDocID Document text\n1 hello\n2 open house\n3 mi casa\n4 hola Profesor\n5 hola y bienvenido\n6 hello and welcome\n\u25eeFigure 18.4 Documents for Exercise 18.11 .\nSpanish English\nmi my\ncasa house\nhola hello\nprofesor professor\ny and\nbienvenido welcome\n\u25eeFigure 18.5 Glossary for Exercise 18.11 .\nprobabilistic latent variable model for dimensionality re duction is the Latent\nDirichlet Allocation (LDA) model ( Blei et al. 2003 ), which is generative and\nassigns probabilities to documents outside of the training set. This model is\nextended to a hierarchical clustering by Rosen-Zvi et al. (2004 ).Wei and Croft\n(2006 ) present the \ufb01rst large scale evaluation of LDA, \ufb01nding it to signi\ufb01-\ncantly outperform the query likelihood model of Section 12.2 (page 242), but\nto not perform quite as well as the relevance model mentioned in Section 12.4\n(page 250) \u2013 but the latter does additional per-query processing unli ke LDA.\nTeh et al. (2006 ) generalize further by presenting Hierarchical Dirichlet Pro-\ncesses, a probabilistic model which allows a group (for us, a document) to\nbe drawn from an in\ufb01nite mixture of latent topics, while stil l allowing these\ntopics to be shared across documents.\n?Exercise 18.11\nAssume you have a set of documents each of which is in either En glish or in Spanish.\nThe collection is given in Figure 18.4.\nFigure 18.5 gives a glossary relating the Spanish and English words abov e for your\nown information. This glossary is NOT available to the retri eval system:\n1.Construct the appropriate term-document matrix Cto use for a collection con-\nsisting of these documents. For simplicity, use raw term fre quencies rather than\nnormalized tf-idf weights. Make sure to clearly label the di mensions of your ma-\ntrix.\nOnline edition (c)\n2009 Cambridge UP18.5 References and further reading 419\n2.Write down the matrices U2,\u03a3\u2032\n2and V2and from these derive the rank 2 approxi-\nmation C2.\n3.State succinctly what the (i,j)entry in the matrix CTCrepresents.\n4.State succinctly what the (i,j)entry in the matrix CT\n2C2represents, and why it\ndiffers from that in CTC.\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 421\n19 Web search basics\nIn this and the following two chapters, we consider web searc h engines. Sec-\ntions 19.1\u201319.4 provide some background and history to help the reader ap-\npreciate the forces that conspire to make the Web chaotic, fa st-changing and\n(from the standpoint of information retrieval) very differ ent from the \u201ctradi-\ntional\u201d collections studied thus far in this book. Sections 19.5\u201319.6 deal with\nestimating the number of documents indexed by web search eng ines, and the\nelimination of duplicate documents in web indexes, respect ively. These two\nlatter sections serve as background material for the follow ing two chapters.\n19.1 Background and history\nThe Web is unprecedented in many ways: unprecedented in scal e, unprece-\ndented in the almost-complete lack of coordination in its cr eation, and un-\nprecedented in the diversity of backgrounds and motives of i ts participants.\nEach of these contributes to making web search different \u2013 an d generally far\nharder \u2013 than searching \u201ctraditional\u201d documents.\nThe invention of hypertext, envisioned by Vannevar Bush in t he 1940\u2019s and\n\ufb01rst realized in working systems in the 1970\u2019s, signi\ufb01cantl y precedes the for-\nmation of the World Wide Web (which we will simply refer to as t he Web), in\nthe 1990\u2019s. Web usage has shown tremendous growth to the poin t where it\nnow claims a good fraction of humanity as participants, by re lying on a sim-\nple, open client-server design: (1) the server communicate s with the client\nvia a protocol (the http or hypertext transfer protocol) that is lightweight and HTTP\nsimple, asynchronously carrying a variety of payloads (tex t, images and \u2013\nover time \u2013 richer media such as audio and video \ufb01les) encoded in a sim-\nple markup language called HTML (for hypertext markup language); (2) the HTML\nclient \u2013 generally a browser , an application within a graphical user environ-\nment \u2013 can ignore what it does not understand. Each of these se emingly\ninnocuous features has contributed enormously to the growt h of the Web, so\nit is worthwhile to examine them further.\nOnline edition (c)\n2009 Cambridge UP422 19 Web search basics\nThe basic operation is as follows: a client (such as a browser ) sends an http\nrequest to aweb server . The browser speci\ufb01es a URL (forUniversal Resource Lo- URL\ncator ) such as http://www.stanford.edu/home/atoz/contact.html .\nIn this example URL, the string http refers to the protocol to be used for\ntransmitting the data. The string www.stanford.edu is known as the do-\nmain and speci\ufb01es the root of a hierarchy of web pages (typically m irroring a\n\ufb01lesystem hierarchy underlying the web server). In this exa mple,/home/atoz/contact.html\nis a path in this hierarchy with a \ufb01le contact.html that contains the infor-\nmation to be returned by the web server at www.stanford.edu in response\nto this request. The HTML-encoded \ufb01le contact.html holds the hyper-\nlinks and the content (in this instance, contact informatio n for Stanford Uni-\nversity), as well as formatting rules for rendering this con tent in a browser.\nSuch an http request thus allows us to fetch the content of a pa ge, some-\nthing that will prove to be useful to us for crawling and index ing documents\n(Chapter 20).\nThe designers of the \ufb01rst browsers made it easy to view the HTM L markup\ntags on the content of a URL. This simple convenience allowed new users to\ncreate their own HTML content without extensive training or experience;\nrather, they learned from example content that they liked. A s they did so, a\nsecond feature of browsers supported the rapid proliferati on of web content\ncreation and usage: browsers ignored what they did not under stand. This\ndid not, as one might fear, lead to the creation of numerous in compatible\ndialects of HTML. What it did promote was amateur content cre ators who\ncould freely experiment with and learn from their newly crea ted web pages\nwithout fear that a simple syntax error would \u201cbring the syst em down.\u201d Pub-\nlishing on the Web became a mass activity that was not limited to a few\ntrained programmers, but rather open to tens and eventually hundreds of\nmillions of individuals. For most users and for most informa tion needs, the\nWeb quickly became the best way to supply and consume informa tion on\neverything from rare ailments to subway schedules.\nThe mass publishing of information on the Web is essentially useless un-\nless this wealth of information can be discovered and consum ed by other\nusers. Early attempts at making web information \u201cdiscovera ble\u201d fell into two\nbroad categories: (1) full-text index search engines such a s Altavista, Excite\nand Infoseek and (2) taxonomies populated with web pages in c ategories,\nsuch as Yahoo! The former presented the user with a keyword se arch in-\nterface supported by inverted indexes and ranking mechanis ms building on\nthose introduced in earlier chapters. The latter allowed th e user to browse\nthrough a hierarchical tree of category labels. While this i s at \ufb01rst blush a\nconvenient and intuitive metaphor for \ufb01nding web pages, it h as a number of\ndrawbacks: \ufb01rst, accurately classifying web pages into tax onomy tree nodes\nis for the most part a manual editorial process, which is dif\ufb01 cult to scale\nwith the size of the Web. Arguably, we only need to have \u201chigh- quality\u201d\nOnline edition (c)\n2009 Cambridge UP19.2 Web characteristics 423\nweb pages in the taxonomy, with only the best web pages for eac h category.\nHowever, just discovering these and classifying them accur ately and consis-\ntently into the taxonomy entails signi\ufb01cant human effort. F urthermore, in\norder for a user to effectively discover web pages classi\ufb01ed into the nodes of\nthe taxonomy tree, the user\u2019s idea of what sub-tree(s) to see k for a particu-\nlar topic should match that of the editors performing the cla ssi\ufb01cation. This\nquickly becomes challenging as the size of the taxonomy grow s; the Yahoo!\ntaxonomy tree surpassed 1000 distinct nodes fairly early on . Given these\nchallenges, the popularity of taxonomies declined over tim e, even though\nvariants (such as About.com and the Open Directory Project) sprang up with\nsubject-matter experts collecting and annotating web page s for each cate-\ngory.\nThe \ufb01rst generation of web search engines transported class ical search\ntechniques such as those in the preceding chapters to the web domain, focus-\ning on the challenge of scale. The earliest web search engine s had to contend\nwith indexes containing tens of millions of documents, whic h was a few or-\nders of magnitude larger than any prior information retriev al system in the\npublic domain. Indexing, query serving and ranking at this s cale required\nthe harnessing together of tens of machines to create highly available sys-\ntems, again at scales not witnessed hitherto in a consumer-f acing search ap-\nplication. The \ufb01rst generation of web search engines was lar gely successful\nat solving these challenges while continually indexing a si gni\ufb01cant fraction\nof the Web, all the while serving queries with sub-second res ponse times.\nHowever, the quality and relevance of web search results lef t much to be\ndesired owing to the idiosyncrasies of content creation on t he Web that we\ndiscuss in Section 19.2. This necessitated the invention of new ranking and\nspam-\ufb01ghting techniques in order to ensure the quality of th e search results.\nWhile classical information retrieval techniques (such as those covered ear-\nlier in this book) continue to be necessary for web search, th ey are not by\nany means suf\ufb01cient. A key aspect (developed further in Chap ter21) is that\nwhereas classical techniques measure the relevance of a doc ument to a query,\nthere remains a need to gauge the authoritativeness of a document based on\ncues such as which website hosts it.\n19.2 Web characteristics\nThe essential feature that led to the explosive growth of the web \u2013 decentral-\nized content publishing with essentially no central contro l of authorship \u2013\nturned out to be the biggest challenge for web search engines in their quest to\nindex and retrieve this content. Web page authors created co ntent in dozens\nof (natural) languages and thousands of dialects, thus dema nding many dif-\nferent forms of stemming and other linguistic operations. B ecause publish-\nOnline edition (c)\n2009 Cambridge UP424 19 Web search basics\ning was now open to tens of millions, web pages exhibited hete rogeneity at a\ndaunting scale, in many crucial aspects. First, content-cr eation was no longer\nthe privy of editorially-trained writers; while this repre sented a tremendous\ndemocratization of content creation, it also resulted in a t remendous varia-\ntion in grammar and style (and in many cases, no recognizable grammar or\nstyle). Indeed, web publishing in a sense unleashed the best and worst of\ndesktop publishing on a planetary scale, so that pages quick ly became rid-\ndled with wild variations in colors, fonts and structure. So me web pages,\nincluding the professionally created home pages of some lar ge corporations,\nconsisted entirely of images (which, when clicked, led to ri cher textual con-\ntent) \u2013 and therefore, no indexable text.\nWhat about the substance of the text in web pages? The democra tization\nof content creation on the web meant a new level of granularit y in opinion on\nvirtually any subject. This meant that the web contained tru th, lies, contra-\ndictions and suppositions on a grand scale. This gives rise t o the question:\nwhich web pages does one trust? In a simplistic approach, one might argue\nthat some publishers are trustworthy and others not \u2013 beggin g the question\nof how a search engine is to assign such a measure of trust to ea ch website\nor web page. In Chapter 21we will examine approaches to understanding\nthis question. More subtly, there may be no universal, user- independent no-\ntion of trust; a web page whose contents are trustworthy to on e user may\nnot be so to another. In traditional (non-web) publishing th is is not an issue:\nusers self-select sources they \ufb01nd trustworthy. Thus one re ader may \ufb01nd\nthe reporting of The New York Times to be reliable, while another may prefer\nThe Wall Street Journal . But when a search engine is the only viable means\nfor a user to become aware of (let alone select) most content, this challenge\nbecomes signi\ufb01cant.\nWhile the question \u201chow big is the Web?\u201d has no easy answer (se e Sec-\ntion 19.5), the question \u201chow many web pages are in a search engine\u2019s in dex\u201d\nis more precise, although, even this question has issues. By the end of 1995,\nAltavista reported that it had crawled and indexed approxim ately 30 million\nstatic web pages . Static web pages are those whose content does not vary from STATIC WEB PAGES\none request for that page to the next. For this purpose, a prof essor who man-\nually updates his home page every week is considered to have a static web\npage, but an airport\u2019s \ufb02ight status page is considered to be d ynamic. Dy-\nnamic pages are typically mechanically generated by an appl ication server\nin response to a query to a database, as show in Figure 19.1. One sign of\nsuch a page is that the URL has the character \"?\" in it. Since th e number\nof static web pages was believed to be doubling every few mont hs in 1995,\nearly web search engines such as Altavista had to constantly add hardware\nand bandwidth for crawling and indexing web pages.\nOnline edition (c)\n2009 Cambridge UP19.2 Web characteristics 425\n\u25eeFigure 19.1 A dynamically generated web page. The browser sends a reques t for\n\ufb02ight information on \ufb02ight AA129 to the web application, tha t fetches the informa-\ntion from back-end databases then creates a dynamic web page that it returns to the\nbrowser.\n&%'$\n&%'$\n-anchor\n\u25eeFigure 19.2 Two nodes of the web graph joined by a link.\n19.2.1 The web graph\nWe can view the static Web consisting of static HTML pages tog ether with\nthe hyperlinks between them as a directed graph in which each web page is\na node and each hyperlink a directed edge.\nFigure 19.2 shows two nodes A and B from the web graph, each corre-\nsponding to a web page, with a hyperlink from A to B. We refer to the set of\nall such nodes and directed edges as the web graph. Figure 19.2 also shows\nthat (as is the case with most links on web pages) there is some text surround-\ning the origin of the hyperlink on page A. This text is general ly encapsulated\nin thehref attribute of the <a> (for anchor) tag that encodes the hyperl ink\nin the HTML code of page A, and is referred to as anchor text . As one might ANCHOR TEXT\nsuspect, this directed graph is not strongly connected : there are pairs of pages\nsuch that one cannot proceed from one page of the pair to the ot her by follow-\ning hyperlinks. We refer to the hyperlinks into a page as in-links and those IN-LINKS\nout of a page as out-links . The number of in-links to a page (also known as OUT -LINKS\nitsin-degree ) has averaged from roughly 8 to 15, in a range of studies. We\nsimilarly de\ufb01ne the out-degree of a web page to be the number o f links out\nOnline edition (c)\n2009 Cambridge UP426 19 Web search basics\n\u25eeFigure 19.3 A sample small web graph. In this example we have six pages lab eled\nA-F. Page B has in-degree 3 and out-degree 1. This example gra ph is not strongly\nconnected: there is no path from any of pages B-F to page A.\nof it. These notions are represented in Figure 19.3.\nThere is ample evidence that these links are not randomly dis tributed; for\none thing, the distribution of the number of links into a web p age does not\nfollow the Poisson distribution one would expect if every we b page were\nto pick the destinations of its links uniformly at random. Ra ther, this dis-\ntribution is widely reported to be a power law , in which the total number of POWER LAW\nweb pages with in-degree iis proportional to 1/ i\u03b1; the value of \u03b1typically\nreported by studies is 2.1.1Furthermore, several studies have suggested that\nthe directed graph connecting web pages has a bowtie shape: there are three BOWTIE\nmajor categories of web pages that are sometimes referred to as IN, OUT\nand SCC. A web surfer can pass from any page in IN to any page in S CC, by\nfollowing hyperlinks. Likewise, a surfer can pass from page in SCC to any\npage in OUT. Finally, the surfer can surf from any page in SCC t o any other\npage in SCC. However, it is not possible to pass from a page in S CC to any\npage in IN, or from a page in OUT to a page in SCC (or, consequent ly, IN).\nNotably, in several studies IN and OUT are roughly equal in si ze, whereas\n1. Cf. Zipf\u2019s law of the distribution of words in text in Chapt er5(page 90), which is a power\nlaw with \u03b1=1.\nOnline edition (c)\n2009 Cambridge UP19.2 Web characteristics 427\n\u25eeFigure 19.4 The bowtie structure of the Web. Here we show one tube and thre e\ntendrils.\nSCC is somewhat larger; most web pages fall into one of these t hree sets. The\nremaining pages form into tubes that are small sets of pages outside SCC that\nlead directly from IN to OUT, and tendrils that either lead nowhere from IN,\nor from nowhere to OUT. Figure 19.4 illustrates this structure of the Web.\n19.2.2 Spam\nEarly in the history of web search, it became clear that web se arch engines\nwere an important means for connecting advertisers to prosp ective buyers.\nA user searching for maui golf real estate is not merely seeking news or en-\ntertainment on the subject of housing on golf courses on the i sland of Maui,\nbut instead likely to be seeking to purchase such a property. Sellers of such\nproperty and their agents, therefore, have a strong incenti ve to create web\npages that rank highly on this query. In a search engine whose scoring was\nbased on term frequencies, a web page with numerous repetiti ons ofmauigolf\nreal estate would rank highly. This led to the \ufb01rst generation of spam , which SPAM\n(in the context of web search) is the manipulation of web page content for\nthe purpose of appearing high up in search results for select ed keywords.\nTo avoid irritating users with these repetitions, sophisti cated spammers re-\nsorted to such tricks as rendering these repeated terms in th e same color as\nthe background. Despite these words being consequently inv isible to the hu-\nman user, a search engine indexer would parse the invisible w ords out of\nOnline edition (c)\n2009 Cambridge UP428 19 Web search basics\n\u25eeFigure 19.5 Cloaking as used by spammers.\nthe HTML representation of the web page and index these words as being\npresent in the page.\nAt its root, spam stems from the heterogeneity of motives in c ontent cre-\nation on the Web. In particular, many web content creators ha ve commercial\nmotives and therefore stand to gain from manipulating searc h engine results.\nYou might argue that this is no different from a company that u ses large fonts\nto list its phone numbers in the yellow pages; but this genera lly costs the\ncompany more and is thus a fairer mechanism. A more apt analog y, perhaps,\nis the use of company names beginning with a long string of A\u2019s to be listed\nearly in a yellow pages category. In fact, the yellow pages\u2019 m odel of com-\npanies paying for larger/darker fonts has been replicated i n web search: in\nmany search engines, it is possible to pay to have one\u2019s web pa ge included\nin the search engine\u2019s index \u2013 a model known as paid inclusion . Different PAID INCLUSION\nsearch engines have different policies on whether to allow p aid inclusion,\nand whether such a payment has any effect on ranking in search results.\nSearch engines soon became sophisticated enough in their sp am detection\nto screen out a large number of repetitions of particular key words. Spam-\nmers responded with a richer set of spam techniques, the best known of\nwhich we now describe. The \ufb01rst of these techniques is cloaking , shown in\nFigure 19.5. Here, the spammer\u2019s web server returns different pages dep end-\ning on whether the http request comes from a web search engine \u2019s crawler\n(the part of the search engine that gathers web pages, to be de scribed in\nChapter 20), or from a human user\u2019s browser. The former causes the web\npage to be indexed by the search engine under misleading keyw ords. When\nthe user searches for these keywords and elects to view the pa ge, he receives\na web page that has altogether different content than that in dexed by the\nsearch engine. Such deception of search indexers is unknown in the tra-\nditional world of information retrieval; it stems from the f act that the rela-\ntionship between page publishers and web search engines is n ot completely\ncollaborative.\nAdoorway page contains text and metadata carefully chosen to rank highly\nOnline edition (c)\n2009 Cambridge UP19.3 Advertising as the economic model 429\non selected search keywords. When a browser requests the doo rway page, it\nis redirected to a page containing content of a more commerci al nature. More\ncomplex spamming techniques involve manipulation of the me tadata related\nto a page including (for reasons we will see in Chapter 21) the links into a\nweb page. Given that spamming is inherently an economically motivated\nactivity, there has sprung around it an industry of Search Engine Optimizers , SEARCH ENGINE\nOPTIMIZERS or SEOs to provide consultancy services for clients who seek to have their\nweb pages rank highly on selected keywords. Web search engin es frown on\nthis business of attempting to decipher and adapt to their pr oprietary rank-\ning techniques and indeed announce policies on forms of SEO b ehavior they\ndo not tolerate (and have been known to shut down search reque sts from cer-\ntain SEOs for violation of these). Inevitably, the parrying between such SEOs\n(who gradually infer features of each web search engine\u2019s ra nking methods)\nand the web search engines (who adapt in response) is an unend ing struggle;\nindeed, the research sub-area of adversarial information retrieval has sprung up ADVERSARIAL\nINFORMATION\nRETRIEVALaround this battle. To combat spammers who manipulate the te xt of their\nweb pages is the exploitation of the link structure of the Web \u2013 a technique\nknown as link analysis . The \ufb01rst web search engine known to apply link anal-\nysis on a large scale (to be detailed in Chapter 21) was Google, although all\nweb search engines currently make use of it (and correspondi ngly, spam-\nmers now invest considerable effort in subverting it \u2013 this i s known as link LINK SPAM\nspam ).\n?Exercise 19.1\nIf the number of pages with in-degree iis proportional to 1/ i2.1, what is the probabil-\nity that a randomly chosen web page has in-degree 1?\nExercise 19.2\nIf the number of pages with in-degree iis proportional to 1/ i2.1, what is the average\nin-degree of a web page?\nExercise 19.3\nIf the number of pages with in-degree iis proportional to 1/ i2.1, then as the largest\nin-degree goes to in\ufb01nity, does the fraction of pages with in -degree igrow, stay the\nsame, or diminish? How would your answer change for values of the exponent other\nthan 2.1?\nExercise 19.4\nThe average in-degree of all nodes in a snapshot of the web gra ph is 9. What can we\nsay about the average out-degree of all nodes in this snapsho t?\n19.3 Advertising as the economic model\nEarly in the history of the Web, companies used graphical ban ner advertise-\nments on web pages at popular websites (news and entertainme nt sites such\nas MSN, America Online, Yahoo! and CNN). The primary purpose of these\nadvertisements was branding : to convey to the viewer a positive feeling about\nOnline edition (c)\n2009 Cambridge UP430 19 Web search basics\nthe brand of the company placing the advertisement. Typical ly these adver-\ntisements are priced on a cost per mil (CPM ) basis: the cost to the company of CPM\nhaving its banner advertisement displayed 1000 times. Some websites struck\ncontracts with their advertisers in which an advertisement was priced not by\nthe number of times it is displayed (also known as impressions ), but rather\nby the number of times it was clicked on by the user. This pricing model is\nknown as the cost per click (CPC ) model. In such cases, clicking on the adver- CPC\ntisement leads the user to a web page set up by the advertiser, where the user\nis induced to make a purchase. Here the goal of the advertisem ent is not so\nmuch brand promotion as to induce a transaction. This distin ction between\nbrand and transaction-oriented advertising was already wi dely recognized\nin the context of conventional media such as broadcast and pr int. The inter-\nactivity of the web allowed the CPC billing model \u2013 clicks cou ld be metered\nand monitored by the website and billed to the advertiser.\nThe pioneer in this direction was a company named Goto, which changed\nits name to Overture prior to eventual acquisition by Yahoo! Goto was not,\nin the traditional sense, a search engine; rather, for every query term qit ac-\ncepted bidsfrom companies who wanted their web page shown on the query\nq. In response to the query q, Goto would return the pages of all advertisers\nwho bid for q, ordered by their bids. Furthermore, when the user clicked\non one of the returned results, the corresponding advertise r would make a\npayment to Goto (in the initial implementation, this paymen t equaled the\nadvertiser\u2019s bid for q).\nSeveral aspects of Goto\u2019s model are worth highlighting. Fir st, a user typing\nthe query qinto Goto\u2019s search interface was actively expressing an int erest\nand intent related to the query q. For instance, a user typing golfclubs is more\nlikely to be imminently purchasing a set than one who is simpl y browsing\nnews on golf. Second, Goto only got compensated when a user ac tually ex-\npressed interest in an advertisement \u2013 as evinced by the user clicking the ad-\nvertisement. Taken together, these created a powerful mech anism by which\nto connect advertisers to consumers, quickly raising the an nual revenues of\nGoto/Overture into hundreds of millions of dollars. This st yle of search en-\ngine came to be known variously as sponsored search orsearch advertising . SPONSORED SEARCH\nSEARCH ADVERTISING Given these two kinds of search engines \u2013 the \u201cpure\u201d search en gines such\nas Google and Altavista, versus the sponsored search engine s \u2013 the logi-\ncal next step was to combine them into a single user experienc e. Current\nsearch engines follow precisely this model: they provide pu re search results\n(generally known as algorithmic search results) as the primary response to a ALGORITHMIC SEARCH\nuser\u2019s search, together with sponsored search results disp layed separately\nand distinctively to the right of the algorithmic results. T his is shown in Fig-\nure19.6. Retrieving sponsored search results and ranking them in re sponse\nto a query has now become considerably more sophisticated th an the sim-\nple Goto scheme; the process entails a blending of ideas from information\nOnline edition (c)\n2009 Cambridge UP19.3 Advertising as the economic model 431\n\u25eeFigure 19.6 Search advertising triggered by query keywords. Here the qu eryA320\nreturns algorithmic search results about the Airbus aircra ft, together with advertise-\nments for various non-aircraft goods numbered A320, that ad vertisers seek to market\nto those querying on this query. The lack of advertisements f or the aircraft re\ufb02ects the\nfact that few marketers attempt to sell A320 aircraft on the w eb.\nretrieval and microeconomics, and is beyond the scope of thi s book. For\nadvertisers, understanding how search engines do this rank ing and how to\nallocate marketing campaign budgets to different keywords and to different\nsponsored search engines has become a profession known as search engine SEARCH ENGINE\nMARKETING marketing (SEM).\nThe inherently economic motives underlying sponsored sear ch give rise\nto attempts by some participants to subvert the system to the ir advantage.\nThis can take many forms, one of which is known as click spam . There is CLICK SPAM\ncurrently no universally accepted de\ufb01nition of click spam. It refers (as the\nname suggests) to clicks on sponsored search results that ar e not from bona\n\ufb01de search users. For instance, a devious advertiser may att empt to exhaust\nthe advertising budget of a competitor by clicking repeated ly (through the\nuse of a robotic click generator) on that competitor\u2019s spons ored search ad-\nvertisements. Search engines face the challenge of discern ing which of the\nclicks they observe are part of a pattern of click spam, to avo id charging their\nadvertiser clients for such clicks.\n?Exercise 19.5\nThe Goto method ranked advertisements matching a query by bid: the highest-bidding\nadvertiser got the top position, the second-highest the nex t, and so on. What can go\nwrong with this when the highest-bidding advertiser places an advertisement that is\nirrelevant to the query? Why might an advertiser with an irre levant advertisement\nbid high in this manner?\nExercise 19.6\nSuppose that, in addition to bids, we had for each advertiser their click-through rate :\nthe ratio of the historical number of times users click on the ir advertisement to the\nnumber of times the advertisement was shown. Suggest a modi\ufb01 cation of the Goto\nscheme that exploits this data to avoid the problem in Exerci se19.5 above.\nOnline edition (c)\n2009 Cambridge UP432 19 Web search basics\n19.4 The search user experience\nIt is crucial that we understand the users of web search as wel l. This is\nagain a signi\ufb01cant change from traditional information ret rieval, where users\nwere typically professionals with at least some training in the art of phrasing\nqueries over a well-authored collection whose style and str ucture they un-\nderstood well. In contrast, web search users tend to not know (or care) about\nthe heterogeneity of web content, the syntax of query langua ges and the art\nof phrasing queries; indeed, a mainstream tool (as web searc h has come to\nbecome) should not place such onerous demands on billions of people. A\nrange of studies has concluded that the average number of key words in a\nweb search is somewhere between 2 and 3. Syntax operators (Bo olean con-\nnectives, wildcards, etc.) are seldom used, again a result o f the composition\nof the audience \u2013 \u201cnormal\u201d people, not information scientis ts.\nIt is clear that the more user traf\ufb01c a web search engine can at tract, the\nmore revenue it stands to earn from sponsored search. How do s earch en-\ngines differentiate themselves and grow their traf\ufb01c? Here Google identi\ufb01ed\ntwo principles that helped it grow at the expense of its compe titors: (1) a\nfocus on relevance, speci\ufb01cally precision rather than reca ll in the \ufb01rst few re-\nsults; (2) a user experience that is lightweight, meaning th at both the search\nquery page and the search results page are uncluttered and al most entirely\ntextual, with very few graphical elements. The effect of the \ufb01rst was simply\nto save users time in locating the information they sought. T he effect of the\nsecond is to provide a user experience that is extremely resp onsive, or at any\nrate not bottlenecked by the time to load the search query or r esults page.\n19.4.1 User query needs\nThere appear to be three broad categories into which common w eb search\nqueries can be grouped: (i) informational, (ii) navigation al and (iii) transac-\ntional. We now explain these categories; it should be clear t hat some queries\nwill fall in more than one of these categories, while others w ill fall outside\nthem.\nInformational queries seek general information on a broad topic, such as INFORMATIONAL\nQUERIES leukemia or Provence. There is typically not a single web pag e that con-\ntains all the information sought; indeed, users with inform ational queries\ntypically try to assimilate information from multiple web p ages.\nNavigational queries seek the website or home page of a single entity that the NAVIGATIONAL\nQUERIES user has in mind, say Lufthansa airlines . In such cases, the user\u2019s expectation\nis that the very \ufb01rst search result should be the home page of L ufthansa.\nThe user is not interested in a plethora of documents contain ing the term\nLufthansa ; for such a user, the best measure of user satisfaction is pre cision at\n1.\nOnline edition (c)\n2009 Cambridge UP19.5 Index size and estimation 433\nAtransactional query is one that is a prelude to the user performing a trans- TRANSACTIONAL\nQUERY action on the Web \u2013 such as purchasing a product, downloading a \ufb01le or\nmaking a reservation. In such cases, the search engine shoul d return results\nlisting services that provide form interfaces for such tran sactions.\nDiscerning which of these categories a query falls into can b e challeng-\ning. The category not only governs the algorithmic search re sults, but the\nsuitability of the query for sponsored search results (sinc e the query may re-\nveal an intent to purchase). For navigational queries, some have argued that\nthe search engine should return only a single result or even t he target web\npage directly. Nevertheless, web search engines have histo rically engaged in\na battle of bragging rights over which one indexes more web pa ges. Does\nthe user really care? Perhaps not, but the media does highlig ht estimates\n(often statistically indefensible) of the sizes of various search engines. Users\nare in\ufb02uenced by these reports and thus, search engines do ha ve to pay at-\ntention to how their index sizes compare to competitors\u2019. Fo r informational\n(and to a lesser extent, transactional) queries, the user do es care about the\ncomprehensiveness of the search engine.\nFigure 19.7 shows a composite picture of a web search engine including\nthe crawler, as well as both the web page and advertisement in dexes. The\nportion of the \ufb01gure under the curved dashed line is internal to the search\nengine.\n19.5 Index size and estimation\nTo a \ufb01rst approximation, comprehensiveness grows with inde x size, although\nit does matter which speci\ufb01c pages a search engine indexes \u2013 s ome pages are\nmore informative than others. It is also dif\ufb01cult to reason a bout the fraction\nof the Web indexed by a search engine, because there is an in\ufb01n ite number of\ndynamic web pages; for instance, http://www.yahoo.com/any_string\nreturns a valid HTML page rather than an error, politely info rming the user\nthat there is no such page at Yahoo! Such a \"soft 404 error\" is o nly one exam-\nple of many ways in which web servers can generate an in\ufb01nite n umber of\nvalid web pages. Indeed, some of these are malicious spider t raps devised\nto cause a search engine\u2019s crawler (the component that syste matically gath-\ners web pages for the search engine\u2019s index, described in Cha pter 20) to stay\nwithin a spammer\u2019s website and index many pages from that sit e.\nWe could ask the following better-de\ufb01ned question: given tw o search en-\ngines, what are the relative sizes of their indexes? Even thi s question turns\nout to be imprecise, because:\n1.In response to queries a search engine can return web pages wh ose con-\ntents it has not (fully or even partially) indexed. For one th ing, search\nengines generally index only the \ufb01rst few thousand words in a web page.\nOnline edition (c)\n2009 Cambridge UP434 19 Web search basicsT h e W e b\nA d i n d e x e s\nW e b c r a w l e rI n d e x e r\nI n d e x e s\nS e a r c h\nU s e r\n\u25eeFigure 19.7 The various components of a web search engine.\nIn some cases, a search engine is aware of a page pthat is linked to by pages\nit has indexed, but has not indexed pitself. As we will see in Chapter 21,\nit is still possible to meaningfully return pin search results.\n2.Search engines generally organize their indexes in various tiers and parti-\ntions, not all of which are examined on every search (recall t iered indexes\nfrom Section 7.2.1 ). For instance, a web page deep inside a website may be\nindexed but not retrieved on general web searches; it is howe ver retrieved\nas a result on a search that a user has explicitly restricted t o that website\n(such site-speci\ufb01c search is offered by most web search engi nes).\nThus, search engine indexes include multiple classes of ind exed pages, so\nthat there is no single measure of index size. These issues no twithstanding,\na number of techniques have been devised for crude estimates of the ratio of\nthe index sizes of two search engines, E1and E2. The basic hypothesis under-\nlying these techniques is that each search engine indexes a f raction of the Web\nchosen independently and uniformly at random. This involve s some ques-\ntionable assumptions: \ufb01rst, that there is a \ufb01nite size for th e Web from which\neach search engine chooses a subset, and second, that each en gine chooses\nan independent, uniformly chosen subset. As will be clear fr om the discus-\nsion of crawling in Chapter 20, this is far from true. However, if we begin\nOnline edition (c)\n2009 Cambridge UP19.5 Index size and estimation 435\nwith these assumptions, then we can invoke a classical estim ation technique\nknown as the capture-recapture method . CAPTURE -RECAPTURE\nMETHOD Suppose that we could pick a random page from the index of E1and test\nwhether it is in E2\u2019s index and symmetrically, test whether a random page\nfrom E2is in E1. These experiments give us fractions xand ysuch that our\nestimate is that a fraction xof the pages in E1are in E2, while a fraction yof\nthe pages in E2are in E1. Then, letting|Ei|denote the size of the index of\nsearch engine Ei, we have\nx|E1|\u2248y|E2|,\nfrom which we have the form we will use\n|E1|\n|E2|\u2248y\nx. (19.1)\nIf our assumption about E1and E2being independent and uniform random\nsubsets of the Web were true, and our sampling process unbias ed, then Equa-\ntion ( 19.1) should give us an unbiased estimator for |E1|/|E2|. We distinguish\nbetween two scenarios here. Either the measurement is perfo rmed by some-\none with access to the index of one of the search engines (say a n employee of\nE1), or the measurement is performed by an independent party wi th no ac-\ncess to the innards of either search engine. In the former cas e, we can simply\npick a random document from one index. The latter case is more challeng-\ning; by picking a random page from one search engine from outside the search\nengine , then verify whether the random page is present in the other s earch\nengine.\nTo implement the sampling phase, we might generate a random p age from\nthe entire (idealized, \ufb01nite) Web and test it for presence in each search engine.\nUnfortunately, picking a web page uniformly at random is a di f\ufb01cult prob-\nlem. We brie\ufb02y outline several attempts to achieve such a sam ple, pointing\nout the biases inherent to each; following this we describe i n some detail one\ntechnique that much research has built on.\n1.Random searches: Begin with a search log of web searches; send a random\nsearch from this log to E1and a random page from the results. Since such\nlogs are not widely available outside a search engine, one im plementation\nis to trap all search queries going out of a work group (say sci entists in a\nresearch center) that agrees to have all its searches logged . This approach\nhas a number of issues, including the bias from the types of se arches made\nby the work group. Further, a random document from the result s of such\na random search to E1is not the same as a random document from E1.\n2.Random IP addresses: A second approach is to generate random IP ad-\ndresses and send a request to a web server residing at the rand om ad-\ndress, collecting all pages at that server. The biases here i nclude the fact\nOnline edition (c)\n2009 Cambridge UP436 19 Web search basics\nthat many hosts might share one IP (due to a practice known as v irtual\nhosting) or not accept http requests from the host where the e xperiment\nis conducted. Furthermore, this technique is more likely to hit one of the\nmany sites with few pages, skewing the document probabiliti es; we may\nbe able to correct for this effect if we understand the distri bution of the\nnumber of pages on websites.\n3.Random walks: If the web graph were a strongly connected directed graph,\nwe could run a random walk starting at an arbitrary web page. T his\nwalk would converge to a steady state distribution (see Chap ter21, Sec-\ntion 21.2.1 for more background material on this), from which we could in\nprinciple pick a web page with a \ufb01xed probability. This metho d, too has\na number of biases. First, the Web is not strongly connected s o that, even\nwith various corrective rules, it is dif\ufb01cult to argue that w e can reach a\nsteady state distribution starting from any page. Second, t he time it takes\nfor the random walk to settle into this steady state is unknow n and could\nexceed the length of the experiment.\nClearly each of these approaches is far from perfect. We now d escribe a\nfourth sampling approach, random queries . This approach is noteworthy for\ntwo reasons: it has been successfully built upon for a series of increasingly\nre\ufb01ned estimates, and conversely it has turned out to be the a pproach most\nlikely to be misinterpreted and carelessly implemented, le ading to mislead-\ning measurements. The idea is to pick a page (almost) uniform ly at random\nfrom a search engine\u2019s index by posing a random query to it. It should be\nclear that picking a set of random terms from (say) Webster\u2019s dictionary is\nnot a good way of implementing this idea. For one thing, not al l vocabulary\nterms occur equally often, so this approach will not result i n documents be-\ning chosen uniformly at random from the search engine. For an other, there\nare a great many terms in web documents that do not occur in a st andard\ndictionary such as Webster\u2019s. To address the problem of voca bulary terms\nnot in a standard dictionary, we begin by amassing a sample we b dictionary.\nThis could be done by crawling a limited portion of the Web, or by crawling a\nmanually-assembled representative subset of the Web such a s Yahoo! (as was\ndone in the earliest experiments with this method). Conside r a conjunctive\nquery with two or more randomly chosen words from this dictio nary.\nOperationally, we proceed as follows: we use a random conjun ctive query\nonE1and pick from the top 100 returned results a page pat random. We\nthen test pfor presence in E2by choosing 6-8 low-frequency terms in pand\nusing them in a conjunctive query for E2. We can improve the estimate by\nrepeating the experiment a large number of times. Both the sa mpling process\nand the testing process have a number of issues.\n1.Our sample is biased towards longer documents.\nOnline edition (c)\n2009 Cambridge UP19.6 Near-duplicates and shingling 437\n2.Picking from the top 100 results of E1induces a bias from the ranking\nalgorithm of E1. Picking from all the results of E1makes the experiment\nslower. This is particularly so because most web search engi nes put up\ndefenses against excessive robotic querying.\n3.During the checking phase, a number of additional biases are introduced:\nfor instance, E2may not handle 8-word conjunctive queries properly.\n4.Either E1orE2may refuse to respond to the test queries, treating them as\nrobotic spam rather than as bona \ufb01de queries.\n5.There could be operational problems like connection time-o uts.\nA sequence of research has built on this basic paradigm to eli minate some\nof these issues; there is no perfect solution yet, but the lev el of sophistica-\ntion in statistics for understanding the biases is increasi ng. The main idea\nis to address biases by estimating, for each document, the ma gnitude of the\nbias. From this, standard statistical sampling methods can generate unbi-\nased samples. In the checking phase, the newer work moves awa y from\nconjunctive queries to phrase and other queries that appear to be better-\nbehaved. Finally, newer experiments use other sampling met hods besides\nrandom queries. The best known of these is document random walk sampling ,\nin which a document is chosen by a random walk on a virtual grap h de-\nrived from documents. In this graph, nodes are documents; tw o documents\nare connected by an edge if they share two or more words in comm on. The\ngraph is never instantiated; rather, a random walk on it can b e performed by\nmoving from a document dto another by picking a pair of keywords in d,\nrunning a query on a search engine and picking a random docume nt from\nthe results. Details may be found in the references in Sectio n19.7.\n?Exercise 19.7\nTwo web search engines A and B each generate a large number of p ages uniformly at\nrandom from their indexes. 30% of A\u2019s pages are present in B\u2019s index, while 50% of\nB\u2019s pages are present in A\u2019s index. What is the number of pages in A\u2019s index relative\nto B\u2019s?\n19.6 Near-duplicates and shingling\nOne aspect we have ignored in the discussion of index size in S ection 19.5 is\nduplication : the Web contains multiple copies of the same content. By som e\nestimates, as many as 40% of the pages on the Web are duplicate s of other\npages. Many of these are legitimate copies; for instance, ce rtain information\nrepositories are mirrored simply to provide redundancy and access reliabil-\nity. Search engines try to avoid indexing multiple copies of the same content,\nto keep down storage and processing overheads.\nOnline edition (c)\n2009 Cambridge UP438 19 Web search basics\nThe simplest approach to detecting duplicates is to compute , for each web\npage, a \ufb01ngerprint that is a succinct (say 64-bit) digest of the characters on th at\npage. Then, whenever the \ufb01ngerprints of two web pages are equ al, we test\nwhether the pages themselves are equal and if so declare one o f them to be a\nduplicate copy of the other. This simplistic approach fails to capture a crucial\nand widespread phenomenon on the Web: near duplication . In many cases,\nthe contents of one web page are identical to those of another except for a\nfew characters \u2013 say, a notation showing the date and time at w hich the page\nwas last modi\ufb01ed. Even in such cases, we want to be able to decl are the two\npages to be close enough that we only index one copy. Short of e xhaustively\ncomparing all pairs of web pages, an infeasible task at the sc ale of billions of\npages, how can we detect and \ufb01lter out such near duplicates?\nWe now describe a solution to the problem of detecting near-d uplicate web\npages. The answer lies in a technique known as shingling . Given a positive SHINGLING\ninteger kand a sequence of terms in a document d, de\ufb01ne the k-shingles of\ndto be the set of all consecutive sequences of kterms in d. As an example,\nconsider the following text: aroseisaroseisarose . The 4-shingles for this text\n(k=4 is a typical value used in the detection of near-duplicate w eb pages)\narea rose is a ,rose is a rose andis a rose is . The \ufb01rst two of these shingles\neach occur twice in the text. Intuitively, two documents are near duplicates if\nthe sets of shingles generated from them are nearly the same. We now make\nthis intuition precise, then develop a method for ef\ufb01cientl y computing and\ncomparing the sets of shingles for all web pages.\nLetS(dj)denote the set of shingles of document dj. Recall the Jaccard\ncoef\ufb01cient from page 61, which measures the degree of overlap between\nthe sets S(d1)and S(d2)as|S(d1)\u2229S(d2)|/|S(d1)\u222aS(d2)|; denote this by\nJ(S(d1),S(d2)). Our test for near duplication between d1and d2is to com-\npute this Jaccard coef\ufb01cient; if it exceeds a preset thresho ld (say, 0.9), we\ndeclare them near duplicates and eliminate one from indexin g. However,\nthis does not appear to have simpli\ufb01ed matters: we still have to compute\nJaccard coef\ufb01cients pairwise.\nTo avoid this, we use a form of hashing. First, we map every shi ngle into\na hash value over a large space, say 64 bits. For j=1, 2, let H(dj)be the\ncorresponding set of 64-bit hash values derived from S(dj). We now invoke\nthe following trick to detect document pairs whose sets H()have large Jac-\ncard overlaps. Let \u03c0be a random permutation from the 64-bit integers to the\n64-bit integers. Denote by \u03a0(dj)the set of permuted hash values in H(dj);\nthus for each h\u2208H(dj), there is a corresponding value \u03c0(h)\u2208\u03a0(dj).\nLetx\u03c0\njbe the smallest integer in \u03a0(dj). Then\nTheorem 19.1.\nJ(S(d1),S(d2)) = P(x\u03c0\n1=x\u03c0\n2).\nOnline edition (c)\n2009 Cambridge UP19.6 Near-duplicates and shingling 439\n----\n----\n0000\n0000\n264\u22121264\u22121264\u22121264\u22121\n264\u22121264\u22121264\u22121264\u22121\nDocument 1 Document 2H(d1) H(d2)\nu\n1u\n1u\n2u\n2u\n3u\n3u\n4u\n4\nH(d1)and \u03a0(d1) H(d2)and \u03a0(d2)\nu u u u u u u u3 3 1 1 4 4 2 2\n3 3 1 1 4 4 2 2\n3 3\u03a0(d1) \u03a0(d2)\nx\u03c0\n1x\u03c0\n2\n\u25eeFigure 19.8 Illustration of shingle sketches. We see two documents goin g through\nfour stages of shingle sketch computation. In the \ufb01rst step ( top row), we apply a 64-bit\nhash to each shingle from each document to obtain H(d1)and H(d2)(circles). Next,\nwe apply a random permutation \u03a0to permute H(d1)and H(d2), obtaining \u03a0(d1)\nand \u03a0(d2)(squares). The third row shows only \u03a0(d1)and \u03a0(d2), while the bottom\nrow shows the minimum values x\u03c0\n1and x\u03c0\n2for each document.\nProof. We give the proof in a slightly more general setting: conside r a family\nof sets whose elements are drawn from a common universe. View the sets\nas columns of a matrix A, with one row for each element in the universe.\nThe element aij=1 if element iis present in the set Sjthat the jth column\nrepresents.\nLet\u03a0be a random permutation of the rows of A; denote by \u03a0(Sj)the\ncolumn that results from applying \u03a0to the jth column. Finally, let x\u03c0\njbe the\nindex of the \ufb01rst row in which the column \u03a0(Sj)has a 1. We then prove that\nfor any two columns j1,j2,\nP(x\u03c0\nj1=x\u03c0\nj2) = J(Sj1,Sj2).\nIf we can prove this, the theorem follows.\nConsider two columns j1,j2as shown in Figure 19.9. The ordered pairs of\nentries of Sj1and Sj2partition the rows into four types: those with 0\u2019s in both\nof these columns, those with a 0 in Sj1and a 1 in Sj2, those with a 1 in Sj1\nand a 0 in Sj2, and \ufb01nally those with 1\u2019s in both of these columns. Indeed,\nthe \ufb01rst four rows of Figure 19.9 exemplify all of these four types of rows.\nOnline edition (c)\n2009 Cambridge UP440 19 Web search basics\nSj1Sj2\n0 1\n1 0\n1 1\n0 0\n1 1\n0 1\n\u25eeFigure 19.9 Two sets Sj1and Sj2; their Jaccard coef\ufb01cient is 2/5.\nDenote by C00the number of rows with 0\u2019s in both columns, C01the second,\nC10the third and C11the fourth. Then,\nJ(Sj1,Sj2) =C11\nC01+C10+C11. (19.2)\nTo complete the proof by showing that the right-hand side of E quation ( 19.2)\nequals P(x\u03c0\nj1=x\u03c0\nj2), consider scanning columns j1,j2in increasing row in-\ndex until the \ufb01rst non-zero entry is found in either column. B ecause \u03a0is a\nrandom permutation, the probability that this smallest row has a 1 in both\ncolumns is exactly the right-hand side of Equation ( 19.2).\nThus, our test for the Jaccard coef\ufb01cient of the shingle sets is probabilis-\ntic: we compare the computed values x\u03c0\nifrom different documents. If a pair\ncoincides, we have candidate near duplicates. Repeat the pr ocess indepen-\ndently for 200 random permutations \u03c0(a choice suggested in the literature).\nCall the set of the 200 resulting values of x\u03c0\nithesketch \u03c8(di)ofdi. We can\nthen estimate the Jaccard coef\ufb01cient for any pair of documen tsdi,djto be\n|\u03c8i\u2229\u03c8j|/200; if this exceeds a preset threshold, we declare that diand djare\nsimilar.\nHow can we quickly compute |\u03c8i\u2229\u03c8j|/200 for all pairs i,j? Indeed, how\ndo we represent all pairs of documents that are similar, with out incurring\na blowup that is quadratic in the number of documents? First, we use \ufb01n-\ngerprints to remove all but one copy of identical documents. We may also\nremove common HTML tags and integers from the shingle comput ation, to\neliminate shingles that occur very commonly in documents wi thout telling\nus anything about duplication. Next we use a union-\ufb01nd algorithm to create\nclusters that contain documents that are similar. To do this , we must accom-\nplish a crucial step: going from the set of sketches to the set of pairs i,jsuch\nthat diand djare similar.\nTo this end, we compute the number of shingles in common for an y pair of\ndocuments whose sketches have any members in common. We begi n with\nthe list <x\u03c0\ni,di>sorted by x\u03c0\nipairs. For each x\u03c0\ni, we can now generate\nOnline edition (c)\n2009 Cambridge UP19.7 References and further reading 441\nall pairs i,jfor which x\u03c0\niis present in both their sketches. From these we\ncan compute, for each pair i,jwith non-zero sketch overlap, a count of the\nnumber of x\u03c0\nivalues they have in common. By applying a preset threshold,\nwe know which pairs i,jhave heavily overlapping sketches. For instance, if\nthe threshold were 80%, we would need the count to be at least 1 60 for any\ni,j. As we identify such pairs, we run the union-\ufb01nd to group docu ments\ninto near-duplicate \u201csyntactic clusters\u201d. This is essenti ally a variant of the\nsingle-link clustering algorithm introduced in Section 17.2 (page 382).\nOne \ufb01nal trick cuts down the space needed in the computation o f|\u03c8i\u2229\n\u03c8j|/200 for pairs i,j, which in principle could still demand space quadratic\nin the number of documents. To remove from consideration tho se pairs i,j\nwhose sketches have few shingles in common, we preprocess th e sketch for\neach document as follows: sort the x\u03c0\niin the sketch, then shingle this sorted\nsequence to generate a set of super-shingles for each document. If two docu-\nments have a super-shingle in common, we proceed to compute t he precise\nvalue of|\u03c8i\u2229\u03c8j|/200. This again is a heuristic but can be highly effective\nin cutting down the number of i,jpairs for which we accumulate the sketch\noverlap counts.\n?Exercise 19.8\nWeb search engines A and B each crawl a random subset of the sam e size of the Web.\nSome of the pages crawled are duplicates \u2013 exact textual copi es of each other at dif-\nferent URLs. Assume that duplicates are distributed unifor mly amongst the pages\ncrawled by A and B. Further, assume that a duplicate is a page t hat has exactly two\ncopies \u2013 no pages have more than two copies. A indexes pages wi thout duplicate\nelimination whereas B indexes only one copy of each duplicat e page. The two ran-\ndom subsets have the same size before duplicate elimination . If, 45% of A\u2019s indexed\nURLs are present in B\u2019s index, while 50% of B\u2019s indexed URLs ar e present in A\u2019s\nindex, what fraction of the Web consists of pages that do not h ave a duplicate?\nExercise 19.9\nInstead of using the process depicted in Figure 19.8, consider instead the following\nprocess for estimating the Jaccard coef\ufb01cient of the overla p between two sets S1and\nS2. We pick a random subset of the elements of the universe from w hich S1and S2\nare drawn; this corresponds to picking a random subset of the rows of the matrix Ain\nthe proof. We exhaustively compute the Jaccard coef\ufb01cient o f these random subsets.\nWhy is this estimate an unbiased estimator of the Jaccard coe f\ufb01cient for S1and S2?\nExercise 19.10\nExplain why this estimator would be very dif\ufb01cult to use in pr actice.\n19.7 References and further reading\nBush (1945 ) foreshadowed the Web when he described an information man-\nagement system that he called memex .Berners-Lee et al. (1992 ) describes\none of the earliest incarnations of the Web. Kumar et al. (2000 ) and Broder\nOnline edition (c)\n2009 Cambridge UP442 19 Web search basics\net al. (2000 ) provide comprehensive studies of the Web as a graph. The use\nof anchor text was \ufb01rst described in McBryan (1994 ). The taxonomy of web\nqueries in Section 19.4 is due to Broder (2002 ). The observation of the power\nlaw with exponent 2.1 in Section 19.2.1 appeared in Kumar et al. (1999 ).\nChakrabarti (2002 ) is a good reference for many aspects of web search and\nanalysis.\nThe estimation of web search index sizes has a long history of develop-\nment covered by Bharat and Broder (1998 ),Lawrence and Giles (1998 ),Rus-\nmevichientong et al. (2001 ),Lawrence and Giles (1999 ),Henzinger et al. (2000 ),\nBar-Yossef and Gurevich (2006 ). The state of the art is Bar-Yossef and Gure-\nvich (2006 ), including several of the bias-removal techniques mentio ned at\nthe end of Section 19.5. Shingling was introduced by Broder et al. (1997 ) and\nused for detecting websites (rather than simply pages) that are identical by\nBharat et al. (2000 ).\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 443\n20 Web crawling and indexes\n20.1 Overview\nWeb crawling is the process by which we gather pages from the W eb, in\norder to index them and support a search engine. The objectiv e of crawling\nis to quickly and ef\ufb01ciently gather as many useful web pages a s possible,\ntogether with the link structure that interconnects them. I n Chapter 19we\nstudied the complexities of the Web stemming from its creati on by millions of\nuncoordinated individuals. In this chapter we study the res ulting dif\ufb01culties\nfor crawling the Web. The focus of this chapter is the compone nt shown in\nFigure 19.7 asweb crawler ; it is sometimes referred to as a spider . WEB CRAWLER\nSPIDER The goal of this chapter is not to describe how to build the cra wler for\na full-scale commercial web search engine. We focus instead on a range of\nissues that are generic to crawling from the student project scale to substan-\ntial research projects. We begin (Section 20.1.1 ) by listing desiderata for web\ncrawlers, and then discuss in Section 20.2 how each of these issues is ad-\ndressed. The remainder of this chapter describes the archit ecture and some\nimplementation details for a distributed web crawler that s atis\ufb01es these fea-\ntures. Section 20.3 discusses distributing indexes across many machines for\na web-scale implementation.\n20.1.1 Features a crawler must provide\nWe list the desiderata for web crawlers in two categories: fe atures that web\ncrawlers must provide, followed by features they should provide.\nRobustness: The Web contains servers that create spider traps , which are gen-\nerators of web pages that mislead crawlers into getting stuc k fetching an\nin\ufb01nite number of pages in a particular domain. Crawlers mus t be de-\nsigned to be resilient to such traps. Not all such traps are ma licious; some\nare the inadvertent side-effect of faulty website developm ent.\nOnline edition (c)\n2009 Cambridge UP444 20 Web crawling and indexes\nPoliteness: Web servers have both implicit and explicit policies regula ting\nthe rate at which a crawler can visit them. These politeness p olicies must\nbe respected.\n20.1.2 Features a crawler should provide\nDistributed: The crawler should have the ability to execute in a distribut ed\nfashion across multiple machines.\nScalable: The crawler architecture should permit scaling up the crawl rate\nby adding extra machines and bandwidth.\nPerformance and ef\ufb01ciency: The crawl system should make ef\ufb01cient use of\nvarious system resources including processor, storage and network band-\nwidth.\nQuality: Given that a signi\ufb01cant fraction of all web pages are of poor u til-\nity for serving user query needs, the crawler should be biase d towards\nfetching \u201cuseful\u201d pages \ufb01rst.\nFreshness: In many applications, the crawler should operate in continu ous\nmode: it should obtain fresh copies of previously fetched pa ges. A search\nengine crawler, for instance, can thus ensure that the searc h engine\u2019s index\ncontains a fairly current representation of each indexed we b page. For\nsuch continuous crawling, a crawler should be able to crawl a page with\na frequency that approximates the rate of change of that page .\nExtensible: Crawlers should be designed to be extensible in many ways \u2013\nto cope with new data formats, new fetch protocols, and so on. This de-\nmands that the crawler architecture be modular.\n20.2 Crawling\nThe basic operation of any hypertext crawler (whether for th e Web, an in-\ntranet or other hypertext document collection) is as follow s. The crawler\nbegins with one or more URLs that constitute a seed set . It picks a URL from\nthis seed set, then fetches the web page at that URL. The fetch ed page is then\nparsed, to extract both the text and the links from the page (e ach of which\npoints to another URL). The extracted text is fed to a text ind exer (described\nin Chapters 4and 5). The extracted links (URLs) are then added to a URL\nfrontier , which at all times consists of URLs whose corresponding pag es have\nyet to be fetched by the crawler. Initially, the URL frontier contains the seed\nset; as pages are fetched, the corresponding URLs are delete d from the URL\nfrontier. The entire process may be viewed as traversing the web graph (see\nOnline edition (c)\n2009 Cambridge UP20.2 Crawling 445\nChapter 19). In continuous crawling, the URL of a fetched page is added\nback to the frontier for fetching again in the future.\nThis seemingly simple recursive traversal of the web graph i s complicated\nby the many demands on a practical web crawling system: the cr awler has to\nbe distributed, scalable, ef\ufb01cient, polite, robust and ext ensible while fetching\npages of high quality. We examine the effects of each of these issues. Our\ntreatment follows the design of the Mercator crawler that has formed the ba- MERCATOR\nsis of a number of research and commercial crawlers. As a refe rence point,\nfetching a billion pages (a small fraction of the static Web a t present) in a\nmonth-long crawl requires fetching several hundred pages e ach second. We\nwill see how to use a multi-threaded design to address severa l bottlenecks in\nthe overall crawler system in order to attain this fetch rate .\nBefore proceeding to this detailed description, we reitera te for readers who\nmay attempt to build crawlers of some basic properties any no n-professional\ncrawler should satisfy:\n1.Only one connection should be open to any given host at a time.\n2.A waiting time of a few seconds should occur between successi ve requests\nto a host.\n3.Politeness restrictions detailed in Section 20.2.1 should be obeyed.\n20.2.1 Crawler architecture\nThe simple scheme outlined above for crawling demands sever al modules\nthat \ufb01t together as shown in Figure 20.1.\n1.The URL frontier, containing URLs yet to be fetched in the cur rent crawl\n(in the case of continuous crawling, a URL may have been fetch ed previ-\nously but is back in the frontier for re-fetching). We descri be this further\nin Section 20.2.3 .\n2.ADNS resolution module that determines the web server from which to\nfetch the page speci\ufb01ed by a URL. We describe this further in S ection 20.2.2 .\n3.A fetch module that uses the http protocol to retrieve the web page at a\nURL.\n4.A parsing module that extracts the text and set of links from a fetched web\npage.\n5.A duplicate elimination module that determines whether an e xtracted\nlink is already in the URL frontier or has recently been fetch ed.\nOnline edition (c)\n2009 Cambridge UP446 20 Web crawling and indexes\nwww\nFetchDNS\nParse\nURL FrontierContent\nSeen?\u0013\n\u0012\u0010\n\u0011\u0012\u0011Doc\nFP\u2019s\u0013\n\u0012\u0010\n\u0011\u0012\u0011robots\ntemplates\u0013\n\u0012\u0010\n\u0011\u0012\u0011URL\nset\nURL\nFilterDup\nURL\nElim-\u001b\n-\n6\u001b-\n?6\n- - -\n\u001b6?6?6?\n\u25eeFigure 20.1 The basic crawler architecture.\nCrawling is performed by anywhere from one to potentially hu ndreds of\nthreads, each of which loops through the logical cycle in Fig ure20.1. These\nthreads may be run in a single process, or be partitioned amon gst multiple\nprocesses running at different nodes of a distributed syste m. We begin by\nassuming that the URL frontier is in place and non-empty and d efer our de-\nscription of the implementation of the URL frontier to Secti on20.2.3 . We\nfollow the progress of a single URL through the cycle of being fetched, pass-\ning through various checks and \ufb01lters, then \ufb01nally (for cont inuous crawling)\nbeing returned to the URL frontier.\nA crawler thread begins by taking a URL from the frontier and f etching\nthe web page at that URL, generally using the http protocol. T he fetched\npage is then written into a temporary store, where a number of operations\nare performed on it. Next, the page is parsed and the text as we ll as the\nlinks in it are extracted. The text (with any tag information \u2013 e.g., terms in\nboldface) is passed on to the indexer. Link information incl uding anchor text\nis also passed on to the indexer for use in ranking in ways that are described\nin Chapter 21. In addition, each extracted link goes through a series of te sts\nto determine whether the link should be added to the URL front ier.\nFirst, the thread tests whether a web page with the same conte nt has al-\nready been seen at another URL. The simplest implementation for this would\nuse a simple \ufb01ngerprint such as a checksum (placed in a store l abeled \"Doc\nFP\u2019s\" in Figure 20.1). A more sophisticated test would use shingles instead\nOnline edition (c)\n2009 Cambridge UP20.2 Crawling 447\nof \ufb01ngerprints, as described in Chapter 19.\nNext, a URL \ufb01lter is used to determine whether the extracted URL should\nbe excluded from the frontier based on one of several tests. F or instance, the\ncrawl may seek to exclude certain domains (say, all .com URLs ) \u2013 in this case\nthe test would simply \ufb01lter out the URL if it were from the .com domain.\nA similar test could be inclusive rather than exclusive. Man y hosts on the\nWeb place certain portions of their websites off-limits to c rawling, under a\nstandard known as the Robots Exclusion Protocol . This is done by placing a ROBOTS EXCLUSION\nPROTOCOL \ufb01le with the name robots.txt at the root of the URL hierarchy a t the site. Here\nis an example robots.txt \ufb01le that speci\ufb01es that no robot shou ld visit any URL\nwhose position in the \ufb01le hierarchy starts with /yoursite/temp/ , except for the\nrobot called \u201csearchengine\u201d.\nUser-agent: *\nDisallow: /yoursite/temp/\nUser-agent: searchengine\nDisallow:\nThe robots.txt \ufb01le must be fetched from a website in order to t est whether\nthe URL under consideration passes the robot restrictions, and can there-\nfore be added to the URL frontier. Rather than fetch it afresh for testing on\neach URL to be added to the frontier, a cache can be used to obta in a re-\ncently fetched copy of the \ufb01le for the host. This is especiall y important since\nmany of the links extracted from a page fall within the host fr om which the\npage was fetched and therefore can be tested against the host \u2019s robots.txt\n\ufb01le. Thus, by performing the \ufb01ltering during the link extrac tion process, we\nwould have especially high locality in the stream of hosts th at we need to test\nfor robots.txt \ufb01les, leading to high cache hit rates. Unfort unately, this runs\nafoul of webmasters\u2019 politeness expectations. A URL (parti cularly one refer-\nring to a low-quality or rarely changing document) may be in t he frontier for\ndays or even weeks. If we were to perform the robots \ufb01ltering before adding\nsuch a URL to the frontier, its robots.txt \ufb01le could have chan ged by the time\nthe URL is dequeued from the frontier and fetched. We must con sequently\nperform robots-\ufb01ltering immediately before attempting to fetch a web page.\nAs it turns out, maintaining a cache of robots.txt \ufb01les is sti ll highly effective;\nthere is suf\ufb01cient locality even in the stream of URLs dequeu ed from the URL\nfrontier.\nNext, a URL should be normalized in the following sense: often the HTML URL NORMALIZATION\nencoding of a link from a web page pindicates the target of that link relative\nto the page p. Thus, there is a relative link encoded thus in the HTML of the\npageen.wikipedia.org/wiki/Main_Page :\nOnline edition (c)\n2009 Cambridge UP448 20 Web crawling and indexes\n<a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"W ikipedia:General\ndisclaimer\">Disclaimers</a>\npoints to the URL http://en.wikipedia.org/wiki/Wikipedia:General_disc laimer .\nFinally, the URL is checked for duplicate elimination: if th e URL is already\nin the frontier or (in the case of a non-continuous crawl) alr eady crawled,\nwe do not add it to the frontier. When the URL is added to the fro ntier, it is\nassigned a priority based on which it is eventually removed f rom the frontier\nfor fetching. The details of this priority queuing are in Sec tion 20.2.3 .\nCertain housekeeping tasks are typically performed by a ded icated thread.\nThis thread is generally quiescent except that it wakes up on ce every few\nseconds to log crawl progress statistics (URLs crawled, fro ntier size, etc.),\ndecide whether to terminate the crawl, or (once every few hou rs of crawling)\ncheckpoint the crawl. In checkpointing, a snapshot of the cr awler\u2019s state (say,\nthe URL frontier) is committed to disk. In the event of a catas trophic crawler\nfailure, the crawl is restarted from the most recent checkpo int.\nDistributing the crawler\nWe have mentioned that the threads in a crawler could run unde r different\nprocesses, each at a different node of a distributed crawlin g system. Such\ndistribution is essential for scaling; it can also be of use i n a geographically\ndistributed crawler system where each node crawls hosts \u201cne ar\u201d it. Parti-\ntioning the hosts being crawled amongst the crawler nodes ca n be done by\na hash function, or by some more speci\ufb01cally tailored policy . For instance,\nwe may locate a crawler node in Europe to focus on European dom ains, al-\nthough this is not dependable for several reasons \u2013 the route s that packets\ntake through the internet do not always re\ufb02ect geographic pr oximity, and in\nany case the domain of a host does not always re\ufb02ect its physic al location.\nHow do the various nodes of a distributed crawler communicat e and share\nURLs? The idea is to replicate the \ufb02ow of Figure 20.1 at each node, with one\nessential difference: following the URL \ufb01lter, we use a host splitter to dispatch\neach surviving URL to the crawler node responsible for the UR L; thus the set\nof hosts being crawled is partitioned among the nodes. This m odi\ufb01ed \ufb02ow is\nshown in Figure 20.2. The output of the host splitter goes into the Duplicate\nURL Eliminator block of each other node in the distributed sy stem.\nThe \u201cContent Seen?\u201d module in the distributed architecture of Figure 20.2\nis, however, complicated by several factors:\n1.Unlike the URL frontier and the duplicate elimination modul e, document\n\ufb01ngerprints/shingles cannot be partitioned based on host n ame. There is\nnothing preventing the same (or highly similar) content fro m appearing\non different web servers. Consequently, the set of \ufb01ngerpri nts/shingles\nmust be partitioned across the nodes based on some property o f the \ufb01n-\nOnline edition (c)\n2009 Cambridge UP20.2 Crawling 449\nwww\nFetchDNS\nParse\nURL FrontierContent\nSeen?\u0013\n\u0012\u0010\n\u0011\u0012\u0011Doc\nFP\u2019s\u0013\n\u0012\u0010\n\u0011\u0012\u0011URL\nset\nURL\nFilterHost\nsplitterTo\nother\nnodes\nFrom\nother\nnodesDup\nURL\nElim-\u001b\n-\n6\u001b-\n?6\n- - - -\n\u001b6?6?666\n---\n\u25eeFigure 20.2 Distributing the basic crawl architecture.\ngerprint/shingle (say by taking the \ufb01ngerprint modulo the n umber of\nnodes). The result of this locality-mismatch is that most \u201cC ontent Seen?\u201d\ntests result in a remote procedure call (although it is possi ble to batch\nlookup requests).\n2.There is very little locality in the stream of document \ufb01nger prints/shingles.\nThus, caching popular \ufb01ngerprints does not help (since ther e are no pop-\nular \ufb01ngerprints).\n3.Documents change over time and so, in the context of continuo us crawl-\ning, we must be able to delete their outdated \ufb01ngerprints/sh ingles from\nthe content-seen set(s). In order to do so, it is necessary to save the \ufb01nger-\nprint/shingle of the document in the URL frontier, along wit h the URL\nitself.\n20.2.2 DNS resolution\nEach web server (and indeed any host connected to the interne t) has a unique\nIP address : a sequence of four bytes generally represented as four inte gers IPADDRESS\nseparated by dots; for instance 207.142.131.248 is the nume rical IP address as-\nsociated with the host www.wikipedia.org. Given a URL such a swww.wikipedia.org\nin textual form, translating it to an IP address (in this case , 207.142.131.248) is\nOnline edition (c)\n2009 Cambridge UP450 20 Web crawling and indexes\na process known as DNS resolution or DNS lookup; here DNS stands for Do- DNS RESOLUTION\nmain Name Service . During DNS resolution, the program that wishes to per-\nform this translation (in our case, a component of the web cra wler) contacts a\nDNS server that returns the translated IP address. (In practice the ent ire trans- DNS SERVER\nlation may not occur at a single DNS server; rather, the DNS se rver contacted\ninitially may recursively call upon other DNS servers to com plete the transla-\ntion.) For a more complex URL such as en.wikipedia.org/wiki/Domain_Name_System ,\nthe crawler component responsible for DNS resolution extra cts the host name\n\u2013 in this case en.wikipedia.org \u2013 and looks up the IP address f or the host\nen.wikipedia.org.\nDNS resolution is a well-known bottleneck in web crawling. D ue to the\ndistributed nature of the Domain Name Service, DNS resoluti on may entail\nmultiple requests and round-trips across the internet, req uiring seconds and\nsometimes even longer. Right away, this puts in jeopardy our goal of fetching\nseveral hundred documents a second. A standard remedy is to i ntroduce\ncaching: URLs for which we have recently performed DNS looku ps are likely\nto be found in the DNS cache, avoiding the need to go to the DNS s ervers\non the internet. However, obeying politeness constraints ( see Section 20.2.3 )\nlimits the of cache hit rate.\nThere is another important dif\ufb01culty in DNS resolution; the lookup imple-\nmentations in standard libraries (likely to be used by anyon e developing a\ncrawler) are generally synchronous. This means that once a r equest is made\nto the Domain Name Service, other crawler threads at that nod e are blocked\nuntil the \ufb01rst request is completed. To circumvent this, mos t web crawlers\nimplement their own DNS resolver as a component of the crawle r. Thread\niexecuting the resolver code sends a message to the DNS server and then\nperforms a timed wait: it resumes either when being signaled by another\nthread or when a set time quantum expires. A single, separate DNS thread\nlistens on the standard DNS port (port 53) for incoming respo nse packets\nfrom the name service. Upon receiving a response, it signals the appropriate\ncrawler thread (in this case, i) and hands it the response packet if ihas not\nyet resumed because its time quantum has expired. A crawler t hread that re-\nsumes because its wait time quantum has expired retries for a \ufb01xed number\nof attempts, sending out a new message to the DNS server and pe rforming\na timed wait each time; the designers of Mercator recommend o f the order\nof \ufb01ve attempts. The time quantum of the wait increases expon entially with\neach of these attempts; Mercator started with one second and ended with\nroughly 90 seconds, in consideration of the fact that there a re host names\nthat take tens of seconds to resolve.\nOnline edition (c)\n2009 Cambridge UP20.2 Crawling 451\n20.2.3 The URL frontier\nThe URL frontier at a node is given a URL by its crawl process (o r by the\nhost splitter of another crawl process). It maintains the UR Ls in the frontier\nand regurgitates them in some order whenever a crawler threa d seeks a URL.\nTwo important considerations govern the order in which URLs are returned\nby the frontier. First, high-quality pages that change freq uently should be\nprioritized for frequent crawling. Thus, the priority of a p age should be a\nfunction of both its change rate and its quality (using some r easonable quality\nestimate). The combination is necessary because a large num ber of spam\npages change completely on every fetch.\nThe second consideration is politeness: we must avoid repea ted fetch re-\nquests to a host within a short time span. The likelihood of th is is exacerbated\nbecause of a form of locality of reference: many URLs link to o ther URLs at\nthe same host. As a result, a URL frontier implemented as a sim ple priority\nqueue might result in a burst of fetch requests to a host. This might occur\neven if we were to constrain the crawler so that at most one thr ead could\nfetch from any single host at any time. A common heuristic is t o insert a\ngap between successive fetch requests to a host that is an ord er of magnitude\nlarger than the time taken for the most recent fetch from that host.\nFigure 20.3 shows a polite and prioritizing implementation of a URL fron -\ntier. Its goals are to ensure that (i) only one connection is o pen at a time to any\nhost; (ii) a waiting time of a few seconds occurs between succ essive requests\nto a host and (iii) high-priority pages are crawled preferen tially.\nThe two major sub-modules are a set of F front queues in the upper por-\ntion of the \ufb01gure, and a set of B back queues in the lower part; all of these are\nFIFO queues. The front queues implement the prioritization , while the back\nqueues implement politeness. In the \ufb02ow of a URL added to the f rontier as\nit makes its way through the front and back queues, a prioritizer \ufb01rst assigns\nto the URL an integer priority ibetween 1 and Fbased on its fetch history\n(taking into account the rate at which the web page at this URL has changed\nbetween previous crawls). For instance, a document that has exhibited fre-\nquent change would be assigned a higher priority. Other heur istics could be\napplication-dependent and explicit \u2013 for instance, URLs fr om news services\nmay always be assigned the highest priority. Now that it has b een assigned\npriority i, the URL is now appended to the ith of the front queues.\nEach of the Bback queues maintains the following invariants: (i) it is no n-\nempty while the crawl is in progress and (ii) it only contains URLs from a\nsingle host1. An auxiliary table T(Figure 20.4) is used to maintain the map-\nping from hosts to back queues. Whenever a back-queue is empt y and is\nbeing re-\ufb01lled from a front-queue, table Tmust be updated accordingly.\n1. The number of hosts is assumed to far exceed B.\nOnline edition (c)\n2009 Cambridge UP452 20 Web crawling and indexes\nBack queue\nselector-\u001bBiased front queue selector\nBack queue routerPrioritizer\nr r r rBback queues\nSingle host on eachr r r\nrrFfront queues1 2 F\n1 2 B\n?XXXXXXXXXXXXzXXXXXXXXXXXXz\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u00189\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u00189\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u00189XXXXXXXXXXXXXXz\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010)\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010)PPPPPPPPPPPq?\nHHHHHHHHHHHjHHHHHHHHHHHj\b\b\b\b\b\b\b\b\b\b\b\u0019\n@\n@@\u0000\n\u0000\u0000Heap\n\u25eeFigure 20.3 The URL frontier. URLs extracted from already crawled pages \ufb02ow in\nat the top of the \ufb01gure. A crawl thread requesting a URL extrac ts it from the bottom of\nthe \ufb01gure. En route, a URL \ufb02ows through one of several front queues that manage its\npriority for crawling, followed by one of several back queues that manage the crawler\u2019s\npoliteness.\nOnline edition (c)\n2009 Cambridge UP20.2 Crawling 453\nHost Back queue\nstanford.edu 23\nmicrosoft.com 47\nacm.org 12\n\u25eeFigure 20.4 Example of an auxiliary hosts-to-back queues table.\nIn addition, we maintain a heap with one entry for each back qu eue, the\nentry being the earliest time teat which the host corresponding to that queue\ncan be contacted again.\nA crawler thread requesting a URL from the frontier extracts the root of\nthis heap and (if necessary) waits until the corresponding t ime entry te. It\nthen takes the URL uat the head of the back queue jcorresponding to the\nextracted heap root, and proceeds to fetch the URL u. After fetching u, the\ncalling thread checks whether jis empty. If so, it picks a front queue and\nextracts from its head a URL v. The choice of front queue is biased (usually\nby a random process) towards queues of higher priority, ensu ring that URLs\nof high priority \ufb02ow more quickly into the back queues. We exa mine vto\ncheck whether there is already a back queue holding URLs from its host.\nIf so, vis added to that queue and we reach back to the front queues to\n\ufb01nd another candidate URL for insertion into the now-empty q ueue j. This\nprocess continues until jis non-empty again. In any case, the thread inserts\na heap entry for jwith a new earliest time tebased on the properties of the\nURL in jthat was last fetched (such as when its host was last contacte d as\nwell as the time taken for the last fetch), then continues wit h its processing.\nFor instance, the new entry tecould be the current time plus ten times the\nlast fetch time.\nThe number of front queues, together with the policy of assig ning priori-\nties and picking queues, determines the priority propertie s we wish to build\ninto the system. The number of back queues governs the extent to which we\ncan keep all crawl threads busy while respecting politeness . The designers\nof Mercator recommend a rough rule of three times as many back queues as\ncrawler threads.\nOn a Web-scale crawl, the URL frontier may grow to the point wh ere it\ndemands more memory at a node than is available. The solution is to let\nmost of the URL frontier reside on disk. A portion of each queu e is kept in\nmemory, with more brought in from disk as it is drained in memo ry.\n?Exercise 20.1\nWhy is it better to partition hosts (rather than individual U RLs) between the nodes of\na distributed crawl system?\nExercise 20.2\nWhy should the host splitter precede the Duplicate URL Elimi nator?\nOnline edition (c)\n2009 Cambridge UP454 20 Web crawling and indexes\nExercise 20.3 [\u22c6 \u22c6 \u22c6 ]\nIn the preceding discussion we encountered two recommended \u201chard constants\u201d \u2013\nthe increment on tebeingten times the last fetch time, and the number of back\nqueues being three times the number of crawl threads. How are these two constant s\nrelated?\n20.3 Distributing indexes\nIn Section 4.4we described distributed indexing. We now consider the dist ri-\nbution of the index across a large computer cluster2that supports querying.\nTwo obvious alternative index implementations suggest the mselves: parti- TERM PARTITIONING\ntioning by terms , also known as global index organization, and partitioning by DOCUMENT\nPARTITIONING documents , also know as local index organization. In the former, the di ction-\nary of index terms is partitioned into subsets, each subset r esiding at a node.\nAlong with the terms at a node, we keep the postings for those t erms. A\nquery is routed to the nodes corresponding to its query terms . In principle,\nthis allows greater concurrency since a stream of queries wi th different query\nterms would hit different sets of machines.\nIn practice, partitioning indexes by vocabulary terms turn s out to be non-\ntrivial. Multi-word queries require the sending of long pos tings lists between\nsets of nodes for merging, and the cost of this can outweigh th e greater con-\ncurrency. Load balancing the partition is governed not by an a priori analysis\nof relative term frequencies, but rather by the distributio n of query terms\nand their co-occurrences, which can drift with time or exhib it sudden bursts.\nAchieving good partitions is a function of the co-occurrenc es of query terms\nand entails the clustering of terms to optimize objectives t hat are not easy to\nquantify. Finally, this strategy makes implementation of d ynamic indexing\nmore dif\ufb01cult.\nA more common implementation is to partition by documents: e ach node\ncontains the index for a subset of all documents. Each query i s distributed to\nall nodes, with the results from various nodes being merged b efore presenta-\ntion to the user. This strategy trades more local disk seeks f or less inter-node\ncommunication. One dif\ufb01culty in this approach is that globa l statistics used\nin scoring \u2013 such as idf \u2013 must be computed across the entire do cument col-\nlection even though the index at any single node only contain s a subset of\nthe documents. These are computed by distributed \u201cbackgrou nd\u201d processes\nthat periodically refresh the node indexes with fresh globa l statistics.\nHow do we decide the partition of documents to nodes? Based on our de-\nvelopment of the crawler architecture in Section 20.2.1 , one simple approach\nwould be to assign all pages from a host to a single node. This p artitioning\n2. Please note the different usage of \u201cclusters\u201d elsewhere i n this book, in the sense of Chapters\n16and 17.\nOnline edition (c)\n2009 Cambridge UP20.4 Connectivity servers 455\ncould follow the partitioning of hosts to crawler nodes. A da nger of such\npartitioning is that on many queries, a preponderance of the results would\ncome from documents at a small number of hosts (and hence a sma ll number\nof index nodes).\nA hash of each URL into the space of index nodes results in a mor e uni-\nform distribution of query-time computation across nodes. At query time,\nthe query is broadcast to each of the nodes, with the top kresults from each\nnode being merged to \ufb01nd the top kdocuments for the query. A common\nimplementation heuristic is to partition the document coll ection into indexes\nof documents that are more likely to score highly on most quer ies (using,\nfor instance, techniques in Chapter 21) and low-scoring indexes with the re-\nmaining documents. We only search the low-scoring indexes w hen there are\ntoo few matches in the high-scoring indexes, as described in Section 7.2.1 .\n20.4 Connectivity servers\nFor reasons to become clearer in Chapter 21, web search engines require a\nconnectivity server that supports fast connectivity queries on the web graph. CONNECTIVITY SERVER\nCONNECTIVITY\nQUERIESTypical connectivity queries are which URLs link to a given URL? and which\nURLs does a given URL link to? To this end, we wish to store mappings in\nmemory from URL to out-links, and from URL to in-links. Appli cations in-\nclude crawl control, web graph analysis, sophisticated cra wl optimization\nand link analysis (to be covered in Chapter 21).\nSuppose that the Web had four billion pages, each with ten lin ks to other\npages. In the simplest form, we would require 32 bits or 4 byte s to specify\neach end (source and destination) of each link, requiring a t otal of\n4\u00d7109\u00d710\u00d78=3.2\u00d71011\nbytes of memory. Some basic properties of the web graph can be exploited to\nuse well under 10% of this memory requirement. At \ufb01rst sight, we appear to\nhave a data compression problem \u2013 which is amenable to a varie ty of stan-\ndard solutions. However, our goal is not to simply compress t he web graph\nto \ufb01t into memory; we must do so in a way that ef\ufb01ciently suppor ts connec-\ntivity queries; this challenge is reminiscent of index comp ression (Chapter 5).\nWe assume that each web page is represented by a unique intege r; the\nspeci\ufb01c scheme used to assign these integers is described be low. We build\nanadjacency table that resembles an inverted index: it has a row for each web\npage, with the rows ordered by the corresponding integers. T he row for any\npage pcontains a sorted list of integers, each corresponding to a w eb page\nthat links to p. This table permits us to respond to queries of the form which\npages link to p? In similar fashion we build a table whose entries are the page s\nlinked to by p.\nOnline edition (c)\n2009 Cambridge UP456 20 Web crawling and indexes\n1: www.stanford.edu/alchemy\n2: www.stanford.edu/biology\n3: www.stanford.edu/biology/plant\n4: www.stanford.edu/biology/plant/copyright\n5: www.stanford.edu/biology/plant/people\n6: www.stanford.edu/chemistry\n\u25eeFigure 20.5 A lexicographically ordered set of URLs.\nThis table representation cuts the space taken by the naive r epresentation\n(in which we explicitly represent each link by its two end poi nts, each a 32-bit\ninteger) by 50%. Our description below will focus on the tabl e for the links\nfrom each page; it should be clear that the techniques apply just a s well to\nthe table of links to each page. To further reduce the storage for the table, we\nexploit several ideas:\n1.Similarity between lists: Many rows of the table have many en tries in\ncommon. Thus, if we explicitly represent a prototype row for several\nsimilar rows, the remainder can be succinctly expressed in t erms of the\nprototypical row.\n2.Locality: many links from a page go to \u201cnearby\u201d pages \u2013 pages o n the\nsame host, for instance. This suggests that in encoding the d estination of\na link, we can often use small integers and thereby save space .\n3.We use gap encodings in sorted lists: rather than store the de stination of\neach link, we store the offset from the previous entry in the r ow.\nWe now develop each of these techniques.\nIn a lexicographic ordering of all URLs, we treat each URL as an alphanu-\nmeric string and sort these strings. Figure 20.5 shows a segment of this sorted\norder. For a true lexicographic sort of web pages, the domain name part of\nthe URL should be inverted, so that www.stanford.edu becomes edu.stanford.www ,\nbut this is not necessary here since we are mainly concerned w ith links local\nto a single host.\nTo each URL, we assign its position in this ordering as the uni que identi-\nfying integer. Figure 20.6 shows an example of such a numbering and the\nresulting table. In this example sequence, www.stanford.edu/biology\nis assigned the integer 2 since it is second in the sequence.\nWe next exploit a property that stems from the way most websit es are\nstructured to get similarity and locality. Most websites ha ve a template with\na set of links from each page in the site to a \ufb01xed set of pages on the site (such\nOnline edition (c)\n2009 Cambridge UP20.4 Connectivity servers 457\n1: 1, 2, 4, 8, 16, 32, 64\n2: 1, 4, 9, 16, 25, 36, 49, 64\n3: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\n4: 1, 4, 8, 16, 25, 36, 49, 64\n\u25eeFigure 20.6 A four-row segment of the table of links.\nas its copyright notice, terms of use, and so on). In this case , the rows cor-\nresponding to pages in a website will have many table entries in common.\nMoreover, under the lexicographic ordering of URLs, it is ve ry likely that the\npages from a website appear as contiguous rows in the table.\nWe adopt the following strategy: we walk down the table, enco ding each\ntable row in terms of the seven preceding rows. In the example of Figure 20.6,\nwe could encode the fourth row as \u201cthe same as the row at offset 2 (mean-\ning, two rows earlier in the table), with 9 replaced by 8\u201d. Thi s requires the\nspeci\ufb01cation of the offset, the integer(s) dropped (in this case 9) and the in-\nteger(s) added (in this case 8). The use of only the seven prec eding rows has\ntwo advantages: (i) the offset can be expressed with only 3 bi ts; this choice\nis optimized empirically (the reason for seven and not eight preceding rows\nis the subject of Exercise 20.4) and (ii) \ufb01xing the maximum offset to a small\nvalue like seven avoids having to perform an expensive searc h among many\ncandidate prototypes in terms of which to express the curren t row.\nWhat if none of the preceding seven rows is a good prototype fo r express-\ning the current row? This would happen, for instance, at each boundary\nbetween different websites as we walk down the rows of the tab le. In this\ncase we simply express the row as starting from the empty set a nd \u201cadding\nin\u201d each integer in that row. By using gap encodings to store t he gaps (rather\nthan the actual integers) in each row, and encoding these gap s tightly based\non the distribution of their values, we obtain further space reduction. In ex-\nperiments mentioned in Section 20.5, the series of techniques outlined here\nappears to use as few as 3 bits per link, on average \u2013 a dramatic reduction\nfrom the 64 required in the naive representation.\nWhile these ideas give us a representation of sizable web gra phs that com-\nfortably \ufb01t in memory, we still need to support connectivity queries. What\nis entailed in retrieving from this representation the set o f links from a page?\nFirst, we need an index lookup from (a hash of) the URL to its ro w number\nin the table. Next, we need to reconstruct these entries, whi ch may be en-\ncoded in terms of entries in other rows. This entails followi ng the offsets to\nreconstruct these other rows \u2013 a process that in principle co uld lead through\nmany levels of indirection. In practice however, this does n ot happen very\noften. A heuristic for controlling this can be introduced in to the construc-\nOnline edition (c)\n2009 Cambridge UP458 20 Web crawling and indexes\ntion of the table: when examining the preceding seven rows as candidates\nfrom which to model the current row, we demand a threshold of s imilarity\nbetween the current row and the candidate prototype. This th reshold must\nbe chosen with care. If the threshold is set too high, we seldo m use proto-\ntypes and express many rows afresh. If the threshold is too lo w, most rows\nget expressed in terms of prototypes, so that at query time th e reconstruction\nof a row leads to many levels of indirection through precedin g prototypes.\n?Exercise 20.4\nWe noted that expressing a row in terms of one of seven precedi ng rows allowed us\nto use no more than three bits to specify which of the precedin g rows we are using\nas prototype. Why seven and not eight preceding rows? (Hint: consider the case when\nnone of the preceding seven rows is a good prototype.)\nExercise 20.5\nWe noted that for the scheme in Section 20.4, decoding the links incident on a URL\ncould result in many levels of indirection. Construct an exa mple in which the number\nof levels of indirection grows linearly with the number of UR Ls.\n20.5 References and further reading\nThe \ufb01rst web crawler appears to be Matthew Gray\u2019s Wanderer, w ritten in the\nspring of 1993. The Mercator crawler is due to Najork and Heyd on (Najork\nand Heydon 2001 ;2002 ); the treatment in this chapter follows their work.\nOther classic early descriptions of web crawling include Burner (1997 ),Brin\nand Page (1998 ),Cho et al. (1998 ) and the creators of the Webbase system\nat Stanford ( Hirai et al. 2000 ).Cho and Garcia-Molina (2002 ) give a taxon-\nomy and comparative study of different modes of communicati on between\nthe nodes of a distributed crawler. The Robots Exclusion Pro tocol standard\nis described at http://www.robotstxt.org/wc/exclusion.html .Boldi et al. (2002 ) and\nShkapenyuk and Suel (2002 ) provide more recent details of implementing\nlarge-scale distributed web crawlers.\nOur discussion of DNS resolution (Section 20.2.2 ) uses the current conven-\ntion for internet addresses, known as IPv4 (for Internet Pro tocol version 4) \u2013\neach IP address is a sequence of four bytes. In the future, the convention for\naddresses (collectively known as the internet address space ) is likely to use a\nnew standard known as IPv6 ( http://www.ipv6.org/ ).\nTomasic and Garcia-Molina (1993 ) and Jeong and Omiecinski (1995 ) are\nkey early papers evaluating term partitioning versus docum ent partitioning\nfor distributed indexes. Document partitioning is found to be superior, at\nleast when the distribution of terms is skewed, as it typical ly is in practice.\nThis result has generally been con\ufb01rmed in more recent work ( MacFarlane\net al. 2000 ). But the outcome depends on the details of the distributed s ystem;\nOnline edition (c)\n2009 Cambridge UP20.5 References and further reading 459\nat least one thread of work has reached the opposite conclusi on (Ribeiro-\nNeto and Barbosa 1998 ,Badue et al. 2001 ).Sornil (2001 ) argues for a par-\ntitioning scheme that is a hybrid between term and document p artitioning.\nBarroso et al. (2003 ) describe the distribution methods used at Google. The\n\ufb01rst implementation of a connectivity server was described byBharat et al.\n(1998 ). The scheme discussed in this chapter, currently believed to be the\nbest published scheme (achieving as few as 3 bits per link for encoding), is\ndescribed in a series of papers by Boldi and Vigna (2004a ;b).\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 461\n21 Link analysis\nThe analysis of hyperlinks and the graph structure of the Web has been in-\nstrumental in the development of web search. In this chapter we focus on the\nuse of hyperlinks for ranking web search results. Such link a nalysis is one\nof many factors considered by web search engines in computin g a compos-\nite score for a web page on any given query. We begin by reviewi ng some\nbasics of the Web as a graph in Section 21.1, then proceed to the technical\ndevelopment of the elements of link analysis for ranking.\nLink analysis for web search has intellectual antecedents i n the \ufb01eld of cita-\ntion analysis, aspects of which overlap with an area known as bibliometrics.\nThese disciplines seek to quantify the in\ufb02uence of scholarl y articles by ana-\nlyzing the pattern of citations amongst them. Much as citati ons represent the\nconferral of authority from a scholarly article to others, l ink analysis on the\nWeb treats hyperlinks from a web page to another as a conferra l of authority.\nClearly, not every citation or hyperlink implies such autho rity conferral; for\nthis reason, simply measuring the quality of a web page by the number of\nin-links (citations from other pages) is not robust enough. For instance, one\nmay contrive to set up multiple web pages pointing to a target web page,\nwith the intent of arti\ufb01cially boosting the latter\u2019s tally o f in-links. This phe-\nnomenon is referred to as link spam. Nevertheless, the pheno menon of ci-\ntation is prevalent and dependable enough that it is feasibl e for web search\nengines to derive useful signals for ranking from more sophi sticated link\nanalysis. Link analysis also proves to be a useful indicator of what page(s)\nto crawl next while crawling the web; this is done by using lin k analysis to\nguide the priority assignment in the front queues of Chapter 20.\nSection 21.1 develops the basic ideas underlying the use of the web graph\nin link analysis. Sections 21.2 and 21.3 then develop two distinct methods for\nlink analysis, PageRank and HITS.\nOnline edition (c)\n2009 Cambridge UP462 21 Link analysis\n21.1 The Web as a graph\nRecall the notion of the web graph from Section 19.2.1 and particularly Fig-\nure19.2. Our study of link analysis builds on two intuitions:\n1.The anchor text pointing to page B is a good description of pag e B.\n2.The hyperlink from A to B represents an endorsement of page B, by the\ncreator of page A. This is not always the case; for instance, m any links\namongst pages within a single website stem from the user of a c ommon\ntemplate. For instance, most corporate websites have a poin ter from ev-\nery page to a page containing a copyright notice \u2013 this is clea rly not an\nendorsement. Accordingly, implementations of link analys is algorithms\nwill typical discount such \u201cinternal\u201d links.\n21.1.1 Anchor text and the web graph\nThe following fragment of HTML code from a web page shows a hyp erlink\npointing to the home page of the Journal of the ACM:\n<a href=\"http://www.acm.org/jacm/\">Journal of the ACM.< /a>\nIn this case, the link points to the page http://www.acm.org/jacm/ and\nthe anchor text is Journal of the ACM. Clearly, in this example the anchor is de-\nscriptive of the target page. But then the target page (B = http://www.acm.org/jacm/ )\nitself contains the same description as well as considerabl e additional infor-\nmation on the journal. So what use is the anchor text?\nThe Web is full of instances where the page B does not provide a n accu-\nrate description of itself. In many cases this is a matter of h ow the publish-\ners of page B choose to present themselves; this is especiall y common with\ncorporate web pages, where a web presence is a marketing stat ement. For\nexample, at the time of the writing of this book the home page o f the IBM\ncorporation ( http://www.ibm.com ) did not contain the term computer any-\nwhere in its HTML code, despite the fact that IBM is widely vie wed as the\nworld\u2019s largest computer maker. Similarly, the HTML code fo r the home\npage of Yahoo! ( http://www.yahoo.com ) does not at this time contain the\nwordportal .\nThus, there is often a gap between the terms in a web page, and h ow web\nusers would describe that web page. Consequently, web searc hers need not\nuse the terms in a page to query for it. In addition, many web pa ges are rich\nin graphics and images, and/or embed their text in these imag es; in such\ncases, the HTML parsing performed when crawling will not ext ract text that\nis useful for indexing these pages. The \u201cstandard IR\u201d approa ch to this would\nbe to use the methods outlined in Chapter 9and Section 12.4. The insight\nOnline edition (c)\n2009 Cambridge UP21.1 The Web as a graph 463\nbehind anchor text is that such methods can be supplanted by a nchor text,\nthereby tapping the power of the community of web page author s.\nThe fact that the anchors of many hyperlinks pointing to http://www.ibm.com\ninclude the word computer can be exploited by web search engines. For in-\nstance, the anchor text terms can be included as terms under w hich to index\nthe target web page. Thus, the postings for the term computer would include\nthe document http://www.ibm.com and that for the term portal would in-\nclude the document http://www.yahoo.com , using a special indicator to\nshow that these terms occur as anchor (rather than in-page) t ext. As with\nin-page terms, anchor text terms are generally weighted bas ed on frequency,\nwith a penalty for terms that occur very often (the most commo n terms in an-\nchor text across the Web are Click andhere, using methods very similar to idf).\nThe actual weighting of terms is determined by machine-lear ned scoring, as\nin Section 15.4.1 ; current web search engines appear to assign a substantial\nweighting to anchor text terms.\nThe use of anchor text has some interesting side-effects. Se arching for big\nblue on most web search engines returns the home page of the IBM cor pora-\ntion as the top hit; this is consistent with the popular nickn ame that many\npeople use to refer to IBM. On the other hand, there have been ( and con-\ntinue to be) many instances where derogatory anchor text suc h asevil empire\nleads to somewhat unexpected results on querying for these t erms on web\nsearch engines. This phenomenon has been exploited in orche strated cam-\npaigns against speci\ufb01c sites. Such orchestrated anchor tex t may be a form\nof spamming, since a website can create misleading anchor te xt pointing to\nitself, to boost its ranking on selected query terms. Detect ing and combating\nsuch systematic abuse of anchor text is another form of spam d etection that\nweb search engines perform.\nThe window of text surrounding anchor text (sometimes refer red to as ex-\ntended anchor text ) is often usable in the same manner as anchor text itself;\nconsider for instance the fragment of web text there is good discussion\nof vedic scripture <a>here</a> . This has been considered in a num-\nber of settings and the useful width of this window has been st udied; see\nSection 21.4 for references.\n?Exercise 21.1\nIs it always possible to follow directed edges (hyperlinks) in the web graph from any\nnode (web page) to any other? Why or why not?\nExercise 21.2\nFind an instance of misleading anchor-text on the Web.\nExercise 21.3\nGiven the collection of anchor-text phrases for a web page x, suggest a heuristic for\nchoosing one term or phrase from this collection that is most descriptive of x.\nOnline edition (c)\n2009 Cambridge UP464 21 Link analysis\n\u0012\u0011\u0013\u0010\nA\n\u0012\u0011\u0013\u0010C\u0012\u0011\u0013\u0010\nB\n\u0012\u0011\u0013\u0010D-\u0000\u0000\u0012\n@\n@R\n\u25eeFigure 21.1 The random surfer at node A proceeds with probability 1/3 to e ach\nof B, C and D.\nExercise 21.4\nDoes your heuristic in the previous exercise take into accou nt a single domain D\nrepeating anchor text for xfrom multiple pages in D?\n21.2 PageRank\nWe now focus on scoring and ranking measures derived from the link struc-\nture alone. Our \ufb01rst technique for link analysis assigns to e very node in\nthe web graph a numerical score between 0 and 1, known as its PageRank . PAGE RANK\nThe PageRank of a node will depend on the link structure of the web graph.\nGiven a query, a web search engine computes a composite score for each\nweb page that combines hundreds of features such as cosine si milarity (Sec-\ntion 6.3) and term proximity (Section 7.2.2 ), together with the PageRank score.\nThis composite score, developed using the methods of Sectio n15.4.1 , is used\nto provide a ranked list of results for the query.\nConsider a random surfer who begins at a web page (a node of the web\ngraph) and executes a random walk on the Web as follows. At eac h time\nstep, the surfer proceeds from his current page A to a randoml y chosen web\npage that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of\nwhich there are three hyperlinks to nodes B, C and D; the surfe r proceeds at\nthe next time step to one of these three nodes, with equal prob abilities 1/3.\nAs the surfer proceeds in this random walk from node to node, h e visits\nsome nodes more often than others; intuitively, these are no des with many\nlinks coming in from other frequently visited nodes. The ide a behind Page-\nRank is that pages visited more often in this walk are more imp ortant.\nWhat if the current location of the surfer, the node A, has no o ut-links?\nTo address this we introduce an additional operation for our random surfer:\ntheteleport operation. In the teleport operation the surfer jumps from a node TELEPORT\nto any other node in the web graph. This could happen because h e types\nOnline edition (c)\n2009 Cambridge UP21.2 PageRank 465\nan address into the URL bar of his browser. The destination of a teleport\noperation is modeled as being chosen uniformly at random fro m all web\npages. In other words, if Nis the total number of nodes in the web graph1,\nthe teleport operation takes the surfer to each node with pro bability 1/ N.\nThe surfer would also teleport to his present position with p robability 1/ N.\nIn assigning a PageRank score to each node of the web graph, we use the\nteleport operation in two ways: (1) When at a node with no out- links, the\nsurfer invokes the teleport operation. (2) At any node that h as outgoing links,\nthe surfer invokes the teleport operation with probability 0<\u03b1<1 and the\nstandard random walk (follow an out-link chosen uniformly a t random as in\nFigure 21.1) with probability 1 \u2212\u03b1, where \u03b1is a \ufb01xed parameter chosen in\nadvance. Typically, \u03b1might be 0.1.\nIn Section 21.2.1 , we will use the theory of Markov chains to argue that\nwhen the surfer follows this combined process (random walk p lus teleport)\nhe visits each node vof the web graph a \ufb01xed fraction of the time \u03c0(v)that\ndepends on (1) the structure of the web graph and (2) the value of\u03b1. We call\nthis value \u03c0(v)the PageRank of vand will show how to compute this value\nin Section 21.2.2 .\n21.2.1 Markov chains\nA Markov chain is a discrete-time stochastic process: a process that occurs in\na series of time-steps in each of which a random choice is made . A Markov\nchain consists of N states . Each web page will correspond to a state in the\nMarkov chain we will formulate.\nA Markov chain is characterized by an N\u00d7N transition probability matrix P\neach of whose entries is in the interval [0, 1]; the entries in each row of Padd\nup to 1. The Markov chain can be in one of the Nstates at any given time-\nstep; then, the entry Pijtells us the probability that the state at the next time-\nstep is j, conditioned on the current state being i. Each entry Pijis known as a\ntransition probability and depends only on the current stat ei; this is known\nas the Markov property. Thus, by the Markov property,\n\u2200i,j,Pij\u2208[0, 1]\nand\n\u2200i,N\n\u2211\nj=1Pij=1. (21.1)\nA matrix with non-negative entries that satis\ufb01es Equation ( 21.1) is known\nas a stochastic matrix . A key property of a stochastic matrix is that it has a STOCHASTIC MATRIX\nprincipal left eigenvector corresponding to its largest eigenvalue, which is 1. PRINCIPAL LEFT\nEIGENVECTOR\n1. This is consistent with our usage of Nfor the number of documents in the collection.\nOnline edition (c)\n2009 Cambridge UP466 21 Link analysis\n\u0016\u0015\u0017\u0014\nA\u0016\u0015\u0017\u0014\nB\u0016\u0015\u0017\u0014\nC-1-0.5\n\u001b\n0.5\u001b\n1\n\u25eeFigure 21.2 A simple Markov chain with three states; the numbers on the li nks\nindicate the transition probabilities.\nIn a Markov chain, the probability distribution of next stat es for a Markov\nchain depends only on the current state, and not on how the Mar kov chain\narrived at the current state. Figure 21.2 shows a simple Markov chain with\nthree states. From the middle state A, we proceed with (equal ) probabilities\nof 0.5 to either B or C. From either B or C, we proceed with proba bility 1 to\nA. The transition probability matrix of this Markov chain is then\n\uf8eb\n\uf8ed0 0.5 0.5\n1 0 0\n1 0 0\uf8f6\n\uf8f8\nA Markov chain\u2019s probability distribution over its states m ay be viewed as\naprobability vector : a vector all of whose entries are in the interval [0, 1], and PROBABILITY VECTOR\nthe entries add up to 1. An N-dimensional probability vector each of whose\ncomponents corresponds to one of the Nstates of a Markov chain can be\nviewed as a probability distribution over its states. For ou r simple Markov\nchain of Figure 21.2, the probability vector would have 3 components that\nsum to 1.\nWe can view a random surfer on the web graph as a Markov chain, w ith\none state for each web page, and each transition probability representing the\nprobability of moving from one web page to another. The telep ort operation\ncontributes to these transition probabilities. The adjace ncy matrix Aof the\nweb graph is de\ufb01ned as follows: if there is a hyperlink from pa geito page\nj, then Aij=1, otherwise Aij=0. We can readily derive the transition\nprobability matrix Pfor our Markov chain from the N\u00d7Nmatrix A:\n1.If a row of Ahas no 1\u2019s, then replace each element by 1/N. For all other\nrows proceed as follows.\n2.Divide each 1 in Aby the number of 1\u2019s in its row. Thus, if there is a row\nwith three 1\u2019s, then each of them is replaced by 1/3.\n3.Multiply the resulting matrix by 1 \u2212\u03b1.\nOnline edition (c)\n2009 Cambridge UP21.2 PageRank 467\n4.Add \u03b1/Nto every entry of the resulting matrix, to obtain P.\nWe can depict the probability distribution of the surfer\u2019s p osition at any\ntime by a probability vector /vectorx. At t=0 the surfer may begin at a state whose\ncorresponding entry in /vectorxis 1 while all others are zero. By de\ufb01nition, the\nsurfer\u2019s distribution at t=1 is given by the probability vector /vectorxP; att=2\nby(/vectorxP)P=/vectorxP2, and so on. We will detail this process in Section 21.2.2 . We\ncan thus compute the surfer\u2019s distribution over the states a t any time, given\nonly the initial distribution and the transition probabili ty matrix P.\nIf a Markov chain is allowed to run for many time steps, each st ate is vis-\nited at a (different) frequency that depends on the structur e of the Markov\nchain. In our running analogy, the surfer visits certain web pages (say, pop-\nular news home pages) more often than other pages. We now make this in-\ntuition precise, establishing conditions under which such the visit frequency\nconverges to \ufb01xed, steady-state quantity. Following this, we set the Page-\nRank of each node vto this steady-state visit frequency and show how it can\nbe computed.\nDe\ufb01nition: A Markov chain is said to be ergodic if there exists a positive ERGODIC MARKOV\nCHAIN integer T0such that for all pairs of states i,jin the Markov chain, if it is\nstarted at time 0 in state ithen for all t>T0, the probability of being in state\njat time tis greater than 0.\nFor a Markov chain to be ergodic, two technical conditions ar e required\nof its states and the non-zero transition probabilities; th ese conditions are\nknown as irreducibility and aperiodicity . Informally, the \ufb01rst ensures that there\nis a sequence of transitions of non-zero probability from an y state to any\nother, while the latter ensures that the states are not parti tioned into sets\nsuch that all state transitions occur cyclically from one se t to another.\nTheorem 21.1. For any ergodic Markov chain, there is a unique steady-state prob- STEADY -STATE\nability vector /vector\u03c0that is the principal left eigenvector of P, such that if \u03b7(i,t)is the\nnumber of visits to state i in t steps, then\nlim\nt\u2192\u221e\u03b7(i,t)\nt=\u03c0(i),\nwhere \u03c0(i)>0is the steady-state probability for state i.\nIt follows from Theorem 21.1 that the random walk with teleporting re-\nsults in a unique distribution of steady-state probabiliti es over the states of\nthe induced Markov chain. This steady-state probability fo r a state is the\nPageRank of the corresponding web page.\nOnline edition (c)\n2009 Cambridge UP468 21 Link analysis\n21.2.2 The PageRank computation\nHow do we compute PageRank values? Recall the de\ufb01nition of a l eft eigen-\nvector from Equation 18.2; the left eigenvectors of the transition probability\nmatrix PareN-vectors /vector\u03c0such that\n/vector\u03c0P=\u03bb/vector\u03c0. (21.2)\nThe Nentries in the principal eigenvector /vector\u03c0are the steady-state proba-\nbilities of the random walk with teleporting, and thus the Pa geRank values\nfor the corresponding web pages. We may interpret Equation ( 21.2) as fol-\nlows: if /vector\u03c0is the probability distribution of the surfer across the web pages,\nhe remains in the steady-state distribution /vector\u03c0. Given that /vector\u03c0is the steady-state\ndistribution, we have that \u03c0P=1\u03c0, so 1 is an eigenvalue of P . Thus if we\nwere to compute the principal left eigenvector of the matrix P\u2014 the one with\neigenvalue 1 \u2014 we would have computed the PageRank values.\nThere are many algorithms available for computing left eige nvectors; the\nreferences at the end of Chapter 18and the present chapter are a guide to\nthese. We give here a rather elementary method, sometimes kn own as power\niteration . If/vectorxis the initial distribution over the states, then the distri bution at\ntime tis/vectorxPt. As tgrows large, we would expect that the distribution /vectorxPt2\nis very similar to the distribution /vectorxPt+1, since for large twe would expect\nthe Markov chain to attain its steady state. By Theorem 21.1 this is indepen-\ndent of the initial distribution /vectorx. The power iteration method simulates the\nsurfer\u2019s walk: begin at a state and run the walk for a large num ber of steps\nt, keeping track of the visit frequencies for each of the state s. After a large\nnumber of steps t, these frequencies \u201csettle down\u201d so that the variation in th e\ncomputed frequencies is below some predetermined threshol d. We declare\nthese tabulated frequencies to be the PageRank values.\nWe consider the web graph in Exercise 21.6 with \u03b1=0.5. The transition\nprobability matrix of the surfer\u2019s walk with teleportation is then\nP=\uf8eb\n\uf8ed1/6 2/3 1/6\n5/12 1/6 5/12\n1/6 2/3 1/6\uf8f6\n\uf8f8. (21.3)\nImagine that the surfer starts in state 1, corresponding to t he initial proba-\nbility distribution vector /vectorx0= (1 0 0). Then, after one step the distribution\nis\n/vectorx0P=/parenleftbig\n1/6 2/3 1/6/parenrightbig=/vectorx1. (21.4)\n2. Note that Ptrepresents Praised to the tth power, not the transpose of Pwhich is denoted PT.\nOnline edition (c)\n2009 Cambridge UP21.2 PageRank 469\n/vectorx0 1 0 0\n/vectorx1 1/6 2/3 1/6\n/vectorx2 1/3 1/3 1/3\n/vectorx3 1/4 1/2 1/4\n/vectorx4 7/24 5/12 7/24\n. . .\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n/vectorx 5/18 4/9 5/18\n\u25eeFigure 21.3 The sequence of probability vectors.\nAfter two steps it is\n/vectorx1P=/parenleftbig1/6 2/3 1/6/parenrightbig\uf8eb\n\uf8ed1/6 2/3 1/6\n5/12 1/6 5/12\n1/6 2/3 1/6\uf8f6\n\uf8f8=/parenleftbig1/3 1/3 1/3/parenrightbig=/vectorx2. (21.5)\nContinuing in this fashion gives a sequence of probability v ectors as shown\nin Figure 21.3.\nContinuing for several steps, we see that the distribution c onverges to the\nsteady state of /vectorx= (5/18 4/9 5/18 ). In this simple example, we may\ndirectly calculate this steady-state probability distrib ution by observing the\nsymmetry of the Markov chain: states 1 and 3 are symmetric, as evident from\nthe fact that the \ufb01rst and third rows of the transition probab ility matrix in\nEquation ( 21.3) are identical. Postulating, then, that they both have the s ame\nsteady-state probability and denoting this probability by p, we know that the\nsteady-state distribution is of the form /vector\u03c0= (p1\u22122p p). Now, using the\nidentity /vector\u03c0=/vector\u03c0P, we solve a simple linear equation to obtain p=5/18 and\nconsequently, /vector\u03c0= (5/18 4/9 5/18 ).\nThe PageRank values of pages (and the implicit ordering amon gst them)\nare independent of any query a user might pose; PageRank is th us a query-\nindependent measure of the static quality of each web page (r ecall such static\nquality measures from Section 7.1.4 ). On the other hand, the relative order-\ning of pages should, intuitively, depend on the query being s erved. For this\nreason, search engines use static quality measures such as P ageRank as just\none of many factors in scoring a web page on a query. Indeed, th e relative\ncontribution of PageRank to the overall score may again be de termined by\nmachine-learned scoring as in Section 15.4.1 .\nOnline edition (c)\n2009 Cambridge UP470 21 Link analysis\nd0\nd2 d1\nd5\nd3 d6\nd4car benz\nfordgm\nhonda\njaguar\njag\ncatleopard\ntiger\njaguar\nlioncheetah\nspeed\n\u25eeFigure 21.4 A small web graph. Arcs are annotated with the word that occur s in\nthe anchor text of the corresponding link.\n\u270eExample 21.1: Consider the graph in Figure 21.4. For a teleportation rate of 0.14\nits (stochastic) transition probability matrix is:\n0.02 0.02 0.88 0.02 0.02 0.02 0.02\n0.02 0.45 0.45 0.02 0.02 0.02 0.02\n0.31 0.02 0.31 0.31 0.02 0.02 0.02\n0.02 0.02 0.02 0.45 0.45 0.02 0.02\n0.02 0.02 0.02 0.02 0.02 0.02 0.88\n0.02 0.02 0.02 0.02 0.02 0.45 0.45\n0.02 0.02 0.02 0.31 0.31 0.02 0.31\nThe PageRank vector of this matrix is:\n/vectorx= (0.05 0.04 0.11 0.25 0.21 0.04 0.31 ) (21.6)\nObserve that in Figure 21.4,q2,q3,q4and q6are the nodes with at least two in-links.\nOf these, q2has the lowest PageRank since the random walk tends to drift o ut of the\ntop part of the graph \u2013 the walker can only return there throug h teleportation.\nOnline edition (c)\n2009 Cambridge UP21.2 PageRank 471\n21.2.3 Topic-speci\ufb01c PageRank\nThus far we have discussed the PageRank computation with a te leport op-\neration in which the surfer jumps to a random web page chosen u niformly\nat random. We now consider teleporting to a random web page ch osen non-\nuniformly . In doing so, we are able to derive PageRank values tailored t o\nparticular interests. For instance, a sports a\ufb01cionado mig ht wish that pages\non sports be ranked higher than non-sports pages. Suppose th at web pages\non sports are \u201cnear\u201d one another in the web graph. Then, a rand om surfer\nwho frequently \ufb01nds himself on random sports pages is likely (in the course\nof the random walk) to spend most of his time at sports pages, s o that the\nsteady-state distribution of sports pages is boosted.\nSuppose our random surfer, endowed with a teleport operatio n as before,\nteleports to a random web page on the topic of sports instead of teleporting to a\nuniformly chosen random web page. We will not focus on how we c ollect all\nweb pages on the topic of sports; in fact, we only need a non-ze ro subset Sof\nsports-related web pages, so that the teleport operation is feasible. This may\nbe obtained, for instance, from a manually built directory o f sports pages\nsuch as the open directory project ( http://www.dmoz.org/ ) or that of Yahoo.\nProvided the set Sof sports-related pages is non-empty, it follows that\nthere is a non-empty set of web pages Y\u2287Sover which the random walk\nhas a steady-state distribution; let us denote this sports PageRank distribution\nby/vector\u03c0s. For web pages not in Y, we set the PageRank values to zero. We call\n/vector\u03c0sthetopic-speci\ufb01c PageRank for sports. TOPIC -SPECIFIC\nPAGE RANK We do not demand that teleporting takes the random surfer to a uniformly\nchosen sports page; the distribution over teleporting targ etsScould in fact\nbe arbitrary.\nIn like manner we can envision topic-speci\ufb01c PageRank distr ibutions for\neach of several topics such as science, religion, politics a nd so on. Each of\nthese distributions assigns to each web page a PageRank valu e in the interval\n[0, 1). For a user interested in only a single topic from among these topics,\nwe may invoke the corresponding PageRank distribution when scoring and\nranking search results. This gives us the potential of consi dering settings in\nwhich the search engine knows what topic a user is interested in. This may\nhappen because users either explicitly register their inte rests, or because the\nsystem learns by observing each user\u2019s behavior over time.\nBut what if a user is known to have a mixture of interests from m ultiple\ntopics? For instance, a user may have an interest mixture (or pro\ufb01le ) that is\n60% sports and 40% politics; can we compute a personalized PageRank for this PERSONALIZED\nPAGE RANK user? At \ufb01rst glance, this appears daunting: how could we pos sibly compute\na different PageRank distribution for each user pro\ufb01le (wit h, potentially, in-\n\ufb01nitely many possible pro\ufb01les)? We can in fact address this p rovided we\nassume that an individual\u2019s interests can be well-approxim ated as a linear\nOnline edition (c)\n2009 Cambridge UP472 21 Link analysis\n\u25eeFigure 21.5 Topic-speci\ufb01c PageRank. In this example we consider a user w hose\ninterests are 60% sports and 40% politics. If the teleportat ion probability is 10%, this\nuser is modeled as teleporting 6% to sports pages and 4% to pol itics pages.\ncombination of a small number of topic page distributions. A user with this\nmixture of interests could teleport as follows: determine \ufb01 rst whether to tele-\nport to the set Sof known sports pages, or to the set of known politics pages.\nThis choice is made at random, choosing sports pages 60% of th e time and\npolitics pages 40% of the time. Once we choose that a particul ar teleport step\nis to (say) a random sports page, we choose a web page in Suniformly at\nrandom to teleport to. This in turn leads to an ergodic Markov chain with a\nsteady-state distribution that is personalized to this use r\u2019s preferences over\ntopics (see Exercise 21.16 ).\nWhile this idea has intuitive appeal, its implementation ap pears cumber-\nsome: it seems to demand that for each user, we compute a trans ition prob-\nOnline edition (c)\n2009 Cambridge UP21.2 PageRank 473\nability matrix and compute its steady-state distribution. We are rescued by\nthe fact that the evolution of the probability distribution over the states of\na Markov chain can be viewed as a linear system. In Exercise 21.16 we will\nshow that it is not necessary to compute a PageRank vector for every distinct\ncombination of user interests over topics; the personalize d PageRank vector\nfor any user can be expressed as a linear combination of the un derlying topic-\nspeci\ufb01c PageRanks. For instance, the personalized PageRan k vector for the\nuser whose interests are 60% sports and 40% politics can be co mputed as\n0.6/vector\u03c0s+0.4/vector\u03c0p, (21.7)\nwhere /vector\u03c0sand/vector\u03c0pare the topic-speci\ufb01c PageRank vectors for sports and for\npolitics, respectively.\n?Exercise 21.5\nWrite down the transition probability matrix for the exampl e in Figure 21.2.\nExercise 21.6\nConsider a web graph with three nodes 1, 2 and 3. The links are a s follows: 1\u2192\n2, 3\u21922, 2\u21921, 2\u21923. Write down the transition probability matrices for the su rfer\u2019s\nwalk with teleporting, for the following three values of the teleport probability: (a)\n\u03b1=0; (b) \u03b1=0.5 and (c) \u03b1=1.\nExercise 21.7\nA user of a browser can, in addition to clicking a hyperlink on the page xhe is cur-\nrently browsing, use the back button to go back to the page from which he arrived at\nx. Can such a user of back buttons be modeled as a Markov chain? H ow would we\nmodel repeated invocations of the back button?\nExercise 21.8\nConsider a Markov chain with three states A, B and C, and trans ition probabilities as\nfollows. From state A, the next state is B with probability 1. From B, the next state is\neither A with probability pA, or state C with probability 1 \u2212pA. From C the next state\nis A with probability 1. For what values of pA\u2208[0, 1]is this Markov chain ergodic?\nExercise 21.9\nShow that for any directed graph, the Markov chain induced by a random walk with\nthe teleport operation is ergodic.\nExercise 21.10\nShow that the PageRank of every page is at least \u03b1/N. What does this imply about\nthe difference in PageRank values (over the various pages) a s\u03b1becomes close to 1?\nExercise 21.11\nFor the data in Example 21.1, write a small routine or use a scienti\ufb01c calculator to\ncompute the PageRank values stated in Equation ( 21.6).\nOnline edition (c)\n2009 Cambridge UP474 21 Link analysis\nExercise 21.12\nSuppose that the web graph is stored on disk as an adjacency li st, in such a way that\nyou may only query for the out-neighbors of pages in the order in which they are\nstored. You cannot load the graph in main memory but you may do multiple reads\nover the full graph. Write the algorithm for computing the Pa geRank in this setting.\nExercise 21.13\nRecall the sets Sand Yintroduced near the beginning of Section 21.2.3 . How does the\nsetYrelate to S?\nExercise 21.14\nIs the set Yalways the set of all web pages? Why or why not?\nExercise 21.15 [\u22c6 \u22c6 \u22c6 ]\nIs the sports PageRank of any page in Sat least as large as its PageRank?\nExercise 21.16 [\u22c6 \u22c6 \u22c6 ]\nConsider a setting where we have two topic-speci\ufb01c PageRank values for each web\npage: a sports PageRank /vector\u03c0s, and a politics PageRank /vector\u03c0p. Let \u03b1be the (common)\nteleportation probability used in computing both sets of to pic-speci\ufb01c PageRanks.\nForq\u2208[0, 1], consider a user whose interest pro\ufb01le is divided between a f raction qin\nsports and a fraction 1 \u2212qin politics. Show that the user\u2019s personalized PageRank is\nthe steady-state distribution of a random walk in which \u2013 on a teleport step \u2013 the walk\nteleports to a sports page with probability qand to a politics page with probability\n1\u2212q.\nExercise 21.17\nShow that the Markov chain corresponding to the walk in Exerc ise21.16 is ergodic\nand hence the user\u2019s personalized PageRank can be obtained b y computing the steady-\nstate distribution of this Markov chain.\nExercise 21.18\nShow that in the steady-state distribution of Exercise 21.17 , the steady-state probabil-\nity for any web page iequals q\u03c0s(i) + ( 1\u2212q)\u03c0p(i).\n21.3 Hubs and Authorities\nWe now develop a scheme in which, given a query, every web page is as-\nsigned twoscores. One is called its hub score and the other its authority score . HUB SCORE\nAUTHORITY SCORE For any query, we compute two ranked lists of results rather t han one. The\nranking of one list is induced by the hub scores and that of the other by the\nauthority scores.\nThis approach stems from a particular insight into the creat ion of web\npages, that there are two primary kinds of web pages useful as results for\nbroad-topic searches . By a broad topic search we mean an informational query\nsuch as \"I wish to learn about leukemia \". There are authoritative sources of\ninformation on the topic; in this case, the National Cancer I nstitute\u2019s page on\nOnline edition (c)\n2009 Cambridge UP21.3 Hubs and Authorities 475\nleukemia would be such a page. We will call such pages authorities ; in the\ncomputation we are about to describe, they are the pages that will emerge\nwith high authority scores.\nOn the other hand, there are many pages on the Web that are hand -compiled\nlists of links to authoritative web pages on a speci\ufb01c topic. These hubpages\nare not in themselves authoritative sources of topic-speci \ufb01c information, but\nrather compilations that someone with an interest in the top ic has spent time\nputting together. The approach we will take, then, is to use t hese hub pages\nto discover the authority pages. In the computation we now de velop, these\nhub pages are the pages that will emerge with high hub scores.\nA good hub page is one that points to many good authorities; a g ood au-\nthority page is one that is pointed to by many good hub pages. W e thus\nappear to have a circular de\ufb01nition of hubs and authorities; we will turn this\ninto an iterative computation. Suppose that we have a subset of the web con-\ntaining good hub and authority pages, together with the hype rlinks amongst\nthem. We will iteratively compute a hub score and an authorit y score for ev-\nery web page in this subset, deferring the discussion of how w e pick this\nsubset until Section 21.3.1 .\nFor a web page vin our subset of the web, we use h(v)to denote its hub\nscore and a(v)its authority score. Initially, we set h(v) = a(v) = 1 for all\nnodes v. We also denote by v/ma\u221asto\u2192ythe existence of a hyperlink from vto\ny. The core of the iterative algorithm is a pair of updates to th e hub and au-\nthority scores of all pages given by Equation 21.8, which capture the intuitive\nnotions that good hubs point to good authorities and that goo d authorities\nare pointed to by good hubs.\nh(v)\u2190\u2211\nv/ma\u221asto\u2192ya(y) (21.8)\na(v)\u2190\u2211\ny/ma\u221asto\u2192vh(y).\nThus, the \ufb01rst line of Equation ( 21.8) sets the hub score of page vto the sum\nof the authority scores of the pages it links to. In other word s, if vlinks to\npages with high authority scores, its hub score increases. T he second line\nplays the reverse role; if page vis linked to by good hubs, its authority score\nincreases.\nWhat happens as we perform these updates iteratively, recom puting hub\nscores, then new authority scores based on the recomputed hu b scores, and\nso on? Let us recast the equations Equation ( 21.8) into matrix-vector form.\nLet/vectorhand/vectoradenote the vectors of all hub and all authority scores respec tively,\nfor the pages in our subset of the web graph. Let Adenote the adjacency\nmatrix of the subset of the web graph that we are dealing with: Ais a square\nmatrix with one row and one column for each page in the subset. The entry\nOnline edition (c)\n2009 Cambridge UP476 21 Link analysis\nAijis 1 if there is a hyperlink from page ito page j, and 0 otherwise. Then,\nwe may write Equation ( 21.8)\n/vectorh\u2190 A/vectora (21.9)\n/vectora\u2190 AT/vectorh,\nwhere ATdenotes the transpose of the matrix A. Now the right hand side of\neach line of Equation ( 21.9) is a vector that is the left hand side of the other\nline of Equation ( 21.9). Substituting these into one another, we may rewrite\nEquation ( 21.9) as\n/vectorh\u2190 AAT/vectorh (21.10)\n/vectora\u2190 ATA/vectora.\nNow, Equation ( 21.10 ) bears an uncanny resemblance to a pair of eigenvector\nequations (Section 18.1); indeed, if we replace the \u2190symbols by =symbols\nand introduce the (unknown) eigenvalue, the \ufb01rst line of Equ ation ( 21.10 )\nbecomes the equation for the eigenvectors of AAT, while the second becomes\nthe equation for the eigenvectors of ATA:\n/vectorh= ( 1/\u03bbh)AAT/vectorh\n/vectora= ( 1/\u03bba)ATA/vectora. (21.11)\nHere we have used \u03bbhto denote the eigenvalue of AATand \u03bbato denote the\neigenvalue of ATA.\nThis leads to some key consequences:\n1.The iterative updates in Equation ( 21.8) (or equivalently, Equation ( 21.9)),\nif scaled by the appropriate eigenvalues, are equivalent to the power iter-\nation method for computing the eigenvectors of AATand ATA. Provided\nthat the principal eigenvalue of AATis unique, the iteratively computed\nentries of /vectorhand/vectorasettle into unique steady-state values determined by the\nentries of Aand hence the link structure of the graph.\n2.In computing these eigenvector entries, we are not restrict ed to using the\npower iteration method; indeed, we could use any fast method for com-\nputing the principal eigenvector of a stochastic matrix.\nThe resulting computation thus takes the following form:\n1.Assemble the target subset of web pages, form the graph induc ed by their\nhyperlinks and compute AATand ATA.\n2.Compute the principal eigenvectors of AATand ATAto form the vector\nof hub scores /vectorhand authority scores /vectora.\nOnline edition (c)\n2009 Cambridge UP21.3 Hubs and Authorities 477\n3.Output the top-scoring hubs and the top-scoring authoritie s.\nThis method of link analysis is known as HITS , which is an acronym for HITS\nHyperlink-Induced Topic Search .\n\u270eExample 21.2: Assuming the query jaguar and double-weighting of links whose\nanchors contain the query word, the matrix Afor Figure 21.4 is as follows:\n0 0 1 0 0 0 0\n0 1 1 0 0 0 0\n1 0 1 2 0 0 0\n0 0 0 1 1 0 0\n0 0 0 0 0 0 1\n0 0 0 0 0 1 1\n0 0 0 2 1 0 1\nThe hub and authority vectors are:\n/vectorh= (0.03 0.04 0.33 0.18 0.04 0.04 0.35 )\n/vectora= (0.10 0.01 0.12 0.47 0.16 0.01 0.13 )\nHere, q3is the main authority \u2013 two hubs ( q2and q6) are pointing to it via highly\nweighted jaguar links.\nSince the iterative updates captured the intuition of good h ubs and good\nauthorities, the high-scoring pages we output would give us good hubs and\nauthorities from the target subset of web pages. In Section 21.3.1 we describe\nthe remaining detail: how do we gather a target subset of web p ages around\na topic such as leukemia ?\n21.3.1 Choosing the subset of the Web\nIn assembling a subset of web pages around a topic such as leukemia , we must\ncope with the fact that good authority pages may not contain t he speci\ufb01c\nquery term leukemia . This is especially true, as we noted in Section 21.1.1 ,\nwhen an authority page uses its web presence to project a cert ain market-\ning image. For instance, many pages on the IBM website are aut horitative\nsources of information on computer hardware, even though th ese pages may\nnot contain the term computer orhardware . However, a hub compiling com-\nputer hardware resources is likely to use these terms and als o link to the\nrelevant pages on the IBM website.\nBuilding on these observations, the following procedure ha s been sug-\ngested for compiling the subset of the Web for which to comput e hub and\nauthority scores.\nOnline edition (c)\n2009 Cambridge UP478 21 Link analysis\n1.Given a query (say leukemia ), use a text index to get all pages containing\nleukemia . Call this the root set of pages.\n2.Build the base set of pages, to include the root set as well as any page that\neither links to a page in the root set, or is linked to by a page i n the root\nset.\nWe then use the base set for computing hub and authority score s. The base\nset is constructed in this manner for three reasons:\n1.A good authority page may not contain the query text (such as computer\nhardware ).\n2.If the text query manages to capture a good hub page vhin the root set,\nthen the inclusion of all pages linked to by any page in the roo t set will\ncapture all the good authorities linked to by vhin the base set.\n3.Conversely, if the text query manages to capture a good autho rity page\nvain the root set, then the inclusion of pages which point to vawill bring\nother good hubs into the base set. In other words, the \u201cexpans ion\u201d of\nthe root set into the base set enriches the common pool of good hubs and\nauthorities.\nRunning HITS across a variety of queries reveals some intere sting insights\nabout link analysis. Frequently, the documents that emerge as top hubs and\nauthorities include languages other than the language of th e query. These\npages were presumably drawn into the base set, following the assembly of\nthe root set. Thus, some elements of cross-language retrieval (where a query\nin one language retrieves documents in another) are evident here; interest-\ningly, this cross-language effect resulted purely from lin k analysis, with no\nlinguistic translation taking place.\nWe conclude this section with some notes on implementing thi s algorithm.\nThe root set consists of all pages matching the text query; in fact, implemen-\ntations (see the references in Section 21.4) suggest that it suf\ufb01ces to use 200 or\nso web pages for the root set, rather than all pages matching t he text query.\nAny algorithm for computing eigenvectors may be used for com puting the\nhub/authority score vector. In fact, we need not compute the exact values\nof these scores; it suf\ufb01ces to know the relative values of the scores so that\nwe may identify the top hubs and authorities. To this end, it i s possible that\na small number of iterations of the power iteration method yi elds the rela-\ntive ordering of the top hubs and authorities. Experiments h ave suggested\nthat in practice, about \ufb01ve iterations of Equation ( 21.8) yield fairly good re-\nsults. Moreover, since the link structure of the web graph is fairly sparse\n(the average web page links to about ten others), we do not per form these as\nmatrix-vector products but rather as additive updates as in Equation ( 21.8).\nOnline edition (c)\n2009 Cambridge UP21.3 Hubs and Authorities 479\n\u25eeFigure 21.6 A sample run of HITS on the query japan elementary schools .\nFigure 21.6 shows the results of running HITS on the query japan elemen-\ntaryschools . The \ufb01gure shows the top hubs and authorities; each row lists the\ntitle tag from the corresponding HTML page. Because the resulting string\nis not necessarily in Latin characters, the resulting print is (in many cases)\na string of gibberish. Each of these corresponds to a web page that does\nnot use Latin characters, in this case very likely pages in Ja panese. There\nalso appear to be pages in other non-English languages, whic h seems sur-\nprising given that the query string is in English. In fact, th is result is em-\nblematic of the functioning of HITS \u2013 following the assembly of the root set,\nthe (English) query string is ignored. The base set is likely to contain pages\nin other languages, for instance if an English-language hub page links to\nthe Japanese-language home pages of Japanese elementary sc hools. Because\nthe subsequent computation of the top hubs and authorities i s entirely link-\nbased, some of these non-English pages will appear among the top hubs and\nauthorities.\n?Exercise 21.19\nIf all the hub and authority scores are initialized to 1, what is the hub/authority score\nof a node after one iteration?\nOnline edition (c)\n2009 Cambridge UP480 21 Link analysis\nExercise 21.20\nHow would you interpret the entries of the matrices AATand ATA? What is the\nconnection to the co-occurrence matrix CCTin Chapter 18?\nExercise 21.21\nWhat are the principal eigenvalues of AATand ATA?\nd1 d2\nd3\n\u25eeFigure 21.7 Web graph for Exercise 21.22 .\nExercise 21.22\nFor the web graph in Figure 21.7, compute PageRank, hub and authority scores for\neach of the three pages. Also give the relative ordering of th e 3 nodes for each of these\nscores, indicating any ties.\nPageRank: Assume that at each step of the PageRank random wal k, we teleport to a\nrandom page with probability 0.1, with a uniform distributi on over which particular\npage we teleport to.\nHubs/Authorities: Normalize the hub (authority) scores so that the maximum hub\n(authority) score is 1.\nHint 1: Using symmetries to simplify and solving with linear equations might be\neasier than using iterative methods.\nHint 2: Provide the relative ordering (indicating any ties) of the three nodes for each\nof the three scoring measures.\n21.4 References and further reading\nGar\ufb01eld (1955 ) is seminal in the science of citation analysis. This was bui lt\non by Pinski and Narin (1976 ) to develop a journal in\ufb02uence weight , whose\nde\ufb01nition is remarkably similar to that of the PageRank meas ure.\nThe use of anchor text as an aid to searching and ranking stems from the\nwork of McBryan (1994 ). Extended anchor-text was implicit in his work, with\nsystematic experiments reported in Chakrabarti et al. (1998 ).\nKemeny and Snell (1976 ) is a classic text on Markov chains. The PageRank\nmeasure was developed in Brin and Page (1998 ) and in Page et al. (1998 ).\nOnline edition (c)\n2009 Cambridge UP21.4 References and further reading 481\nA number of methods for the fast computation of PageRank valu es are sur-\nveyed in Berkhin (2005 ) and in Langville and Meyer (2006 ); the former also\ndetails how the PageRank eigenvector solution may be viewed as solving a\nlinear system, leading to one way of solving Exercise 21.16 . The effect of the\nteleport probability \u03b1has been studied by Baeza-Yates et al. (2005 ) and by\nBoldi et al. (2005 ). Topic-speci\ufb01c PageRank and variants were developed in\nHaveliwala (2002 ),Haveliwala (2003 ) and in Jeh and Widom (2003 ).Berkhin\n(2006a ) develops an alternate view of topic-speci\ufb01c PageRank.\nNg et al. (2001b ) suggests that the PageRank score assignment is more ro-\nbust than HITS in the sense that scores are less sensitive to s mall changes in\ngraph topology. However, it has also been noted that the tele port operation\ncontributes signi\ufb01cantly to PageRank\u2019s robustness in this sense. Both Page-\nRank and HITS can be \u201cspammed\u201d by the orchestrated insertion of links into\nthe web graph; indeed, the Web is known to have such link farms that col- LINK FARMS\nlude to increase the score assigned to certain pages by vario us link analysis\nalgorithms.\nThe HITS algorithm is due to Kleinberg (1999 ).Chakrabarti et al. (1998 ) de-\nveloped variants that weighted links in the iterative compu tation based on\nthe presence of query terms in the pages being linked and comp ared these\nto results from several web search engines. Bharat and Henzinger (1998 ) fur-\nther developed these and other heuristics, showing that cer tain combinations\noutperformed the basic HITS algorithm. Borodin et al. (2001 ) provides a sys-\ntematic study of several variants of the HITS algorithm. Ng et al. (2001b )\nintroduces a notion of stability for link analysis, arguing that small changes\nto link topology should not lead to signi\ufb01cant changes in the ranked list of\nresults for a query. Numerous other variants of HITS have bee n developed\nby a number of authors, the best know of which is perhaps SALSA (Lempel\nand Moran 2000 ).\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPDRAFT!\u00a9April1, 2009CambridgeUniversityPress. Feedbackwelcome . 483\nBibliography\nWe use the following abbreviated journal and conference nam es in the bibliography:\nCACM Communications of the Association for Computing Machinery .\nIP&M Information Processing and Management.\nIRInformation Retrieval.\nJACM Journal of the Association for Computing Machinery.\nJASIS Journal of the American Society for Information Science.\nJASIST Journal of the American Society for Information Science and Technology.\nJMLR Journal of Machine Learning Research.\nTOIS ACM Transactions on Information Systems.\nProc. ACL Proceedings of the Annual Meeting of the Association for Com putational\nLinguistics. Available from: http://www.aclweb.org/anthology-index/\nProc. CIKM Proceedings of the ACM CIKM Conference on Information and Kn ow-\nledge Management. ACM Press.\nProc. ECIR Proceedings of the European Conference on Information Retr ieval.\nProc. ECML Proceedings of the European Conference on Machine Learning .\nProc. ICML Proceedings of the International Conference on Machine Lea rning.\nProc. IJCAI Proceedings of the International Joint Conference on Arti\ufb01 cial Intelli-\ngence.\nProc. INEX Proceedings of the Initiative for the Evaluation of XML Retr ieval.\nProc. KDD Proceedings of the ACM SIGKDD International Conference on K now-\nledge Discovery and Data Mining.\nProc. NIPS Proceedings of the Neural Information Processing Systems C onference.\nProc. PODS Proceedings of the ACM Conference on Principles of Database Systems.\nProc. SDAIR Proceedings of the Annual Symposium on Document Analysis an d In-\nformation Retrieval.\nOnline edition (c)\n2009 Cambridge UP484 Bibliography\nProc. SIGIR Proceedings of the Annual International ACM/SIGIR Confere nce on\nResearch and Development in Information Retrieval. Availa ble from: http://www.sigir.org/proceedings/Proc-\nBrowse.html\nProc. SPIRE Proceedings of the Symposium on String Processing and Infor mation\nRetrieval.\nProc. TREC Proceedings of the Text Retrieval Conference.\nProc. UAI Proceedings of the Conference on Uncertainty in Arti\ufb01cial I ntelligence.\nProc. VLDB Proceedings of the Very Large Data Bases Conference.\nProc. WWW Proceedings of the International World Wide Web Conference .\nAberer, Karl. 2001. P-Grid: A self-organizing access struc ture for P2P information\nsystems. In Proc. International Conference on Cooperative Informatio n Systems , pp.\n179\u2013194. Springer. xxxiv ,519\nAizerman, Mark A., Emmanuel M. Braverman, and Lev I. Rozono\u00e9 r. 1964. Theoret-\nical foundations of the potential function method in patter n recognition learning.\nAutomation and Remote Control 25:821\u2013837. 347,519,520,530\nAkaike, Hirotugu. 1974. A new look at the statistical model i denti\ufb01cation. IEEE\nTransactions on automatic control 19(6):716\u2013723. 373,519\nAllan, James. 2005. HARD track overview in TREC 2005: High ac curacy retrieval\nfrom documents. In Proc. TREC .174,519\nAllan, James, Ron Papka, and Victor Lavrenko. 1998. On-line new event\ndetection and tracking. In Proc. SIGIR , pp. 37\u201345. ACM Press. DOI:\ndoi.acm.org/10.1145/290941.290954 .399,519,526,528\nAllwein, Erin L., Robert E. Schapire, and Yoram Singer. 2000 . Reducing multiclass\nto binary: A unifying approach for margin classi\ufb01ers. JMLR 1:113\u2013141. URL:\nwww.jmlr.org/papers/volume1/allwein00a/allwein00a.p df.315,519,530,531\nAlonso, Omar, Sandeepan Banerjee, and Mark Drake. 2006. GIO : A semantic web\napplication using the information grid framework. In Proc. WWW , pp. 857\u2013858.\nACM Press. DOI:doi.acm.org/10.1145/1135777.1135913 .373,519,522\nAlting\u00f6vde, Ismail Seng\u00f6r, Engin Demir, Fazli Can, and \u00d6zg\u00fc r Ulusoy. 2008. In-\ncremental cluster-based retrieval using compressed clust er-skipping inverted \ufb01les.\nTOIS . To appear. 372\nAlting\u00f6vde, Ismail Seng\u00f6r, Rifat Ozcan, Huseyin Cagdas Oca lan, Fazli Can, and\n\u00d6zg\u00fcr Ulusoy. 2007. Large-scale cluster-based retrieval e xperiments on Turkish\ntexts. In Proc. SIGIR , pp. 891\u2013892. ACM Press. 519,521,528,532\nAmer-Yahia, Sihem, Chavdar Botev, Jochen D\u00f6rre, and Jayave l Shanmugasundaram.\n2006. XQuery full-text extensions explained. IBM Systems Journal 45(2):335\u2013352.\n217,519,520,522,530\nAmer-Yahia, Sihem, Pat Case, Thomas R\u00f6lleke, Jayavel Shanm ugasundaram, and\nGerhard Weikum. 2005. Report on the DB/IR panel at SIGMOD 200 5.SIGMOD\nRecord 34(4):71\u201374. DOI:doi.acm.org/10.1145/1107499.1107514 .217,519,521,530,532\nOnline edition (c)\n2009 Cambridge UPBibliography 485\nAmer-Yahia, Sihem, and Mounia Lalmas. 2006. XML search: Lan guages, INEX and\nscoring. SIGMOD Record 35(4):16\u201323. DOI:doi.acm.org/10.1145/1228268.1228271 .\n217,519,526\nAnagnostopoulos, Aris, Andrei Z. Broder, and Kunal Punera. 2006. Effective and\nef\ufb01cient classi\ufb01cation on a search-engine model. In Proc. CIKM , pp. 208\u2013217. ACM\nPress. DOI:doi.acm.org/10.1145/1183614.1183648 .315,519,520,529\nAnderberg, Michael R. 1973. Cluster analysis for applications . Academic Press. 372,519\nAndoni, Alexandr, Mayur Datar, Nicole Immorlica, Piotr Ind yk, and Vahab Mirrokni.\n2006. Locality-sensitive hashing using stable distributi ons. In Nearest Neighbor\nMethods in Learning and Vision: Theory and Practice . MIT Press. 314,519,522,524,\n527\nAnh, Vo Ngoc, Owen de Kretser, and Alistair Moffat. 2001. Vec tor-space ranking with\neffective early termination. In Proc. SIGIR , pp. 35\u201342. ACM Press. 149,519,526,527\nAnh, Vo Ngoc, and Alistair Moffat. 2005. Inverted index com-\npression using word-aligned binary codes. IR 8(1):151\u2013166. DOI:\ndx.doi.org/10.1023/B:INRT.0000048490.99518.5c .106,519,527\nAnh, Vo Ngoc, and Alistair Moffat. 2006a. Improved word-ali gned binary compres-\nsion for text indexing. IEEE Transactions on Knowledge and Data Engineering 18(6):\n857\u2013861. 106,519,527\nAnh, Vo Ngoc, and Alistair Moffat. 2006b. Pruned query evalu ation us-\ning pre-computed impacts. In Proc. SIGIR , pp. 372\u2013379. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148235 .149,519,527\nAnh, Vo Ngoc, and Alistair Moffat. 2006c. Structured index o rganizations for high-\nthroughput text querying. In Proc. SPIRE , pp. 304\u2013315. Springer. 149,519,527\nApt\u00e9, Chidanand, Fred Damerau, and Sholom M. Weiss. 1994. Au tomated learning\nof decision rules for text categorization. TOIS 12(1):233\u2013251. 286,519,522,532\nArthur, David, and Sergei Vassilvitskii. 2006. How slow is t hek-means method? In\nProc. ACM Symposium on Computational Geometry , pp. 144\u2013153. 373,519,532\nArvola, Paavo, Marko Junkkari, and Jaana Kek\u00e4l\u00e4inen. 2005. Generalized contextual-\nization method for XML information retrieval. In Proc. CIKM , pp. 20\u201327. 216,519,\n525\nAslam, Javed A., and Emine Yilmaz. 2005. A geometric interpr etation and analysis\nof R-precision. In Proc. CIKM , pp. 664\u2013671. ACM Press. 174,519,533\nAult, Thomas Galen, and Yiming Yang. 2002. Information \ufb01lte ring in TREC-9 and\nTDT-3: A comparative analysis. IR5(2-3):159\u2013187. 315,519,533\nBadue, Claudine Santos, Ricardo A. Baeza-Yates, Berthier R ibeiro-Neto, and Nivio\nZiviani. 2001. Distributed query processing using partiti oned inverted \ufb01les. In\nProc. SPIRE , pp. 10\u201320. 459,519,529,533\nBaeza-Yates, Ricardo, Paolo Boldi, and Carlos Castillo. 20 05. The choice of a damp-\ning function for propagating importance in link-based rank ing. Technical report,\nDipartimento di Scienze dell\u2019Informazione, Universit\u00e0 de gli Studi di Milano. 481,\n519,520,521\nOnline edition (c)\n2009 Cambridge UP486 Bibliography\nBaeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval .\nAddison Wesley. xxxiv ,84,105,175,400,519,529\nBahle, Dirk, Hugh E. Williams, and Justin Zobel. 2002. Ef\ufb01ci ent phrase querying with\nan auxiliary index. In Proc. SIGIR , pp. 215\u2013221. ACM Press. 47,519,533\nBaldridge, Jason, and Miles Osborne. 2004. Active learning and the total cost of\nannotation. In Proc. Empirical Methods in Natural Language Processing , pp. 9\u201316. 348,\n519,528\nBall, G. H. 1965. Data analysis in the social sciences: What a bout the details? In Proc.\nFall Joint Computer Conference , pp. 533\u2013560. Spartan Books. 373,519\nBanko, Michele, and Eric Brill. 2001. Scaling to very very la rge corpora for natural\nlanguage disambiguation. In Proc. ACL .337,519,520\nBar-Ilan, Judit, and Tatyana Gutman. 2005. How do search eng ines respond to some\nnon-English queries? Journal of Information Science 31(1):13\u201328. 46,519,523\nBar-Yossef, Ziv, and Maxim Gurevich. 2006. Random sampling from a\nsearch engine\u2019s index. In Proc. WWW , pp. 367\u2013376. ACM Press. DOI:\ndoi.acm.org/10.1145/1135777.1135833 .442,519,523\nBarroso, Luiz Andr\u00e9, Jeffrey Dean, and Urs H\u00f6lzle. 2003. Web search for\na planet: The Google cluster architecture. IEEE Micro 23(2):22\u201328. DOI:\ndx.doi.org/10.1109/MM.2003.1196112 .459,519,522,524\nBartell, Brian Theodore. 1994. Optimizing ranking functions: A connectionist approach to\nadaptive information retrieval . PhD thesis, University of California at San Diego, La\nJolla, CA. 150,519\nBartell, Brian T., Garrison W. Cottrell, and Richard K. Bele w. 1998. Optimizing sim-\nilarity using multi-query relevance feedback. JASIS 49(8):742\u2013761. 150,519,520,\n521\nBarzilay, Regina, and Michael Elhadad. 1997. Using lexical chains for text summa-\nrization. In Workshop on Intelligent Scalable Text Summarization , pp. 10\u201317. 174,520,\n522\nBast, Holger, and Debapriyo Majumdar. 2005. Why spectral re trieval works. In Proc.\nSIGIR , pp. 11\u201318. ACM Press. DOI:doi.acm.org/10.1145/1076034.1076040 .417,520,\n527\nBasu, Sugato, Arindam Banerjee, and Raymond J. Mooney. 2004 . Active semi-\nsupervision for pairwise constrained clustering. In Proc. SIAM International Con-\nference on Data Mining , pp. 333\u2013344. 373,519,520,528\nBeesley, Kenneth R. 1998. Language identi\ufb01er: A computer pr ogram for automatic\nnatural-language identi\ufb01cation of on-line text. In Languages at Crossroads: Proc.\nAnnual Conference of the American Translators Association , pp. 47\u201354. 46,520\nBeesley, Kenneth R., and Lauri Karttunen. 2003. Finite State Morphology . CSLI Publi-\ncations. 46,520,525\nBennett, Paul N. 2000. Assessing the calibration of naive Ba yes\u2019 posterior estimates.\nTechnical Report CMU-CS-00-155, School of Computer Scienc e, Carnegie Mellon\nUniversity. 286,520\nOnline edition (c)\n2009 Cambridge UPBibliography 487\nBerger, Adam, and John Lafferty. 1999. Information retriev al as statistical translation.\nInProc. SIGIR , pp. 222\u2013229. ACM Press. 251,252,520,526\nBerkhin, Pavel. 2005. A survey on pagerank computing. Internet Mathematics 2(1):\n73\u2013120. 481,520\nBerkhin, Pavel. 2006a. Bookmark-coloring algorithm for pe rsonalized pagerank com-\nputing. Internet Mathematics 3(1):41\u201362. 481,520\nBerkhin, Pavel. 2006b. A survey of clustering data mining te chniques. In Jacob Kogan,\nCharles Nicholas, and Marc Teboulle (eds.), Grouping Multidimensional Data: Recent\nAdvances in Clustering , pp. 25\u201371. Springer. 372,520\nBerners-Lee, Tim, Robert Cailliau, Jean-Francois Groff, a nd Bernd Pollermann.\n1992. World-Wide Web: The information universe. Electronic Networking: Re-\nsearch, Applications and Policy 1(2):74\u201382. URL:citeseer.ist.psu.edu/article/berners-\nlee92worldwide.html .441,520,521,523,529\nBerry, Michael, and Paul Young. 1995. Using latent semantic indexing for multilan-\nguage information retrieval. Computers and the Humanities 29(6):413\u2013429. 417,520,\n533\nBerry, Michael W., Susan T. Dumais, and Gavin W. O\u2019Brien. 199 5. Using linear algebra\nfor intelligent information retrieval. SIAM Review 37(4):573\u2013595. 417,520,522,528\nBetsi, Stamatina, Mounia Lalmas, Anastasios Tombros, and T heodora Tsikrika. 2006.\nUser expectations from XML element retrieval. In Proc. SIGIR , pp. 611\u2013612. ACM\nPress. 217,520,526,531,532\nBharat, Krishna, and Andrei Broder. 1998. A technique for me asuring the relative size\nand overlap of public web search engines. Computer Networks and ISDN Systems 30\n(1-7):379\u2013388. DOI:dx.doi.org/10.1016/S0169-7552(98)00127-5 .442,520\nBharat, Krishna, Andrei Broder, Monika Henzinger, Puneet K umar, and Suresh\nVenkatasubramanian. 1998. The connectivity server: Fast a ccess to linkage in-\nformation on the web. In Proc. WWW , pp. 469\u2013477. 459,520,524,526,532\nBharat, Krishna, Andrei Z. Broder, Jeffrey Dean, and Monika Rauch Henzinger. 2000.\nA comparison of techniques to \ufb01nd mirrored hosts on the WWW. JASIS 51(12):\n1114\u20131122. URL:citeseer.ist.psu.edu/bharat99comparison.html .442,520,522,524\nBharat, Krishna, and Monika R. Henzinger. 1998. Improved al gorithms for topic\ndistillation in a hyperlinked environment. In Proc. SIGIR , pp. 104\u2013111. ACM Press.\nURL:citeseer.ist.psu.edu/bharat98improved.html .481,520,524\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning . Springer. 315,\n520\nBlair, David C., and M. E. Maron. 1985. An evaluation of retri eval effectiveness for a\nfull-text document-retrieval system. CACM 28(3):289\u2013299. 193,520,527\nBlanco, Roi, and Alvaro Barreiro. 2006. TSP and cluster-bas ed solutions to the reas-\nsignment of document identi\ufb01ers. IR9(4):499\u2013517. 106,519,520\nBlanco, Roi, and Alvaro Barreiro. 2007. Boosting static pru ning of inverted \ufb01les. In\nProc. SIGIR . ACM Press. 105,519,520\nOnline edition (c)\n2009 Cambridge UP488 Bibliography\nBlandford, Dan, and Guy Blelloch. 2002. Index compression t hrough document re-\nordering. In Proc. Data Compression Conference , p. 342. IEEE Computer Society. 106,\n520\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Lat ent Dirichlet allocation.\nJMLR 3:993\u20131022. 418,520,525,528\nBoldi, Paolo, Bruno Codenotti, Massimo Santini, and Sebast iano Vigna. 2002. Ubi-\ncrawler: A scalable fully distributed web crawler. In Proc. Australian World Wide\nWeb Conference .URL:citeseer.ist.psu.edu/article/boldi03ubicrawler.html .458,520,521,\n530,532\nBoldi, Paolo, Massimo Santini, and Sebastiano Vigna. 2005. PageRank as a function\nof the damping factor. In Proc. WWW .URL:citeseer.ist.psu.edu/boldi05pagerank.html .\n481,520,530,532\nBoldi, Paolo, and Sebastiano Vigna. 2004a. Codes for the Wor ld-Wide Web. Internet\nMathematics 2(4):405\u2013427. 459,520,532\nBoldi, Paolo, and Sebastiano Vigna. 2004b. The WebGraph fra mework I: Compression\ntechniques. In Proc. WWW , pp. 595\u2013601. ACM Press. 459,520,532\nBoldi, Paolo, and Sebastiano Vigna. 2005. Compressed perfe ct embedded skip lists\nfor quick inverted-index lookups. In Proc. SPIRE . Springer. 46,520,532\nBoley, Daniel. 1998. Principal direction divisive partiti oning. Data Mining and Know-\nledge Discovery 2(4):325\u2013344. DOI:dx.doi.org/10.1023/A:1009740529316 .400,520\nBorodin, Allan, Gareth O. Roberts, Jeffrey S. Rosenthal, an d Panayiotis Tsaparas.\n2001. Finding authorities and hubs from link structures on t he World Wide Web.\nInProc. WWW , pp. 415\u2013429. 481,520,529,530,532\nBourne, Charles P ., and Donald F. Ford. 1961. A study of metho ds for sys-\ntematically abbreviating English words and names. JACM 8(4):538\u2013552. DOI:\ndoi.acm.org/10.1145/321088.321094 .65,520,523\nBradley, Paul S., and Usama M. Fayyad. 1998. Re\ufb01ning initial points for K-means\nclustering. In Proc. ICML , pp. 91\u201399. 373,520,522\nBradley, Paul S., Usama M. Fayyad, and Cory Reina. 1998. Scal ing clustering algo-\nrithms to large databases. In Proc. KDD , pp. 9\u201315. 374,520,522,529\nBrill, Eric, and Robert C. Moore. 2000. An improved error mod el for noisy channel\nspelling correction. In Proc. ACL , pp. 286\u2013293. 65,520,528\nBrin, Sergey, and Lawrence Page. 1998. The anatomy of a large -scale hypertextual\nweb search engine. In Proc. WWW , pp. 107\u2013117. 149,458,480,520,528\nBrisaboa, Nieves R., Antonio Fari\u00f1a, Gonzalo Navarro, and J os\u00e9 R. Param\u00e1. 2007.\nLightweight natural language text compression. IR10(1):1\u201333. 107,520,522,528\nBroder, Andrei. 2002. A taxonomy of web search. SIGIR Forum 36(2):3\u201310. DOI:\ndoi.acm.org/10.1145/792550.792552 .442,520\nBroder, Andrei, S. Ravi Kumar, Farzin Maghoul, Prabhakar Ra ghavan, Sridhar Ra-\njagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph struc-\nture in the web. Computer Networks 33(1):309\u2013320. 441,520,526,527,529,531,\n532\nOnline edition (c)\n2009 Cambridge UPBibliography 489\nBroder, Andrei Z., Steven C. Glassman, Mark S. Manasse, and G eoffrey Zweig. 1997.\nSyntactic clustering of the web. In Proc. WWW , pp. 391\u2013404. 442,520,523,527,533\nBrown, Eric W. 1995. Execution Performance Issues in Full-Text Information Ret rieval .\nPhD thesis, University of Massachusetts, Amherst. 149,520\nBuckley, Chris, James Allan, and Gerard Salton. 1994a. Auto matic routing and ad-hoc\nretrieval using SMART: TREC 2. In Proc. TREC , pp. 45\u201355. 314,519,520,530\nBuckley, Chris, and Gerard Salton. 1995. Optimization of re levance feedback weights.\nInProc. SIGIR , pp. 351\u2013357. ACM Press. DOI:doi.acm.org/10.1145/215206.215383 .\n315,520,530\nBuckley, Chris, Gerard Salton, and James Allan. 1994b. The e ffect of adding relevance\ninformation in a relevance feedback environment. In Proc. SIGIR , pp. 292\u2013300.\nACM Press. 185,194,314,519,520,530\nBuckley, Chris, Amit Singhal, and Mandar Mitra. 1995. New re trieval approaches\nusing SMART: TREC 4. In Proc. TREC .187,520,527,531\nBuckley, Chris, and Ellen M. Voorhees. 2000. Evaluating eva luation measure stability.\nInProc. SIGIR , pp. 33\u201340. 173,174,520,532\nBurges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier, Matt De eds, Nicole Hamilton,\nand Greg Hullender. 2005. Learning to rank using gradient de scent. In Proc. ICML .\n348,520,522,523,524,526,529,530\nBurges, Christopher J. C. 1998. A tutorial on support vector machines for pattern\nrecognition. Data Mining and Knowledge Discovery 2(2):121\u2013167. 346,520\nBurner, Mike. 1997. Crawling towards eternity: Building an archive of the World\nWide Web. Web Techniques Magazine 2(5). 458,520\nBurnham, Kenneth P ., and David Anderson. 2002. Model Selection and Multi-Model\nInference . Springer. 373,519,521\nBush, Vannevar. 1945. As we may think. The Atlantic Monthly . URL:\nwww.theatlantic.com/doc/194507/bush .17,441,521\nB\u00fcttcher, Stefan, and Charles L. A. Clarke. 2005a. Indexing time vs. query time:\nTrade-offs in dynamic information retrieval systems. In Proc. CIKM , pp. 317\u2013318.\nACM Press. DOI:doi.acm.org/10.1145/1099554.1099645 .84,521\nB\u00fcttcher, Stefan, and Charles L. A. Clarke. 2005b. A securit y model for full-\ntext \ufb01le system search in multi-user environments. In Proc. FAST . URL:\nwww.usenix.org/events/fast05/tech/buettcher.html .84,521\nB\u00fcttcher, Stefan, and Charles L. A. Clarke. 2006. A document -centric approach to\nstatic index pruning in text retrieval systems. In Proc. CIKM , pp. 182\u2013189. DOI:\ndoi.acm.org/10.1145/1183614.1183644 .105,521\nB\u00fcttcher, Stefan, Charles L. A. Clarke, and Brad Lushman. 20 06. Hybrid index main-\ntenance for growing text collections. In Proc. SIGIR , pp. 356\u2013363. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148233 .84,521,527\nCacheda, Fidel, Victor Carneiro, Carmen Guerrero, and \u00c1nge l Vi\u00f1a. 2003. Optimiza-\ntion of restricted searches in web directories using hybrid data structures. In Proc.\nECIR , pp. 436\u2013451. 372,521,523,532\nOnline edition (c)\n2009 Cambridge UP490 Bibliography\nCallan, Jamie. 2000. Distributed information retrieval. I n W. Bruce Croft (ed.), Ad-\nvances in information retrieval , pp. 127\u2013150. Kluwer. 84,521\nCan, Fazli, Ismail Seng\u00f6r Alting\u00f6vde, and Engin Demir. 2004 . Ef\ufb01ciency and effec-\ntiveness of query processing in cluster-based retrieval. Information Systems 29(8):\n697\u2013717. DOI:dx.doi.org/10.1016/S0306-4379(03)00062-0 .372,519,521,522\nCan, Fazli, and Esen A. Ozkarahan. 1990. Concepts and effect iveness of the cover-\ncoef\ufb01cient-based clustering methodology for text databas es.ACM Trans. Database\nSyst. 15(4):483\u2013517. 372,521,528\nCao, Guihong, Jian-Yun Nie, and Jing Bai. 2005. Integrating word relationships into\nlanguage models. In Proc. SIGIR , pp. 298\u2013305. ACM Press. 252,519,521,528\nCao, Yunbo, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsi ao-Wuen Hon. 2006.\nAdapting Ranking SVM to document retrieval. In Proc. SIGIR . ACM Press. 348,521,\n524,526,533\nCarbonell, Jaime, and Jade Goldstein. 1998. The use of MMR, d iversity-based rerank-\ning for reordering documents and producing summaries. In Proc. SIGIR , pp. 335\u2013\n336. ACM Press. DOI:doi.acm.org/10.1145/290941.291025 .167,521,523\nCarletta, Jean. 1996. Assessing agreement on classi\ufb01catio n tasks: The kappa statistic.\nComputational Linguistics 22:249\u2013254. 174,521\nCarmel, David, Doron Cohen, Ronald Fagin, Eitan Farchi, Mic hael Herscovici,\nYoelle S. Maarek, and Aya Soffer. 2001. Static index pruning for infor-\nmation retrieval systems. In Proc. SIGIR , pp. 43\u201350. ACM Press. DOI:\ndoi.acm.org/10.1145/383952.383958 .105,149,521,522,524,527,531\nCarmel, David, Yoelle S. Maarek, Matan Mandelbrod, Yosi Mas s, and Aya Soffer.\n2003. Searching XML documents via XML fragments. In Proc. SIGIR , pp. 151\u2013158.\nACM Press. DOI:doi.acm.org/10.1145/860435.860464 .216,521,527,531\nCaruana, Rich, and Alexandru Niculescu-Mizil. 2006. An emp irical comparison of\nsupervised learning algorithms. In Proc. ICML .347,521,528\nCastro, R. M., M. J. Coates, and R. D. Nowak. 2004. Likelihood based hierarchical\nclustering. IEEE Transactions in Signal Processing 52(8):2308\u20132321. 400,521,528\nCavnar, William B., and John M. Trenkle. 1994. N-gram-based text categorization. In\nProc. SDAIR , pp. 161\u2013175. 46,521,532\nChakrabarti, Soumen. 2002. Mining the Web: Analysis of Hypertext and Semi Structured\nData . Morgan Kaufmann. 442,521\nChakrabarti, Soumen, Byron Dom, David Gibson, Jon Kleinber g, Prabhakar Ragha-\nvan, and Sridhar Rajagopalan. 1998. Automatic resource lis t compilation by\nanalyzing hyperlink structure and associated text. In Proc. WWW .URL:cite-\nseer.ist.psu.edu/chakrabarti98automatic.html .480,481,521,522,523,525,529\nChapelle, Olivier, Bernhard Sch\u00f6lkopf, and Alexander Zien (eds.). 2006. Semi-\nSupervised Learning . MIT Press. 347,500,507,521,533\nChaudhuri, Surajit, Gautam Das, Vagelis Hristidis, and Ger hard Weikum.\n2006. Probabilistic information retrieval approach for ra nking of database\nquery results. ACM Transactions on Database Systems 31(3):1134\u20131168. DOI:\ndoi.acm.org/10.1145/1166074.1166085 .217,521,522,524,532\nOnline edition (c)\n2009 Cambridge UPBibliography 491\nCheeseman, Peter, and John Stutz. 1996. Bayesian classi\ufb01ca tion (AutoClass): Theory\nand results. In Advances in Knowledge Discovery and Data Mining , pp. 153\u2013180. MIT\nPress. 374,521,531\nChen, Hsin-Hsi, and Chuan-Jie Lin. 2000. A multilingual new s summarizer. In Proc.\nCOLING , pp. 159\u2013165. 373,521,526\nChen, Pai-Hsuen, Chih-Jen Lin, and Bernhard Sch\u00f6lkopf. 200 5. A tutorial on \u03bd-\nsupport vector machines. Applied Stochastic Models in Business and Industry 21:\n111\u2013136. 346,521,526,530\nChiaramella, Yves, Philippe Mulhem, and Franck Fourel. 199 6. A model for multime-\ndia information retrieval. Technical Report 4-96, Univers ity of Glasgow. 216,521,\n523,528\nChierichetti, Flavio, Alessandro Panconesi, Prabhakar Ra ghavan, Mauro Sozio,\nAlessandro Tiberi, and Eli Upfal. 2007. Finding near neighb ors through cluster\npruning. In Proc. PODS .149,521,528,529,531,532\nCho, Junghoo, and Hector Garcia-Molina. 2002. Parallel cra wlers. In Proc. WWW , pp.\n124\u2013135. ACM Press. DOI:doi.acm.org/10.1145/511446.511464 .458,521,523\nCho, Junghoo, Hector Garcia-Molina, and Lawrence Page. 199 8. Ef\ufb01cient crawling\nthrough URL ordering. In Proc. WWW , pp. 161\u2013172. 458,521,523,528\nChu-Carroll, Jennifer, John Prager, Krzysztof Czuba, Davi d Ferrucci, and\nPablo Duboue. 2006. Semantic search via XML fragments: A hig h-\nprecision approach to IR. In Proc. SIGIR , pp. 445\u2013452. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148247 .216,521,522,529\nClarke, Charles L.A., Gordon V . Cormack, and Elizabeth A. Tu dhope. 2000. Relevance\nranking for one to three term queries. IP&M 36:291\u2013311. 149,521,532\nCleverdon, Cyril W. 1991. The signi\ufb01cance of the Cran\ufb01eld te sts on index languages.\nInProc. SIGIR , pp. 3\u201312. ACM Press. 17,173,521\nCoden, Anni R., Eric W. Brown, and Savitha Srinivasan (eds.) . 2002. Information\nRetrieval Techniques for Speech Applications . Springer. xxxiv ,520,521,531\nCohen, Paul R. 1995. Empirical methods for arti\ufb01cial intelligence . MIT Press. 286,521\nCohen, William W. 1998. Integration of heterogeneous datab ases without common\ndomains using queries based on textual similarity. In Proc. SIGMOD , pp. 201\u2013212.\nACM Press. 217,521\nCohen, William W., Robert E. Schapire, and Yoram Singer. 199 8. Learn-\ning to order things. In Proc. NIPS . The MIT Press. URL:cite-\nseer.ist.psu.edu/article/cohen98learning.html .150,521,530,531\nCohen, William W., and Yoram Singer. 1999. Context-sensiti ve learning methods for\ntext categorization. TOIS 17(2):141\u2013173. 339,521,531\nComtet, Louis. 1974. Advanced Combinatorics . Reidel. 356,521\nCooper, William S., Aitao Chen, and Fredric C. Gey. 1994. Ful l text retrieval based on\nprobabilistic equations with coef\ufb01cients \ufb01tted by logisti c regression. In Proc. TREC ,\npp. 57\u201366. 150,521,523\nOnline edition (c)\n2009 Cambridge UP492 Bibliography\nCormen, Thomas H., Charles Eric Leiserson, and Ronald L. Riv est. 1990. Introduction\nto Algorithms . MIT Press. 11,79,399,521,526,529\nCover, Thomas M., and Peter E. Hart. 1967. Nearest neighbor p attern classi\ufb01cation.\nIEEE Transactions on Information Theory 13(1):21\u201327. 315,521,524\nCover, Thomas M., and Joy A. Thomas. 1991. Elements of Information Theory . Wiley.\n106,251,521,531\nCrammer, Koby, and Yoram Singer. 2001. On the algorithmic im plementation of\nmulticlass kernel-based machines. JMLR 2:265\u2013292. 347,521,531\nCreecy, Robert H., Brij M. Masand, Stephen J. Smith, and Davi d L. Waltz. 1992.\nTrading MIPS and memory for knowledge engineering. CACM 35(8):48\u201364. DOI:\ndoi.acm.org/10.1145/135226.135228 .314,521,527,531,532\nCrestani, Fabio, Mounia Lalmas, Cornelis J. Van Rijsbergen , and Iain Campbell.\n1998. Is this document relevant? . . . probably: A survey of pr obabilistic mod-\nels in information retrieval. ACM Computing Surveys 30(4):528\u2013552. DOI:\ndoi.acm.org/10.1145/299917.299920 .235,521,526,529\nCristianini, Nello, and John Shawe-Taylor. 2000. Introduction to Support Vector Ma-\nchines and Other Kernel-based Learning Methods . Cambridge University Press. 346,\n521,530\nCroft, W. Bruce. 1978. A \ufb01le organization for cluster-based retrieval. In Proc. SIGIR ,\npp. 65\u201382. ACM Press. 372,521\nCroft, W. Bruce, and David J. Harper. 1979. Using probabilis tic models of document\nretrieval without relevance information. Journal of Documentation 35(4):285\u2013295.\n133,227,521,524\nCroft, W. Bruce, and John Lafferty (eds.). 2003. Language Modeling for Information\nRetrieval . Springer. 252,522,526\nCrouch, Carolyn J. 1988. A cluster-based approach to thesau rus construction. In Proc.\nSIGIR , pp. 309\u2013320. ACM Press. DOI:doi.acm.org/10.1145/62437.62467 .374,522\nCucerzan, Silviu, and Eric Brill. 2004. Spelling correctio n as an iterative process that\nexploits the collective knowledge of web users. In Proc. Empirical Methods in Natural\nLanguage Processing .65,520,522\nCutting, Douglas R., David R. Karger, and Jan O. Pedersen. 19 93. Constant\ninteraction-time Scatter/Gather browsing of very large do cument collections. In\nProc. SIGIR , pp. 126\u2013134. ACM Press. 399,522,525,528\nCutting, Douglas R., Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scat-\nter/Gather: A cluster-based approach to browsing large doc ument collections. In\nProc. SIGIR , pp. 318\u2013329. ACM Press. 372,399,522,525,528,532\nDamerau, Fred J. 1964. A technique for computer detection an d correction of spelling\nerrors. CACM 7(3):171\u2013176. DOI:doi.acm.org/10.1145/363958.363994 .65,522\nDavidson, Ian, and Ashwin Satyanarayana. 2003. Speeding up k-means clustering\nby bootstrap averaging. In ICDM 2003 Workshop on Clustering Large Data Sets .373,\n522,530\nOnline edition (c)\n2009 Cambridge UPBibliography 493\nDay, William H., and Herbert Edelsbrunner. 1984. Ef\ufb01cient a lgorithms for agglomer-\native hierarchical clustering methods. Journal of Classi\ufb01cation 1:1\u201324. 399,522\nde Moura, Edleno Silva, Gonzalo Navarro, Nivio Ziviani, and Ricardo Baeza-Yates.\n2000. Fast and \ufb02exible word searching on compressed text. TOIS 18(2):113\u2013139.\nDOI:doi.acm.org/10.1145/348751.348754 .107,519,528,533\nDean, Jeffrey, and Sanjay Ghemawat. 2004. MapReduce: Simpl i\ufb01ed data processing\non large clusters. In Proc. Symposium on Operating System Design and Implementat ion.\nxx,76,83,522,523\nDeerwester, Scott, Susan T. Dumais, George W. Furnas, Thoma s K. Landauer, and\nRichard Harshman. 1990. Indexing by latent semantic analys is.JASIS 41(6):391\u2013\n407. 417,522,523,524,526\ndel Bimbo, Alberto. 1999. Visual Information Retrieval . Morgan Kaufmann. xxxiv ,533\nDempster, A.P ., N.M. Laird, and D.B. Rubin. 1977. Maximum li kelihood from incom-\nplete data via the EM algorithm. Journal of the Royal Statistical Society Series B 39:\n1\u201338. 373,522,526,530\nDhillon, Inderjit S. 2001. Co-clustering documents and wor ds using bipartite spectral\ngraph partitioning. In Proc. KDD , pp. 269\u2013274. 374,400,522\nDhillon, Inderjit S., and Dharmendra S. Modha. 2001. Concep t decompositions for\nlarge sparse text data using clustering. Machine Learning 42(1/2):143\u2013175. DOI:\ndx.doi.org/10.1023/A:1007612920971 .373,522,527\nDi Eugenio, Barbara, and Michael Glass. 2004. The kappa stat istic: A second look.\nComputational Linguistics 30(1):95\u2013101. DOI:dx.doi.org/10.1162/089120104773633402 .\n174,522,523\nDietterich, Thomas G. 2002. Ensemble learning. In Michael A . Arbib (ed.), The Hand-\nbook of Brain Theory and Neural Networks , 2nd edition. MIT Press. 347,522\nDietterich, Thomas G., and Ghulum Bakiri. 1995. Solving mul ticlass learning prob-\nlems via error-correcting output codes. Journal of Arti\ufb01cial Intelligence Research 2:\n263\u2013286. 315,519,522\nDom, Byron E. 2002. An information-theoretic external clus ter-validity measure. In\nProc. UAI .373,522\nDomingos, Pedro. 2000. A uni\ufb01ed bias-variance decompositi on for zero-one and\nsquared loss. In Proc. National Conference on Arti\ufb01cial Intelligence and Pr oc. Conference\nInnovative Applications of Arti\ufb01cial Intelligence , pp. 564\u2013569. AAAI Press / The MIT\nPress. 315,522\nDomingos, Pedro, and Michael J. Pazzani. 1997. On the optima lity of the simple\nBayesian classi\ufb01er under zero-one loss. Machine Learning 29(2-3):103\u2013130. URL:\nciteseer.ist.psu.edu/domingos97optimality.html .286,522,528\nDownie, J. Stephen. 2006. The Music Information Retrieval E valuation eXchange\n(MIREX). D-Lib Magazine 12(12). xxxiv ,522\nDuda, Richard O., Peter E. Hart, and David G. Stork. 2000. Pattern Classi\ufb01cation , 2nd\nedition. Wiley-Interscience. 286,372,522,524,531\nOnline edition (c)\n2009 Cambridge UP494 Bibliography\nDumais, Susan, John Platt, David Heckerman, and Mehran Saha mi. 1998. Inductive\nlearning algorithms and representations for text categori zation. In Proc. CIKM , pp.\n148\u2013155. ACM Press. DOI:doi.acm.org/10.1145/288627.288651 .xvii,282,333,334,\n347,522,524,529,530\nDumais, Susan T. 1993. Latent semantic indexing (LSI) and TR EC-2. In Proc. TREC ,\npp. 105\u2013115. 415,417,522\nDumais, Susan T. 1995. Latent semantic indexing (LSI): TREC -3 report. In Proc. TREC ,\npp. 219\u2013230. 416,417,522\nDumais, Susan T., and Hao Chen. 2000. Hierarchical classi\ufb01c ation of Web content. In\nProc. SIGIR , pp. 256\u2013263. ACM Press. 347,521,522\nDunning, Ted. 1993. Accurate methods for the statistics of s urprise and coincidence.\nComputational Linguistics 19(1):61\u201374. 286,522\nDunning, Ted. 1994. Statistical identi\ufb01cation of language . Technical Report 94-273,\nComputing Research Laboratory, New Mexico State Universit y.46,522\nEckart, Carl, and Gale Young. 1936. The approximation of a ma trix by another of\nlower rank. Psychometrika 1:211\u2013218. 417,522,533\nEl-Hamdouchi, Abdelmoula, and Peter Willett. 1986. Hierar chic document classi\ufb01ca-\ntion using Ward\u2019s clustering method. In Proc. SIGIR , pp. 149\u2013156. ACM Press. DOI:\ndoi.acm.org/10.1145/253168.253200 .399,522,532\nElias, Peter. 1975. Universal code word sets and representa tions of the integers. IEEE\nTransactions on Information Theory 21(2):194\u2013203. 106,522\nEyheramendy, Susana, David Lewis, and David Madigan. 2003. On the Naive Bayes\nmodel for text categorization. In International Workshop on Arti\ufb01cial Intelligence and\nStatistics . Society for Arti\ufb01cial Intelligence and Statistics. 286,522,526,527\nFallows, Deborah, 2004. The internet and daily life. URL:\nwww.pewinternet.org/pdfs/PIP_Internet_and_Daily_Lif e.pdf . Pew/Internet and American\nLife Project. xxxi,522\nFayyad, Usama M., Cory Reina, and Paul S. Bradley. 1998. Init ialization of iterative\nre\ufb01nement clustering algorithms. In Proc. KDD , pp. 194\u2013198. 374,520,522,529\nFellbaum, Christiane D. 1998. WordNet \u2013 An Electronic Lexical Database . MIT Press.\n194,522\nFerragina, Paolo, and Rossano Venturini. 2007. Compressed permuterm indexes. In\nProc. SIGIR . ACM Press. 65,522,532\nForman, George. 2004. A pitfall and solution in multi-class feature selection for text\nclassi\ufb01cation. In Proc. ICML .286,523\nForman, George. 2006. Tackling concept drift by temporal in ductive transfer. In Proc.\nSIGIR , pp. 252\u2013259. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148216 .286,523\nForman, George, and Ira Cohen. 2004. Learning from little: C omparison of classi\ufb01ers\ngiven little training. In Proc. PKDD , pp. 161\u2013172. 336,521,523\nFowlkes, Edward B., and Colin L. Mallows. 1983. A method for c omparing two\nhierarchical clusterings. Journal of the American Statistical Association 78(383):553\u2013\n569. URL:www.jstor.org/view/01621459/di985957/98p0926l/0 .400,523,527\nOnline edition (c)\n2009 Cambridge UPBibliography 495\nFox, Edward A., and Whay C. Lee. 1991. FAST-INV: A fast algori thm for building\nlarge inverted \ufb01les. Technical report, Virginia Polytechn ic Institute & State Univer-\nsity, Blacksburg, VA, USA. 83,523,526\nFraenkel, Aviezri S., and Shmuel T. Klein. 1985. Novel compr ession of sparse bit-\nstrings \u2013 preliminary report. In Combinatorial Algorithms on Words, NATO ASI Series\nVol F12 , pp. 169\u2013183. Springer. 106,523,525\nFrakes, William B., and Ricardo Baeza-Yates (eds.). 1992. Information Retrieval: Data\nStructures and Algorithms . Prentice Hall. 497,509,519,523\nFraley, Chris, and Adrian E. Raftery. 1998. How many cluster s? Which clustering\nmethod? Answers via model-based cluster analysis. Computer Journal 41(8):578\u2013\n588. 373,523,529\nFriedl, Jeffrey E. F. 2006. Mastering Regular Expressions , 3rd edition. O\u2019Reilly. 18,523\nFriedman, Jerome H. 1997. On bias, variance, 0/1\u2013loss, and t he curse-of-\ndimensionality. Data Mining and Knowledge Discovery 1(1):55\u201377. 286,315,523\nFriedman, Nir, and Moises Goldszmidt. 1996. Building class i\ufb01ers using Bayesian\nnetworks. In Proc. National Conference on Arti\ufb01cial Intelligence , pp. 1277\u20131284. 231,\n523\nFuhr, Norbert. 1989. Optimum polynomial retrieval functio ns based on the probabil-\nity ranking principle. TOIS 7(3):183\u2013204. 150,523\nFuhr, Norbert. 1992. Probabilistic models in information r etrieval. Computer Journal\n35(3):243\u2013255. 235,348,523\nFuhr, Norbert, Norbert G\u00f6vert, Gabriella Kazai, and Mounia Lalmas (eds.). 2003a.\nINitiative for the Evaluation of XML Retrieval (INEX). Proc . First INEX Workshop .\nERCIM. 216,523,525,526\nFuhr, Norbert, and Kai Gro\u00dfjohann. 2004. XIRQL: An XML query lan-\nguage based on information retrieval concepts. TOIS 22(2):313\u2013356. URL:\ndoi.acm.org/10.1145/984321.984326 .216,523\nFuhr, Norbert, and Mounia Lalmas. 2007. Advances in XML retr ieval: The INEX\ninitiative. In International Workshop on Research Issues in Digital Libra ries.216,523,\n526\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Gabriella K azai (eds.). 2006. Ad-\nvances in XML Information Retrieval and Evaluation, 4th Int ernational Workshop of the\nInitiative for the Evaluation of XML Retrieval, INEX 2005 . Springer. 216,523,525,526,\n527\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Zolt\u00e1n Szl\u00e1 vik (eds.). 2005. Ad-\nvances in XML Information Retrieval, Third International W orkshop of the Initiative for\nthe Evaluation of XML Retrieval, INEX 2004 . Springer. 216,507,515,523,526,527,\n531\nFuhr, Norbert, Mounia Lalmas, and Andrew Trotman (eds.). 20 07.Comparative Evalu-\nation of XML Information Retrieval Systems, 5th Internatio nal Workshop of the Initiative\nfor the Evaluation of XML Retrieval, INEX 2006 . Springer. 216,502,504,523,526,532\nOnline edition (c)\n2009 Cambridge UP496 Bibliography\nFuhr, Norbert, Saadia Malik, and Mounia Lalmas (eds.). 2003 b.INEX 2003 Workshop .\nURL:inex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,496,505,523,526,527\nFuhr, Norbert, and Ulrich Pfeifer. 1994. Probabilistic inf ormation retrieval as a com-\nbination of abstraction, inductive learning, and probabil istic assumptions. TOIS 12\n(1):92\u2013115. DOI:doi.acm.org/10.1145/174608.174612 .150,523,529\nFuhr, Norbert, and Thomas R\u00f6lleke. 1997. A probabilistic re lational algebra for the\nintegration of information retrieval and database systems .TOIS 15(1):32\u201366. DOI:\ndoi.acm.org/10.1145/239041.239045 .217,523,530\nGaertner, Thomas, John W. Lloyd, and Peter A. Flach. 2002. Ke rnels for structured\ndata. In Proc. International Conference on Inductive Logic Program ming , pp. 66\u201383. 347,\n522,523,527\nGao, Jianfeng, Mu Li, Chang-Ning Huang, and Andi Wu. 2005. Ch inese word seg-\nmentation and named entity recognition: A pragmatic approa ch. Computational\nLinguistics 31(4):531\u2013574. 46,523,524,526,533\nGao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. 2004. Dependence\nlanguage model for information retrieval. In Proc. SIGIR , pp. 170\u2013177. ACM Press.\n252,521,523,528,533\nGarcia, Steven, Hugh E. Williams, and Adam Cannane. 2004. Ac cess-ordered indexes.\nInProc. Australasian Conference on Computer Science , pp. 7\u201314. 149,521,523,533\nGarcia-Molina, Hector, Jennifer Widom, and Jeffrey D. Ullm an. 1999. Database System\nImplementation . Prentice Hall. 84,523,532\nGar\ufb01eld, Eugene. 1955. Citation indexes to science: A new di mension in documenta-\ntion through association of ideas. Science 122:108\u2013111. 480,523\nGar\ufb01eld, Eugene. 1976. The permuterm subject index: An auto biographic review.\nJASIS 27(5-6):288\u2013291. 65,523\nGeman, Stuart, Elie Bienenstock, and Ren\u00e9 Doursat. 1992. Ne ural networks and the\nbias/variance dilemma. Neural Computation 4(1):1\u201358. 315,520,522,523\nGeng, Xiubo, Tie-Yan Liu, Tao Qin, and Hang Li. 2007. Feature selection for ranking.\nInProc. SIGIR , pp. 407\u2013414. ACM Press. 348,523,526,529\nGerrand, Peter. 2007. Estimating linguistic diversity on t he internet: A taxonomy\nto avoid pitfalls and paradoxes. Journal of Computer-Mediated Communication 12(4).\nURL:jcmc.indiana.edu/vol12/issue4/gerrand.html . article 8. 30,523\nGey, Fredric C. 1994. Inferring probability of relevance us ing the method of logistic\nregression. In Proc. SIGIR , pp. 222\u2013231. ACM Press. 348,523\nGhamrawi, Nadia, and Andrew McCallum. 2005. Collective mul ti-label classi\ufb01cation.\nInProc. CIKM , pp. 195\u2013200. ACM Press. DOI:doi.acm.org/10.1145/1099554.1099591 .\n315,523,527\nGlover, Eric, David M. Pennock, Steve Lawrence, and Robert K rovetz. 2002a. In-\nferring hierarchical descriptions. In Proc. CIKM , pp. 507\u2013514. ACM Press. DOI:\ndoi.acm.org/10.1145/584792.584876 .400,523,526,529\nOnline edition (c)\n2009 Cambridge UPBibliography 497\nGlover, Eric J., Kostas Tsioutsiouliklis, Steve Lawrence, David M. Pennock,\nand Gary W. Flake. 2002b. Using web structure for classifyin g and de-\nscribing web pages. In Proc. WWW , pp. 562\u2013569. ACM Press. DOI:\ndoi.acm.org/10.1145/511446.511520 .400,522,523,526,529,532\nG\u00f6vert, Norbert, and Gabriella Kazai. 2003. Overview of the INitiative for the\nEvaluation of XML retrieval (INEX) 2002. In Fuhr et al. (2003b ), pp. 1\u201317. URL:\ninex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,523,525\nGrabs, Torsten, and Hans-J\u00f6rg Schek. 2002. Generating vect or spaces on-the-\ufb02y for\n\ufb02exible XML retrieval. In XML and Information Retrieval Workshop at SIGIR 2002 .\n216,523,530\nGreiff, Warren R. 1998. A theory of term weighting based on ex ploratory data analy-\nsis. In Proc. SIGIR , pp. 11\u201319. ACM Press. 227,523\nGrinstead, Charles M., and J. Laurie Snell. 1997. Introduction to\nProbability , 2nd edition. American Mathematical Society. URL:\nwww.dartmouth.edu/ ~chance/teaching_aids/books_articles/probability_boo k/amsbook.mac.pdf .\n235,523,531\nGrossman, David A., and Ophir Frieder. 2004. Information Retrieval: Algorithms and\nHeuristics , 2nd edition. Springer. xxxiv ,84,217,523\nGus\ufb01eld, Dan. 1997. Algorithms on Strings, Trees and Sequences: Computer Scien ce and\nComputational Biology . Cambridge University Press. 65,523\nHamerly, Greg, and Charles Elkan. 2003. Learning the kink-means. In Proc. NIPS .\nURL:books.nips.cc/papers/\ufb01les/nips16/NIPS2003_AA36.pdf .373,522,523\nHan, Eui-Hong, and George Karypis. 2000. Centroid-based do cument classi\ufb01cation:\nAnalysis and experimental results. In Proc. PKDD , pp. 424\u2013431. 314,524,525\nHand, David J. 2006. Classi\ufb01er technology and the illusion o f progress. Statistical\nScience 21:1\u201314. 286,524\nHand, David J., and Keming Yu. 2001. Idiot\u2019s Bayes: Not so stu pid after all. Interna-\ntional Statistical Review 69(3):385\u2013398. 286,524,533\nHarman, Donna. 1991. How effective is suf\ufb01xing? JASIS 42:7\u201315. 46,524\nHarman, Donna. 1992. Relevance feedback revisited. In Proc. SIGIR , pp. 1\u201310. ACM\nPress. 185,194,524\nHarman, Donna, Ricardo Baeza-Yates, Edward Fox, and W. Lee. 1992. Inverted \ufb01les.\nInFrakes and Baeza-Yates (1992 ), pp. 28\u201343. 83,519,523,524,526\nHarman, Donna, and Gerald Candela. 1990. Retrieving record s from a gigabyte of\ntext on a minicomputer using statistical ranking. JASIS 41(8):581\u2013589. 83,521,524\nHarold, Elliotte Rusty, and Scott W. Means. 2004. XML in a Nutshell , 3rd edition.\nO\u2019Reilly. 216,524,527\nHarter, Stephen P . 1998. Variations in relevance assessmen ts and the measurement of\nretrieval effectiveness. JASIS 47:37\u201349. 174,524\nHartigan, J. A., and M. A. Wong. 1979. A K-means clustering al gorithm. Applied\nStatistics 28:100\u2013108. 373,524,533\nOnline edition (c)\n2009 Cambridge UP498 Bibliography\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2001. The Elements of\nStatistical Learning: Data Mining, Inference, and Predict ion. Springer. 286,314,315,\n347,523,524,531\nHatzivassiloglou, Vasileios, Luis Gravano, and Ankineedu Maganti. 2000.\nAn investigation of linguistic features and clustering alg orithms for topi-\ncal document clustering. In Proc. SIGIR , pp. 224\u2013231. ACM Press. DOI:\ndoi.acm.org/10.1145/345508.345582 .373,523,524,527\nHaveliwala, Taher. 2003. Topic-sensitive PageRank: A cont ext-sensitive ranking al-\ngorithm for web search. IEEE Transactions on Knowledge and Data Engineering 15(4):\n784\u2013796. URL:citeseer.ist.psu.edu/article/haveliwala03topicsensi tive.html .481,524\nHaveliwala, Taher H. 2002. Topic-sensitive PageRank. In Proc. WWW .URL:cite-\nseer.ist.psu.edu/haveliwala02topicsensitive.html .481,524\nHayes, Philip J., and Steven P . Weinstein. 1990. CONSTRUE/T IS: A system for\ncontent-based indexing of a database of news stories. In Proc. Conference on In-\nnovative Applications of Arti\ufb01cial Intelligence , pp. 49\u201366. 335,524,532\nHeaps, Harold S. 1978. Information Retrieval: Computational and Theoretical Asp ects.\nAcademic Press. 105,524\nHearst, Marti A. 1997. TextTiling: Segmenting text into mul ti-paragraph subtopic\npassages. Computational Linguistics 23(1):33\u201364. 217,524\nHearst, Marti A. 2006. Clustering versus faceted categorie s for information explo-\nration. CACM 49(4):59\u201361. DOI:doi.acm.org/10.1145/1121949.1121983 .372,524\nHearst, Marti A., and Jan O. Pedersen. 1996. Reexamining the cluster hypothesis. In\nProc. SIGIR , pp. 76\u201384. ACM Press. 372,524,528\nHearst, Marti A., and Christian Plaunt. 1993. Subtopic stru cturing for full-\nlength document access. In Proc. SIGIR , pp. 59\u201368. ACM Press. DOI:\ndoi.acm.org/10.1145/160688.160695 .217,524,529\nHeinz, Steffen, and Justin Zobel. 2003. Ef\ufb01cient single-pa ss index construction for\ntext databases. JASIST 54(8):713\u2013729. DOI:dx.doi.org/10.1002/asi.10268 .83,524,533\nHeinz, Steffen, Justin Zobel, and Hugh E. Williams. 2002. Bu rst tries: A\nfast, ef\ufb01cient data structure for string keys. TOIS 20(2):192\u2013223. DOI:\ndoi.acm.org/10.1145/506309.506312 .84,524,533\nHenzinger, Monika R., Allan Heydon, Michael Mitzenmacher, and Marc Najork.\n2000. On near-uniform URL sampling. In Proc. WWW , pp. 295\u2013308. North-Holland.\nDOI:dx.doi.org/10.1016/S1389-1286(00)00055-4 .442,524,527,528\nHerbrich, Ralf, Thore Graepel, and Klaus Obermayer. 2000. L arge margin rank\nboundaries for ordinal regression. In Advances in Large Margin Classi\ufb01ers , pp. 115\u2013\n132. MIT Press. 348,523,524,528\nHersh, William, Chris Buckley, T. J. Leone, and David Hickam . 1994. OHSUMED: An\ninteractive retrieval evaluation and new large test collec tion for research. In Proc.\nSIGIR , pp. 192\u2013201. ACM Press. 174,520,524,526\nHersh, William R., Andrew Turpin, Susan Price, Benjamin Cha n, Dale Kraemer,\nLynetta Sacherek, and Daniel Olson. 2000a. Do batch and user evaluation give\nthe same results? In Proc. SIGIR , pp. 17\u201324. 175,521,524,526,528,529,530,532\nOnline edition (c)\n2009 Cambridge UPBibliography 499\nHersh, William R., Andrew Turpin, Susan Price, Dale Kraemer , Daniel Olson, Ben-\njamin Chan, and Lynetta Sacherek. 2001. Challenging conven tional assumptions\nof automated information retrieval with real users: Boolea n searching and batch\nretrieval evaluations. IP&M 37(3):383\u2013402. 175,521,524,526,528,529,530,532\nHersh, William R., Andrew Turpin, Lynetta Sacherek, Daniel Olson, Susan Price, Ben-\njamin Chan, and Dale Kraemer. 2000b. Further analysis of whe ther batch and user\nevaluations give the same results with a question-answerin g task. In Proc. TREC .\n175,521,524,526,528,529,530,532\nHiemstra, Djoerd. 1998. A linguistically motivated probab ilistic model of information\nretrieval. In Proc. ECDL , volume 1513 of LNCS , pp. 569\u2013584. 252,524\nHiemstra, Djoerd. 2000. A probabilistic justi\ufb01cation for u sing tf.idf term weighting in\ninformation retrieval. International Journal on Digital Libraries 3(2):131\u2013139. 246,524\nHiemstra, Djoerd, and Wessel Kraaij. 2005. A language-mode ling approach to TREC.\nInVoorhees and Harman (2005 ), pp. 373\u2013395. 252,524,526\nHirai, Jun, Sriram Raghavan, Hector Garcia-Molina, and And reas Paepcke. 2000.\nWebBase: A repository of web pages. In Proc. WWW , pp. 277\u2013293. 458,523,524,\n528,529\nHofmann, Thomas. 1999a. Probabilistic Latent Semantic Ind exing. In Proc. UAI .URL:\nciteseer.ist.psu.edu/hofmann99probabilistic.html .417,524\nHofmann, Thomas. 1999b. Probabilistic Latent Semantic Ind exing. In Proc. SIGIR , pp.\n50\u201357. ACM Press. URL:citeseer.ist.psu.edu/article/hofmann99probabilistic .html .417,\n524\nHollink, Vera, Jaap Kamps, Christof Monz, and Maarten de Rij ke. 2004. Monolingual\ndocument retrieval for European languages. IR7(1):33\u201352. 46,524,525,528,529\nHopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman. 20 00.Introduction to Au-\ntomata Theory, Languages, and Computation , 2nd edition. Addison Wesley. 18,524,\n528,532\nHuang, Yifen, and Tom M. Mitchell. 2006. Text clustering wit h ex-\ntended user feedback. In Proc. SIGIR , pp. 413\u2013420. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148242 .374,524,527\nHubert, Lawrence, and Phipps Arabie. 1985. Comparing parti tions. Journal of Classi-\n\ufb01cation 2:193\u2013218. 373,519,524\nHughes, Baden, Timothy Baldwin, Steven Bird, Jeremy Nichol son, and Andrew\nMacKinlay. 2006. Reconsidering language identi\ufb01cation fo r written language re-\nsources. In Proc. International Conference on Language Resources and E valuation , pp.\n485\u2013488. 46,519,520,524,527,528\nHull, David. 1993. Using statistical testing in the evaluat ion of retrieval performance.\nInProc. SIGIR , pp. 329\u2013338. ACM Press. 173,524\nHull, David. 1996. Stemming algorithms \u2013 A case study for det ailed evaluation. JASIS\n47(1):70\u201384. 46,524\nIde, E. 1971. New experiments in relevance feedback. In Salton (1971b ), pp. 337\u2013354.\n193,524\nOnline edition (c)\n2009 Cambridge UP500 Bibliography\nIndyk, Piotr. 2004. Nearest neighbors in high-dimensional spaces. In J. E. Good-\nman and J. O\u2019Rourke (eds.), Handbook of Discrete and Computational Geometry , 2nd\nedition, pp. 877\u2013892. Chapman and Hall/CRC Press. 314,524\nIngwersen, Peter, and Kalervo J\u00e4rvelin. 2005. The Turn: Integration of Information\nSeeking and Retrieval in Context . Springer. xxxiv ,524\nIttner, David J., David D. Lewis, and David D. Ahn. 1995. Text categorization of low\nquality images. In Proc. SDAIR , pp. 301\u2013315. 314,519,524,526\nIwayama, Makoto, and Takenobu Tokunaga. 1995. Cluster-bas ed text categorization:\nA comparison of category search strategies. In Proc. SIGIR , pp. 273\u2013280. ACM\nPress. 314,524,531\nJackson, Peter, and Isabelle Moulinier. 2002. Natural Language Processing for Online\nApplications: Text Retrieval, Extraction and Categorizat ion. John Benjamins. 334,335,\n524,528\nJacobs, Paul S., and Lisa F. Rau. 1990. SCISOR: Extracting in formation from on-line\nnews. CACM 33:88\u201397. 335,524,529\nJain, Anil, M. Narasimha Murty, and Patrick Flynn. 1999. Dat a clustering: A review.\nACM Computing Surveys 31(3):264\u2013323. 399,523,524,528\nJain, Anil K., and Richard C. Dubes. 1988. Algorithms for Clustering Data . Prentice\nHall. 399,522,524\nJardine, N., and Cornelis Joost van Rijsbergen. 1971. The us e of hierarchic clustering\nin information retrieval. Information Storage and Retrieval 7:217\u2013240. 372,525,529\nJ\u00e4rvelin, Kalervo, and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated g ain-based evaluation of\nIR techniques. TOIS 20(4):422\u2013446. 174,525\nJeh, Glen, and Jennifer Widom. 2003. Scaling personalized w eb search. In Proc.\nWWW , pp. 271\u2013279. ACM Press. 481,525,532\nJensen, Finn V ., and Finn B. Jensen. 2001. Bayesian Networks and Decision Graphs .\nSpringer. 234,525\nJeong, Byeong-Soo, and Edward Omiecinski. 1995. Inverted \ufb01 le partitioning schemes\nin multiple disk systems. IEEE Transactions on Parallel and Distributed Systems 6(2):\n142\u2013153. 458,525,528\nJi, Xiang, and Wei Xu. 2006. Document clustering with prior k nowledge. In Proc.\nSIGIR , pp. 405\u2013412. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148241 .374,\n525,533\nJing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proc.\nConference on Applied Natural Language Processing , pp. 310\u2013315. 174,525\nJoachims, Thorsten. 1997. A probabilistic analysis of the R occhio algorithm with t\ufb01df\nfor text categorization. In Proc. ICML , pp. 143\u2013151. Morgan Kaufmann. 314,525\nJoachims, Thorsten. 1998. Text categorization with suppor t vector machines: Learn-\ning with many relevant features. In Proc. ECML , pp. 137\u2013142. Springer. xvii,282,\n333,334,525\nOnline edition (c)\n2009 Cambridge UPBibliography 501\nJoachims, Thorsten. 1999. Making large-scale SVM learning practical. In B. Sch\u00f6lkopf,\nC. Burges, and A. Smola (eds.), Advances in Kernel Methods - Support Vector Learning .\nMIT Press. 347,525\nJoachims, Thorsten. 2002a. Learning to Classify Text Using Support Vector Machines .\nKluwer. xvii,334,347,525\nJoachims, Thorsten. 2002b. Optimizing search engines usin g clickthrough data. In\nProc. KDD , pp. 133\u2013142. 175,185,348,525\nJoachims, Thorsten. 2006a. Training linear SVMs in linear t ime. In Proc. KDD , pp.\n217\u2013226. ACM Press. DOI:doi.acm.org/10.1145/1150402.1150429 .286,329,347,525\nJoachims, Thorsten. 2006b. Transductive support vector ma chines. In Chapelle et al.\n(2006 ), pp. 105\u2013118. 347,525\nJoachims, Thorsten, Laura Granka, Bing Pan, Helene Hembroo ke, and Geri Gay. 2005.\nAccurately interpreting clickthrough data as implicit fee dback. In Proc. SIGIR , pp.\n154\u2013161. ACM Press. 175,185,523,524,525,528\nJohnson, David, Vishv Malhotra, and Peter Vamplew. 2006. Mo re effective web search\nusing bigrams and trigrams. Webology 3(4). URL:www.webology.ir/2006/v3n4/a35.html .\nArticle 35. 47,525,527,532\nJurafsky, Dan, and James H. Martin. 2008. Speech and Language Processing: An Introduc-\ntion to Natural Language Processing, Computational Lingui stics and Speech Recognition ,\n2nd edition. Prentice Hall. xxxiv ,252,525,527\nK\u00e4ki, Mika. 2005. Findex: Search result categories help use rs when doc-\nument ranking fails. In Proc. SIGCHI , pp. 131\u2013140. ACM Press. DOI:\ndoi.acm.org/10.1145/1054972.1054991 .372,400,526\nKammenhuber, Nils, Julia Luxenburger, Anja Feldmann, and G erhard Weikum. 2006.\nWeb search clickstreams. In Proc. ACM SIGCOMM on Internet Measurement , pp.\n245\u2013250. ACM Press. 47,522,525,527,532\nKamps, Jaap, Maarten de Rijke, and B\u00f6rkur Sigurbj\u00f6rnsson. 2 004. Length nor-\nmalization in XML retrieval. In Proc. SIGIR , pp. 80\u201387. ACM Press. DOI:\ndoi.acm.org/10.1145/1008992.1009009 .216,525,529,530\nKamps, Jaap, Maarten Marx, Maarten de Rijke, and B\u00f6rkur Sigu rbj\u00f6rnsson. 2006.\nArticulating information needs in XML query languages. TOIS 24(4):407\u2013436. DOI:\ndoi.acm.org/10.1145/1185877.1185879 .216,525,527,529,530\nKamvar, Sepandar D., Dan Klein, and Christopher D. Manning. 2002. Interpreting\nand extending classical agglomerative clustering algorit hms using a model-based\napproach. In Proc. ICML , pp. 283\u2013290. Morgan Kaufmann. 400,525,527\nKannan, Ravi, Santosh Vempala, and Adrian Vetta. 2000. On cl usterings \u2013 Good, bad\nand spectral. In Proc. Symposium on Foundations of Computer Science , pp. 367\u2013377.\nIEEE Computer Society. 400,525,532\nKaszkiel, Marcin, and Justin Zobel. 1997. Passage retrieva l revisited. In Proc. SIGIR ,\npp. 178\u2013185. ACM Press. DOI:doi.acm.org/10.1145/258525.258561 .217,525,533\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding groups in data . Wiley. 373,\n525,530\nOnline edition (c)\n2009 Cambridge UP502 Bibliography\nKazai, Gabriella, and Mounia Lalmas. 2006. eXtended cumula ted gain measures\nfor the evaluation of content-oriented XML retrieval. TOIS 24(4):503\u2013542. DOI:\ndoi.acm.org/10.1145/1185883 .217,525,526\nKek\u00e4l\u00e4inen, Jaana. 2005. Binary and graded relevance in IR e valuations \u2013 Comparison\nof the effects on ranking of IR systems. IP&M 41:1019\u20131033. 174,525\nKek\u00e4l\u00e4inen, Jaana, and Kalervo J\u00e4rvelin. 2002. Using grade d relevance assessments\nin IR evaluation. JASIST 53(13):1120\u20131129. 174,525\nKemeny, John G., and J. Laurie Snell. 1976. Finite Markov Chains . Springer. 480,525,\n531\nKent, Allen, Madeline M. Berry, Fred U. Luehrs, Jr., and J. W. Perry. 1955. Machine\nliterature searching VIII. Operational criteria for desig ning information retrieval\nsystems. American Documentation 6(2):93\u2013101. 173,520,525,527,529\nKernighan, Mark D., Kenneth W. Church, and William A. Gale. 1 990. A spelling\ncorrection program based on a noisy channel model. In Proc. ACL , pp. 205\u2013210. 65,\n521,523,525\nKing, Benjamin. 1967. Step-wise clustering procedures. Journal of the American Statis-\ntical Association 69:86\u2013101. 399,525\nKishida, Kazuaki, Kuang-Hua Chen, Sukhoon Lee, Kazuko Kuri yama, Noriko\nKando, Hsin-Hsi Chen, and Sung Hyon Myaeng. 2005. Overview o f CLIR task\nat the \ufb01fth NTCIR workshop. In NTCIR Workshop Meeting on Evaluation of Informa-\ntion Access Technologies: Information Retrieval, Questio n Answering and Cross-Lingual\nInformation Access . National Institute of Informatics. 45,521,525,526,528\nKlein, Dan, and Christopher D. Manning. 2002. Conditional s tructure versus con-\nditional estimation in NLP models. In Proc. Empirical Methods in Natural Language\nProcessing , pp. 9\u201316. 336,525,527\nKleinberg, Jon M. 1997. Two algorithms for nearest-neighbo r search in high dimen-\nsions. In Proc. ACM Symposium on Theory of Computing , pp. 599\u2013608. ACM Press.\nDOI:doi.acm.org/10.1145/258533.258653 .314,525\nKleinberg, Jon M. 1999. Authoritative sources in a hyperlin ked environment. JACM\n46(5):604\u2013632. URL:citeseer.ist.psu.edu/article/kleinberg98authoritati ve.html .481,525\nKleinberg, Jon M. 2002. An impossibility theorem for cluste ring. In Proc. NIPS .373,\n525\nKnuth, Donald E. 1997. The Art of Computer Programming, Volume 3: Sorting and\nSearching , 3rd edition. Addison Wesley. 65,525\nKo, Youngjoong, Jinwoo Park, and Jungyun Seo. 2004. Improvi ng text categorization\nusing the importance of sentences. IP&M 40(1):65\u201379. 340,525,528,530\nKoenemann, J\u00fcrgen, and Nicholas J. Belkin. 1996. A case for i nteraction: A study of\ninteractive information retrieval behavior and effective ness. In Proc. SIGCHI , pp.\n205\u2013212. ACM Press. DOI:doi.acm.org/10.1145/238386.238487 .194,520,525\nKo\u0142cz, Aleksander, Vidya Prabakarmurthi, and Jugal Kalita . 2000. Summarization as\nfeature selection for text categorization. In Proc. CIKM , pp. 365\u2013370. ACM Press.\n340,525,529\nOnline edition (c)\n2009 Cambridge UPBibliography 503\nKo\u0142cz, Aleksander, and Wen-Tau Yih. 2007. Raising the basel ine for high-precision\ntext classi\ufb01ers. In Proc. KDD .286,525,533\nKoller, Daphne, and Mehran Sahami. 1997. Hierarchically cl assifying documents\nusing very few words. In Proc. ICML , pp. 170\u2013178. 347,525,530\nKonheim, Alan G. 1981. Cryptography: A Primer . John Wiley & Sons. 46,525\nKorfhage, Robert R. 1997. Information Storage and Retrieval . Wiley. xxxiv ,175,525\nKozlov, M. K., S. P . Tarasov, and L. G. Khachiyan. 1979. Polyn omial solvability of con-\nvex quadratic programming. Soviet Mathematics Doklady 20:1108\u20131111. Translated\nfrom original in Doklady Akademiia Nauk SSR , 228 (1979). 328,525,531\nKraaij, Wessel, and Martijn Spitters. 2003. Language model s for topic tracking. In\nW. B. Croft and J. Lafferty (eds.), Language Modeling for Information Retrieval , pp.\n95\u2013124. Kluwer. 251,526,531\nKraaij, Wessel, Thijs Westerveld, and Djoerd Hiemstra. 200 2. The importance of prior\nprobabilities for entry page search. In Proc. SIGIR , pp. 27\u201334. ACM Press. 252,524,\n526,532\nKrippendorff, Klaus. 2003. Content Analysis: An Introduction to its Methodology . Sage.\n174,526\nKrovetz, Bob. 1995. Word sense disambiguation for large text databases . PhD thesis,\nUniversity of Massachusetts Amherst. 46,526\nKukich, Karen. 1992. Techniques for automatically correct ing words in text. ACM\nComputing Surveys 24(4):377\u2013439. DOI:doi.acm.org/10.1145/146370.146380 .65,526\nKumar, Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins. 1999.\nTrawling the Web for emerging cyber-communities. Computer Networks 31(11\u201316):\n1481\u20131493. URL:citeseer.ist.psu.edu/kumar99trawling.html .442,526,529,531\nKumar, S. Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, D andapani Sivakumar,\nAndrew Tomkins, and Eli Upfal. 2000. The Web as a graph. In Proc. PODS , pp.\n1\u201310. ACM Press. URL:citeseer.ist.psu.edu/article/kumar00web.html .441,526,529,531,\n532\nKupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A tra inable document sum-\nmarizer. In Proc. SIGIR , pp. 68\u201373. ACM Press. 174,521,526,529\nKurland, Oren, and Lillian Lee. 2004. Corpus structure, lan guage models, and\nad hoc information retrieval. In Proc. SIGIR , pp. 194\u2013201. ACM Press. DOI:\ndoi.acm.org/10.1145/1008992.1009027 .372,526\nLafferty, John, and Chengxiang Zhai. 2001. Document langua ge models, query mod-\nels, and risk minimization for information retrieval. In Proc. SIGIR , pp. 111\u2013119.\nACM Press. 250,251,526,533\nLafferty, John, and Chengxiang Zhai. 2003. Probabilistic r elevance models based\non document and query generation. In W. Bruce Croft and John L afferty (eds.),\nLanguage Modeling for Information Retrieval . Kluwer. 252,526,533\nLalmas, Mounia, Gabriella Kazai, Jaap Kamps, Jovan Pehcevs ki, Benjamin Pi-\nwowarski, and Stephen E. Robertson. 2007. INEX 2006 evaluat ion measures. In\nFuhr et al. (2007 ), pp. 20\u201334. 217,525,526,529\nOnline edition (c)\n2009 Cambridge UP504 Bibliography\nLalmas, Mounia, and Anastasios Tombros. 2007. Evaluating X ML retrieval effective-\nness at INEX. SIGIR Forum 41(1):40\u201357. DOI:doi.acm.org/10.1145/1273221.1273225 .\n216,526,531\nLance, G. N., and W. T. Williams. 1967. A general theory of cla ssi\ufb01catory sorting\nstrategies 1. Hierarchical systems. Computer Journal 9(4):373\u2013380. 399,526,533\nLangville, Amy, and Carl Meyer. 2006. Google\u2019s PageRank and Beyond: The Science of\nSearch Engine Rankings . Princeton University Press. 481,526,527\nLarsen, Bjornar, and Chinatsu Aone. 1999. Fast and effectiv e text mining using\nlinear-time document clustering. In Proc. KDD , pp. 16\u201322. ACM Press. DOI:\ndoi.acm.org/10.1145/312129.312186 .399,400,519,526\nLarson, Ray R. 2005. A fusion approach to XML structured docu ment retrieval. IR8\n(4):601\u2013629. DOI:dx.doi.org/10.1007/s10791-005-0749-0 .216,526\nLavrenko, Victor, and W. Bruce Croft. 2001. Relevance-base d language models. In\nProc. SIGIR , pp. 120\u2013127. ACM Press. 250,522,526\nLawrence, Steve, and C. Lee Giles. 1998. Searching the World Wide Web. Science 280\n(5360):98\u2013100. URL:citeseer.ist.psu.edu/lawrence98searching.html .442,523,526\nLawrence, Steve, and C. Lee Giles. 1999. Accessibility of in formation on the web.\nNature 500:107\u2013109. 442,523,526\nLee, Whay C., and Edward A. Fox. 1988. Experimental comparis on of schemes for in-\nterpreting Boolean queries. Technical Report TR-88-27, Co mputer Science, Virginia\nPolytechnic Institute and State University. 17,523,526\nLempel, Ronny, and Shlomo Moran. 2000. The stochastic appro ach for link-structure\nanalysis (SALSA) and the TKC effect. Computer Networks 33(1\u20136):387\u2013401. URL:\nciteseer.ist.psu.edu/lempel00stochastic.html .481,526,528\nLesk, Michael. 1988. Grab \u2013 Inverted indexes with low storag e overhead. Computing\nSystems 1:207\u2013220. 83,526\nLesk, Michael. 2004. Understanding Digital Libraries , 2nd edition. Morgan Kaufmann.\nxxxiv ,526\nLester, Nicholas, Alistair Moffat, and Justin Zobel. 2005. Fast on-line index con-\nstruction by geometric partitioning. In Proc. CIKM , pp. 776\u2013783. ACM Press. DOI:\ndoi.acm.org/10.1145/1099554.1099739 .84,526,527,533\nLester, Nicholas, Justin Zobel, and Hugh E. Williams. 2006. Ef\ufb01cient online\nindex maintenance for contiguous inverted lists. IP&M 42(4):916\u2013933. DOI:\ndx.doi.org/10.1016/j.ipm.2005.09.005 .84,526,533\nLevenshtein, Vladimir I. 1965. Binary codes capable of corr ecting spurious insertions\nand deletions of ones. Problems of Information Transmission 1:8\u201317. 65,526\nLew, Michael S. 2001. Principles of Visual Information Retrieval . Springer. xxxiv ,526\nLewis, David D. 1995. Evaluating and optimizing autonomous text classi\ufb01cation\nsystems. In Proc. SIGIR . ACM Press. 286,526\nLewis, David D. 1998. Naive (Bayes) at forty: The independen ce assumption in\ninformation retrieval. In Proc. ECML , pp. 4\u201315. Springer. 286,526\nOnline edition (c)\n2009 Cambridge UPBibliography 505\nLewis, David D., and Karen Sp\u00e4rck Jones. 1996. Natural langu age processing for\ninformation retrieval. CACM 39(1):92\u2013101. DOI:doi.acm.org/10.1145/234173.234210 .\nxxxiv ,525,526\nLewis, David D., and Marc Ringuette. 1994. A comparison of tw o learning algorithms\nfor text categorization. In Proc. SDAIR , pp. 81\u201393. 286,526,529\nLewis, David D., Robert E. Schapire, James P . Callan, and Ron Papka. 1996. Training\nalgorithms for linear text classi\ufb01ers. In Proc. SIGIR , pp. 298\u2013306. ACM Press. DOI:\ndoi.acm.org/10.1145/243199.243277 .315,521,526,528,530\nLewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. R CV1: A new bench-\nmark collection for text categorization research. JMLR 5:361\u2013397. 84,287,526,530,\n533\nLi, Fan, and Yiming Yang. 2003. A loss function analysis for c lassi\ufb01cation methods in\ntext categorization. In Proc. ICML , pp. 472\u2013479. xvii,282,347,526,533\nLiddy, Elizabeth D. 2005. Automatic document retrieval. In Encyclopedia of Language\nand Linguistics , 2nd edition. Elsevier. 17,526\nList, Johan, Vojkan Mihajlovic, Georgina Ram\u00edrez, Arjen P . Vries, Djoerd Hiemstra,\nand Henk Ernst Blok. 2005. TIJAH: Embracing IR methods in XML databases. IR\n8(4):547\u2013570. DOI:dx.doi.org/10.1007/s10791-005-0747-2 .216,520,524,526,527,529,\n532\nLita, Lucian Vlad, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuE-\ncasIng. In Proc. ACL , pp. 152\u2013159. 46,524,525,526,530\nLittman, Michael L., Susan T. Dumais, and Thomas K. Landauer . 1998. Automatic\ncross-language information retrieval using latent semant ic indexing. In Gregory\nGrefenstette (ed.), Proc. Cross-Language Information Retrieval . Kluwer. URL:cite-\nseer.ist.psu.edu/littman98automatic.html .417,522,526\nLiu, Tie-Yan, Yiming Yang, Hao Wan, Hua-Jun Zeng, Zheng Chen , and Wei-Ying Ma.\n2005. Support vector machines classi\ufb01cation with very larg e scale taxonomy. ACM\nSIGKDD Explorations 7(1):36\u201343. 347,521,526,527,532,533\nLiu, Xiaoyong, and W. Bruce Croft. 2004. Cluster-based retr ieval us-\ning language models. In Proc. SIGIR , pp. 186\u2013193. ACM Press. DOI:\ndoi.acm.org/10.1145/1008992.1009026 .252,351,372,522,526\nLloyd, Stuart P . 1982. Least squares quantization in PCM. IEEE Transactions on Infor-\nmation Theory 28(2):129\u2013136. 373,527\nLodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cris tianini, and Chris\nWatkins. 2002. Text classi\ufb01cation using string kernels. JMLR 2:419\u2013444. 347,521,\n527,530,532\nLombard, Matthew, Cheryl C. Bracken, and Jennifer Snyder-D uch. 2002. Content\nanalysis in mass communication: Assessment and reporting o f intercoder reliabil-\nity.Human Communication Research 28:587\u2013604. 174,520,527,531\nLong, Xiaohui, and Torsten Suel. 2003. Optimized query exec ution in large\nsearch engines with global page ordering. In Proc. VLDB . URL:cite-\nseer.ist.psu.edu/long03optimized.html .149,527,531\nOnline edition (c)\n2009 Cambridge UP506 Bibliography\nLovins, Julie Beth. 1968. Development of a stemming algorit hm. Translation and\nComputational Linguistics 11(1):22\u201331. 33,527\nLu, Wei, Stephen E. Robertson, and Andrew MacFarlane. 2007. CISR at INEX 2006.\nInFuhr et al. (2007 ), pp. 57\u201363. 216,527,529\nLuhn, Hans Peter. 1957. A statistical approach to mechanize d encoding and searching\nof literary information. IBM Journal of Research and Development 1(4):309\u2013317. 133,\n527\nLuhn, Hans Peter. 1958. The automatic creation of literatur e abstracts. IBM Journal of\nResearch and Development 2(2):159\u2013165, 317. 133,527\nLuk, Robert W. P ., and Kui-Lam Kwok. 2002. A comparison of Chi nese document\nindexing strategies and retrieval models. ACM Transactions on Asian Language In-\nformation Processing 1(3):225\u2013268. 45,526,527\nLunde, Ken. 1998. CJKV Information Processing . O\u2019Reilly. 45,527\nMacFarlane, A., J.A. McCann, and S.E. Robertson. 2000. Para llel search using parti-\ntioned inverted \ufb01les. In Proc. SPIRE , pp. 209\u2013220. 458,527,529\nMacQueen, James B. 1967. Some methods for classi\ufb01cation and analysis of mul-\ntivariate observations. In Proc. Berkeley Symposium on Mathematics, Statistics and\nProbability , pp. 281\u2013297. University of California Press. 373,527\nManning, Christopher D., and Hinrich Sch\u00fctze. 1999. Foundations of Statistical Natural\nLanguage Processing . MIT Press. xxxiv ,40,105,251,252,286,372,527,530\nMaron, M. E., and J. L. Kuhns. 1960. On relevance, probabilis tic indexing, and infor-\nmation retrieval. JACM 7(3):216\u2013244. 235,286,526,527\nMass, Yosi, Matan Mandelbrod, Einat Amitay, David Carmel, Y o\u00eblle S. Maarek, and\nAya Soffer. 2003. JuruXML \u2013 An XML retrieval system at INEX\u20190 2. In Fuhr et al.\n(2003b ), pp. 73\u201380. URL:inex.is.informatik.uni-duisburg.de:2003/proceedings .pdf.216,\n519,521,527,531\nMcBryan, Oliver A. 1994. GENVL and WWWW: Tools for Taming the Web. In Proc.\nWWW .URL:citeseer.ist.psu.edu/mcbryan94genvl.html .442,480,527\nMcCallum, Andrew, and Kamal Nigam. 1998. A comparison of eve nt models for\nNaive Bayes text classi\ufb01cation. In AAAI/ICML Workshop on Learning for Text Cate-\ngorization , pp. 41\u201348. 286,527,528\nMcCallum, Andrew, Ronald Rosenfeld, Tom M. Mitchell, and An drew Y. Ng. 1998.\nImproving text classi\ufb01cation by shrinkage in a hierarchy of classes. In Proc. ICML ,\npp. 359\u2013367. Morgan Kaufmann. 347,527,528,530\nMcCallum, Andrew Kachites. 1996. Bow: A toolkit for statist ical language modeling,\ntext retrieval, classi\ufb01cation and clustering. www.cs.cmu.edu/ ~mccallum/bow .316,527\nMcKeown, Kathleen, and Dragomir R. Radev. 1995. Generating summaries\nof multiple news articles. In Proc. SIGIR , pp. 74\u201382. ACM Press. DOI:\ndoi.acm.org/10.1145/215206.215334 .400,527,529\nMcKeown, Kathleen R., Regina Barzilay, David Evans, Vasile ios Hatzivassiloglou, Ju-\ndith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, a nd Sergey Sigelman.\nOnline edition (c)\n2009 Cambridge UPBibliography 507\n2002. Tracking and summarizing news on a daily basis with Col umbia\u2019s News-\nblaster. In Proc. Human Language Technology Conference .351,373,520,522,524,525,\n527,528,530\nMcLachlan, Geoffrey J., and Thiriyambakam Krishnan. 1996. The EM Algorithm and\nExtensions . John Wiley & Sons. 373,526,527\nMeadow, Charles T., Donald H. Kraft, and Bert R. Boyce. 1999. Text Information Re-\ntrieval Systems . Academic Press. xxxiv ,520,526,527\nMeil\u02d8 a, Marina. 2005. Comparing clusterings \u2013 An axiomatic view. In Proc. ICML .373,\n527\nMelnik, Sergey, Sriram Raghavan, Beverly Yang, and Hector G arcia-Molina. 2001.\nBuilding a distributed full-text index for the web. In Proc. WWW , pp. 396\u2013406.\nACM Press. DOI:doi.acm.org/10.1145/371920.372095 .83,523,527,529,533\nMihajlovi\u00b4 c, Vojkan, Henk Ernst Blok, Djoerd Hiemstra, and Peter M. G. Apers. 2005.\nScore region algebra: Building a transparent XML-R databas e. In Proc. CIKM , pp.\n12\u201319. DOI:doi.acm.org/10.1145/1099554.1099560 .216,519,520,524,527\nMiller, David R. H., Tim Leek, and Richard M. Schwartz. 1999. A hidden Markov\nmodel information retrieval system. In Proc. SIGIR , pp. 214\u2013221. ACM Press. 246,\n252,526,527,530\nMinsky, Marvin Lee, and Seymour Papert (eds.). 1988. Perceptrons: An introduction to\ncomputational geometry . MIT Press. Expanded edition. 315,527,528\nMitchell, Tom M. 1997. Machine Learning . McGraw Hill. 286,527\nMoffat, Alistair, and Timothy A. H. Bell. 1995. In situ gener ation of compressed\ninverted \ufb01les. JASIS 46(7):537\u2013550. 83,520,527\nMoffat, Alistair, and Lang Stuiver. 1996. Exploiting clust ering in inverted \ufb01le com-\npression. In Proc. Conference on Data Compression , pp. 82\u201391. IEEE Computer Soci-\nety.106,527,531\nMoffat, Alistair, and Justin Zobel. 1992. Parameterised co mpression\nfor sparse bitmaps. In Proc. SIGIR , pp. 274\u2013285. ACM Press. DOI:\ndoi.acm.org/10.1145/133160.133210 .106,528,533\nMoffat, Alistair, and Justin Zobel. 1996. Self-indexing in verted \ufb01les for fast text re-\ntrieval. TOIS 14(4):349\u2013379. 46,47,528,533\nMoffat, Alistair, and Justin Zobel. 1998. Exploring the sim ilarity space. SIGIR Forum\n32(1). 133,528,533\nMooers, Calvin. 1961. From a point of view of mathematical et c. techniques. In R. A.\nFairthorne (ed.), Towards information retrieval , pp. xvii\u2013xxiii. Butterworths. 17,528\nMooers, Calvin E. 1950. Coding, information retrieval, and the rapid selector. Ameri-\ncan Documentation 1(4):225\u2013229. 17,528\nMoschitti, Alessandro. 2003. A study on optimal parameter t uning for Rocchio text\nclassi\ufb01er. In Proc. ECIR , pp. 420\u2013435. 315,528\nMoschitti, Alessandro, and Roberto Basili. 2004. Complex l inguistic features for text\nclassi\ufb01cation: A comprehensive study. In Proc. ECIR , pp. 181\u2013196. 347,520,528\nOnline edition (c)\n2009 Cambridge UP508 Bibliography\nMurata, Masaki, Qing Ma, Kiyotaka Uchimoto, Hiromi Ozaku, M asao Utiyama, and\nHitoshi Isahara. 2000. Japanese probabilistic informatio n retrieval using location\nand category information. In International Workshop on Information Retrieval With\nAsian Languages , pp. 81\u201388. URL:portal.acm.org/citation.cfm?doid=355214.355226 .340,\n524,527,528,532\nMuresan, Gheorghe, and David J. Harper. 2004. Topic modelin g for medi-\nated access to very large document collections. JASIST 55(10):892\u2013910. DOI:\ndx.doi.org/10.1002/asi.20034 .372,524,528\nMurtagh, Fionn. 1983. A survey of recent advances in hierarc hical clustering algo-\nrithms. Computer Journal 26(4):354\u2013359. 399,528\nNajork, Marc, and Allan Heydon. 2001. High-performance web crawling. Technical\nReport 173, Compaq Systems Research Center. 458,524,528\nNajork, Marc, and Allan Heydon. 2002. High-performance web crawling. In James\nAbello, Panos Pardalos, and Mauricio Resende (eds.), Handbook of Massive Data\nSets, chapter 2. Kluwer. 458,524,528\nNavarro, Gonzalo, and Ricardo Baeza-Yates. 1997. Proximal nodes: A model to\nquery document databases by content and structure. TOIS 15(4):400\u2013435. DOI:\ndoi.acm.org/10.1145/263479.263482 .217,519,528\nNewsam, Shawn, Sitaram Bhagavathy, and B. S. Manjunath. 200 1. Category-based\nimage retrieval. In Proc. IEEE International Conference on Image Processing, S pecial\nSession on Multimedia Indexing, Browsing and Retrieval , pp. 596\u2013599. 179,520,527,\n528\nNg, Andrew Y., and Michael I. Jordan. 2001. On discriminativ e vs. generative clas-\nsi\ufb01ers: A comparison of logistic regression and naive Bayes . In Proc. NIPS , pp.\n841\u2013848. URL:www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/A A28.ps.gz .286,\n336,525,528\nNg, Andrew Y., Michael I. Jordan, and Yair Weiss. 2001a. On sp ectral clustering:\nAnalysis and an algorithm. In Proc. NIPS , pp. 849\u2013856. 400,525,528,532\nNg, Andrew Y., Alice X. Zheng, and Michael I. Jordan. 2001b. L ink analysis, eigenvec-\ntors and stability. In Proc. IJCAI , pp. 903\u2013910. URL:citeseer.ist.psu.edu/ng01link.html .\n481,525,528,533\nNigam, Kamal, Andrew McCallum, and Tom Mitchell. 2006. Semi -supervised text\nclassi\ufb01cation using EM. In Chapelle et al. (2006 ), pp. 33\u201356. 347,527,528\nNtoulas, Alexandros, and Junghoo Cho. 2007. Pruning polici es for two-tiered in-\nverted index with correctness guarantee. In Proc. SIGIR , pp. 191\u2013198. ACM Press.\n105,521,528\nOard, Douglas W., and Bonnie J. Dorr. 1996. A survey of multil ingual text retrieval.\nTechnical Report UMIACS-TR-96-19, Institute for Advanced Computer Studies,\nUniversity of Maryland, College Park, MD, USA. xxxiv ,522,528\nOgilvie, Paul, and Jamie Callan. 2005. Parameter estimatio n for a simple hierar-\nchical generative model for XML retrieval. In Proc. INEX , pp. 211\u2013224. DOI:\ndx.doi.org/10.1007/11766278_16 .216,521,528\nOnline edition (c)\n2009 Cambridge UPBibliography 509\nO\u2019Keefe, Richard A., and Andrew Trotman. 2004. The simplest query language that\ncould possibly work. In Fuhr et al. (2005 ), pp. 167\u2013174. 217,528,532\nOsi\u00b4 nski, Stanis\u0142aw, and Dawid Weiss. 2005. A concept-driv en algorithm for clustering\nsearch results. IEEE Intelligent Systems 20(3):48\u201354. 400,528,532\nPage, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Wino grad. 1998. The Page-\nRank citation ranking: Bringing order to the web. Technical report, Stanford Digital\nLibrary Technologies Project. URL:citeseer.ist.psu.edu/page98pagerank.html .480,520,\n528,533\nPaice, Chris D. 1990. Another stemmer. SIGIR Forum 24(3):56\u201361. 33,528\nPapineni, Kishore. 2001. Why inverse document frequency? I nProc. North American\nChapter of the Association for Computational Linguistics , pp. 1\u20138. 133,528\nPavlov, Dmitry, Ramnath Balasubramanyan, Byron Dom, Shyam Kapur, and Jig-\nnashu Parikh. 2004. Document preprocessing for naive Bayes classi\ufb01cation and\nclustering with mixture of multinomials. In Proc. KDD , pp. 829\u2013834. 286,519,522,\n525,528\nPelleg, Dan, and Andrew Moore. 1999. Accelerating exact k-m eans algorithms\nwith geometric reasoning. In Proc. KDD , pp. 277\u2013281. ACM Press. DOI:\ndoi.acm.org/10.1145/312129.312248 .373,528,529\nPelleg, Dan, and Andrew Moore. 2000. X-means: Extending k-m eans with ef\ufb01cient es-\ntimation of the number of clusters. In Proc. ICML , pp. 727\u2013734. Morgan Kaufmann.\n373,528,529\nPerkins, Simon, Kevin Lacker, and James Theiler. 2003. Graf ting: Fast, incremental\nfeature selection by gradient descent in function space. JMLR 3:1333\u20131356. 286,\n526,529,531\nPersin, Michael. 1994. Document \ufb01ltering for fast ranking. InProc. SIGIR , pp. 339\u2013348.\nACM Press. 149,529\nPersin, Michael, Justin Zobel, and Ron Sacks-Davis. 1996. F iltered document retrieval\nwith frequency-sorted indexes. JASIS 47(10):749\u2013764. 149,529,530,533\nPeterson, James L. 1980. Computer programs for detecting an d correcting spelling\nerrors. CACM 23(12):676\u2013687. DOI:doi.acm.org/10.1145/359038.359041 .65,529\nPicca, Davide, Beno\u00eet Curdy, and Fran\u00e7ois Bavaud. 2006. Non -linear correspondence\nanalysis in text retrieval: A kernel view. In Proc. JADT .308,520,522,529\nPinski, Gabriel, and Francis Narin. 1976. Citation in\ufb02uenc e for journal aggregates of\nscienti\ufb01c publications: Theory, with application to the li terature of Physics. IP&M\n12:297\u2013326. 480,528,529\nPirolli, Peter L. T. 2007. Information Foraging Theory: Adaptive Interaction With In forma-\ntion. Oxford University Press. 373,529\nPlatt, John. 2000. Probabilistic outputs for support vecto r machines and comparisons\nto regularized likelihood methods. In A.J. Smola, P .L. Bart lett, B. Sch\u00f6lkopf, and\nD. Schuurmans (eds.), Advances in Large Margin Classi\ufb01ers , pp. 61\u201374. MIT Press.\n325,529\nOnline edition (c)\n2009 Cambridge UP510 Bibliography\nPonte, Jay M., and W. Bruce Croft. 1998. A language modeling a pproach to informa-\ntion retrieval. In Proc. SIGIR , pp. 275\u2013281. ACM Press. xxii,246,247,249,252,522,\n529\nPopescul, Alexandrin, and Lyle H. Ungar. 2000. Automatic la beling of document\nclusters. Unpublished MS, U. Pennsylvania. URL:http://www.cis.upenn.edu/ popes-\ncul/Publications/popescul00labeling.pdf .400,529,532\nPorter, Martin F. 1980. An algorithm for suf\ufb01x stripping. Program 14(3):130\u2013137. 33,\n529\nPugh, William. 1990. Skip lists: A probabilistic alternati ve to balanced trees. CACM\n33(6):668\u2013676. 46,529\nQin, Tao, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-Sheng Wang , and Hang Li. 2007.\nRanking with multiple hyperplanes. In Proc. SIGIR . ACM Press. 348,526,529,532,\n533\nQiu, Yonggang, and H.P . Frei. 1993. Concept based query expa nsion. In Proc. SIGIR ,\npp. 160\u2013169. ACM Press. 194,523,529\nR Development Core Team. 2005. R: A language and environment for statistical comput-\ning. R Foundation for Statistical Computing, Vienna. URL:www.R-project.org . ISBN\n3-900051-07-0. 374,400,529\nRadev, Dragomir R., Sasha Blair-Goldensohn, Zhu Zhang, and Revathi Sundara\nRaghavan. 2001. Interactive, domain-independent identi\ufb01 cation and summariza-\ntion of topically related news articles. In Proc. European Conference on Research and\nAdvanced Technology for Digital Libraries , pp. 225\u2013238. 373,520,529,533\nRahm, Erhard, and Philip A. Bernstein. 2001. A survey of appr oaches\nto automatic schema matching. VLDB Journal 10(4):334\u2013350. URL:cite-\nseer.ist.psu.edu/rahm01survey.html .216,520,529\nRand, William M. 1971. Objective criteria for the evaluatio n of clustering methods.\nJournal of the American Statistical Association 66(336):846\u2013850. 373,529\nRasmussen, Edie. 1992. Clustering algorithms. In Frakes and Baeza-Yates (1992 ), pp.\n419\u2013442. 372,529\nRennie, Jason D., Lawrence Shih, Jaime Teevan, and David R. K arger. 2003. Tackling\nthe poor assumptions of naive Bayes text classi\ufb01ers. In Proc. ICML , pp. 616\u2013623.\n286,525,529,530,531\nRibeiro-Neto, Berthier, Edleno S. Moura, Marden S. Neubert , and Nivio Ziviani. 1999.\nEf\ufb01cient distributed algorithms to build inverted \ufb01les. In Proc. SIGIR , pp. 105\u2013112.\nACM Press. DOI:doi.acm.org/10.1145/312624.312663 .83,528,529,533\nRibeiro-Neto, Berthier A., and Ramurti A. Barbosa. 1998. Qu ery performance for\ntightly coupled distributed digital libraries. In Proc. ACM Conference on Digital\nLibraries , pp. 182\u2013190. 459,519,529\nRice, John A. 2006. Mathematical Statistics and Data Analysis . Duxbury Press. 99,235,\n276,529\nRichardson, M., A. Prakash, and E. Brill. 2006. Beyond PageR ank: machine learning\nfor static ranking. In Proc. WWW , pp. 707\u2013715. 348,520,529\nOnline edition (c)\n2009 Cambridge UPBibliography 511\nRiezler, Stefan, Alexander Vasserman, Ioannis Tsochantar idis, Vibhu Mittal, and\nYi Liu. 2007. Statistical machine translation for query exp ansion in answer re-\ntrieval. In Proc. ACL , pp. 464\u2013471. Association for Computational Linguistics. URL:\nwww.aclweb.org/anthology/P/P07/P07-1059 .194,527,529,532\nRipley, B. D. 1996. Pattern Recognition and Neural Networks . Cambridge University\nPress. 222,235,529\nRobertson, Stephen. 2005. How Okapi came to TREC. In Voorhees and Harman\n(2005 ), pp. 287\u2013299. 235,529\nRobertson, Stephen, Hugo Zaragoza, and Michael Taylor. 200 4. Simple BM25\nextension to multiple weighted \ufb01elds. In Proc. CIKM , pp. 42\u201349. DOI:\ndoi.acm.org/10.1145/1031171.1031181 .235,529,531,533\nRobertson, Stephen E., and Karen Sp\u00e4rck Jones. 1976. Releva nce weighting of search\nterms. JASIS 27:129\u2013146. 133,235,525,529\nRocchio, J. J. 1971. Relevance feedback in information retr ieval. In Salton (1971b ), pp.\n313\u2013323. 181,193,314,530\nRoget, P . M. 1946. Roget\u2019s International Thesaurus . Thomas Y. Crowell. 194,530\nRosen-Zvi, Michal, Thomas Grif\ufb01ths, Mark Steyvers, and Pad hraic Smyth. 2004. The\nauthor-topic model for authors and documents. In Proc. UAI , pp. 487\u2013494. 418,\n523,530,531\nRoss, Sheldon. 2006. A First Course in Probability . Pearson Prentice Hall. 99,235,530\nRusmevichientong, Paat, David M. Pennock, Steve Lawrence, and C. Lee Giles. 2001.\nMethods for sampling pages uniformly from the world wide web . In Proc. AAAI\nFall Symposium on Using Uncertainty Within Computation , pp. 121\u2013128. URL:cite-\nseer.ist.psu.edu/rusmevichientong01methods.html .442,523,526,529,530\nRuthven, Ian, and Mounia Lalmas. 2003. A survey on the use of r elevance feedback\nfor information access systems. Knowledge Engineering Review 18(1). 194,526,530\nSahoo, Nachiketa, Jamie Callan, Ramayya Krishnan, George D uncan, and Rema Pad-\nman. 2006. Incremental hierarchical clustering of text doc uments. In Proc. CIKM ,\npp. 357\u2013366. DOI:doi.acm.org/10.1145/1183614.1183667 .400,521,522,526,528,530\nSakai, Tetsuya. 2007. On the reliability of information ret rieval metrics based on\ngraded relevance. IP&M 43(2):531\u2013548. 174,530\nSalton, Gerard. 1971a. Cluster search strategies and the op timization of retrieval\neffectiveness. In The SMART Retrieval System \u2013 Experiments in Automatic Docum ent\nProcessing Salton (1971b ), pp. 223\u2013242. 351,372,530\nSalton, Gerard (ed.). 1971b. The SMART Retrieval System \u2013 Experiments in Automatic\nDocument Processing . Prentice Hall. 133,173,193,499,509,510,530\nSalton, Gerard. 1975. Dynamic information and library processing . Prentice Hall. 372,\n530\nSalton, Gerard. 1989. Automatic Text Processing: The Transformation, Analysis, and\nRetrieval of Information by Computer . Addison Wesley. 46,194,530\nOnline edition (c)\n2009 Cambridge UP512 Bibliography\nSalton, Gerard. 1991. The Smart project in automatic docume nt retrieval. In Proc.\nSIGIR , pp. 356\u2013358. ACM Press. 173,530\nSalton, Gerard, James Allan, and Chris Buckley. 1993. Appro aches to passage re-\ntrieval in full text information systems. In Proc. SIGIR , pp. 49\u201358. ACM Press. DOI:\ndoi.acm.org/10.1145/160688.160693 .217,519,520,530\nSalton, Gerard, and Chris Buckley. 1987. Term weighting app roaches in automatic\ntext retrieval. Technical report, Cornell University, Ith aca, NY, USA. 133,520,530\nSalton, Gerard, and Christopher Buckley. 1988. Term-weigh ting approaches in auto-\nmatic text retrieval. IP&M 24(5):513\u2013523. 133,520,530\nSalton, Gerard, and Chris Buckley. 1990. Improving retriev al performance by rele-\nvance feedback. JASIS 41(4):288\u2013297. 194,520,530\nSaracevic, Tefko, and Paul Kantor. 1988. A study of informat ion seeking and retriev-\ning. II: Users, questions and effectiveness. JASIS 39:177\u2013196. 173,525,530\nSaracevic, Tefko, and Paul Kantor. 1996. A study of informat ion seeking and retriev-\ning. III: Searchers, searches, overlap. JASIS 39(3):197\u2013216. 173,525,530\nSavaresi, Sergio M., and Daniel Boley. 2004. A comparative a nalysis on the bisecting\nK-means and the PDDP clustering algorithms. Intelligent Data Analysis 8(4):345\u2013\n362. 400,520,530\nSchamber, Linda, Michael Eisenberg, and Michael S. Nilan. 1 990. A re-examination\nof relevance: toward a dynamic, situational de\ufb01nition. IP&M 26(6):755\u2013776. 174,\n522,528,530\nSchapire, Robert E. 2003. The boosting approach to machine l earning: An overview.\nIn D. D. Denison, M. H. Hansen, C. Holmes, B. Mallick, and B. Yu (eds.), Nonlinear\nEstimation and Classi\ufb01cation . Springer. 347,530\nSchapire, Robert E., and Yoram Singer. 2000. Boostexter: A b oosting-based system\nfor text categorization. Machine Learning 39(2/3):135\u2013168. 347,530,531\nSchapire, Robert E., Yoram Singer, and Amit Singhal. 1998. B oosting and Rocchio\napplied to text \ufb01ltering. In Proc. SIGIR , pp. 215\u2013223. ACM Press. 314,315,530,531\nSchlieder, Torsten, and Holger Meuss. 2002. Querying and ra nking XML documents.\nJASIST 53(6):489\u2013503. DOI:dx.doi.org/10.1002/asi.10060 .216,527,530\nScholer, Falk, Hugh E. Williams, John Yiannis, and Justin Zo bel. 2002. Compression\nof inverted indexes for fast query evaluation. In Proc. SIGIR , pp. 222\u2013229. ACM\nPress. DOI:doi.acm.org/10.1145/564376.564416 .106,530,533\nSch\u00f6lkopf, Bernhard, and Alexander J. Smola. 2001. Learning with Kernels: Support\nVector Machines, Regularization, Optimization, and Beyon d. MIT Press. 346,530,531\nSch\u00fctze, Hinrich. 1998. Automatic word sense discriminati on.Computational Linguis-\ntics24(1):97\u2013124. 192,194,530\nSch\u00fctze, Hinrich, David A. Hull, and Jan O. Pedersen. 1995. A comparison of clas-\nsi\ufb01ers and document representations for the routing proble m. In Proc. SIGIR , pp.\n229\u2013237. ACM Press. 193,286,315,524,529,530\nOnline edition (c)\n2009 Cambridge UPBibliography 513\nSch\u00fctze, Hinrich, and Jan O. Pedersen. 1995. Information re trieval based on word\nsenses. In Proc. SDAIR , pp. 161\u2013175. 374,529,530\nSch\u00fctze, Hinrich, and Craig Silverstein. 1997. Projection s for ef\ufb01cient document\nclustering. In Proc. SIGIR , pp. 74\u201381. ACM Press. 373,417,530\nSchwarz, Gideon. 1978. Estimating the dimension of a model. Annals of Statistics 6\n(2):461\u2013464. 373,530\nSebastiani, Fabrizio. 2002. Machine learning in automated text categorization. ACM\nComputing Surveys 34(1):1\u201347. 286,530\nShawe-Taylor, John, and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis .\nCambridge University Press. 346,521,530\nShkapenyuk, Vladislav, and Torsten Suel. 2002. Design and i mplementation of a\nhigh-performance distributed web crawler. In Proc. International Conference on Data\nEngineering .URL:citeseer.ist.psu.edu/shkapenyuk02design.html .458,530,531\nSiegel, Sidney, and N. John Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral\nSciences , 2nd edition. McGraw Hill. 174,521,530\nSifry, Dave, 2007. The state of the Live Web, April 2007. URL:techno-\nrati.com/weblog/2007/04/328.html .30,530\nSigurbj\u00f6rnsson, B\u00f6rkur, Jaap Kamps, and Maarten de Rijke. 2 004. Mixture models,\noverlap, and structural hints in XML element retrieval. In Proc. INEX , pp. 196\u2013210.\n216,525,529,530\nSilverstein, Craig, Monika Rauch Henzinger, Hannes Marais , and Michael Moricz.\n1999. Analysis of a very large web search engine query log. SIGIR Forum 33(1):\n6\u201312. 47,524,527,528,530\nSilvestri, Fabrizio. 2007. Sorting out the document identi \ufb01er assignment problem. In\nProc. ECIR , pp. 101\u2013112. 106,531\nSilvestri, Fabrizio, Raffaele Perego, and Salvatore Orlan do. 2004. Assigning docu-\nment identi\ufb01ers to enhance compressibility of web search en gines indexes. In Proc.\nACM Symposium on Applied Computing , pp. 600\u2013605. 106,528,529,531\nSindhwani, V ., and S. S. Keerthi. 2006. Large scale semi-sup ervised linear SVMs. In\nProc. SIGIR , pp. 477\u2013484. 348,525,531\nSinghal, Amit, Chris Buckley, and Mandar Mitra. 1996a. Pivo ted document\nlength normalization. In Proc. SIGIR , pp. 21\u201329. ACM Press. URL:cite-\nseer.ist.psu.edu/singhal96pivoted.html .133,520,527,531\nSinghal, Amit, Mandar Mitra, and Chris Buckley. 1997. Learn ing routing queries in a\nquery zone. In Proc. SIGIR , pp. 25\u201332. ACM Press. 193,520,527,531\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1995. Leng th normalization in\ndegraded text collections. Technical report, Cornell Univ ersity, Ithaca, NY. 133,\n520,530,531\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1996b. Len gth normalization in\ndegraded text collections. In Proc. SDAIR , pp. 149\u2013162. 133,520,530,531\nOnline edition (c)\n2009 Cambridge UP514 Bibliography\nSingitham, Pavan Kumar C., Mahathi S. Mahabhashyam, and Pra bhakar Raghavan.\n2004. Ef\ufb01ciency-quality tradeoffs for vector score aggreg ation. In Proc. VLDB , pp.\n624\u2013635. URL:www.vldb.org/conf/2004/RS17P1.PDF .149,372,527,529,531\nSmeulders, Arnold W. M., Marcel Worring, Simone Santini, Am arnath Gupta,\nand Ramesh Jain. 2000. Content-based image retrieval at the end of the\nearly years. IEEE Trans. Pattern Anal. Mach. Intell. 22(12):1349\u20131380. DOI:\ndx.doi.org/10.1109/34.895972 .xxxiv ,523,524,530,531,533\nSneath, Peter H.A., and Robert R. Sokal. 1973. Numerical Taxonomy: The Principles and\nPractice of Numerical Classi\ufb01cation . W.H. Freeman. 399,531\nSnedecor, George Waddel, and William G. Cochran. 1989. Statistical methods . Iowa\nState University Press. 286,521,531\nSomogyi, Zoltan. 1990. The Melbourne University bibliogra phy system. Technical\nReport 90/3, Melbourne University, Parkville, Victoria, A ustralia. 83,531\nSong, Ruihua, Ji-Rong Wen, and Wei-Ying Ma. 2005. Viewing te rm proximity from a\ndifferent perspective. Technical Report MSR-TR-2005-69, Microsoft Research. 149,\n527,531,532\nSornil, Ohm. 2001. Parallel Inverted Index for Large-Scale, Dynamic Digital L ibraries . PhD\nthesis, Virginia Tech. URL:scholar.lib.vt.edu/theses/available/etd-02062001-11 4915/ .\n459,531\nSp\u00e4rck Jones, Karen. 1972. A statistical interpretation of term speci\ufb01city and its ap-\nplication in retrieval. Journal of Documentation 28(1):11\u201321. 133,525\nSp\u00e4rck Jones, Karen. 2004. Language modelling\u2019s generativ e model: Is it\nrational? MS, Computer Laboratory, University of Cambridg e. URL:\nwww.cl.cam.ac.uk/ ~ksj21/langmodnote4.pdf .252,525\nSp\u00e4rck Jones, Karen, S. Walker, and Stephen E. Robertson. 20 00. A probabilistic model\nof information retrieval: Development and comparative exp eriments. IP&M 36(6):\n779\u2013808, 809\u2013840. 232,234,235,525,529,532\nSpink, Amanda, and Charles Cole (eds.). 2005. New Directions in Cognitive Information\nRetrieval . Springer. 175,521,531\nSpink, Amanda, Bernard J. Jansen, and H. Cenk Ozmultu. 2000. Use\nof query reformulation and relevance feedback by Excite use rs. Internet\nResearch: Electronic Networking Applications and Policy 10(4):317\u2013328. URL:\nist.psu.edu/faculty_pages/jjansen/academic/pubs/int ernetresearch2000.pdf .185,524,528,\n531\nSproat, Richard, and Thomas Emerson. 2003. The \ufb01rst interna tional Chinese word\nsegmentation bakeoff. In SIGHAN Workshop on Chinese Language Processing .46,\n522,531\nSproat, Richard, William Gale, Chilin Shih, and Nancy Chang . 1996. A stochastic\n\ufb01nite-state word-segmentation algorithm for Chinese. Computational Linguistics 22\n(3):377\u2013404. 46,521,523,530,531\nSproat, Richard William. 1992. Morphology and computation . MIT Press. 46,531\nOnline edition (c)\n2009 Cambridge UPBibliography 515\nStein, Benno, and Sven Meyer zu Eissen. 2004. Topic identi\ufb01c ation: Framework and\napplication. In Proc. International Conference on Knowledge Management .400,522,\n531\nStein, Benno, Sven Meyer zu Eissen, and Frank Wi\u00dfbrock. 2003 . On cluster validity\nand the information need of users. In Proc. Arti\ufb01cial Intelligence and Applications .\n373,522,531,533\nSteinbach, Michael, George Karypis, and Vipin Kumar. 2000. A comparison of docu-\nment clustering techniques. In KDD Workshop on Text Mining .400,525,526,531\nStrang, Gilbert (ed.). 1986. Introduction to Applied Mathematics . Wellesley-Cambridge\nPress. 417,531\nStrehl, Alexander. 2002. Relationship-based Clustering and Cluster Ensembles for H igh-\ndimensional Data Mining . PhD thesis, The University of Texas at Austin. 373,531\nStrohman, Trevor, and W. Bruce Croft. 2007. Ef\ufb01cient docume nt retrieval in main\nmemory. In Proc. SIGIR , pp. 175\u2013182. ACM Press. 47,522,531\nSwanson, Don R. 1988. Historical note: Information retriev al and the future of an\nillusion. JASIS 39(2):92\u201398. 173,193,531\nTague-Sutcliffe, Jean, and James Blustein. 1995. A statist ical analysis of the TREC-3\ndata. In Proc. TREC , pp. 385\u2013398. 174,520,531\nTan, Songbo, and Xueqi Cheng. 2007. Using hypothesis margin to boost centroid text\nclassi\ufb01er. In Proc. ACM Symposium on Applied Computing , pp. 398\u2013403. ACM Press.\nDOI:doi.acm.org/10.1145/1244002.1244096 .314,521,531\nTannier, Xavier, and Shlomo Geva. 2005. XML retrieval with a natural language\ninterface. In Proc. SPIRE , pp. 29\u201340. 217,523,531\nTao, Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 20 06. Language\nmodel information retrieval with document expansion. In Proc. Human Language\nTechnology Conference / North American Chapter of the Assoc iation for Computational\nLinguistics , pp. 407\u2013414. 252,527,531,532,533\nTaube, Mortimer, and Harold Wooster (eds.). 1958. Information storage and retrieval:\nTheory, systems, and devices . Columbia University Press. 17,531,533\nTaylor, Michael, Hugo Zaragoza, Nick Craswell, Stephen Rob ertson, and Chris\nBurges. 2006. Optimisation methods for ranking functions w ith multiple parame-\nters. In Proc. CIKM . ACM Press. 348,520,521,529,531,533\nTeh, Yee Whye, Michael I. Jordan, Matthew J. Beal, and David M . Blei. 2006. Hier-\narchical Dirichlet processes. Journal of the American Statistical Association 101(476):\n1566\u20131581. 418,520,525,531\nTheobald, Martin, Holger Bast, Debapriyo Majumdar, Ralf Sc henkel, and Gerhard\nWeikum. 2008. TopX: Ef\ufb01cient and versatile top- kquery processing for semistruc-\ntured data. VLDB Journal 17(1):81\u2013115. 216,520,527,530,531,532\nTheobald, Martin, Ralf Schenkel, and Gerhard Weikum. 2005. An ef\ufb01cient and versa-\ntile query engine for TopX search. In Proc. VLDB , pp. 625\u2013636. VLDB Endowment.\n216,530,531,532\nOnline edition (c)\n2009 Cambridge UP516 Bibliography\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2 001. Estimating the num-\nber of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society\nSeries B 63:411\u2013423. 374,524,531,532\nTishby, Naftali, and Noam Slonim. 2000. Data clustering by M arkovian relaxation\nand the information bottleneck method. In Proc. NIPS , pp. 640\u2013646. 374,531\nToda, Hiroyuki, and Ryoji Kataoka. 2005. A search result clu stering method using\ninformatively named entities. In International Workshop on Web Information and Data\nManagement , pp. 81\u201386. ACM Press. DOI:doi.acm.org/10.1145/1097047.1097063 .372,\n525,531\nTomasic, Anthony, and Hector Garcia-Molina. 1993. Query pr ocessing and inverted\nindices in shared-nothing document information retrieval systems. VLDB Journal\n2(3):243\u2013275. 458,523,531\nTombros, Anastasios, and Mark Sanderson. 1998. Advantages of query biased\nsummaries in information retrieval. In Proc. SIGIR , pp. 2\u201310. ACM Press. DOI:\ndoi.acm.org/10.1145/290941.290947 .174,530,531\nTombros, Anastasios, Robert Villa, and Cornelis Joost van R ijsbergen. 2002. The\neffectiveness of query-speci\ufb01c hierarchic clustering in i nformation retrieval. IP&M\n38(4):559\u2013582. DOI:dx.doi.org/10.1016/S0306-4573(01)00048-6 .372,529,531,532\nTomlinson, Stephen. 2003. Lexical and algorithmic stemmin g compared for 9 Eu-\nropean languages with Hummingbird Searchserver at CLEF 200 3. In Proc. Cross-\nLanguage Evaluation Forum , pp. 286\u2013300. 46,531\nTong, Simon, and Daphne Koller. 2001. Support vector machin e active learning with\napplications to text classi\ufb01cation. JMLR 2:45\u201366. 348,525,531\nToutanova, Kristina, and Robert C. Moore. 2002. Pronunciat ion modeling for im-\nproved spelling correction. In Proc. ACL , pp. 144\u2013151. 65,528,531\nTreeratpituk, Pucktada, and Jamie Callan. 2006. An experim ental study on automat-\nically labeling hierarchical clusters using statistical f eatures. In Proc. SIGIR , pp.\n707\u2013708. ACM Press. DOI:doi.acm.org/10.1145/1148170.1148328 .400,521,532\nTrotman, Andrew. 2003. Compressing inverted \ufb01les. IR6(1):5\u201319. DOI:\ndx.doi.org/10.1023/A:1022949613039 .106,532\nTrotman, Andrew, and Shlomo Geva. 2006. Passage retrieval a nd other XML-retrieval\ntasks. In SIGIR 2006 Workshop on XML Element Retrieval Methodology , pp. 43\u201350. 217,\n523,532\nTrotman, Andrew, Shlomo Geva, and Jaap Kamps (eds.). 2007. SIGIR Workshop on\nFocused Retrieval . University of Otago. 217,523,525,532\nTrotman, Andrew, Nils Pharo, and Miro Lehtonen. 2006. XML-I R users and use cases.\nInProc. INEX , pp. 400\u2013412. 216,526,529,532\nTrotman, Andrew, and B\u00f6rkur Sigurbj\u00f6rnsson. 2004. Narrowe d Extended XPath I\n(NEXI). In Fuhr et al. (2005 ), pp. 16\u201340. DOI:dx.doi.org/10.1007/11424550_2 .217,\n530,532\nTseng, Huihsin, Pichuan Chang, Galen Andrew, Daniel Jurafs ky, and Christopher\nManning. 2005. A conditional random \ufb01eld word segmenter. In SIGHAN Workshop\non Chinese Language Processing .46,519,521,525,527,532\nOnline edition (c)\n2009 Cambridge UPBibliography 517\nTsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofma nn, and Yasemin Altun.\n2005. Large margin methods for structured and interdepende nt output variables.\nJMLR 6:1453\u20131484. 347,519,524,525,532\nTurpin, Andrew, and William R. Hersh. 2001. Why batch and use r evaluations do not\ngive the same results. In Proc. SIGIR , pp. 225\u2013231. 175,524,532\nTurpin, Andrew, and William R. Hersh. 2002. User interface e ffects in past batch\nversus user experiments. In Proc. SIGIR , pp. 431\u2013432. 175,524,532\nTurpin, Andrew, Yohannes Tsegay, David Hawking, and Hugh E. Williams. 2007.\nFast generation of result snippets in web search. In Proc. SIGIR , pp. 127\u2013134. ACM\nPress. 174,524,532,533\nTurtle, Howard. 1994. Natural language vs. Boolean query ev aluation: A comparison\nof retrieval performance. In Proc. SIGIR , pp. 212\u2013220. ACM Press. 15,532\nTurtle, Howard, and W. Bruce Croft. 1989. Inference network s for document retrieval.\nInProc. SIGIR , pp. 1\u201324. ACM Press. 234,522,532\nTurtle, Howard, and W. Bruce Croft. 1991. Evaluation of an in ference network-based\nretrieval model. TOIS 9(3):187\u2013222. 234,522,532\nTurtle, Howard, and James Flood. 1995. Query evaluation: st rategies and optimiza-\ntions. IP&M 31(6):831\u2013850. DOI:dx.doi.org/10.1016/0306-4573(95)00020-H .133,522,\n532\nVaithyanathan, Shivakumar, and Byron Dom. 2000. Model-bas ed hierarchical clus-\ntering. In Proc. UAI , pp. 599\u2013608. Morgan Kaufmann. 400,522,532\nvan Rijsbergen, Cornelis Joost. 1979. Information Retrieval , 2nd edition. Butterworths.\n173,216,221,231,235,529\nvan Rijsbergen, Cornelis Joost. 1989. Towards an informati on logic. In Proc. SIGIR ,\npp. 77\u201386. ACM Press. DOI:doi.acm.org/10.1145/75334.75344 .xxxiv ,529\nvan Zwol, Roelof, Jeroen Baas, Herre van Oostendorp, and Fra ns Wiering. 2006.\nBricks: The building blocks to tackle query formulation in s tructured document\nretrieval. In Proc. ECIR , pp. 314\u2013325. 217,519,528,532,533\nVapnik, Vladimir N. 1998. Statistical Learning Theory . Wiley-Interscience. 346,532\nVittaut, Jean-No\u00ebl, and Patrick Gallinari. 2006. Machine l earning ranking for struc-\ntured information retrieval. In Proc. ECIR , pp. 338\u2013349. 216,523,532\nVoorhees, Ellen M. 1985a. The cluster hypothesis revisited . In Proc. SIGIR , pp. 188\u2013\n196. ACM Press. 372,532\nVoorhees, Ellen M. 1985b. The effectiveness and ef\ufb01ciency o f agglomerative hierar-\nchic clustering in document retrieval. Technical Report TR 85-705, Cornell. 399,\n532\nVoorhees, Ellen M. 2000. Variations in relevance judgments and the measurement of\nretrieval effectiveness. IP&M 36:697\u2013716. 174,532\nVoorhees, Ellen M., and Donna Harman (eds.). 2005. TREC: Experiment and Evaluation\nin Information Retrieval . MIT Press. 173,314,498,509,524,532\nOnline edition (c)\n2009 Cambridge UP518 Bibliography\nWagner, Robert A., and Michael J. Fischer. 1974. The string- to-string correction prob-\nlem. JACM 21(1):168\u2013173. DOI:doi.acm.org/10.1145/321796.321811 .65,522,532\nWard Jr., J. H. 1963. Hierarchical grouping to optimize an ob jective function. Journal\nof the American Statistical Association 58:236\u2013244. 399,532\nWei, Xing, and W. Bruce Croft. 2006. LDA-based document mode ls\nfor ad-hoc retrieval. In Proc. SIGIR , pp. 178\u2013185. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148204 .418,522,532\nWeigend, Andreas S., Erik D. Wiener, and Jan O. Pedersen. 199 9. Exploiting hierarchy\nin text categorization. IR1(3):193\u2013216. 347,529,532\nWeston, Jason, and Chris Watkins. 1999. Support vector mach ines for multi-class\npattern recognition. In Proc. European Symposium on Arti\ufb01cial Neural Networks , pp.\n219\u2013224. 347,532\nWilliams, Hugh E., and Justin Zobel. 2005. Searchable words on the web. International\nJournal on Digital Libraries 5(2):99\u2013105. DOI:dx.doi.org/10.1007/s00799-003-0050-z .\n105,533\nWilliams, Hugh E., Justin Zobel, and Dirk Bahle. 2004. Fast p hrase querying with\ncombined indexes. TOIS 22(4):573\u2013594. 43,519,533\nWitten, Ian H., and Timothy C. Bell. 1990. Source models for n atural language text.\nInternational Journal Man-Machine Studies 32(5):545\u2013579. 105,520,533\nWitten, Ian H., and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools\nand Techniques , 2nd edition. Morgan Kaufmann. 374,523,533\nWitten, Ian H., Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Com-\npressing and Indexing Documents and Images , 2nd edition. Morgan Kaufmann. 18,\n83,105,106,520,528,533\nWong, S. K. Michael, Yiyu Yao, and Peter Bollmann. 1988. Line ar structure in infor-\nmation retrieval. In Proc. SIGIR , pp. 219\u2013232. ACM Press. 348,520,533\nWoodley, Alan, and Shlomo Geva. 2006. NLPX at INEX 2006. In Proc. INEX , pp.\n302\u2013311. 217,523,533\nXu, Jinxi, and W. Bruce Croft. 1996. Query expansion using lo cal and global document\nanalysis. In Proc. SIGIR , pp. 4\u201311. ACM Press. 194,522,533\nXu, Jinxi, and W. Bruce Croft. 1999. Cluster-based language models for\ndistributed retrieval. In Proc. SIGIR , pp. 254\u2013261. ACM Press. DOI:\ndoi.acm.org/10.1145/312624.312687 .372,522,533\nYang, Hui, and Jamie Callan. 2006. Near-duplicate detectio n by instance-\nlevel constrained clustering. In Proc. SIGIR , pp. 421\u2013428. ACM Press. DOI:\ndoi.acm.org/10.1145/1148170.1148243 .373,521,533\nYang, Yiming. 1994. Expert network: Effective and ef\ufb01cient learning from human\ndecisions in text categorization and retrieval. In Proc. SIGIR , pp. 13\u201322. ACM Press.\n314,533\nYang, Yiming. 1999. An evaluation of statistical approache s to text categorization. IR\n1:69\u201390. 347,533\nOnline edition (c)\n2009 Cambridge UPBibliography 519\nYang, Yiming. 2001. A study of thresholding strategies for t ext categorization. In\nProc. SIGIR , pp. 137\u2013145. ACM Press. DOI:doi.acm.org/10.1145/383952.383975 .315,\n533\nYang, Yiming, and Bryan Kisiel. 2003. Margin-based local re gression for adaptive\n\ufb01ltering. In Proc. CIKM , pp. 191\u2013198. DOI:doi.acm.org/10.1145/956863.956902 .315,\n525,533\nYang, Yiming, and Xin Liu. 1999. A re-examination of text cat egorization methods.\nInProc. SIGIR , pp. 42\u201349. ACM Press. 287,347,527,533\nYang, Yiming, and Jan Pedersen. 1997. Feature selection in s tatistical learning of text\ncategorization. In Proc. ICML .286,529,533\nYue, Yisong, Thomas Finley, Filip Radlinski, and Thorsten J oachims. 2007. A support\nvector method for optimizing average precision. In Proc. SIGIR . ACM Press. 348,\n522,525,529,533\nZamir, Oren, and Oren Etzioni. 1999. Grouper: A dynamic clus tering interface to\nweb search results. In Proc. WWW , pp. 1361\u20131374. Elsevier North-Holland. DOI:\ndx.doi.org/10.1016/S1389-1286(99)00054-7 .372,400,522,533\nZaragoza, Hugo, Djoerd Hiemstra, Michael Tipping, and Step hen Robertson. 2003.\nBayesian extension to the language model for ad hoc informat ion retrieval. In Proc.\nSIGIR , pp. 4\u20139. ACM Press. 252,524,529,531,533\nZavrel, Jakub, Peter Berck, and Willem Lavrijssen. 2000. In formation extraction by\ntext classi\ufb01cation: Corpus mining for features. In Workshop Information Extraction\nMeets Corpus Linguistics .URL:www.cnts.ua.ac.be/Publications/2000/ZBL00 . Held in\nconjunction with LREC-2000. 315,520,526,533\nZha, Hongyuan, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and Hor st D. Simon. 2001.\nBipartite graph partitioning and data clustering. In Proc. CIKM , pp. 25\u201332. 374,\n400,522,523,524,531,533\nZhai, Chengxiang, and John Lafferty. 2001a. Model-based fe edback in the language\nmodeling approach to information retrieval. In Proc. CIKM . ACM Press. 250,526,\n533\nZhai, Chengxiang, and John Lafferty. 2001b. A study of smoot hing methods for\nlanguage models applied to ad hoc information retrieval. In Proc. SIGIR , pp. 334\u2013\n342. ACM Press. 252,526,533\nZhai, ChengXiang, and John Lafferty. 2002. Two-stage langu age models\nfor information retrieval. In Proc. SIGIR , pp. 49\u201356. ACM Press. DOI:\ndoi.acm.org/10.1145/564376.564387 .252,526,533\nZhang, Jiangong, Xiaohui Long, and Torsten Suel. 2007. Perf ormance of compressed\ninverted list caching in search engines. In Proc. CIKM .106,527,531,533\nZhang, Tong, and Frank J. Oles. 2001. Text categorization ba sed on regularized linear\nclassi\ufb01cation methods. IR4(1):5\u201331. URL:citeseer.ist.psu.edu/zhang00text.html .347,\n528,533\nZhao, Ying, and George Karypis. 2002. Evaluation of hierarc hical clustering al-\ngorithms for document datasets. In Proc. CIKM , pp. 515\u2013524. ACM Press. DOI:\ndoi.acm.org/10.1145/584792.584877 .399,525,533\nOnline edition (c)\n2009 Cambridge UP520 Bibliography\nZipf, George Kingsley. 1949. Human Behavior and the Principle of Least Effort . Addison\nWesley. 105,533\nZobel, Justin. 1998. How reliable are the results of large-s cale information retrieval\nexperiments? In Proc. SIGIR , pp. 307\u2013314. 174,533\nZobel, Justin, and Philip Dart. 1995. Finding approximate m atches in\nlarge lexicons. Software Practice and Experience 25(3):331\u2013345. URL:cite-\nseer.i\ufb01.unizh.ch/zobel95\ufb01nding.html .65,522,533\nZobel, Justin, and Philip Dart. 1996. Phonetic string match ing: Lessons from infor-\nmation retrieval. In Proc. SIGIR , pp. 166\u2013173. ACM Press. 65,522,533\nZobel, Justin, and Alistair Moffat. 2006. Inverted \ufb01les for text search engines. ACM\nComputing Surveys 38(2). 18,83,106,133,528,533\nZobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron Sac ks-Davis. 1995. Ef\ufb01cient\nretrieval of partial documents. IP&M 31(3):361\u2013377. DOI:dx.doi.org/10.1016/0306-\n4573(94)00052-5 .217,528,530,532,533\nZukowski, Marcin, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar\nRAM-CPU cache compression. In Proc. International Conference on Data Engineering ,\np. 59. IEEE Computer Society. DOI:dx.doi.org/10.1109/ICDE.2006.150 .106,520,524,\n528,533\nOnline edition (c)\n2009 Cambridge UPAuthor Index\nAberer: Aberer (2001 )\nAhn: Ittner et al. (1995 )\nAizerman: Aizerman et al. (1964 )\nAkaike: Akaike (1974 )\nAllan: Allan (2005 ),Allan et al. (1998 ),\nBuckley et al. (1994a ),Buckley\net al. (1994b ),Salton et al. (1993 )\nAllwein: Allwein et al. (2000 )\nAlonso: Alonso et al. (2006 )\nAlting\u00f6vde: Can et al. (2004 )\nAlting\u00f6vde: Alting\u00f6vde et al. (2007 )\nAltun: Tsochantaridis et al. (2005 )\nAmer-Yahia: Amer-Yahia et al. (2006 ),\nAmer-Yahia et al. (2005 ),\nAmer-Yahia and Lalmas (2006 )\nAmitay: Mass et al. (2003 )\nAnagnostopoulos: Anagnostopoulos\net al. (2006 )\nAnderberg: Anderberg (1973 )\nAnderson: Burnham and Anderson\n(2002 )\nAndoni: Andoni et al. (2006 )\nAndrew: Tseng et al. (2005 )\nAnh: Anh et al. (2001 ),Anh and\nMoffat (2005 ),Anh and Moffat\n(2006a ),Anh and Moffat (2006b ),\nAnh and Moffat (2006c )\nAone: Larsen and Aone (1999 )\nApers: Mihajlovi\u00b4 c et al. (2005 )\nApt\u00e9: Apt\u00e9 et al. (1994 )\nArabie: Hubert and Arabie (1985 )\nArthur: Arthur and Vassilvitskii\n(2006 )\nArvola: Arvola et al. (2005 )Aslam: Aslam and Yilmaz (2005 )\nAult: Ault and Yang (2002 )\nBaas: van Zwol et al. (2006 )\nBadue: Badue et al. (2001 )\nBaeza-Yates: Badue et al. (2001 ),\nBaeza-Yates et al. (2005 ),\nBaeza-Yates and Ribeiro-Neto\n(1999 ),de Moura et al. (2000 ),\nFrakes and Baeza-Yates (1992 ),\nHarman et al. (1992 ),Navarro\nand Baeza-Yates (1997 )\nBahle: Bahle et al. (2002 ),Williams\net al. (2004 )\nBai: Cao et al. (2005 )\nBakiri: Dietterich and Bakiri (1995 )\nBalasubramanyan: Pavlov et al.\n(2004 )\nBaldridge: Baldridge and Osborne\n(2004 )\nBaldwin: Hughes et al. (2006 )\nBall: Ball (1965 )\nBanerjee: Alonso et al. (2006 ),Basu\net al. (2004 )\nBanko: Banko and Brill (2001 )\nBar-Ilan: Bar-Ilan and Gutman (2005 )\nBar-Yossef: Bar-Yossef and Gurevich\n(2006 )\nBarbosa: Ribeiro-Neto and Barbosa\n(1998 )\nBarreiro: Blanco and Barreiro (2006 ),\nBlanco and Barreiro (2007 )\nBarroso: Barroso et al. (2003 )\nBartell: Bartell (1994 ),Bartell et al.\n(1998 )\nOnline edition (c)\n2009 Cambridge UP522 Author Index\nBarzilay: Barzilay and Elhadad\n(1997 ),McKeown et al. (2002 )\nBasili: Moschitti and Basili (2004 )\nBast: Bast and Majumdar (2005 ),\nTheobald et al. (2008 )\nBasu: Basu et al. (2004 )\nBavaud: Picca et al. (2006 )\nBeal: Teh et al. (2006 )\nBeesley: Beesley (1998 ),Beesley and\nKarttunen (2003 )\nBelew: Bartell et al. (1998 )\nBelkin: Koenemann and Belkin (1996 )\nBell: Moffat and Bell (1995 ),Witten\nand Bell (1990 ),Witten et al.\n(1999 )\nBennett: Bennett (2000 )\nBerck: Zavrel et al. (2000 )\nBerger: Berger and Lafferty (1999 )\nBerkhin: Berkhin (2005 ),Berkhin\n(2006a ),Berkhin (2006b )\nBerners-Lee: Berners-Lee et al. (1992 )\nBernstein: Rahm and Bernstein (2001 )\nBerry: Berry and Young (1995 ),Berry\net al. (1995 ),Kent et al. (1955 )\nBetsi: Betsi et al. (2006 )\nBhagavathy: Newsam et al. (2001 )\nBharat: Bharat and Broder (1998 ),\nBharat et al. (1998 ),Bharat et al.\n(2000 ),Bharat and Henzinger\n(1998 )\nBienenstock: Geman et al. (1992 )\nBird: Hughes et al. (2006 )\nBishop: Bishop (2006 )\nBlair: Blair and Maron (1985 )\nBlair-Goldensohn: Radev et al. (2001 )\nBlanco: Blanco and Barreiro (2006 ),\nBlanco and Barreiro (2007 )\nBlandford: Blandford and Blelloch\n(2002 )\nBlei: Blei et al. (2003 ),Teh et al. (2006 )\nBlelloch: Blandford and Blelloch\n(2002 )\nBlok: List et al. (2005 ),Mihajlovi\u00b4 c\net al. (2005 )\nBlustein: Tague-Sutcliffe and Blustein\n(1995 )Boldi: Baeza-Yates et al. (2005 ),Boldi\net al. (2002 ),Boldi et al. (2005 ),\nBoldi and Vigna (2004a ),Boldi\nand Vigna (2004b ),Boldi and\nVigna (2005 )\nBoley: Boley (1998 ),Savaresi and\nBoley (2004 )\nBollmann: Wong et al. (1988 )\nBoncz: Zukowski et al. (2006 )\nBorodin: Borodin et al. (2001 )\nBotev: Amer-Yahia et al. (2006 )\nBourne: Bourne and Ford (1961 )\nBoyce: Meadow et al. (1999 )\nBracken: Lombard et al. (2002 )\nBradley: Bradley and Fayyad (1998 ),\nBradley et al. (1998 ),Fayyad\net al. (1998 )\nBraverman: Aizerman et al. (1964 )\nBrill: Banko and Brill (2001 ),Brill and\nMoore (2000 ),Cucerzan and Brill\n(2004 ),Richardson et al. (2006 )\nBrin: Brin and Page (1998 ),Page et al.\n(1998 )\nBrisaboa: Brisaboa et al. (2007 )\nBroder: Anagnostopoulos et al.\n(2006 ),Bharat and Broder (1998 ),\nBharat et al. (1998 ),Bharat et al.\n(2000 ),Broder (2002 ),Broder\net al. (2000 ),Broder et al. (1997 )\nBrown: Brown (1995 ),Coden et al.\n(2002 )\nBuckley: Buckley et al. (1994a ),\nBuckley and Salton (1995 ),\nBuckley et al. (1994b ),Buckley\net al. (1995 ),Buckley and\nVoorhees (2000 ),Hersh et al.\n(1994 ),Salton et al. (1993 ),Salton\nand Buckley (1987 ),Salton and\nBuckley (1988 ),Salton and\nBuckley (1990 ),Singhal et al.\n(1996a ),Singhal et al. (1997 ),\nSinghal et al. (1995 ),Singhal et al.\n(1996b )\nBurges: Burges et al. (2005 ),Burges\n(1998 ),Taylor et al. (2006 )\nBurner: Burner (1997 )\nOnline edition (c)\n2009 Cambridge UPAuthor Index 523\nBurnham: Burnham and Anderson\n(2002 )\nBush: Bush (1945 )\nB\u00fcttcher: B\u00fcttcher and Clarke\n(2005a ),B\u00fcttcher and Clarke\n(2005b ),B\u00fcttcher and Clarke\n(2006 ),B\u00fcttcher et al. (2006 )\nCacheda: Cacheda et al. (2003 )\nCailliau: Berners-Lee et al. (1992 )\nCallan: Callan (2000 ),Lewis et al.\n(1996 ),Ogilvie and Callan (2005 ),\nSahoo et al. (2006 ),Treeratpituk\nand Callan (2006 ),Yang and\nCallan (2006 )\nCampbell: Crestani et al. (1998 )\nCan: Alting\u00f6vde et al. (2007 ),Can\net al. (2004 ),Can and Ozkarahan\n(1990 )\nCandela: Harman and Candela (1990 )\nCannane: Garcia et al. (2004 )\nCao: Cao et al. (2005 ),Cao et al.\n(2006 ),Gao et al. (2004 )\nCarbonell: Carbonell and Goldstein\n(1998 )\nCarletta: Carletta (1996 )\nCarmel: Carmel et al. (2001 ),Carmel\net al. (2003 ),Mass et al. (2003 )\nCarneiro: Cacheda et al. (2003 )\nCaruana: Caruana and\nNiculescu-Mizil (2006 )\nCase: Amer-Yahia et al. (2005 )\nCastellan: Siegel and Castellan (1988 )\nCastillo: Baeza-Yates et al. (2005 )\nCastro: Castro et al. (2004 )\nCavnar: Cavnar and Trenkle (1994 )\nChakrabarti: Chakrabarti (2002 ),\nChakrabarti et al. (1998 )\nChan: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b )\nChang: Sproat et al. (1996 ),Tseng\net al. (2005 )\nChapelle: Chapelle et al. (2006 )\nChaudhuri: Chaudhuri et al. (2006 )\nCheeseman: Cheeseman and Stutz\n(1996 )\nChen: Chen and Lin (2000 ),Chenet al. (2005 ),Cooper et al. (1994 ),\nDumais and Chen (2000 ),\nKishida et al. (2005 ),Kishida\net al. (2005 ),Kupiec et al. (1995 ),\nLiu et al. (2005 )\nCheng: Tan and Cheng (2007 )\nChiaramella: Chiaramella et al. (1996 )\nChierichetti: Chierichetti et al. (2007 )\nCho: Cho and Garcia-Molina (2002 ),\nCho et al. (1998 ),Ntoulas and\nCho (2007 )\nChu-Carroll: Chu-Carroll et al. (2006 )\nChurch: Kernighan et al. (1990 )\nClarke: B\u00fcttcher and Clarke (2005a ),\nB\u00fcttcher and Clarke (2005b ),\nB\u00fcttcher and Clarke (2006 ),\nB\u00fcttcher et al. (2006 ),Clarke\net al. (2000 )\nCleverdon: Cleverdon (1991 )\nCoates: Castro et al. (2004 )\nCochran: Snedecor and Cochran\n(1989 )\nCoden: Coden et al. (2002 )\nCodenotti: Boldi et al. (2002 )\nCohen: Carmel et al. (2001 ),Cohen\n(1995 ),Cohen (1998 ),Cohen et al.\n(1998 ),Cohen and Singer (1999 ),\nForman and Cohen (2004 )\nCole: Spink and Cole (2005 )\nComtet: Comtet (1974 )\nCooper: Cooper et al. (1994 )\nCormack: Clarke et al. (2000 )\nCormen: Cormen et al. (1990 )\nCottrell: Bartell et al. (1998 )\nCover: Cover and Hart (1967 ),Cover\nand Thomas (1991 )\nCrammer: Crammer and Singer\n(2001 )\nCraswell: Taylor et al. (2006 )\nCreecy: Creecy et al. (1992 )\nCrestani: Crestani et al. (1998 )\nCristianini: Cristianini and\nShawe-Taylor (2000 ),Lodhi et al.\n(2002 ),Shawe-Taylor and\nCristianini (2004 )\nCroft: Croft (1978 ),Croft and Harper\nOnline edition (c)\n2009 Cambridge UP524 Author Index\n(1979 ),Croft and Lafferty (2003 ),\nLavrenko and Croft (2001 ),Liu\nand Croft (2004 ),Ponte and Croft\n(1998 ),Strohman and Croft\n(2007 ),Turtle and Croft (1989 ),\nTurtle and Croft (1991 ),Wei and\nCroft (2006 ),Xu and Croft (1996 ),\nXu and Croft (1999 )\nCrouch: Crouch (1988 )\nCucerzan: Cucerzan and Brill (2004 )\nCurdy: Picca et al. (2006 )\nCutting: Cutting et al. (1993 ),Cutting\net al. (1992 )\nCzuba: Chu-Carroll et al. (2006 )\nDamerau: Apt\u00e9 et al. (1994 ),\nDamerau (1964 )\nDart: Zobel and Dart (1995 ),Zobel\nand Dart (1996 )\nDas: Chaudhuri et al. (2006 )\nDatar: Andoni et al. (2006 )\nDavidson: Davidson and\nSatyanarayana (2003 )\nDay: Day and Edelsbrunner (1984 )\nDean: Barroso et al. (2003 ),Bharat\net al. (2000 ),Dean and\nGhemawat (2004 )\nDeeds: Burges et al. (2005 )\nDeerwester: Deerwester et al. (1990 )\nDemir: Can et al. (2004 )\nDempster: Dempster et al. (1977 )\nDhillon: Dhillon (2001 ),Dhillon and\nModha (2001 )\nDi Eugenio: Di Eugenio and Glass\n(2004 )\nDietterich: Dietterich (2002 ),\nDietterich and Bakiri (1995 )\nDing: Zha et al. (2001 )\nDom: Chakrabarti et al. (1998 ),Dom\n(2002 ),Pavlov et al. (2004 ),\nVaithyanathan and Dom (2000 )\nDomingos: Domingos (2000 ),\nDomingos and Pazzani (1997 )\nDorr: Oard and Dorr (1996 )\nDoursat: Geman et al. (1992 )\nDownie: Downie (2006 )\nDrake: Alonso et al. (2006 )Dubes: Jain and Dubes (1988 )\nDuboue: Chu-Carroll et al. (2006 )\nDuda: Duda et al. (2000 )\nDumais: Berry et al. (1995 ),\nDeerwester et al. (1990 ),Dumais\net al. (1998 ),Dumais (1993 ),\nDumais (1995 ),Dumais and\nChen (2000 ),Littman et al. (1998 )\nDuncan: Sahoo et al. (2006 )\nDunning: Dunning (1993 ),Dunning\n(1994 )\nD\u00f6rre: Amer-Yahia et al. (2006 )\nEckart: Eckart and Young (1936 )\nEdelsbrunner: Day and Edelsbrunner\n(1984 )\nEisenberg: Schamber et al. (1990 )\nEissen: Stein and zu Eissen (2004 ),\nStein et al. (2003 )\nEl-Hamdouchi: El-Hamdouchi and\nWillett (1986 )\nElhadad: Barzilay and Elhadad (1997 )\nElias: Elias (1975 )\nElkan: Hamerly and Elkan (2003 )\nEmerson: Sproat and Emerson (2003 )\nEtzioni: Zamir and Etzioni (1999 )\nEvans: McKeown et al. (2002 )\nEyheramendy: Eyheramendy et al.\n(2003 )\nFagin: Carmel et al. (2001 )\nFallows: Fallows (2004 )\nFarchi: Carmel et al. (2001 )\nFari\u00f1a: Brisaboa et al. (2007 )\nFayyad: Bradley and Fayyad (1998 ),\nBradley et al. (1998 ),Fayyad\net al. (1998 )\nFeldmann: Kammenhuber et al.\n(2006 )\nFellbaum: Fellbaum (1998 )\nFerragina: Ferragina and Venturini\n(2007 )\nFerrucci: Chu-Carroll et al. (2006 )\nFinley: Yue et al. (2007 )\nFischer: Wagner and Fischer (1974 )\nFlach: Gaertner et al. (2002 )\nFlake: Glover et al. (2002b )\nFlood: Turtle and Flood (1995 )\nOnline edition (c)\n2009 Cambridge UPAuthor Index 525\nFlynn: Jain et al. (1999 )\nFord: Bourne and Ford (1961 )\nForman: Forman (2004 ),Forman\n(2006 ),Forman and Cohen (2004 )\nFourel: Chiaramella et al. (1996 )\nFowlkes: Fowlkes and Mallows\n(1983 )\nFox: Fox and Lee (1991 ),Harman\net al. (1992 ),Lee and Fox (1988 )\nFraenkel: Fraenkel and Klein (1985 )\nFrakes: Frakes and Baeza-Yates (1992 )\nFraley: Fraley and Raftery (1998 )\nFrank: Witten and Frank (2005 )\nFrei: Qiu and Frei (1993 )\nFrieder: Grossman and Frieder (2004 )\nFriedl: Friedl (2006 )\nFriedman: Friedman (1997 ),\nFriedman and Goldszmidt\n(1996 ),Hastie et al. (2001 )\nFuhr: Fuhr (1989 ),Fuhr (1992 ),Fuhr\net al. (2003a ),Fuhr and\nGro\u00dfjohann (2004 ),Fuhr and\nLalmas (2007 ),Fuhr et al. (2006 ),\nFuhr et al. (2005 ),Fuhr et al.\n(2007 ),Fuhr et al. (2003b ),Fuhr\nand Pfeifer (1994 ),Fuhr and\nR\u00f6lleke (1997 )\nFurnas: Deerwester et al. (1990 )\nGaertner: Gaertner et al. (2002 )\nGale: Kernighan et al. (1990 ),Sproat\net al. (1996 )\nGallinari: Vittaut and Gallinari (2006 )\nGao: Gao et al. (2005 ),Gao et al.\n(2004 )\nGarcia: Garcia et al. (2004 )\nGarcia-Molina: Cho and\nGarcia-Molina (2002 ),Cho et al.\n(1998 ),Garcia-Molina et al.\n(1999 ),Hirai et al. (2000 ),Melnik\net al. (2001 ),Tomasic and\nGarcia-Molina (1993 )\nGar\ufb01eld: Gar\ufb01eld (1955 ),Gar\ufb01eld\n(1976 )\nGay: Joachims et al. (2005 )\nGeman: Geman et al. (1992 )\nGeng: Geng et al. (2007 )Gerrand: Gerrand (2007 )\nGeva: Tannier and Geva (2005 ),\nTrotman and Geva (2006 ),\nTrotman et al. (2007 ),Woodley\nand Geva (2006 )\nGey: Cooper et al. (1994 ),Gey (1994 )\nGhamrawi: Ghamrawi and\nMcCallum (2005 )\nGhemawat: Dean and Ghemawat\n(2004 )\nGibson: Chakrabarti et al. (1998 )\nGiles: Lawrence and Giles (1998 ),\nLawrence and Giles (1999 ),\nRusmevichientong et al. (2001 )\nGlass: Di Eugenio and Glass (2004 )\nGlassman: Broder et al. (1997 )\nGlover: Glover et al. (2002a ),Glover\net al. (2002b )\nGoldstein: Carbonell and Goldstein\n(1998 )\nGoldszmidt: Friedman and\nGoldszmidt (1996 )\nGrabs: Grabs and Schek (2002 )\nGraepel: Herbrich et al. (2000 )\nGranka: Joachims et al. (2005 )\nGravano: Hatzivassiloglou et al.\n(2000 )\nGreiff: Greiff (1998 )\nGrif\ufb01ths: Rosen-Zvi et al. (2004 )\nGrinstead: Grinstead and Snell (1997 )\nGroff: Berners-Lee et al. (1992 )\nGrossman: Grossman and Frieder\n(2004 )\nGro\u00dfjohann: Fuhr and Gro\u00dfjohann\n(2004 )\nGu: Zha et al. (2001 )\nGuerrero: Cacheda et al. (2003 )\nGupta: Smeulders et al. (2000 )\nGurevich: Bar-Yossef and Gurevich\n(2006 )\nGus\ufb01eld: Gus\ufb01eld (1997 )\nGutman: Bar-Ilan and Gutman (2005 )\nG\u00f6vert: Fuhr et al. (2003a ),G\u00f6vert\nand Kazai (2003 )\nHamerly: Hamerly and Elkan (2003 )\nHamilton: Burges et al. (2005 )\nOnline edition (c)\n2009 Cambridge UP526 Author Index\nHan: Han and Karypis (2000 )\nHand: Hand (2006 ),Hand and Yu\n(2001 )\nHarman: Harman (1991 ),Harman\n(1992 ),Harman et al. (1992 ),\nHarman and Candela (1990 ),\nVoorhees and Harman (2005 )\nHarold: Harold and Means (2004 )\nHarper: Croft and Harper (1979 ),\nMuresan and Harper (2004 )\nHarshman: Deerwester et al. (1990 )\nHart: Cover and Hart (1967 ),Duda\net al. (2000 )\nHarter: Harter (1998 )\nHartigan: Hartigan and Wong (1979 )\nHastie: Hastie et al. (2001 ),Tibshirani\net al. (2001 )\nHatzivassiloglou: Hatzivassiloglou\net al. (2000 ),McKeown et al.\n(2002 )\nHaveliwala: Haveliwala (2003 ),\nHaveliwala (2002 )\nHawking: Turpin et al. (2007 )\nHayes: Hayes and Weinstein (1990 )\nHe:Zha et al. (2001 )\nHeaps: Heaps (1978 )\nHearst: Hearst (1997 ),Hearst (2006 ),\nHearst and Pedersen (1996 ),\nHearst and Plaunt (1993 )\nHeckerman: Dumais et al. (1998 )\nHeinz: Heinz and Zobel (2003 ),Heinz\net al. (2002 )\nHeman: Zukowski et al. (2006 )\nHembrooke: Joachims et al. (2005 )\nHenzinger: Bharat et al. (1998 ),\nBharat et al. (2000 ),Bharat and\nHenzinger (1998 ),Henzinger\net al. (2000 ),Silverstein et al.\n(1999 )\nHerbrich: Herbrich et al. (2000 )\nHerscovici: Carmel et al. (2001 )\nHersh: Hersh et al. (1994 ),Hersh\net al. (2000a ),Hersh et al. (2001 ),\nHersh et al. (2000b ),Turpin and\nHersh (2001 ),Turpin and Hersh\n(2002 )Heydon: Henzinger et al. (2000 ),\nNajork and Heydon (2001 ),\nNajork and Heydon (2002 )\nHickam: Hersh et al. (1994 )\nHiemstra: Hiemstra (1998 ),Hiemstra\n(2000 ),Hiemstra and Kraaij\n(2005 ),Kraaij et al. (2002 ),List\net al. (2005 ),Mihajlovi\u00b4 c et al.\n(2005 ),Zaragoza et al. (2003 )\nHirai: Hirai et al. (2000 )\nHofmann: Hofmann (1999a ),\nHofmann (1999b ),Tsochantaridis\net al. (2005 )\nHollink: Hollink et al. (2004 )\nHon: Cao et al. (2006 )\nHopcroft: Hopcroft et al. (2000 )\nHristidis: Chaudhuri et al. (2006 )\nHuang: Cao et al. (2006 ),Gao et al.\n(2005 ),Huang and Mitchell\n(2006 )\nHubert: Hubert and Arabie (1985 )\nHughes: Hughes et al. (2006 )\nHull: Hull (1993 ),Hull (1996 ),\nSch\u00fctze et al. (1995 )\nHullender: Burges et al. (2005 )\nH\u00f6lzle: Barroso et al. (2003 )\nIde: Ide(1971 )\nImmorlica: Andoni et al. (2006 )\nIndyk: Andoni et al. (2006 ),Indyk\n(2004 )\nIngwersen: Ingwersen and J\u00e4rvelin\n(2005 )\nIsahara: Murata et al. (2000 )\nIttner: Ittner et al. (1995 )\nIttycheriah: Lita et al. (2003 )\nIwayama: Iwayama and Tokunaga\n(1995 )\nJ\u00e4rvelin: Ingwersen and J\u00e4rvelin\n(2005 )\nJackson: Jackson and Moulinier\n(2002 )\nJacobs: Jacobs and Rau (1990 )\nJain: Jain et al. (1999 ),Jain and Dubes\n(1988 ),Smeulders et al. (2000 )\nJansen: Spink et al. (2000 )\nOnline edition (c)\n2009 Cambridge UPAuthor Index 527\nJardine: Jardine and van Rijsbergen\n(1971 )\nJeh: Jeh and Widom (2003 )\nJensen: Jensen and Jensen (2001 ),\nJensen and Jensen (2001 )\nJeong: Jeong and Omiecinski (1995 )\nJi:Ji and Xu (2006 )\nJing: Jing (2000 )\nJoachims: Joachims (1997 ),Joachims\n(1998 ),Joachims (1999 ),Joachims\n(2002a ),Joachims (2002b ),\nJoachims (2006a ),Joachims\n(2006b ),Joachims et al. (2005 ),\nTsochantaridis et al. (2005 ),Yue\net al. (2007 )\nJohnson: Johnson et al. (2006 )\nJones: Lewis and Jones (1996 ),\nRobertson and Jones (1976 ),\nSp\u00e4rck Jones (1972 ),Sp\u00e4rck Jones\n(2004 ),Sp\u00e4rck Jones et al. (2000 )\nJordan: Blei et al. (2003 ),Ng and\nJordan (2001 ),Ng et al. (2001a ),\nNg et al. (2001b ),Teh et al. (2006 )\nJr:Kent et al. (1955 )\nJunkkari: Arvola et al. (2005 )\nJurafsky: Jurafsky and Martin (2008 ),\nTseng et al. (2005 )\nJ\u00e4rvelin: J\u00e4rvelin and Kek\u00e4l\u00e4inen\n(2002 ),Kek\u00e4l\u00e4inen and J\u00e4rvelin\n(2002 )\nKalita: Ko\u0142cz et al. (2000 )\nKambhatla: Lita et al. (2003 )\nKammenhuber: Kammenhuber et al.\n(2006 )\nKamps: Hollink et al. (2004 ),Kamps\net al. (2004 ),Kamps et al. (2006 ),\nLalmas et al. (2007 ),\nSigurbj\u00f6rnsson et al. (2004 ),\nTrotman et al. (2007 )\nKamvar: Kamvar et al. (2002 )\nKando: Kishida et al. (2005 )\nKannan: Kannan et al. (2000 )\nKantor: Saracevic and Kantor (1988 ),\nSaracevic and Kantor (1996 )\nKapur: Pavlov et al. (2004 )Karger: Cutting et al. (1993 ),Cutting\net al. (1992 ),Rennie et al. (2003 )\nKarttunen: Beesley and Karttunen\n(2003 )\nKarypis: Han and Karypis (2000 ),\nSteinbach et al. (2000 ),Zhao and\nKarypis (2002 )\nKaszkiel: Kaszkiel and Zobel (1997 )\nKataoka: Toda and Kataoka (2005 )\nKaufman: Kaufman and Rousseeuw\n(1990 )\nKazai: Fuhr et al. (2003a ),Fuhr et al.\n(2006 ),G\u00f6vert and Kazai (2003 ),\nKazai and Lalmas (2006 ),Lalmas\net al. (2007 )\nKeerthi: Sindhwani and Keerthi\n(2006 )\nKek\u00e4l\u00e4inen: Arvola et al. (2005 ),\nJ\u00e4rvelin and Kek\u00e4l\u00e4inen (2002 ),\nKek\u00e4l\u00e4inen (2005 ),Kek\u00e4l\u00e4inen\nand J\u00e4rvelin (2002 )\nKemeny: Kemeny and Snell (1976 )\nKent: Kent et al. (1955 )\nKernighan: Kernighan et al. (1990 )\nKhachiyan: Kozlov et al. (1979 )\nKing: King (1967 )\nKishida: Kishida et al. (2005 )\nKisiel: Yang and Kisiel (2003 )\nKlavans: McKeown et al. (2002 )\nKlein: Fraenkel and Klein (1985 ),\nKamvar et al. (2002 ),Klein and\nManning (2002 )\nKleinberg: Chakrabarti et al. (1998 ),\nKleinberg (1997 ),Kleinberg\n(1999 ),Kleinberg (2002 )\nKnuth: Knuth (1997 )\nKo:Ko et al. (2004 )\nKoenemann: Koenemann and Belkin\n(1996 )\nKoller: Koller and Sahami (1997 ),\nTong and Koller (2001 )\nKonheim: Konheim (1981 )\nKorfhage: Korfhage (1997 )\nKozlov: Kozlov et al. (1979 )\nKo\u0142cz: Ko\u0142cz et al. (2000 ),Ko\u0142cz and\nYih(2007 )\nOnline edition (c)\n2009 Cambridge UP528 Author Index\nKraaij: Hiemstra and Kraaij (2005 ),\nKraaij and Spitters (2003 ),Kraaij\net al. (2002 )\nKraemer: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b )\nKraft: Meadow et al. (1999 )\nKretser: Anh et al. (2001 )\nKrippendorff: Krippendorff (2003 )\nKrishnan: McLachlan and Krishnan\n(1996 ),Sahoo et al. (2006 )\nKrovetz: Glover et al. (2002a ),\nKrovetz (1995 )\nKuhns: Maron and Kuhns (1960 )\nKukich: Kukich (1992 )\nKumar: Bharat et al. (1998 ),Broder\net al. (2000 ),Kumar et al. (1999 ),\nKumar et al. (2000 ),Steinbach\net al. (2000 )\nKupiec: Kupiec et al. (1995 )\nKuriyama: Kishida et al. (2005 )\nKurland: Kurland and Lee (2004 )\nKwok: Luk and Kwok (2002 )\nK\u00e4ki: K\u00e4ki (2005 )\nLacker: Perkins et al. (2003 )\nLafferty: Berger and Lafferty (1999 ),\nCroft and Lafferty (2003 ),\nLafferty and Zhai (2001 ),Lafferty\nand Zhai (2003 ),Zhai and\nLafferty (2001a ),Zhai and\nLafferty (2001b ),Zhai and\nLafferty (2002 )\nLai: Qin et al. (2007 )\nLaird: Dempster et al. (1977 )\nLalmas: Amer-Yahia and Lalmas\n(2006 ),Betsi et al. (2006 ),Crestani\net al. (1998 ),Fuhr et al. (2003a ),\nFuhr and Lalmas (2007 ),Fuhr\net al. (2006 ),Fuhr et al. (2005 ),\nFuhr et al. (2007 ),Fuhr et al.\n(2003b ),Kazai and Lalmas\n(2006 ),Lalmas et al. (2007 ),\nLalmas and Tombros (2007 ),\nRuthven and Lalmas (2003 )\nLance: Lance and Williams (1967 )\nLandauer: Deerwester et al. (1990 ),\nLittman et al. (1998 )Langville: Langville and Meyer\n(2006 )\nLarsen: Larsen and Aone (1999 )\nLarson: Larson (2005 )\nLavrenko: Allan et al. (1998 ),\nLavrenko and Croft (2001 )\nLavrijssen: Zavrel et al. (2000 )\nLawrence: Glover et al. (2002a ),\nGlover et al. (2002b ),Lawrence\nand Giles (1998 ),Lawrence and\nGiles (1999 ),Rusmevichientong\net al. (2001 )\nLazier: Burges et al. (2005 )\nLee: Fox and Lee (1991 ),Harman\net al. (1992 ),Kishida et al. (2005 ),\nKurland and Lee (2004 ),Lee and\nFox(1988 )\nLeek: Miller et al. (1999 )\nLehtonen: Trotman et al. (2006 )\nLeiserson: Cormen et al. (1990 )\nLempel: Lempel and Moran (2000 )\nLeone: Hersh et al. (1994 )\nLesk: Lesk (1988 ),Lesk (2004 )\nLester: Lester et al. (2005 ),Lester\net al. (2006 )\nLevenshtein: Levenshtein (1965 )\nLew: Lew (2001 )\nLewis: Eyheramendy et al. (2003 ),\nIttner et al. (1995 ),Lewis (1995 ),\nLewis (1998 ),Lewis and Jones\n(1996 ),Lewis and Ringuette\n(1994 ),Lewis et al. (1996 ),Lewis\net al. (2004 )\nLi:Cao et al. (2006 ),Gao et al. (2005 ),\nGeng et al. (2007 ),Lewis et al.\n(2004 ),Li and Yang (2003 ),Qin\net al. (2007 )\nLiddy: Liddy (2005 )\nLin: Chen and Lin (2000 ),Chen et al.\n(2005 )\nList: List et al. (2005 )\nLita: Lita et al. (2003 )\nLittman: Littman et al. (1998 )\nLiu: Cao et al. (2006 ),Geng et al.\n(2007 ),Liu et al. (2005 ),Liu and\nCroft (2004 ),Qin et al. (2007 ),\nOnline edition (c)\n2009 Cambridge UPAuthor Index 529\nRiezler et al. (2007 ),Yang and Liu\n(1999 )\nLloyd: Gaertner et al. (2002 ),Lloyd\n(1982 )\nLodhi: Lodhi et al. (2002 )\nLombard: Lombard et al. (2002 )\nLong: Long and Suel (2003 ),Zhang\net al. (2007 )\nLovins: Lovins (1968 )\nLu:Lu et al. (2007 )\nLuehrs: Kent et al. (1955 )\nLuhn: Luhn (1957 ),Luhn (1958 )\nLuk: Luk and Kwok (2002 )\nLunde: Lunde (1998 )\nLushman: B\u00fcttcher et al. (2006 )\nLuxenburger: Kammenhuber et al.\n(2006 )\nMa: Liu et al. (2005 ),Murata et al.\n(2000 ),Song et al. (2005 )\nMaarek: Carmel et al. (2001 ),Carmel\net al. (2003 ),Mass et al. (2003 )\nMacFarlane: Lu et al. (2007 ),\nMacFarlane et al. (2000 )\nMacKinlay: Hughes et al. (2006 )\nMacQueen: MacQueen (1967 )\nMadigan: Eyheramendy et al. (2003 )\nMaganti: Hatzivassiloglou et al.\n(2000 )\nMaghoul: Broder et al. (2000 )\nMahabhashyam: Singitham et al.\n(2004 )\nMajumdar: Bast and Majumdar\n(2005 ),Theobald et al. (2008 )\nMalhotra: Johnson et al. (2006 )\nMalik: Fuhr et al. (2006 ),Fuhr et al.\n(2005 ),Fuhr et al. (2003b )\nMallows: Fowlkes and Mallows\n(1983 )\nManasse: Broder et al. (1997 )\nMandelbrod: Carmel et al. (2003 ),\nMass et al. (2003 )\nManjunath: Newsam et al. (2001 )\nManning: Kamvar et al. (2002 ),Klein\nand Manning (2002 ),Manning\nand Sch\u00fctze (1999 ),Tseng et al.\n(2005 )Marais: Silverstein et al. (1999 )\nMaron: Blair and Maron (1985 ),\nMaron and Kuhns (1960 )\nMartin: Jurafsky and Martin (2008 )\nMarx: Kamps et al. (2006 )\nMasand: Creecy et al. (1992 )\nMass: Carmel et al. (2003 ),Mass et al.\n(2003 )\nMcBryan: McBryan (1994 )\nMcCallum: Ghamrawi and\nMcCallum (2005 ),McCallum and\nNigam (1998 ),McCallum et al.\n(1998 ),McCallum (1996 ),Nigam\net al. (2006 )\nMcCann: MacFarlane et al. (2000 )\nMcKeown: McKeown and Radev\n(1995 ),McKeown et al. (2002 )\nMcLachlan: McLachlan and Krishnan\n(1996 )\nMeadow: Meadow et al. (1999 )\nMeans: Harold and Means (2004 )\nMei: Tao et al. (2006 )\nMeil\u02d8 a: Meil\u02d8 a (2005 )\nMelnik: Melnik et al. (2001 )\nMeuss: Schlieder and Meuss (2002 )\nMeyer: Langville and Meyer (2006 )\nMihajlovi\u00b4 c: Mihajlovi\u00b4 c et al. (2005 )\nMihajlovic: List et al. (2005 )\nMiller: Miller et al. (1999 )\nMinsky: Minsky and Papert (1988 )\nMirrokni: Andoni et al. (2006 )\nMitchell: Huang and Mitchell (2006 ),\nMcCallum et al. (1998 ),Mitchell\n(1997 ),Nigam et al. (2006 )\nMitra: Buckley et al. (1995 ),Singhal\net al. (1996a ),Singhal et al. (1997 )\nMittal: Riezler et al. (2007 )\nMitzenmacher: Henzinger et al.\n(2000 )\nModha: Dhillon and Modha (2001 )\nMoffat: Anh et al. (2001 ),Anh and\nMoffat (2005 ),Anh and Moffat\n(2006a ),Anh and Moffat (2006b ),\nAnh and Moffat (2006c ),Lester\net al. (2005 ),Moffat and Bell\n(1995 ),Moffat and Stuiver (1996 ),\nOnline edition (c)\n2009 Cambridge UP530 Author Index\nMoffat and Zobel (1992 ),Moffat\nand Zobel (1996 ),Moffat and\nZobel (1998 ),Witten et al. (1999 ),\nZobel and Moffat (2006 ),Zobel\net al. (1995 )\nMonz: Hollink et al. (2004 )\nMooers: Mooers (1961 ),Mooers\n(1950 )\nMooney: Basu et al. (2004 )\nMoore: Brill and Moore (2000 ),Pelleg\nand Moore (1999 ),Pelleg and\nMoore (2000 ),Toutanova and\nMoore (2002 )\nMoran: Lempel and Moran (2000 )\nMoricz: Silverstein et al. (1999 )\nMoschitti: Moschitti (2003 ),Moschitti\nand Basili (2004 )\nMotwani: Hopcroft et al. (2000 ),Page\net al. (1998 )\nMoulinier: Jackson and Moulinier\n(2002 )\nMoura: de Moura et al. (2000 ),\nRibeiro-Neto et al. (1999 )\nMulhem: Chiaramella et al. (1996 )\nMurata: Murata et al. (2000 )\nMuresan: Muresan and Harper (2004 )\nMurtagh: Murtagh (1983 )\nMurty: Jain et al. (1999 )\nMyaeng: Kishida et al. (2005 )\nNajork: Henzinger et al. (2000 ),\nNajork and Heydon (2001 ),\nNajork and Heydon (2002 )\nNarin: Pinski and Narin (1976 )\nNavarro: Brisaboa et al. (2007 ),\nde Moura et al. (2000 ),Navarro\nand Baeza-Yates (1997 )\nNenkova: McKeown et al. (2002 )\nNes: Zukowski et al. (2006 )\nNeubert: Ribeiro-Neto et al. (1999 )\nNewsam: Newsam et al. (2001 )\nNg: Blei et al. (2003 ),McCallum et al.\n(1998 ),Ng and Jordan (2001 ),Ng\net al. (2001a ),Ng et al. (2001b )\nNicholson: Hughes et al. (2006 )\nNiculescu-Mizil: Caruana and\nNiculescu-Mizil (2006 )Nie: Cao et al. (2005 ),Gao et al. (2004 )\nNigam: McCallum and Nigam (1998 ),\nNigam et al. (2006 )\nNilan: Schamber et al. (1990 )\nNowak: Castro et al. (2004 )\nNtoulas: Ntoulas and Cho (2007 )\nO\u2019Brien: Berry et al. (1995 )\nO\u2019Keefe: O\u2019Keefe and Trotman (2004 )\nOard: Oard and Dorr (1996 )\nObermayer: Herbrich et al. (2000 )\nOcalan: Alting\u00f6vde et al. (2007 )\nOgilvie: Ogilvie and Callan (2005 )\nOles: Zhang and Oles (2001 )\nOlson: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b )\nOmiecinski: Jeong and Omiecinski\n(1995 )\nOostendorp: van Zwol et al. (2006 )\nOrlando: Silvestri et al. (2004 )\nOsborne: Baldridge and Osborne\n(2004 )\nOsi\u00b4 nski: Osi\u00b4 nski and Weiss (2005 )\nOzaku: Murata et al. (2000 )\nOzcan: Alting\u00f6vde et al. (2007 )\nOzkarahan: Can and Ozkarahan\n(1990 )\nOzmultu: Spink et al. (2000 )\nPadman: Sahoo et al. (2006 )\nPaepcke: Hirai et al. (2000 )\nPage: Brin and Page (1998 ),Cho et al.\n(1998 ),Page et al. (1998 )\nPaice: Paice (1990 )\nPan: Joachims et al. (2005 )\nPanconesi: Chierichetti et al. (2007 )\nPapert: Minsky and Papert (1988 )\nPapineni: Papineni (2001 )\nPapka: Allan et al. (1998 ),Lewis et al.\n(1996 )\nParam\u00e1: Brisaboa et al. (2007 )\nParikh: Pavlov et al. (2004 )\nPark: Ko et al. (2004 )\nPavlov: Pavlov et al. (2004 )\nPazzani: Domingos and Pazzani\n(1997 )\nPedersen: Cutting et al. (1993 ),\nCutting et al. (1992 ),Hearst and\nOnline edition (c)\n2009 Cambridge UPAuthor Index 531\nPedersen (1996 ),Kupiec et al.\n(1995 ),Sch\u00fctze et al. (1995 ),\nSch\u00fctze and Pedersen (1995 ),\nWeigend et al. (1999 ),Yang and\nPedersen (1997 )\nPehcevski: Lalmas et al. (2007 )\nPelleg: Pelleg and Moore (1999 ),\nPelleg and Moore (2000 )\nPennock: Glover et al. (2002a ),Glover\net al. (2002b ),Rusmevichientong\net al. (2001 )\nPerego: Silvestri et al. (2004 )\nPerkins: Perkins et al. (2003 )\nPerry: Kent et al. (1955 )\nPersin: Persin (1994 ),Persin et al.\n(1996 )\nPeterson: Peterson (1980 )\nPfeifer: Fuhr and Pfeifer (1994 )\nPharo: Trotman et al. (2006 )\nPicca: Picca et al. (2006 )\nPinski: Pinski and Narin (1976 )\nPirolli: Pirolli (2007 )\nPiwowarski: Lalmas et al. (2007 )\nPlatt: Dumais et al. (1998 ),Platt (2000 )\nPlaunt: Hearst and Plaunt (1993 )\nPollermann: Berners-Lee et al. (1992 )\nPonte: Ponte and Croft (1998 )\nPopescul: Popescul and Ungar (2000 )\nPorter: Porter (1980 )\nPrabakarmurthi: Ko\u0142cz et al. (2000 )\nPrager: Chu-Carroll et al. (2006 )\nPrakash: Richardson et al. (2006 )\nPrice: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b )\nPugh: Pugh (1990 )\nPunera: Anagnostopoulos et al.\n(2006 )\nQin: Geng et al. (2007 ),Qin et al.\n(2007 )\nQiu: Qiu and Frei (1993 )\nR Development Core Team: R\nDevelopment Core Team (2005 )\nRadev: McKeown and Radev (1995 ),\nRadev et al. (2001 )\nRadlinski: Yue et al. (2007 )\nRaftery: Fraley and Raftery (1998 )Raghavan: Broder et al. (2000 ),\nChakrabarti et al. (1998 ),\nChierichetti et al. (2007 ),Hirai\net al. (2000 ),Kumar et al. (1999 ),\nKumar et al. (2000 ),Melnik et al.\n(2001 ),Radev et al. (2001 ),\nSingitham et al. (2004 )\nRahm: Rahm and Bernstein (2001 )\nRajagopalan: Broder et al. (2000 ),\nChakrabarti et al. (1998 ),Kumar\net al. (1999 ),Kumar et al. (2000 )\nRam\u00edrez: List et al. (2005 )\nRand: Rand (1971 )\nRasmussen: Rasmussen (1992 )\nRau: Jacobs and Rau (1990 )\nReina: Bradley et al. (1998 ),Fayyad\net al. (1998 )\nRennie: Rennie et al. (2003 )\nRenshaw: Burges et al. (2005 )\nRibeiro-Neto: Badue et al. (2001 ),\nBaeza-Yates and Ribeiro-Neto\n(1999 ),Ribeiro-Neto et al. (1999 ),\nRibeiro-Neto and Barbosa (1998 )\nRice: Rice (2006 )\nRichardson: Richardson et al. (2006 )\nRiezler: Riezler et al. (2007 )\nRijke: Hollink et al. (2004 ),Kamps\net al. (2004 ),Kamps et al. (2006 ),\nSigurbj\u00f6rnsson et al. (2004 )\nRijsbergen: Crestani et al. (1998 ),\nJardine and van Rijsbergen\n(1971 ),Tombros et al. (2002 ),\nvan Rijsbergen (1979 ),\nvan Rijsbergen (1989 )\nRinguette: Lewis and Ringuette\n(1994 )\nRipley: Ripley (1996 )\nRivest: Cormen et al. (1990 )\nRoberts: Borodin et al. (2001 )\nRobertson: Lalmas et al. (2007 ),Lu\net al. (2007 ),MacFarlane et al.\n(2000 ),Robertson (2005 ),\nRobertson et al. (2004 ),Robertson\nand Jones (1976 ),Sp\u00e4rck Jones\net al. (2000 ),Taylor et al. (2006 ),\nZaragoza et al. (2003 )\nOnline edition (c)\n2009 Cambridge UP532 Author Index\nRocchio: Rocchio (1971 )\nRoget: Roget (1946 )\nRose: Lewis et al. (2004 )\nRosen-Zvi: Rosen-Zvi et al. (2004 )\nRosenfeld: McCallum et al. (1998 )\nRosenthal: Borodin et al. (2001 )\nRoss: Ross (2006 )\nRoukos: Lita et al. (2003 )\nRousseeuw: Kaufman and\nRousseeuw (1990 )\nRozono\u00e9r: Aizerman et al. (1964 )\nRubin: Dempster et al. (1977 )\nRusmevichientong:\nRusmevichientong et al. (2001 )\nRuthven: Ruthven and Lalmas (2003 )\nR\u00f6lleke: Amer-Yahia et al. (2005 ),\nFuhr and R\u00f6lleke (1997 )\nSable: McKeown et al. (2002 )\nSacherek: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b )\nSacks-Davis: Persin et al. (1996 ),\nZobel et al. (1995 )\nSahami: Dumais et al. (1998 ),Koller\nand Sahami (1997 )\nSahoo: Sahoo et al. (2006 )\nSakai: Sakai (2007 )\nSalton: Buckley et al. (1994a ),Buckley\nand Salton (1995 ),Buckley et al.\n(1994b ),Salton (1971a ),Salton\n(1971b ),Salton (1975 ),Salton\n(1989 ),Salton (1991 ),Salton et al.\n(1993 ),Salton and Buckley\n(1987 ),Salton and Buckley\n(1988 ),Salton and Buckley\n(1990 ),Singhal et al. (1995 ),\nSinghal et al. (1996b )\nSanderson: Tombros and Sanderson\n(1998 )\nSantini: Boldi et al. (2002 ),Boldi et al.\n(2005 ),Smeulders et al. (2000 )\nSaracevic: Saracevic and Kantor\n(1988 ),Saracevic and Kantor\n(1996 )\nSatyanarayana: Davidson and\nSatyanarayana (2003 )\nSaunders: Lodhi et al. (2002 )Savaresi: Savaresi and Boley (2004 )\nSchamber: Schamber et al. (1990 )\nSchapire: Allwein et al. (2000 ),Cohen\net al. (1998 ),Lewis et al. (1996 ),\nSchapire (2003 ),Schapire and\nSinger (2000 ),Schapire et al.\n(1998 )\nSchek: Grabs and Schek (2002 )\nSchenkel: Theobald et al. (2008 ),\nTheobald et al. (2005 )\nSchiffman: McKeown et al. (2002 )\nSchlieder: Schlieder and Meuss (2002 )\nScholer: Scholer et al. (2002 )\nSchwartz: Miller et al. (1999 )\nSchwarz: Schwarz (1978 )\nSch\u00f6lkopf: Chen et al. (2005 ),\nSch\u00f6lkopf and Smola (2001 )\nSch\u00fctze: Manning and Sch\u00fctze\n(1999 ),Sch\u00fctze (1998 ),Sch\u00fctze\net al. (1995 ),Sch\u00fctze and\nPedersen (1995 ),Sch\u00fctze and\nSilverstein (1997 )\nSebastiani: Sebastiani (2002 )\nSeo: Ko et al. (2004 )\nShaked: Burges et al. (2005 )\nShanmugasundaram: Amer-Yahia\net al. (2006 ),Amer-Yahia et al.\n(2005 )\nShawe-Taylor: Cristianini and\nShawe-Taylor (2000 ),Lodhi et al.\n(2002 ),Shawe-Taylor and\nCristianini (2004 )\nShih: Rennie et al. (2003 ),Sproat et al.\n(1996 )\nShkapenyuk: Shkapenyuk and Suel\n(2002 )\nSiegel: Siegel and Castellan (1988 )\nSifry: Sifry (2007 )\nSigelman: McKeown et al. (2002 )\nSigurbj\u00f6rnsson: Kamps et al. (2004 ),\nKamps et al. (2006 ),\nSigurbj\u00f6rnsson et al. (2004 ),\nTrotman and Sigurbj\u00f6rnsson\n(2004 )\nSilverstein: Sch\u00fctze and Silverstein\n(1997 ),Silverstein et al. (1999 )\nOnline edition (c)\n2009 Cambridge UPAuthor Index 533\nSilvestri: Silvestri (2007 ),Silvestri\net al. (2004 )\nSimon: Zha et al. (2001 )\nSindhwani: Sindhwani and Keerthi\n(2006 )\nSinger: Allwein et al. (2000 ),Cohen\net al. (1998 ),Cohen and Singer\n(1999 ),Crammer and Singer\n(2001 ),Schapire and Singer\n(2000 ),Schapire et al. (1998 )\nSinghal: Buckley et al. (1995 ),\nSchapire et al. (1998 ),Singhal\net al. (1996a ),Singhal et al.\n(1997 ),Singhal et al. (1995 ),\nSinghal et al. (1996b )\nSingitham: Singitham et al. (2004 )\nSivakumar: Kumar et al. (2000 )\nSlonim: Tishby and Slonim (2000 )\nSmeulders: Smeulders et al. (2000 )\nSmith: Creecy et al. (1992 )\nSmola: Sch\u00f6lkopf and Smola (2001 )\nSmyth: Rosen-Zvi et al. (2004 )\nSneath: Sneath and Sokal (1973 )\nSnedecor: Snedecor and Cochran\n(1989 )\nSnell: Grinstead and Snell (1997 ),\nKemeny and Snell (1976 )\nSnyder-Duch: Lombard et al. (2002 )\nSoffer: Carmel et al. (2001 ),Carmel\net al. (2003 ),Mass et al. (2003 )\nSokal: Sneath and Sokal (1973 )\nSomogyi: Somogyi (1990 )\nSong: Song et al. (2005 )\nSornil: Sornil (2001 )\nSozio: Chierichetti et al. (2007 )\nSpink: Spink and Cole (2005 ),Spink\net al. (2000 )\nSpitters: Kraaij and Spitters (2003 )\nSproat: Sproat and Emerson (2003 ),\nSproat et al. (1996 ),Sproat (1992 )\nSrinivasan: Coden et al. (2002 )\nStata: Broder et al. (2000 )\nStein: Stein and zu Eissen (2004 ),\nStein et al. (2003 )\nSteinbach: Steinbach et al. (2000 )\nSteyvers: Rosen-Zvi et al. (2004 )Stork: Duda et al. (2000 )\nStrang: Strang (1986 )\nStrehl: Strehl (2002 )\nStrohman: Strohman and Croft (2007 )\nStuiver: Moffat and Stuiver (1996 )\nStutz: Cheeseman and Stutz (1996 )\nSuel: Long and Suel (2003 ),\nShkapenyuk and Suel (2002 ),\nZhang et al. (2007 )\nSwanson: Swanson (1988 )\nSzl\u00e1vik: Fuhr et al. (2005 )\nTague-Sutcliffe: Tague-Sutcliffe and\nBlustein (1995 )\nTan: Tan and Cheng (2007 )\nTannier: Tannier and Geva (2005 )\nTao: Tao et al. (2006 )\nTarasov: Kozlov et al. (1979 )\nTaube: Taube and Wooster (1958 )\nTaylor: Robertson et al. (2004 ),Taylor\net al. (2006 )\nTeevan: Rennie et al. (2003 )\nTeh: Teh et al. (2006 )\nTheiler: Perkins et al. (2003 )\nTheobald: Theobald et al. (2008 ),\nTheobald et al. (2005 )\nThomas: Cover and Thomas (1991 )\nTiberi: Chierichetti et al. (2007 )\nTibshirani: Hastie et al. (2001 ),\nTibshirani et al. (2001 )\nTipping: Zaragoza et al. (2003 )\nTishby: Tishby and Slonim (2000 )\nToda: Toda and Kataoka (2005 )\nTokunaga: Iwayama and Tokunaga\n(1995 )\nTomasic: Tomasic and Garcia-Molina\n(1993 )\nTombros: Betsi et al. (2006 ),Lalmas\nand Tombros (2007 ),Tombros\nand Sanderson (1998 ),Tombros\net al. (2002 )\nTomkins: Broder et al. (2000 ),Kumar\net al. (1999 ),Kumar et al. (2000 )\nTomlinson: Tomlinson (2003 )\nTong: Tong and Koller (2001 )\nToutanova: Toutanova and Moore\n(2002 )\nOnline edition (c)\n2009 Cambridge UP534 Author Index\nTreeratpituk: Treeratpituk and Callan\n(2006 )\nTrenkle: Cavnar and Trenkle (1994 )\nTrotman: Fuhr et al. (2007 ),O\u2019Keefe\nand Trotman (2004 ),Trotman\n(2003 ),Trotman and Geva (2006 ),\nTrotman et al. (2007 ),Trotman\net al. (2006 ),Trotman and\nSigurbj\u00f6rnsson (2004 )\nTsaparas: Borodin et al. (2001 )\nTsegay: Turpin et al. (2007 )\nTseng: Tseng et al. (2005 )\nTsikrika: Betsi et al. (2006 )\nTsioutsiouliklis: Glover et al. (2002b )\nTsochantaridis: Riezler et al. (2007 ),\nTsochantaridis et al. (2005 )\nTudhope: Clarke et al. (2000 )\nTukey: Cutting et al. (1992 )\nTurpin: Hersh et al. (2000a ),Hersh\net al. (2001 ),Hersh et al. (2000b ),\nTurpin and Hersh (2001 ),Turpin\nand Hersh (2002 ),Turpin et al.\n(2007 )\nTurtle: Turtle (1994 ),Turtle and Croft\n(1989 ),Turtle and Croft (1991 ),\nTurtle and Flood (1995 )\nUchimoto: Murata et al. (2000 )\nUllman: Garcia-Molina et al. (1999 ),\nHopcroft et al. (2000 )\nUlusoy: Alting\u00f6vde et al. (2007 )\nUngar: Popescul and Ungar (2000 )\nUpfal: Chierichetti et al. (2007 ),\nKumar et al. (2000 )\nUtiyama: Murata et al. (2000 )\nVaithyanathan: Vaithyanathan and\nDom (2000 )\nVamplew: Johnson et al. (2006 )\nVapnik: Vapnik (1998 )\nVasserman: Riezler et al. (2007 )\nVassilvitskii: Arthur and Vassilvitskii\n(2006 )\nVempala: Kannan et al. (2000 )\nVenkatasubramanian: Bharat et al.\n(1998 )\nVenturini: Ferragina and Venturini\n(2007 )Veta: Kannan et al. (2000 )\nVigna: Boldi et al. (2002 ),Boldi et al.\n(2005 ),Boldi and Vigna (2004a ),\nBoldi and Vigna (2004b ),Boldi\nand Vigna (2005 )\nVilla: Tombros et al. (2002 )\nVittaut: Vittaut and Gallinari (2006 )\nVi\u00f1a: Cacheda et al. (2003 )\nVoorhees: Buckley and Voorhees\n(2000 ),Voorhees (1985a ),\nVoorhees (1985b ),Voorhees\n(2000 ),Voorhees and Harman\n(2005 )\nVries: List et al. (2005 )\nWagner: Wagner and Fischer (1974 )\nWalker: Sp\u00e4rck Jones et al. (2000 )\nWalther: Tibshirani et al. (2001 )\nWaltz: Creecy et al. (1992 )\nWan: Liu et al. (2005 )\nWang: Qin et al. (2007 ),Tao et al.\n(2006 )\nWard Jr.: Ward Jr. (1963 )\nWatkins: Lodhi et al. (2002 ),Weston\nand Watkins (1999 )\nWei: Wei and Croft (2006 )\nWeigend: Weigend et al. (1999 )\nWeikum: Amer-Yahia et al. (2005 ),\nChaudhuri et al. (2006 ),\nKammenhuber et al. (2006 ),\nTheobald et al. (2008 ),Theobald\net al. (2005 )\nWeinstein: Hayes and Weinstein\n(1990 )\nWeiss: Apt\u00e9 et al. (1994 ),Ng et al.\n(2001a ),Osi\u00b4 nski and Weiss (2005 )\nWen: Song et al. (2005 )\nWesterveld: Kraaij et al. (2002 )\nWeston: Weston and Watkins (1999 )\nWidom: Garcia-Molina et al. (1999 ),\nJeh and Widom (2003 )\nWiener: Broder et al. (2000 ),Weigend\net al. (1999 )\nWiering: van Zwol et al. (2006 )\nWilkinson: Zobel et al. (1995 )\nWillett: El-Hamdouchi and Willett\n(1986 )\nOnline edition (c)\n2009 Cambridge UPAuthor Index 535\nWilliams: Bahle et al. (2002 ),Garcia\net al. (2004 ),Heinz et al. (2002 ),\nLance and Williams (1967 ),\nLester et al. (2006 ),Scholer et al.\n(2002 ),Turpin et al. (2007 ),\nWilliams and Zobel (2005 ),\nWilliams et al. (2004 )\nWinograd: Page et al. (1998 )\nWitten: Witten and Bell (1990 ),Witten\nand Frank (2005 ),Witten et al.\n(1999 )\nWi\u00dfbrock: Stein et al. (2003 )\nWong: Hartigan and Wong (1979 ),\nWong et al. (1988 )\nWoodley: Woodley and Geva (2006 )\nWooster: Taube and Wooster (1958 )\nWorring: Smeulders et al. (2000 )\nWu: Gao et al. (2005 ),Gao et al. (2004 )\nXu:Cao et al. (2006 ),Ji and Xu (2006 ),\nXu and Croft (1996 ),Xu and\nCroft (1999 )\nYang: Ault and Yang (2002 ),Lewis\net al. (2004 ),Li and Yang (2003 ),\nLiu et al. (2005 ),Melnik et al.\n(2001 ),Yang and Callan (2006 ),\nYang (1994 ),Yang (1999 ),Yang\n(2001 ),Yang and Kisiel (2003 ),\nYang and Liu (1999 ),Yang and\nPedersen (1997 )\nYao: Wong et al. (1988 )\nYiannis: Scholer et al. (2002 )\nYih: Ko\u0142cz and Yih (2007 )\nYilmaz: Aslam and Yilmaz (2005 )\nYoung: Berry and Young (1995 ),\nEckart and Young (1936 )\nYu:Hand and Yu (2001 )\nYue: Yue et al. (2007 )\nZamir: Zamir and Etzioni (1999 )\nZaragoza: Robertson et al. (2004 ),\nTaylor et al. (2006 ),Zaragoza\net al. (2003 )\nZavrel: Zavrel et al. (2000 )\nZeng: Liu et al. (2005 )\nZha: Zha et al. (2001 )\nZhai: Lafferty and Zhai (2001 ),\nLafferty and Zhai (2003 ),Taoet al. (2006 ),Zhai and Lafferty\n(2001a ),Zhai and Lafferty\n(2001b ),Zhai and Lafferty (2002 )\nZhang: Qin et al. (2007 ),Radev et al.\n(2001 ),Zhang et al. (2007 ),Zhang\nand Oles (2001 )\nZhao: Zhao and Karypis (2002 )\nZheng: Ng et al. (2001b )\nZien: Chapelle et al. (2006 )\nZipf: Zipf (1949 )\nZiviani: Badue et al. (2001 ),de Moura\net al. (2000 ),Ribeiro-Neto et al.\n(1999 )\nZobel: Bahle et al. (2002 ),Heinz and\nZobel (2003 ),Heinz et al. (2002 ),\nKaszkiel and Zobel (1997 ),Lester\net al. (2005 ),Lester et al. (2006 ),\nMoffat and Zobel (1992 ),Moffat\nand Zobel (1996 ),Moffat and\nZobel (1998 ),Persin et al. (1996 ),\nScholer et al. (2002 ),Williams\nand Zobel (2005 ),Williams et al.\n(2004 ),Zobel (1998 ),Zobel and\nDart (1995 ),Zobel and Dart\n(1996 ),Zobel and Moffat (2006 ),\nZobel et al. (1995 )\nZukowski: Zukowski et al. (2006 )\nZweig: Broder et al. (1997 )\nZwol: van Zwol et al. (2006 )\ndel Bimbo: del Bimbo (1999 )\nOnline edition (c)\n2009 Cambridge UP\nOnline edition (c)\n2009 Cambridge UPIndex\nL2distance, 131\n\u03c72feature selection, 275\n\u03b4codes, 104\n\u03b3encoding, 99\nknearest neighbor classi\ufb01cation, 297\nk-gram index, 54,60\n1/0 loss, 221\n11-point interpolated average\nprecision, 159\n20 Newsgroups, 154\nA/B test, 170\naccess control lists, 81\naccumulator, 113,125\naccuracy, 155\nactive learning, 336\nad hoc retrieval, 5,253\nadd-one smoothing, 260\nadjacency table, 455\nadversarial information retrieval, 429\nAkaike Information Criterion, 367\nalgorithmic search, 430\nanchor text, 425\nany-of classi\ufb01cation, 257,306\nauthority score, 474\nauxiliary index, 78\naverage-link clustering, 389\nB-tree, 50\nbag of words, 117,267\nbag-of-words, 269\nbalanced F measure, 156\nBayes error rate, 300\nBayes Optimal Decision Rule, 222\nBayes risk, 222Bayes\u2019 Rule, 220\nBayesian networks, 234\nBayesian prior, 226\nBernoulli model, 263\nbest-merge persistence, 388\nbias, 311\nbias-variance tradeoff, 241,312,321\nbiclustering, 374\nbigram language model, 240\nBinary Independence Model, 222\nbinary tree, 50,377\nbiword index, 39,43\nblind relevance feedback, seepseudo\nrelevance feedback\nblocked sort-based indexing\nalgorithm, 71\nblocked storage, 92\nblog, 195\nBM25 weights, 232\nboosting, 286\nbottom-up clustering, seehierarchical\nagglomerative clustering\nbowtie, 426\nbreak-even, 334\nbreak-even point, 161\nBSBI, 71\nBuckshot algorithm, 399\nbuffer, 69\ncaching, 9,68,146,447,450\ncapture-recapture method, 435\ncardinality\nin clustering, 355\nCAS topics, 211\ncase-folding, 30\nOnline edition (c)\n2009 Cambridge UP538 Index\ncategory, 256\ncentroid, 292,360\nin relevance feedback, 181\ncentroid-based classi\ufb01cation, 314\nchain rule, 220\nchaining\nin clustering, 385\nchampion lists, 143\nclass boundary, 303\nclassi\ufb01cation, 253,344\nclassi\ufb01cation function, 256\nclassi\ufb01er, 183\nCLEF, 154\nclick spam, 431\nclickstream mining, 170,188\nclickthrough log analysis, 170\nclique, 384\ncluster, 74,349\nin relevance feedback, 184\ncluster hypothesis, 350\ncluster-based classi\ufb01cation, 314\ncluster-internal labeling, 396\nCO topics, 211\nco-clustering, 374\ncollection, 4\ncollection frequency, 27\ncombination similarity, 378,384,393\ncomplete-link clustering, 382\ncomplete-linkage clustering, see\ncomplete-link clustering\ncomponent coverage, 212\ncompound-splitter, 25\ncompounds, 25\nconcept drift, 269,283,286,336\nconditional independence\nassumption, 224,266\nconfusion matrix, 307\nconnected component, 384\nconnectivity queries, 455\nconnectivity server, 455\ncontent management system, 84\ncontext\nXML, 199\ncontext resemblance, 208\ncontiguity hypothesis, 289\ncontinuation bit, 96corpus, 4\ncosine similarity, 121,372\nCPC, 430\nCPM, 430\nCran\ufb01eld, 153\ncross-entropy, 251\ncross-language information retrieval,\n154,417\ncumulative gain, 162\ndata-centric XML, 196,214\ndatabase\nrelational, 1,195,214\ndecision boundary, 292,303\ndecision hyperplane, 290,302\ndecision trees, 282,286\ndendrogram, 378\ndevelopment set, 283\ndevelopment test collection, 153\nDice coef\ufb01cient, 163\ndictionary, 6,7\ndifferential cluster labeling, 396\ndigital libraries, 195\ndistortion, 366\ndistributed index, 74,458\ndistributed indexing, 74\ndistributed information retrieval, see\ndistributed crawling, 458\ndivisive clustering, 395\nDNS resolution, 450\nDNS server, 450\ndocID, 7\ndocument, 4,20\ndocument collection, seecollection\ndocument frequency, 7,118\ndocument likelihood model, 250\ndocument partitioning, 454\ndocument space, 256\ndocument vector, 119,120\ndocument-at-a-time, 126,140\ndocument-partitioned index, 75\ndot product, 121\nEast Asian languages, 45\nedit distance, 58\neffectiveness, 5,280\neigen decomposition, 406\nOnline edition (c)\n2009 Cambridge UPIndex 539\neigenvalue, 404\nEM algorithm, 369\nemail sorting, 254\nenterprise resource planning, 84\nenterprise search, 67\nentropy, 99,106,358\nequivalence classes, 28\nErgodic Markov Chain, 467\nEuclidean distance, 131,372\nEuclidean length, 121\nevidence accumulation, 146\nexclusive clustering, 355\nexhaustive clustering, 355\nexpectation step, 370\nExpectation-Maximization algorithm,\n336,369\nexpected edge density, 373\nextended query, 205\nExtensible Markup Language, 196\nexternal criterion of quality, 356\nexternal sorting algorithm, 70\nF measure, 156,173\nas an evaluation measure in\nclustering, 359\nfalse negative, 359\nfalse positive, 359\nfeature engineering, 338\nfeature selection, 271\n\ufb01eld, 110\n\ufb01ltering, 253,314\n\ufb01rst story detection, 395,399\n\ufb02at clustering, 350\nfocused retrieval, 217\nfree text, 109,148\nfree text query, seequery, free text,\n124,145,196\nfrequency-based feature selection, 277\nFrobenius norm, 410\nfront coding, 93\nfunctional margin, 322\nGAAC, 388\ngenerative model, 237,309,311\ngeometric margin, 323\ngold standard, 152\nGolomb codes, 106GOV2, 154\ngreedy feature selection, 279\ngrep, 3\nground truth, 152\ngroup-average agglomerative\nclustering, 388\ngroup-average clustering, 389\nHAC, 378\nhard assignment, 350\nhard clustering, 350,355\nharmonic number, 101\nHeaps\u2019 law, 88\nheld-out, 298\nheld-out data, 283\nhierarchic clustering, 377\nhierarchical agglomerative clustering,\n378\nhierarchical classi\ufb01cation, 337,347\nhierarchical clustering, 350,377\nHierarchical Dirichlet Processes, 418\nhierarchy\nin clustering, 377\nhighlighting, 203\nHITS, 477\nHTML, 421\nhttp, 421\nhub score, 474\nhyphens, 24\ni.i.d., 283,seeindependent and\nidentically distributed\nIde dec-hi, 183\nidf,83,204,227,232\niid,seeindependent and identically\ndistributed\nimpact, 81\nimplicit relevance feedback, 187\nin-links, 425,461\nincidence matrix, 3,408\nindependence, 275\nindependent and identically\ndistributed, 283\nin clustering, 367\nindex, 3,seepermuterm index, see also\nparametric index, zone index\nindex construction, 67\nOnline edition (c)\n2009 Cambridge UP540 Index\nindexer, 67\nindexing, 67\nsort-based, 7\nindexing granularity, 21\nindexing unit, 201\nINEX, 210\ninformation gain, 285\ninformation need, 5,152\ninformation retrieval, 1\ninformational queries, 432\ninner product, 121\ninstance-based learning, 300\ninter-similarity, 381\ninternal criterion of quality, 356\ninterpolated precision, 158\nintersection\npostings list, 10\ninverse document frequency, 118,125\ninversion, 71,378,391\ninverted \ufb01le, seeinverted index\ninverted index, 6\ninverted list, seepostings list\ninverter, 76\nIP address, 449\nJaccard coef\ufb01cient, 61,438\nK-medoids, 365\nkappa statistic, 165,174,373\nkernel, 332\nkernel function, 332\nkernel trick, 331\nkey-value pairs, 75\nkeyword-in-context, 171\nkNN classi\ufb01cation, 297\nKruskal\u2019s algorithm, 399\nKullback-Leibler divergence, 251,\n317,372\nKWIC, seekeyword-in-context\nlabel, 256\nlabeling, 255\nlanguage, 237\nlanguage identi\ufb01cation, 24,46\nlanguage model, 238\nLaplace smoothing, 260\nLatent Dirichlet Allocation, 418latent semantic indexing, 192,413\nLDA, 418\nlearning algorithm, 256\nlearning error, 310\nlearning method, 256\nlemma, 32\nlemmatization, 32\nlemmatizer, 33\nlength-normalization, 121\nLevenshtein distance, 58\nlexicalized subtree, 206\nlexicon, 6\nlikelihood, 221\nlikelihood ratio, 239\nlinear classi\ufb01er, 301,343\nlinear problem, 303\nlinear separability, 304\nlink farms, 481\nlink spam, 429,461\nLM, 243\nlogarithmic merging, 79\nlossless, 87\nlossy compression, 87\nlow-rank approximation, 410\nLSA, 413\nLSI as soft clustering, 417\nmachine translation, 240,243,251\nmachine-learned relevance, 113,342\nmacroaveraging, 280\nMAP , 159,227,258\nmap phase, 75\nMapReduce, 75\nmargin, 320\nmarginal relevance, 167\nmarginal statistic, 165\nmaster node, 75\nmatrix decomposition, 406\nmaximization step, 370\nmaximum a posteriori, 227,265\nmaximum a posteriori class, 258\nmaximum likelihood estimate, 226,\n259\nmaximum likelihood estimation, 244\nMean Average Precision, seeMAP\nmedoid, 365\nmemory capacity, 312\nOnline edition (c)\n2009 Cambridge UPIndex 541\nmemory-based learning, 300\nMercator, 445\nMercer kernel, 332\nmerge\npostings, 10\nmerge algorithm, 10\nmetadata, 24,110,171,197,373,428\nmicroaveraging, 280\nminimum spanning tree, 399,401\nminimum variance clustering, 399\nMLE, seemaximum likelihood\nestimate\nModApte split, 279,286\nmodel complexity, 312,366\nmodel-based clustering, 368\nmonotonicity, 378\nmulticlass classi\ufb01cation, 306\nmulticlass SVM, 347\nmultilabel classi\ufb01cation, 306\nmultimodal class, 296\nmultinomial classi\ufb01cation, 306\nmultinomial distribution, 241\nmultinomial model, 263,270\nmultinomial Naive Bayes, 258\nmultinomial NB, seemultinomial\nNaive Bayes\nmultivalue classi\ufb01cation, 306\nmultivariate Bernoulli model, 263\nmutual information, 272,358\nNaive Bayes assumption, 224\nnamed entity tagging, 195,339\nNational Institute of Standards and\nTechnology, 153\nnatural language processing, xxxiv ,\n33,171,217,249,372\nnavigational queries, 432\nNDCG, 163\nnested elements, 203\nNEXI, 200\nnext word index, 44\nnibble, 98\nNLP , seenatural language processing\nNMI, 358\nnoise document, 303\nnoise feature, 271\nnonlinear classi\ufb01er, 305nonlinear problem, 305\nnormal vector, 293\nnormalized discounted cumulative\ngain, 163\nnormalized mutual information, 358\nnovelty detection, 395\nNTCIR, 154,174\nobjective function, 354,360\nodds, 221\nodds ratio, 225\nOkapi weighting, 232\none-of classi\ufb01cation, 257,284,306\noptimal classi\ufb01er, 270,310\noptimal clustering, 393\noptimal learning method, 310\nordinal regression, 344\nout-links, 425\noutlier, 363\nover\ufb01tting, 271,312\nPageRank, 464\npaid inclusion, 428\nparameter tuning, 153,314,315,348\nparameter tying, 340\nparameter-free compression, 100\nparameterized compression, 106\nparametric index, 110\nparametric search, 197\nparser, 75\npartition rule, 220\npartitional clustering, 355\npassage retrieval, 217\npatent databases, 195\nperceptron algorithm, 286,315\nperformance, 280\npermuterm index, 53\npersonalized PageRank, 471\nphrase index, 40\nphrase queries, 39,47\nphrase search, 15\npivoted document length\nnormalization, 129\npointwise mutual information, 286\npolychotomous, 306\npolytomous classi\ufb01cation, 306\npolytope, 298\nOnline edition (c)\n2009 Cambridge UP542 Index\npooling, 164,174\npornography \ufb01ltering, 338\nPorter stemmer, 33\npositional independence, 267\npositional index, 41\nposterior probability, 220\nposting, 6,7,71,86\npostings list, 6\npower law, 89,426\nprecision, 5,155\nprecision at k,161\nprecision-recall curve, 158\npre\ufb01x-free code, 100\nprincipal direction divisive\npartitioning, 400\nprincipal left eigenvector, 465\nprior probability, 220\nProbability Ranking Principle, 221\nprobability vector, 466\nprototype, 290\nproximity operator, 14\nproximity weighting, 145\npseudo relevance feedback, 187\npseudocounts, 226\npull model, 314\npurity, 356\npush model, 314\nQuadratic Programming, 324\nquery, 5\nfree text, 14,16,117\nsimple conjunctive, 10\nquery expansion, 189\nquery likelihood model, 242\nquery optimization, 11\nquery-by-example, 201,249\nR-precision, 161,174\nRand index, 359\nadjusted, 373\nrandom variable, 220\nrandom variable C,268\nrandom variable U,266\nrandom variable X,266\nrank, 403\nRanked Boolean retrieval, 112\nranked retrieval, 81,107model, 14\nranking SVM, 345\nrecall, 5,155\nreduce phase, 75\nreduced SVD, 409,412\nregression, 344\nregular expressions, 3,18\nregularization, 328\nrelational database, 195,214\nrelative frequency, 226\nrelevance, 5,152\nrelevance feedback, 178\nresidual sum of squares, 360\nresults snippets, 146\nretrieval model\nBoolean, 4\nRetrieval Status Value, 225\nretrieval systems, 81\nReuters-21578, 154\nReuters-RCV1, 69,154\nRF,178\nRobots Exclusion Protocol, 447\nROC curve, 162\nRocchio algorithm, 181\nRocchio classi\ufb01cation, 292\nrouting, 253,314\nRSS, 360\nrule of 30, 86\nrules in text classi\ufb01cation, 255\nScatter-Gather, 351\nschema, 199\nschema diversity, 204\nschema heterogeneity, 204\nsearch advertising, 430\nsearch engine marketing, 431\nSearch Engine Optimizers, 429\nsearch result clustering, 351\nsearch results, 351\nsecurity, 81\nseed, 361\nseek time, 68\nsegment \ufb01le, 75\nsemi-supervised learning, 336\nsemistructured query, 197\nsemistructured retrieval, 2,197\nsensitivity, 162\nOnline edition (c)\n2009 Cambridge UPIndex 543\nsentiment detection, 254\nsequence model, 267\nshingling, 438\nsingle-label classi\ufb01cation, 306\nsingle-link clustering, 382\nsingle-linkage clustering, see\nsingle-link clustering\nsingle-pass in-memory indexing, 73\nsingleton, 378\nsingleton cluster, 363\nsingular value decomposition, 407\nskip list, 36,46\nslack variables, 327\nSMART, 182\nsmoothing, 127,226\nadd \u03b1,226\nadd1\n2,232\nadd1\n2,226\u2013229,262\nBayesian prior, 226,228,245\nlinear interpolation, 245\nsnippet, 170\nsoft assignment, 350\nsoft clustering, 350,355,377\nsorting\nin index construction, 7\nsoundex, 63\nspam, 338,427\nemail, 254\nweb, 254\nsparseness, 241,244,260\nspeci\ufb01city, 162\nspectral clustering, 400\nspeech recognition, 240\nspelling correction, 147,240,242\nspider, 443\nspider traps, 433\nSPIMI, 73\nsplits, 75\nsponsored search, 430\nstanding query, 253\nstatic quality scores, 138\nstatic web pages, 424\nstatistical signi\ufb01cance, 276\nstatistical text classi\ufb01cation, 255\nsteady-state, 467,468\nstemming, 32,46stochastic matrix, 465\nstop words, 117\nstop list, 27\nstop words, 117\nstop words, 23,27,45,127\nstructural SVM, 345\nstructural SVMs, 330\nstructural term, 207\nstructured document retrieval\nprinciple, 201\nstructured query, 197\nstructured retrieval, 195,197\nsummarization, 400\nsummary\ndynamic, 171\nstatic, 171\nsupervised learning, 256\nsupport vector, 320\nsupport vector machine, 319,346\nmulticlass, 330\nSVD, 373,400,408\nSVM, seesupport vector machine\nsymmetric diagonal decomposition,\n407,408\nsynonymy, 177\nteleport, 464\nterm, 3,19,22\nterm frequency, 16,117\nterm normalization, 28\nterm partitioning, 454\nterm-at-a-time, 125,140\nterm-document matrix, 123\nterm-partitioned index, 74\ntermID, 69\ntest data, 256\ntest set, 256,283\ntext categorization, 253\ntext classi\ufb01cation, 253\ntext summarization, 171\ntext-centric XML, 214\ntf,seeterm frequency\ntf-idf, 119\ntiered indexes, 143\ntoken, 19,22\ntoken normalization, 28\ntop docs, 149\nOnline edition (c)\n2009 Cambridge UP544 Index\ntop-down clustering, 395\ntopic, 153,253\nin XML retrieval, 211\ntopic classi\ufb01cation, 253\ntopic spotting, 253\ntopic-speci\ufb01c PageRank, 471\ntopical relevance, 212\ntraining set, 256,283\ntransactional query, 433\ntransductive SVMs, 336\ntranslation model, 251\nTREC, 153,314\ntrec_eval, 174\ntruecasing, 30,46\ntruncated SVD, 409,412,415\ntwo-class classi\ufb01er, 279\ntype, 22\nunary code, 99\nunigram language model, 240\nunion-\ufb01nd algorithm, 395,440\nuniversal code, 100\nunsupervised learning, 349\nURL, 422\nURL normalization, 447\nutility measure, 286\nvariable byte encoding, 96\nvariance, 311\nvector space model, 120\nvertical search engine, 254\nvocabulary, 6\nVoronoi tessellation, 297\nWard\u2019s method, 399\nweb crawler, 443\nweight vector, 322\nweighted zone scoring, 110\nWikipedia, 211\nwildcard query, 3,49,52\nwithin-point scatter, 375\nword segmentation, 25\nXML, 20,196\nXML attribute, 197\nXML DOM, 197\nXML DTD, 199XML element, 197\nXML fragment, 216\nXML Schema, 199\nXML tag, 197\nXPath, 199\nZipf\u2019s law, 89\nzone, 110,337,339,340\nzone index, 110\nzone search, 197\n", "Natural Language Processing with Python": "\n\nNatural Language Processing with Python\n\nNatural Language Processing\nwith Python\nSteven Bird, Ewan Klein, and Edward Loper\nBeijing \u2022Cambridge \u2022Farnham \u2022K\u00f6ln \u2022Sebastopol \u2022Taipei \u2022Tokyo\nNatural Language Processing with Python\nby Steven Bird, Ewan Klein, and Edward Loper\nCopyright \u00a9 2009 Steven Bird, Ewan Klein, and Edward Loper. All rights reserved.\nPrinted in the United States of America.\nPublished by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO\u2019Reilly books \nmay be purchased for educational, business, or sales promotional use. Online editions\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\nEditor: Julie Steele\nProduction Editor: Loranah Dimant\nCopyeditor: Genevieve d\u2019Entremont\nProofreader: Loranah DimantIndexer: Ellen Troutman Zaig\nCover Designer: Karen Montgomery\nInterior Designer: David Futato\nIllustrator: Robert Romano\nPrinting History:\nJune 2009: First Edition. \nNutshell \nHandbook, the Nutshell Handbook logo, and the O\u2019Reilly logo are registered trademarks of\nO\u2019Reilly Media, \nInc. Natural Language Processing with Python , the image of a right whale, and related\ntrade dress are trademarks of O\u2019Reilly Media, Inc.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and O\u2019Reilly Media, Inc. was aware of a\ntrademark claim, the designations have been printed in caps or initial caps.\nWhile every precaution has been taken in the preparation of this book, the publisher and authors assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\ntained herein.\nISBN: 978-0-596-51649-9\n[M]\n1244726609\nTable of Contents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\n1.\nLanguage Processing and Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n1.1 Computing with Language: Texts and Words 1\n1.2 A Closer Look at Python: Texts as Lists of Words 10\n1.3 Computing with Language: Simple Statistics 16\n1.4 Back to Python: Making Decisions and Taking Control 22\n1.5 Automatic Natural Language Understanding 27\n1.6 Summary 33\n1.7 Further Reading 34\n1.8 Exercises 35\n2. Accessing Text Corpora and Lexical Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.1 Accessing Text Corpora 39\n2.2 Conditional Frequency Distributions 52\n2.3 More Python: Reusing Code 56\n2.4 Lexical Resources 59\n2.5 WordNet 67\n2.6 Summary 73\n2.7 Further Reading 73\n2.8 Exercises 74\n3. Processing Raw Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n3.1 Accessing Text from the Web and from Disk 80\n3.2 Strings: Text Processing at the Lowest Level 87\n3.3 Text Processing with Unicode 93\n3.4 Regular Expressions for Detecting Word Patterns 97\n3.5 Useful Applications of Regular Expressions 102\n3.6 Normalizing Text 107\n3.7 Regular Expressions for Tokenizing Text 109\n3.8 Segmentation 112\n3.9 Formatting: From Lists to Strings 116\nv\n3.10 Summary 121\n3.11 Further Reading 122\n3.12 Exercises 123\n4. Writing Structured Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\n4.1 Back to the Basics 130\n4.2 Sequences 133\n4.3 Questions of Style 138\n4.4 Functions: The Foundation of Structured Programming 142\n4.5 Doing More with Functions 149\n4.6 Program Development 154\n4.7 Algorithm Design 160\n4.8 A Sample of Python Libraries 167\n4.9 Summary 172\n4.10 Further Reading 173\n4.11 Exercises 173\n5. Categorizing and Tagging Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\n5.1 Using a Tagger 179\n5.2 Tagged Corpora 181\n5.3 Mapping Words to Properties Using Python Dictionaries 189\n5.4 Automatic Tagging 198\n5.5 N-Gram Tagging 202\n5.6 Transformation-Based Tagging 208\n5.7 How to Determine the Category of a Word 210\n5.8 Summary 213\n5.9 Further Reading 214\n5.10 Exercises 215\n6. Learning to Classify Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\n6.1 Supervised Classification 221\n6.2 Further Examples of Supervised Classification 233\n6.3 Evaluation 237\n6.4 Decision Trees 242\n6.5 Naive Bayes Classifiers 245\n6.6 Maximum Entropy Classifiers 250\n6.7 Modeling Linguistic Patterns 254\n6.8 Summary 256\n6.9 Further Reading 256\n6.10 Exercises 257\n7. Extracting Information from Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  261\n7.1 Information Extraction 261\nvi | Table of Contents\n7.2 Chunking 264\n7.3 Developing and Evaluating Chunkers 270\n7.4 Recursion in Linguistic Structure 277\n7.5 Named Entity Recognition 281\n7.6 Relation Extraction 284\n7.7 Summary 285\n7.8 Further Reading 286\n7.9 Exercises 286\n8. Analyzing Sentence Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  291\n8.1 Some Grammatical Dilemmas 292\n8.2 What\u2019s the Use of Syntax? 295\n8.3 Context-Free Grammar 298\n8.4 Parsing with Context-Free Grammar 302\n8.5 Dependencies and Dependency Grammar 310\n8.6 Grammar Development 315\n8.7 Summary 321\n8.8 Further Reading 322\n8.9 Exercises 322\n9. Building Feature-Based Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  327\n9.1 Grammatical Features 327\n9.2 Processing Feature Structures 337\n9.3 Extending a Feature-Based Grammar 344\n9.4 Summary 356\n9.5 Further Reading 357\n9.6 Exercises 358\n10. Analyzing the Meaning of Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  361\n10.1 Natural Language Understanding 361\n10.2 Propositional Logic 368\n10.3 First-Order Logic 372\n10.4 The Semantics of English Sentences 385\n10.5 Discourse Semantics 397\n10.6 Summary 402\n10.7 Further Reading 403\n10.8 Exercises 404\n11. Managing Linguistic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\n11.1 Corpus Structure: A Case Study 407\n11.2 The Life Cycle of a Corpus 412\n11.3 Acquiring Data 416\n11.4 Working with XML 425\nTable of Contents | vii\n11.5 Working with Toolbox Data 431\n11.6 Describing Language Resources Using OLAC Metadata 435\n11.7 Summary 437\n11.8 Further Reading 437\n11.9 Exercises 438\nAfterword: The Language Challenge . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  441\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  449\nNLTK Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  459\nGeneral Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  463\nviii | Table of Contents\nPreface\nThis is a book about Natural Language Processing. By \u201cnatural language\u201d we mean a\nlanguage that \nis used for everyday communication by humans; languages such as Eng-\nlish, Hindi, or Portuguese. In contrast to artificial languages such as programming lan-\nguages and mathematical notations, natural languages have evolved as they pass from\ngeneration to generation, and are hard to pin down with explicit rules. We will take\nNatural Language Processing\u2014or NLP for short\u2014in a wide sense to cover any kind of\ncomputer manipulation of natural language. At one extreme, it could be as simple as\ncounting word frequencies to compare different writing styles. At the other extreme,\nNLP involves \u201cunderstanding\u201d complete human utterances, at least to the extent of\nbeing able to give useful responses to them.\nTechnologies based on NLP are becoming increasingly widespread. For example,\nphones and handheld computers support predictive text and handwriting recognition;\nweb search engines give access to information locked up in unstructured text; machine\ntranslation allows us to retrieve texts written in Chinese and read them in Spanish. By\nproviding more natural human-machine interfaces, and more sophisticated access to\nstored information, language processing has come to play a central role in the multi-\nlingual information society.\nThis book provides a highly accessible introduction to the field of NLP. It can be used\nfor individual study or as the textbook for a course on natural language processing or\ncomputational linguistics, or as a supplement to courses in artificial intelligence, text\nmining, or corpus linguistics. The book is intensely practical, containing hundreds of\nfully worked examples and graded exercises.\nThe book is based on the Python programming language together with an open source\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\nencourage you to download Python and NLTK, and try out the examples and exercises\nalong the way.\nix\nAudience\nNLP is important for scientific, economic, social, and cultural reasons. NLP is experi-\nencing rapid \ngrowth as its theories and methods are deployed in a variety of new lan-\nguage technologies. For this reason it is important for a wide range of people to have a\nworking knowledge of NLP. Within industry, this includes people in human-computer\ninteraction, business information analysis, and web software development. Within\nacademia, it includes people in areas from humanities computing and corpus linguistics\nthrough to computer science and artificial intelligence. (To many people in academia,\nNLP is known by the name of \u201cComputational Linguistics.\u201d)\nThis book is intended for a diverse range of people who want to learn how to write\nprograms that analyze written language, regardless of previous programming\nexperience:\nNew to programming?\nThe early chapters of the book are suitable for readers with no prior knowledge of\nprogramming, so long as you aren\u2019t afraid to tackle new concepts and develop new\ncomputing skills. The book is full of examples that you can copy and try for your-\nself, together with hundreds of graded exercises. If you need a more general intro-\nduction to Python, see the list of Python resources at http://docs.python.org/.\nNew to Python?\nExperienced programmers can quickly learn enough Python using this book to get\nimmersed in natural language processing. All relevant Python features are carefully\nexplained and exemplified, and you will quickly come to appreciate Python\u2019s suit-\nability for this application area. The language index will help you locate relevant\ndiscussions in the book.\nAlready dreaming in Python?\nSkim the Python examples and dig into the interesting language analysis material\nthat starts in Chapter 1 . You\u2019ll soon be applying your skills to this fascinating\ndomain.\nEmphasis\nThis book is a practical introduction to NLP. You will learn by example, write real\nprograms, and grasp the value of being able to test an idea through implementation. If\nyou haven\u2019t learned already, this book will teach you programming. Unlike other\nprogramming books, we provide extensive illustrations and exercises from NLP. The\napproach we have taken is also principled, in that we cover the theoretical underpin-\nnings and don\u2019t shy away from careful linguistic and computational analysis. We have\ntried to be pragmatic in striking a balance between theory and application, identifying\nthe connections and the tensions. Finally, we recognize that you won\u2019t get through this\nunless it is also pleasurable, so we have tried to include many applications and ex-\namples that are interesting and entertaining, and sometimes whimsical.\nx | Preface\nNote that this book is not a reference work. Its coverage of Python and NLP is selective,\nand presented in a tutorial style. For reference material, please consult the substantial\nquantity of \nsearchable resources available at http://python.org/ and http://www.nltk\n.org/.\nThis book is not an advanced computer science text. The content ranges from intro-\nductory to intermediate, and is directed at readers who want to learn how to analyze\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\nrithms implemented in NLTK, you can examine the Python code linked from http://\nwww.nltk.org/, and consult the other materials cited in this book.\nWhat You Will Learn\nBy digging into the material presented here, you will learn:\n\u2022 How simple programs can help you manipulate and analyze language data, and\nhow to write these programs\n\u2022 How key concepts from NLP and linguistics are used to describe and analyze\nlanguage\n\u2022 How data structures and algorithms are used in NLP\n\u2022 How language data is stored in standard formats, and how data can be used to\nevaluate the performance of NLP techniques\nDepending on your background, and your motivation for being interested in NLP, you\nwill gain different kinds of skills and knowledge from this book, as set out in Table P-1.\nTable P-1. Skills and knowledge to be gained from reading this book, depending on readers\u2019 goals and\nbackground\nGoals Background in arts and humanities Background in science and engineering\nLanguage\nanalysisManipulating large corpora, exploring linguistic\nmodels, and testing empirical claims.Using techniques in data modeling, data mining, and\nknowledge discovery to analyze natural language.\nLanguage\ntechnologyBuilding robust systems to perform linguistic tasks\nwith technological applications.Using linguistic algorithms and data structures in robust\nlanguage processing software.\nOrganization\nThe early \nchapters are organized in order of conceptual difficulty, starting with a prac-\ntical introduction to language processing that shows how to explore interesting bodies\nof text using tiny Python programs (Chapters 1\u20133). This is followed by a chapter on\nstructured programming ( Chapter 4) that consolidates the programming topics scat-\ntered across the preceding chapters. After this, the pace picks up, and we move on to\na series of chapters covering fundamental topics in language processing: tagging, clas-\nsification, and information extraction (Chapters 5\u20137). The next three chapters look at\nPreface | xi\nways to parse a sentence, recognize its syntactic structure, and construct representa-\ntions of \nmeaning (Chapters 8\u201310). The final chapter is devoted to linguistic data and\nhow it can be managed effectively ( Chapter 11 ). The book concludes with an After-\nword, briefly discussing the past and future of the field.\nWithin each chapter, we switch between different styles of presentation. In one style,\nnatural language is the driver. We analyze language, explore linguistic concepts, and\nuse programming examples to support the discussion. We often employ Python con-\nstructs that have not been introduced systematically, so you can see their purpose before\ndelving into the details of how and why they work. This is just like learning idiomatic\nexpressions in a foreign language: you\u2019re able to buy a nice pastry without first having\nlearned the intricacies of question formation. In the other style of presentation, the\nprogramming language will be the driver. We\u2019ll analyze programs, explore algorithms,\nand the linguistic examples will play a supporting role.\nEach chapter ends with a series of graded exercises, which are useful for consolidating\nthe material. The exercises are graded according to the following scheme: \u25cb is for easy\nexercises that involve minor modifications to supplied code samples or other simple\nactivities; \u25d1 is for intermediate exercises that explore an aspect of the material in more\ndepth, requiring careful analysis and design; \u25cf is for difficult, open-ended tasks that\nwill challenge your understanding of the material and force you to think independently\n(readers new to programming should skip these).\nEach chapter has a further reading section and an online \u201cextras\u201d section at http://www\n.nltk.org/, with pointers to more advanced materials and online resources. Online ver-\nsions of all the code examples are also available there.\nWhy Python?\nPython is a simple yet powerful programming language with excellent functionality for\nprocessing linguistic data. Python can be downloaded for free from http://www.python\n.org/. Installers are available for all platforms.\nHere is a five-line Python program that processes file.txt and prints all the words ending\nin ing:\n>>> for line in open(\"file.txt\"):\n...     for word in line.split():\n...         if word.endswith('ing'):\n...             print word\nThis program illustrates some of the main features of Python. First, whitespace is used\nto nest lines of code; thus the line starting with if falls inside the scope of the previous\nline starting with for; this ensures that the ing test is performed for each word. Second,\nPython is object-oriented; each variable is an entity that has certain defined attributes\nand methods. For example, the value of the variable line is more than a sequence of\ncharacters. It is a string object that has a \u201cmethod\u201d (or operation) called split() that\nxii | Preface\nwe can use to break a line into its words. To apply a method to an object, we write the\nobject name, \nfollowed by a period, followed by the method name, i.e., line.split().\nThird, methods have arguments expressed inside parentheses. For instance, in the ex-\nample, word.endswith('ing') had the argument 'ing' to indicate that we wanted words\nending with ing and not something else. Finally\u2014and most importantly\u2014Python is\nhighly readable, so much so that it is fairly easy to guess what this program does even\nif you have never written a program before.\nWe chose Python because it has a shallow learning curve, its syntax and semantics are\ntransparent, and it has good string-handling functionality. As an interpreted language,\nPython facilitates interactive exploration. As an object-oriented language, Python per-\nmits data and methods to be encapsulated and re-used easily. As a dynamic language,\nPython permits attributes to be added to objects on the fly, and permits variables to be\ntyped dynamically, facilitating rapid development. Python comes with an extensive\nstandard library, including components for graphical programming, numerical pro-\ncessing, and web connectivity.\nPython is heavily used in industry, scientific research, and education around the world.\nPython is often praised for the way it facilitates productivity, quality, and main-\ntainability of software. A collection of Python success stories is posted at http://www\n.python.org/about/success/.\nNLTK defines an infrastructure that can be used to build NLP programs in Python. It\nprovides basic classes for representing data relevant to natural language processing;\nstandard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\ning, and text classification; and standard implementations for each task that can be\ncombined to solve complex problems.\nNLTK comes with extensive documentation. In addition to this book, the website at\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\nalso provides many HOWTOs with extensive examples and test cases, intended for\nusers, developers, and instructors.\nSoftware Requirements\nTo get the most out of this book, you should install several free software packages.\nCurrent download pointers and instructions are available at http://www.nltk.org/.\nPython\nThe material presented in this book assumes that you are using Python version 2.4\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\nNLTK depends on have been ported.\nNLTK\nThe code examples in this book use NLTK version 2.0. Subsequent releases of\nNLTK will be backward-compatible.\nPreface | xiii\nNLTK-Data\nThis contains the linguistic corpora that are analyzed and processed in the book.\nNumPy (recommended)\nThis is \na scientific computing library with support for multidimensional arrays and\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\ntion tasks.\nMatplotlib (recommended)\nThis is a 2D plotting library for data visualization, and is used in some of the book\u2019s\ncode samples that produce line graphs and bar charts.\nNetworkX (optional)\nThis is a library for storing and manipulating network structures consisting of\nnodes and edges. For visualizing semantic networks, also install the Graphviz\nlibrary.\nProver9 (optional)\nThis is an automated theorem prover for first-order and equational logic, used to\nsupport inference in language processing.\nNatural Language Toolkit (NLTK)\nNLTK was originally created in 2001 as part of a computational linguistics course in\nthe Department of Computer and Information Science at the University of Pennsylva-\nnia. Since then it has been developed and expanded with the help of dozens of con-\ntributors. It has now been adopted in courses in dozens of universities, and serves as\nthe basis of many research projects. Table P-2  lists the most important NLTK modules.\nTable P-2. Language processing tasks and corresponding NLTK modules with examples of\nfunctionality\nLanguage processing task NLTK modules Functionality\nAccessing corpora nltk.corpus Standardized interfaces to corpora and lexicons\nString processing nltk.tokenize, nltk.stem Tokenizers, sentence tokenizers, stemmers\nCollocation discovery nltk.collocations t-test, chi-squared, point-wise mutual information\nPart-of-speech tagging nltk.tag n-gram, backoff, Brill, HMM, TnT\nClassification nltk.classify, nltk.cluster Decision tree, maximum entropy, naive Bayes, EM, k-means\nChunking nltk.chunk Regular expression, n-gram, named entity\nParsing nltk.parse Chart, feature-based, unification, probabilistic, dependency\nSemantic interpretation nltk.sem, nltk.inference Lambda calculus, first-order logic, model checking\nEvaluation metrics nltk.metrics Precision, recall, agreement coefficients\nProbability and estimation nltk.probability Frequency distributions, smoothed probability distributions\nApplications nltk.app, nltk.chat Graphical concordancer, parsers, WordNet browser, chatbots\nxiv | Preface\nLanguage processing task NLTK modules Functionality\nLinguistic fieldwork nltk.toolbox Manipulate data in SIL Toolbox format\nNLTK was designed with four primary goals in mind:\nSimplicity\nTo provide \nan intuitive framework along with substantial building blocks, giving\nusers a practical knowledge of NLP without getting bogged down in the tedious\nhouse-keeping usually associated with processing annotated language data\nConsistency\nTo provide a uniform framework with consistent interfaces and data structures,\nand easily guessable method names\nExtensibility\nTo provide a structure into which new software modules can be easily accommo-\ndated, including alternative implementations and competing approaches to the\nsame task\nModularity\nTo provide components that can be used independently without needing to un-\nderstand the rest of the toolkit\nContrasting with these goals are three non-requirements\u2014potentially useful qualities\nthat we have deliberately avoided. First, while the toolkit provides a wide range of\nfunctions, it is not encyclopedic; it is a toolkit, not a system, and it will continue to\nevolve with the field of NLP. Second, while the toolkit is efficient enough to support\nmeaningful tasks, it is not highly optimized for runtime performance; such optimiza-\ntions often involve more complex algorithms, or implementations in lower-level pro-\ngramming languages such as C or C++. This would make the software less readable\nand more difficult to install. Third, we have tried to avoid clever programming tricks,\nsince we believe that clear implementations are preferable to ingenious yet indecipher-\nable ones.\nFor Instructors\nNatural Language Processing is often taught within the confines of a single-semester\ncourse at the advanced undergraduate level or postgraduate level. Many instructors\nhave found that it is difficult to cover both the theoretical and practical sides of the\nsubject in such a short span of time. Some courses focus on theory to the exclusion of\npractical exercises, and deprive students of the challenge and excitement of writing\nprograms to automatically process language. Other courses are simply designed to\nteach programming for linguists, and do not manage to cover any significant NLP con-\ntent. NLTK was originally developed to address this problem, making it feasible to\ncover a substantial amount of theory and practice within a single-semester course, even\nif students have no prior programming experience.\nPreface | xv\nA significant fraction of any NLP syllabus deals with algorithms and data structures.\nOn their \nown these can be rather dry, but NLTK brings them to life with the help of\ninteractive graphical user interfaces that make it possible to view algorithms step-by-\nstep. Most NLTK components include a demonstration that performs an interesting\ntask without requiring any special input from the user. An effective way to deliver the\nmaterials is through interactive presentation of the examples in this book, entering\nthem in a Python session, observing what they do, and modifying them to explore some\nempirical or theoretical issue.\nThis book contains hundreds of exercises that can be used as the basis for student\nassignments. The simplest exercises involve modifying a supplied program fragment in\na specified way in order to answer a concrete question. At the other end of the spectrum,\nNLTK provides a flexible framework for graduate-level research projects, with standard\nimplementations of all the basic data structures and algorithms, interfaces to dozens\nof widely used datasets (corpora), and a flexible and extensible architecture. Additional\nsupport for teaching using NLTK is available on the NLTK website.\nWe believe this book is unique in providing a comprehensive framework for students\nto learn about NLP in the context of learning to program. What sets these materials\napart is the tight coupling of the chapters and exercises with NLTK, giving students\u2014\neven those with no prior programming experience\u2014a practical introduction to NLP.\nAfter completing these materials, students will be ready to attempt one of the more\nadvanced textbooks, such as Speech and Language Processing , by Jurafsky and Martin\n(Prentice Hall, 2008).\nThis book presents programming concepts in an unusual order, beginning with a non-\ntrivial data type\u2014lists of strings\u2014then introducing non-trivial control structures such\nas comprehensions and conditionals. These idioms permit us to do useful language\nprocessing from the start. Once this motivation is in place, we return to a systematic\npresentation of fundamental concepts such as strings, loops, files, and so forth. In this\nway, we cover the same ground as more conventional approaches, without expecting\nreaders to be interested in the programming language for its own sake.\nTwo possible course plans are illustrated in Table P-3 . The first one presumes an arts/\nhumanities audience, whereas the second one presumes a science/engineering audi-\nence. Other course plans could cover the first five chapters, then devote the remaining\ntime to a single area, such as text classification (Chapters 6 and 7), syntax (Chapters\n8 and 9), semantics (Chapter 10), or linguistic data management (Chapter 11).\nTable P-3. Suggested course plans; approximate number of lectures per chapter\nChapter Arts and Humanities Science and Engineering\nChapter 1, Language Processing and Python 2\u20134 2\nChapter 2, Accessing Text Corpora and Lexical Resources 2\u20134 2\nChapter 3, Processing Raw Text 2\u20134 2\nChapter 4, Writing Structured Programs 2\u20134 1\u20132\nxvi | Preface\nChapter Arts and Humanities Science and Engineering\nChapter 5, Categorizing and Tagging Words 2\u20134 2\u20134\nChapter 6, Learning to Classify Text 0\u20132 2\u20134\nChapter 7, Extracting Information from Text 2 2\u20134\nChapter 8, Analyzing Sentence Structure 2\u20134 2\u20134\nChapter 9, Building Feature-Based Grammars 2\u20134 1\u20134\nChapter 10, Analyzing the Meaning of Sentences 1\u20132 1\u20134\nChapter 11, Managing Linguistic Data 1\u20132 1\u20134\nTotal 18\u201336 18\u201336\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nBold\nIndicates new terms.\nItalic\nUsed within \nparagraphs to refer to linguistic examples, the names of texts, and\nURLs; also used for filenames and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, statements, and keywords; also used for pro-\ngram names.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter-\nmined by context; also used for metavariables within program code examples.\nThis icon signifies a tip, suggestion, or general note.\nThis icon indicates a warning or caution.\nUsing Code Examples\nThis book \nis here to help you get your job done. In general, you may use the code in\nthis book in your programs and documentation. You do not need to contact us for\npermission unless you\u2019re reproducing a significant portion of the code. For example,\nPreface | xvii\nwriting a program that uses several chunks of code from this book does not require\npermission. Selling \nor distributing a CD-ROM of examples from O\u2019Reilly books does\nrequire permission. Answering a question by citing this book and quoting example\ncode does not require permission. Incorporating a significant amount of example code\nfrom this book into your product\u2019s documentation does require permission.\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\nauthor, publisher, and ISBN. For example: \u201c Natural Language Processing with Py-\nthon, by Steven Bird, Ewan Klein, and Edward Loper. Copyright 2009 Steven Bird,\nEwan Klein, and Edward Loper, 978-0-596-51649-9.\u201d\nIf you feel your use of code examples falls outside fair use or the permission given above,\nfeel free to contact us at permissions@oreilly.com.\nSafari\u00ae Books Online\nWhen you see a Safari\u00ae Books Online icon on the cover of your favorite\ntechnology book, \nthat means the book is available online through the\nO\u2019Reilly Network Safari Bookshelf.\nSafari offers a solution that\u2019s better than e-books. It\u2019s a virtual library that lets you easily\nsearch thousands of top tech books, cut and paste code samples, download chapters,\nand find quick answers when you need the most accurate, current information. Try it\nfor free at http://my.safaribooksonline.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO\u2019Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at:\nhttp://www.oreilly.com/catalog/9780596516499\nxviii | Preface\nThe authors provide additional materials for each chapter via the NLTK website at:\nhttp://www.nltk.org/\nTo comment or ask technical questions about this book, send email to:\nbookquestions@oreilly.com\nFor more information about our books, conferences, Resource Centers, and the\nO\u2019Reilly Network, see our website at:\nhttp://www.oreilly.com\nAcknowledgments\nThe authors \nare indebted to the following people for feedback on earlier drafts of this\nbook: Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven Bethard,\nOndrej Bojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan Garrette,\nJean Mark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, Peter Ljungl\u00f6f,\nStefan M\u00fcller, Robin Munn, Joel Nothman, Adam Przepiorkowski, Brandon Rhodes,\nStuart Robinson, Jussi Salmela, Kyle Schlansker, Rob Speer, and Richard Sproat. We\nare thankful to many students and colleagues for their comments on the class materials\nthat evolved into these chapters, including participants at NLP and linguistics summer\nschools in Brazil, India, and the USA. This book would not exist without the members\nof the nltk-dev developer community, named on the NLTK website, who have given\nso freely of their time and expertise in building and extending NLTK.\nWe are grateful to the U.S. National Science Foundation, the Linguistic Data Consor-\ntium, an Edward Clarence Dyason Fellowship, and the Universities of Pennsylvania,\nEdinburgh, and Melbourne for supporting our work on this book.\nWe thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O\u2019Reilly team,\nfor organizing comprehensive reviews of our drafts from people across the NLP and\nPython communities, for cheerfully customizing O\u2019Reilly\u2019s production tools to accom-\nmodate our needs, and for meticulous copyediting work.\nFinally, we owe a huge debt of gratitude to our partners, Kay, Mimo, and Jee, for their\nlove, patience, and support over the many years that we worked on this book. We hope\nthat our children\u2014Andrew, Alison, Kirsten, Leonie, and Maaike\u2014catch our enthusi-\nasm for language and computation from these pages.\nRoyalties\nRoyalties from the sale of this book are being used to support the development of the\nNatural Language Toolkit.\nPreface | xix\nFigure P-1. Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2007\nxx | Preface\nCHAPTER 1\nLanguage Processing and Python\nIt is easy to get our hands on millions of words of text. What can we do with it, assuming\nwe can \nwrite some simple programs? In this chapter, we\u2019ll address the following\nquestions:\n1. What can we achieve by combining simple programming techniques with large\nquantities of text?\n2. How can we automatically extract key words and phrases that sum up the style\nand content of a text?\n3. What tools and techniques does the Python programming language provide for\nsuch work?\n4. What are some of the interesting challenges of natural language processing?\nThis chapter is divided into sections that skip between two quite different styles. In the\n\u201ccomputing with language\u201d sections, we will take on some linguistically motivated\nprogramming tasks without necessarily explaining how they work. In the \u201ccloser look\nat Python\u201d sections we will systematically review key programming concepts. We\u2019ll\nflag the two styles in the section titles, but later chapters will mix both styles without\nbeing so up-front about it. We hope this style of introduction gives you an authentic\ntaste of what will come later, while covering a range of elementary concepts in linguis-\ntics and computer science. If you have basic familiarity with both areas, you can skip\nto Section 1.5; we will repeat any important points in later chapters, and if you miss\nanything you can easily consult the online reference material at http://www.nltk.org/. If\nthe material is completely new to you, this chapter will raise more questions than it\nanswers, questions that are addressed in the rest of this book.\n1.1  Computing with Language: Texts and Words\nWe\u2019re all very familiar with text, since we read and write it every day. Here we will treat\ntext as raw data  for the programs we write, programs that manipulate and analyze it in\na variety of interesting ways. But before we can do this, we have to get started with the\nPython interpreter.\n1\nGetting Started with Python\nOne of \nthe friendly things about Python is that it allows you to type directly into the\ninteractive interpreter\u2014the program that will be running your Python programs. You\ncan access the Python interpreter using a simple graphical interface called the In-\nteractive DeveLopment Environment (IDLE). On a Mac you can find this under Ap-\nplications\u2192MacPython, and on Windows under All Programs \u2192Python. Under Unix\nyou can run Python from the shell by typing idle (if this is not installed, try typing\npython). The interpreter will print a blurb about your Python version; simply check that\nyou are running Python 2.4 or 2.5 (here it is 2.5.1):\nPython 2.5.1 (r251:54863, Apr 15 2008, 22:57:26)\n[GCC 4.0.1 (Apple Inc. build 5465)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nIf you are unable to run the Python interpreter, you probably don\u2019t have\nPython installed \ncorrectly. Please visit http://python.org/ for detailed in-\nstructions.\nThe >>> prompt indicates that the Python interpreter is now waiting for input. When\ncopying examples from this book, don\u2019t type the \u201c>>>\u201d yourself. Now, let\u2019s begin by\nusing Python as a calculator:\n>>> 1 + 5 * 2 - 3\n8\n>>>\nOnce the interpreter has finished calculating the answer and displaying it, the prompt\nreappears. This means the Python interpreter is waiting for another instruction.\nYour Turn: Enter a few more expressions of your own. You can use\nasterisk ( *) for multiplication and slash ( /) for division, and parentheses\nfor bracketing expressions. Note that division doesn\u2019t always behave as\nyou might expect\u2014it does integer division (with rounding of fractions\ndownwards) when you type 1/3 and \u201cfloating-point\u201d (or decimal) divi-\nsion when you type 1.0/3.0. In order to get the expected behavior of\ndivision (standard in Python 3.0), you need to type: from __future__\nimport division.\nThe preceding examples demonstrate how you can work interactively with the Python\ninterpreter, experimenting with various expressions in the language to see what they\ndo. Now let\u2019s try a non-sensical expression to see how the interpreter handles it:\n2 | Chapter 1: \u2002Language Processing and Python\n>>> 1 +\n  File \"<stdin>\", line 1\n    1 +\n      ^\nSyntaxError: invalid syntax\n>>>\nThis produced a syntax error . In \nPython, it doesn\u2019t make sense to end an instruction\nwith a plus sign. The Python interpreter indicates the line where the problem occurred\n(line 1 of <stdin>, which stands for \u201cstandard input\u201d).\nNow that we can use the Python interpreter, we\u2019re ready to start working with language\ndata.\nGetting Started with NLTK\nBefore going further you should install NLTK, downloadable for free from http://www\n.nltk.org/. Follow the instructions there to download the version required for your\nplatform.\nOnce you\u2019ve installed NLTK, start up the Python interpreter as before, and install the\ndata required for the book by typing the following two commands at the Python\nprompt, then selecting the book collection as shown in Figure 1-1.\n>>> import nltk\n>>> nltk.download()\nFigure 1-1. Downloading the NLTK Book Collection: Browse the available packages using\nnl\ntk.download() . The Collections tab on the downloader shows how the packages are grouped into\nsets, and you should select the line labeled book to obtain all data required for the examples and\nexercises in this book. It consists of about 30 compressed files requiring about 100Mb disk space. The\nfull collection of data (i.e., all in the downloader) is about five times this size (at the time of writing)\nand continues to expand.\nOnce the data is downloaded to your machine, you can load some of it using the Python\ninterpreter. The first step is to type a special command at the Python prompt, which\n1.1  Computing with Language: Texts and Words | 3\ntells the interpreter to load some texts for us to explore: from nltk.book import *. This\nsays \n\u201cfrom NLTK\u2019s book module, load all items.\u201d The book module contains all the data\nyou will need as you read this chapter. After printing a welcome message, it loads the\ntext of several books (this will take a few seconds). Here\u2019s the command again, together\nwith the output that you will see. Take care to get spelling and punctuation right, and\nremember that you don\u2019t type the >>>.\n>>> from nltk.book import *\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n>>>\nAny time we want to find out about these texts, we just have to enter their names at\nthe Python prompt:\n>>> text1\n<Text: Moby Dick by Herman Melville 1851>\n>>> text2\n<Text: Sense and Sensibility by Jane Austen 1811>\n>>>\nNow that we can use the Python interpreter, and have some data to work with, we\u2019re\nready to get started.\nSearching Text\nThere are many ways to examine the context of a text apart from simply reading it. A\nconcordance view shows us every occurrence of a given word, together with some\ncontext. Here we look up the word monstrous in Moby Dick by entering text1 followed\nby a period, then the term concordance, and then placing \"monstrous\" in parentheses:\n>>> text1.concordance(\"monstrous\")\nBuilding index...\nDisplaying 11 of 11 matches:\nong the former , one was of a most monstrous size . ... This came towards us ,\nON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\nll over with a heathenish array of monstrous clubs and spears . Some were thick\nd as you gazed , and wondered what monstrous cannibal and savage could ever hav\nthat has survived the flood ; most monstrous and most mountainous ! That Himmal\nthey might scout at Moby Dick as a monstrous fable , or still worse and more de\nth of Radney .'\" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l\ning Scenes . In connexion with the monstrous pictures of whales , I am strongly\nere to enter upon those still more monstrous stories of them which are to be fo\n4 | Chapter 1: \u2002Language Processing and Python\nght have been rummaged out of this monstrous cabinet there is no telling . But\nof Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n>>>\nYour Turn:  Try searching for other words; to save re-typing, you might\nbe able to use up-arrow, Ctrl-up-arrow, or Alt-p to access the previous\ncommand and modify the word being searched. You can also try search-\nes on some of the other texts we have included. For example, search\nSense and Sensibility  for the word affection, using text2.concord\nance(\"affection\"). Search the book of Genesis to find out how long\nsome people lived, using: text3.concordance(\"lived\"). You could look\nat text4, the Inaugural Address Corpus , to see examples of English going\nback to 1789, and search for words like nation, terror, god to see how\nthese words have been used differently over time. We\u2019ve also included\ntext5, the NPS Chat Corpus : search this for unconventional words like\nim, ur, lol. (Note that this corpus is uncensored!)\nOnce you\u2019ve spent a little while examining these texts, we hope you have a new sense\nof the richness and diversity of language. In the next chapter you will learn how to\naccess a broader range of text, including text in languages other than English.\nA concordance permits us to see words in context. For example, we saw that mon-\nstrous occurred in contexts such as the ___ pictures and the ___ size. What other words\nappear in a similar range of contexts? We can find out by appending the term\nsimilar to the name of the text in question, then inserting the relevant word in\nparentheses:\n>>> text1.similar(\"monstrous\")\nBuilding word-context index...\nsubtly impalpable pitiable curious imperial perilous trustworthy\nabundant untoward singular lamentable few maddens horrible loving lazy\nmystifying christian exasperate puzzled\n>>> text2.similar(\"monstrous\")\nBuilding word-context index...\nvery exceedingly so heartily a great good amazingly as sweet\nremarkably extremely vast\n>>>\nObserve that we get different results for different texts. Austen uses this word quite\ndifferently from Melville; for her, monstrous has positive connotations, and sometimes\nfunctions as an intensifier like the word very.\nThe term common_contexts allows us to examine just the contexts that are shared by\ntwo or more words, such as monstrous and very. We have to enclose these words by\nsquare brackets as well as parentheses, and separate them with a comma:\n>>> text2.common_contexts([\"monstrous\", \"very\"])\nbe_glad am_glad a_pretty is_pretty a_lucky\n>>>\n1.1  Computing with Language: Texts and Words | 5\nYour Turn:  Pick another pair of words and compare their usage in two\ndifferent texts, using the similar() and common_contexts() functions.\nIt is one thing to automatically detect that a particular word occurs in a text, and to\ndisplay some words that appear in the same context. However, we can also determine\nthe location of a word in the text: how many words from the beginning it appears. This\npositional information can be displayed using a dispersion plot. Each stripe represents\nan instance of a word, and each row represents the entire text. In Figure 1-2 we see\nsome striking patterns of word usage over the last 220 years (in an artificial text con-\nstructed by joining the texts of the Inaugural Address Corpus end-to-end). You can\nproduce this plot as shown below. You might like to try more words (e.g., liberty,\nconstitution) and different texts. Can you predict the dispersion of a word before you\nview it? As before, take care to get the quotes, commas, brackets, and parentheses\nexactly right.\n>>> text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n>>>\nImportant: You need to have Python\u2019s NumPy and Matplotlib pack-\nages installed in order to produce the graphical plots used in this book.\nPlease see http://www.nltk.org/ for installation instructions.\nNow, just for fun, let\u2019s try generating some random text in the various styles we have\njust seen. To do this, we type the name of the text followed by the term generate. (We\nneed to include the parentheses, but there\u2019s nothing that goes between them.)\nFigure 1-2. Lexical dispersion plot for words in U.S. Presidential Inaugural Addresses: This can be\nused to investigate changes in language use over time.\n6 | Chapter 1: \u2002Language Processing and Python\n>>> text3.generate()\nIn the beginning of his brother is a hairy man , whose top may reach\nunto heaven ; and ye shall sow the land of Egypt there was no bread in\nall that he was taken out of the month , upon the earth . So shall thy\nwages be ? And they made their father ; and Isaac was old , and kissed\nhim : and Laban with his cattle in the midst of the hands of Esau thy\nfirst born , and Phichol the chief butler unto his son Isaac , she\n>>>\nNote that \nthe first time you run this command, it is slow because it gathers statistics\nabout word sequences. Each time you run it, you will get different output text. Now\ntry generating random text in the style of an inaugural address or an Internet chat room.\nAlthough the text is random, it reuses common words and phrases from the source text\nand gives us a sense of its style and content. (What is lacking in this randomly generated\ntext?)\nWhen generate produces its output, punctuation is split off from the\npreceding word. While this is not correct formatting for English text,\nwe do it to make clear that words and punctuation are independent of\none another. You will learn more about this in Chapter 3.\nCounting Vocabulary\nThe most obvious fact about texts that emerges from the preceding examples is that\nthey differ in the vocabulary they use. In this section, we will see how to use the com-\nputer to count the words in a text in a variety of useful ways. As before, you will jump\nright in and experiment with the Python interpreter, even though you may not have\nstudied Python systematically yet. Test your understanding by modifying the examples,\nand trying the exercises at the end of the chapter.\nLet\u2019s begin by finding out the length of a text from start to finish, in terms of the words\nand punctuation symbols that appear. We use the term len to get the length of some-\nthing, which we\u2019ll apply here to the book of Genesis:\n>>> len(text3)\n44764\n>>>\nSo Genesis has 44,764 words and punctuation symbols, or \u201ctokens.\u201d A token is the\ntechnical name for a sequence of characters\u2014such as hairy, his, or :)\u2014that we want\nto treat as a group. When we count the number of tokens in a text, say, the phrase to\nbe or not to be , we are counting occurrences of these sequences. Thus, in our example\nphrase there are two occurrences of to, two of be, and one each of or and not. But there\nare only four distinct vocabulary items in this phrase. How many distinct words does\nthe book of Genesis contain? To work this out in Python, we have to pose the question\nslightly differently. The vocabulary of a text is just the set of tokens that it uses, since\nin a set, all duplicates are collapsed together. In Python we can obtain the vocabulary\n1.1  Computing with Language: Texts and Words | 7\nitems of text3 with the command: set(text3). When you do this, many screens of\nwords will fly past. Now try the following:\n>>> sorted(set(text3)) \n['!', \"'\", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',\n'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',\n'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]\n>>> len(set(text3)) \n2789\n>>>\nBy wrapping sorted()\n around the Python expression set(text3) \n , we obtain a sorted\nlist of \nvocabulary items, beginning with various punctuation symbols and continuing\nwith words starting with A. All capitalized words precede lowercase words. We dis-\ncover the size of the vocabulary indirectly, by asking for the number of items in the set,\nand again we can use len to obtain this number \n . Although it has 44,764 tokens, this\nbook has \nonly 2,789 distinct words, or \u201cword types.\u201d A word type  is the form or\nspelling of the word independently of its specific occurrences in a text\u2014that is, the\nword considered as a unique item of vocabulary. Our count of 2,789 items will include\npunctuation symbols, so we will generally call these unique items types instead of word\ntypes.\nNow, let\u2019s calculate a measure of the lexical richness of the text. The next example\nshows us that each word is used 16 times on average (we need to make sure Python\nuses floating-point division):\n>>> from __future__ import division\n>>> len(text3) / len(set(text3))\n16.050197203298673\n>>>\nNext, let\u2019s focus on particular words. We can count how often a word occurs in a text,\nand compute what percentage of the text is taken up by a specific word:\n>>> text3.count(\"smote\")\n5\n>>> 100 * text4.count('a') / len(text4)\n1.4643016433938312\n>>>\nYour Turn:  How many times does the word lol appear in text5? How\nmuch is this as a percentage of the total number of words in this text?\nYou may want to repeat such calculations on several texts, but it is tedious to keep\nretyping the formula. Instead, you can come up with your own name for a task, like\n\u201clexical_diversity\u201d or \u201cpercentage\u201d, and associate it with a block of code. Now you\nonly have to type a short name instead of one or more complete lines of Python code,\nand you can reuse it as often as you like. The block of code that does a task for us is\n8 | Chapter 1: \u2002Language Processing and Python\ncalled a function, and we define a short name for our function with the keyword def.\nThe next \nexample shows how to define two new functions, lexical_diversity() and\npercentage():\n>>> def lexical_diversity(text): \n...     return len(text) / len(set(text)) \n...\n>>> def percentage(count, total): \n...     return 100 * count / total\n...\nCaution!\nThe Python \ninterpreter changes the prompt from >>> to ... after en-\ncountering the colon at the end of the first line. The ... prompt indicates\nthat Python expects an indented code block  to appear next. It is up to\nyou to do the indentation, by typing four spaces or hitting the Tab key.\nTo finish the indented block, just enter a blank line.\nIn the definition of lexical diversity()  \n, we specify a parameter labeled text . This\nparameter is a \u201cplaceholder\u201d for the actual text whose lexical diversity we want to\ncompute, and reoccurs in the block of code that will run when the function is used, in\nline \n . Similarly, percentage() is defined to take two parameters, labeled count and\ntotal \n .\nOnce Python \nknows that lexical_diversity() and percentage() are the names for spe-\ncific blocks of code, we can go ahead and use these functions:\n>>> lexical_diversity(text3)\n16.050197203298673\n>>> lexical_diversity(text5)\n7.4200461589185629\n>>> percentage(4, 5)\n80.0\n>>> percentage(text4.count('a'), len(text4))\n1.4643016433938312\n>>>\nTo recap, we use or call a function such as lexical_diversity() by typing its name,\nfollowed by an open parenthesis, the name of the text, and then a close parenthesis.\nThese parentheses will show up often; their role is to separate the name of a task\u2014such\nas lexical_diversity()\u2014from the data that the task is to be performed on\u2014such as\ntext3. The data value that we place in the parentheses when we call a function is an\nargument to the function.\nYou have already encountered several functions in this chapter, such as len(), set(),\nand sorted(). By convention, we will always add an empty pair of parentheses after a\nfunction name, as in len(), just to make clear that what we are talking about is a func-\ntion rather than some other kind of Python expression. Functions are an important\nconcept in programming, and we only mention them at the outset to give newcomers\n1.1  Computing with Language: Texts and Words | 9\na sense of the power and creativity of programming. Don\u2019t worry if you find it a bit\nconfusing right now.\nLater we\u2019ll \nsee how to use functions when tabulating data, as in Table 1-1 . Each row\nof the table will involve the same computation but with different data, and we\u2019ll do this\nrepetitive work using a function.\nTable 1-1. Lexical diversity of various genres in the Brown Corpus\nGenre Tokens Types Lexical diversity\nskill and hobbies 82345 11935 6.9\nhumor 21695 5017 4.3\nfiction: science 14470 3233 4.5\npress: reportage 100554 14394 7.0\nfiction: romance 70022 8452 8.3\nreligion 39399 6373 6.2\n1.2  A Closer Look at Python: Texts as Lists of Words\nYou\u2019ve seen \nsome important elements of the Python programming language. Let\u2019s take\na few moments to review them systematically.\nLists\nWhat is a text? At one level, it is a sequence of symbols on a page such as this one. At\nanother level, it is a sequence of chapters, made up of a sequence of sections, where\neach section is a sequence of paragraphs, and so on. However, for our purposes, we\nwill think of a text as nothing more than a sequence of words and punctuation. Here\u2019s\nhow we represent text in Python, in this case the opening sentence of Moby Dick:\n>>> sent1 = ['Call', 'me', 'Ishmael', '.']\n>>>\nAfter the prompt we\u2019ve given a name we made up, sent1, followed by the equals sign,\nand then some quoted words, separated with commas, and surrounded with brackets.\nThis bracketed material is known as a list in Python: it is how we store a text. We can\ninspect it by typing the name \n . We can ask for its length \n . We can even apply our\nown lexical_diversity() function to it \n .\n>>> sent1 \n['Call', 'me', 'Ishmael', '.']\n>>> len(sent1) \n4\n>>> lexical_diversity(sent1) \n1.0\n>>>\n10 | Chapter 1: \u2002Language Processing and Python\nSome more lists have been defined for you, one for the opening sentence of each of our\ntexts, sent2 \u2026 sent9\n. We inspect two of them here; you can see the rest for yourself\nusing the Python interpreter (if you get an error saying that sent2 is not defined, you\nneed to first type from nltk.book import *).\n>>> sent2\n['The', 'family', 'of', 'Dashwood', 'had', 'long',\n'been', 'settled', 'in', 'Sussex', '.']\n>>> sent3\n['In', 'the', 'beginning', 'God', 'created', 'the',\n'heaven', 'and', 'the', 'earth', '.']\n>>>\nYour Turn:  Make up a few sentences of your own, by typing a name,\nequals sign, and a list of words, like this: ex1 = ['Monty', 'Python',\n'and', 'the', 'Holy', 'Grail'] . Repeat some of the other Python op-\nerations we saw earlier in Section 1.1 , e.g., sorted(ex1), len(set(ex1)),\nex1.count('the').\nA pleasant surprise is that we can use Python\u2019s addition operator on lists. Adding two\nlists \n  creates a new list with everything from the first list, followed by everything from\nthe second list:\n>>> ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail'] \n['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']\nThis special use of the addition operation is called concatenation; it\ncombines \nthe lists together into a single list. We can concatenate sen-\ntences to build up a text.\nWe don\u2019t have to literally type the lists either; we can use short names that refer to pre-\ndefined lists.\n>>> sent4 + sent1\n['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',\n'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']\n>>>\nWhat if we want to add a single item to a list? This is known as appending. When we\nappend() to a list, the list itself is updated as a result of the operation.\n>>> sent1.append(\"Some\")\n>>> sent1\n['Call', 'me', 'Ishmael', '.', 'Some']\n>>>\n1.2  A Closer Look at Python: Texts as Lists of Words | 11\nIndexing Lists\nAs we \nhave seen, a text in Python is a list of words, represented using a combination\nof brackets and quotes. Just as with an ordinary page of text, we can count up the total\nnumber of words in text1 with len(text1), and count the occurrences in a text of a\nparticular word\u2014say, heaven\u2014using text1.count('heaven').\nWith some patience, we can pick out the 1st, 173rd, or even 14,278th word in a printed\ntext. Analogously, we can identify the elements of a Python list by their order of oc-\ncurrence in the list. The number that represents this position is the item\u2019s index. We\ninstruct Python to show us the item that occurs at an index such as 173 in a text by\nwriting the name of the text followed by the index inside square brackets:\n>>> text4[173]\n'awaken'\n>>>\nWe can do the converse; given a word, find the index of when it first occurs:\n>>> text4.index('awaken')\n173\n>>>\nIndexes are a common way to access the words of a text, or, more generally, the ele-\nments of any list. Python permits us to access sublists as well, extracting manageable\npieces of language from large texts, a technique known as slicing.\n>>> text5[16715:16735]\n['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good',\n'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without',\n'buying', 'it']\n>>> text6[1600:1625]\n['We', \"'\", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We',\n'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive',\n'officer', 'for', 'the', 'week']\n>>>\nIndexes have some subtleties, and we\u2019ll explore these with the help of an artificial\nsentence:\n>>> sent = ['word1', 'word2', 'word3', 'word4', 'word5',\n...         'word6', 'word7', 'word8', 'word9', 'word10']\n>>> sent[0]\n'word1'\n>>> sent[9]\n'word10'\n>>>\nNotice that our indexes start from zero: sent element zero, written sent[0], is the first\nword, 'word1', whereas sent element 9 is 'word10'. The reason is simple: the moment\nPython accesses the content of a list from the computer\u2019s memory, it is already at the\nfirst element; we have to tell it how many elements forward to go. Thus, zero steps\nforward leaves it at the first element.\n12 | Chapter 1: \u2002Language Processing and Python\nThis practice of counting from zero is initially confusing, but typical of\nmodern programming \nlanguages. You\u2019ll quickly get the hang of it if\nyou\u2019ve mastered the system of counting centuries where 19XY is a year\nin the 20th century, or if you live in a country where the floors of a\nbuilding are numbered from 1, and so walking up n-1 flights of stairs\ntakes you to level n.\nNow, if we accidentally use an index that is too large, we get an error:\n>>> sent[10]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\nIndexError: list index out of range\n>>>\nThis time it is not a syntax error, because the program fragment is syntactically correct.\nInstead, it is a runtime error , and it produces a Traceback message that shows the\ncontext of the error, followed by the name of the error, IndexError, and a brief\nexplanation.\nLet\u2019s take a closer look at slicing, using our artificial sentence again. Here we verify that\nthe slice 5:8 includes sent elements at indexes 5, 6, and 7:\n>>> sent[5:8]\n['word6', 'word7', 'word8']\n>>> sent[5]\n'word6'\n>>> sent[6]\n'word7'\n>>> sent[7]\n'word8'\n>>>\nBy convention, m:n means elements m\u2026n-1. As the next example shows, we can omit\nthe first number if the slice begins at the start of the list \n , and we can omit the second\nnumber if the slice goes to the end \n :\n>>> sent[:3] \n['word1', 'word2', 'word3']\n>>> text2[141525:] \n['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor', 'and', 'Marianne',\n',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the', 'least', 'considerable', ',',\n'that', 'though', 'sisters', ',', 'and', 'living', 'almost', 'within', 'sight', 'of',\n'each', 'other', ',', 'they', 'could', 'live', 'without', 'disagreement', 'between',\n'themselves', ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',\n'THE', 'END']\n>>>\nWe can \nmodify an element of a list by assigning to one of its index values. In the next\nexample, we put sent[0] on the left of the equals sign \n . We can also replace an entire\nslice with \nnew material \n . A consequence of this last change is that the list only has\nfour elements, and accessing a later value generates an error \n .\n1.2  A Closer Look at Python: Texts as Lists of Words | 13\n>>> sent[0] = 'First' \n>>> sent[9] = 'Last'\n>>> len(sent)\n10\n>>> sent[1:9] = ['Second', 'Third'] \n>>> sent\n['First', 'Second', 'Third', 'Last']\n>>> sent[9] \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\nIndexError: list index out of range\n>>>\nYour Turn:  Take a few minutes to define a sentence of your own and\nmodify individual words and groups of words (slices) using the same\nmethods used earlier. Check your understanding by trying the exercises\non lists at the end of this chapter.\nVariables\nFrom the start of Section 1.1 , you have had access to texts called text1, text2, and so\non. It saved a lot of typing to be able to refer to a 250,000-word book with a short name\nlike this! In general, we can make up names for anything we care to calculate. We did\nthis ourselves in the previous sections, e.g., defining a variable sent1, as follows:\n>>> sent1 = ['Call', 'me', 'Ishmael', '.']\n>>>\nSuch lines have the form: variable = expression . Python will evaluate the expression,\nand save its result to the variable. This process is called assignment. It does not gen-\nerate any output; you have to type the variable on a line of its own to inspect its contents.\nThe equals sign is slightly misleading, since information is moving from the right side\nto the left. It might help to think of it as a left-arrow. The name of the variable can be\nanything you like, e.g., my_sent, sentence, xyzzy. It must start with a letter, and can\ninclude numbers and underscores. Here are some examples of variables and\nassignments:\n>>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',\n... 'forth', 'from', 'Camelot', '.']\n>>> noun_phrase = my_sent[1:4]\n>>> noun_phrase\n['bold', 'Sir', 'Robin']\n>>> wOrDs = sorted(noun_phrase)\n>>> wOrDs\n['Robin', 'Sir', 'bold']\n>>>\nRemember that capitalized words appear before lowercase words in sorted lists.\n14 | Chapter 1: \u2002Language Processing and Python\nNotice in the previous example that we split the definition of my_sent\nover two \nlines. Python expressions can be split across multiple lines, so\nlong as this happens within any kind of brackets. Python uses the ...\nprompt to indicate that more input is expected. It doesn\u2019t matter how\nmuch indentation is used in these continuation lines, but some inden-\ntation usually makes them easier to read.\nIt is good to choose meaningful variable names to remind you\u2014and to help anyone\nelse who reads your Python code\u2014what your code is meant to do. Python does not try\nto make sense of the names; it blindly follows your instructions, and does not object if\nyou do something confusing, such as one = 'two' or two = 3. The only restriction is\nthat a variable name cannot be any of Python\u2019s reserved words, such as def, if, not,\nand import. If you use a reserved word, Python will produce a syntax error:\n>>> not = 'Camelot'\nFile \"<stdin>\", line 1\n    not = 'Camelot'\n        ^\nSyntaxError: invalid syntax\n>>>\nWe will often use variables to hold intermediate steps of a computation, especially\nwhen this makes the code easier to follow. Thus len(set(text1)) could also be written:\n>>> vocab = set(text1)\n>>> vocab_size = len(vocab)\n>>> vocab_size\n19317\n>>>\nCaution!\nTake care \nwith your choice of names (or identifiers) for Python varia-\nbles. First, you should start the name with a letter, optionally followed\nby digits ( 0 to 9) or letters. Thus, abc23 is fine, but 23abc will cause a\nsyntax error. Names are case-sensitive, which means that myVar and\nmyvar are distinct variables. Variable names cannot contain whitespace,\nbut you can separate words using an underscore, e.g., my_var. Be careful\nnot to insert a hyphen instead of an underscore: my-var is wrong, since\nPython interprets the - as a minus sign.\nStrings\nSome of the methods we used to access the elements of a list also work with individual\nwords, or strings. For example, we can assign a string to a variable \n , index a string\n, and slice a string \n .\n1.2  A Closer Look at Python: Texts as Lists of Words | 15\n>>> name = 'Monty' \n>>> name[0] \n'M'\n>>> name[:4] \n'Mont'\n>>>\nWe can also perform multiplication and addition with strings:\n>>> name * 2\n'MontyMonty'\n>>> name + '!'\n'Monty!'\n>>>\nWe can \njoin the words of a list to make a single string, or split a string into a list, as\nfollows:\n>>> ' '.join(['Monty', 'Python'])\n'Monty Python'\n>>> 'Monty Python'.split()\n['Monty', 'Python']\n>>>\nWe will come back to the topic of strings in Chapter 3. For the time being, we have\ntwo important building blocks\u2014lists and strings\u2014and are ready to get back to some\nlanguage analysis.\n1.3  Computing with Language: Simple Statistics\nLet\u2019s return to our exploration of the ways we can bring our computational resources\nto bear on large quantities of text. We began this discussion in Section 1.1, and saw\nhow to search for words in context, how to compile the vocabulary of a text, how to\ngenerate random text in the same style, and so on.\nIn this section, we pick up the question of what makes a text distinct, and use automatic\nmethods to find characteristic words and expressions of a text. As in Section 1.1 , you\ncan try new features of the Python language by copying them into the interpreter, and\nyou\u2019ll learn about these features systematically in the following section.\nBefore continuing further, you might like to check your understanding of the last sec-\ntion by predicting the output of the following code. You can use the interpreter to check\nwhether you got it right. If you\u2019re not sure how to do this task, it would be a good idea\nto review the previous section before continuing further.\n>>> saying = ['After', 'all', 'is', 'said', 'and', 'done',\n...           'more', 'is', 'said', 'than', 'done']\n>>> tokens = set(saying)\n>>> tokens = sorted(tokens)\n>>> tokens[-2:]\nwhat output do you expect here?\n>>>\n16 | Chapter 1: \u2002Language Processing and Python\nFrequency Distributions\nHow can \nwe automatically identify the words of a text that are most informative about\nthe topic and genre of the text? Imagine how you might go about finding the 50 most\nfrequent words of a book. One method would be to keep a tally for each vocabulary\nitem, like that shown in Figure 1-3. The tally would need thousands of rows, and it\nwould be an exceedingly laborious process\u2014so laborious that we would rather assign\nthe task to a machine.\nFigure 1-3. Counting words appearing in a text (a frequency distribution).\nThe table \nin Figure 1-3 is known as a frequency distribution , and it tells us the\nfrequency of each vocabulary item in the text. (In general, it could count any kind of\nobservable event.) It is a \u201cdistribution\u201d since it tells us how the total number of word\ntokens in the text are distributed across the vocabulary items. Since we often need\nfrequency distributions in language processing, NLTK provides built-in support for\nthem. Let\u2019s use a FreqDist to find the 50 most frequent words of Moby Dick . Try to\nwork out what is going on here, then read the explanation that follows.\n>>> fdist1 = FreqDist(text1) \n>>> fdist1 \n<FreqDist with 260819 outcomes>\n>>> vocabulary1 = fdist1.keys() \n>>> vocabulary1[:50] \n[',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', \"'\", '-',\n'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '\"', 'all', 'for',\n'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',\n'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',\n'now', 'which', '?', 'me', 'like']\n>>> fdist1['whale']\n906\n>>>\nWhen we \nfirst invoke FreqDist, we pass the name of the text as an argument \n . We\ncan inspect \nthe total number of words (\u201coutcomes\u201d) that have been counted up \n \u2014\n260,819 in \nthe case of Moby Dick. The expression keys() gives us a list of all the distinct\ntypes in the text \n , and we can look at the first 50 of these by slicing the list \n .\n1.3  Computing with Language: Simple Statistics | 17\nYour Turn: Try the preceding frequency distribution example for your-\nself, for text2. Be careful to use the correct parentheses and uppercase\nletters. If you get an error message NameError: name 'FreqDist' is not\ndefined, you need to start your work with from nltk.book import *.\nDo any words produced in the last example help us grasp the topic or genre of this text?\nOnly one word, whale, is slightly informative! It occurs over 900 times. The rest of the\nwords tell us nothing about the text; they\u2019re just English \u201cplumbing.\u201d What proportion\nof the text is taken up with such words? We can generate a cumulative frequency plot\nfor these words, using fdist1.plot(50, cumulative=True), to produce the graph in\nFigure 1-4. These 50 words account for nearly half the book!\nFigure 1-4. Cumulative frequency plot for the 50 most frequently used words in Moby Dick , which\naccount for nearly half of the tokens.\n18 | Chapter 1: \u2002Language Processing and Python\nIf the frequent words don\u2019t help us, how about the words that occur once only, the so-\ncalled hapaxes? View \nthem by typing fdist1.hapaxes(). This list contains\nlexicographer, cetological, contraband, expostulations, and about 9,000 others. It seems\nthat there are too many rare words, and without seeing the context we probably can\u2019t\nguess what half of the hapaxes mean in any case! Since neither frequent nor infrequent\nwords help, we need to try something else.\nFine-Grained Selection of Words\nNext, let\u2019s look at the long words of a text; perhaps these will be more characteristic\nand informative. For this we adapt some notation from set theory. We would like to\nfind the words from the vocabulary of the text that are more than 15 characters long.\nLet\u2019s call this property P, so that P(w) is true if and only if w is more than 15 characters\nlong. Now we can express the words of interest using mathematical set notation as\nshown in (1a). This means \u201cthe set of all w such that w is an element of V (the vocabu-\nlary) and w has property P.\u201d\n(1) a. { w | w \u2208 V & P(w)}\nb.[w for w in V if p(w)]\nThe corresponding Python expression is given in (1b). (Note that it produces a list, not\na set, which means that duplicates are possible.) Observe how similar the two notations\nare. Let\u2019s go one more step and write executable Python code:\n>>> V = set(text1)\n>>> long_words = [w for w in V if len(w) > 15]\n>>> sorted(long_words)\n['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',\n'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',\n'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',\n'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',\n'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',\n'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']\n>>>\nFor each word w in the vocabulary V, we check whether len(w) is greater than 15; all\nother words will be ignored. We will discuss this syntax more carefully later.\nYour Turn:  Try out the previous statements in the Python interpreter,\nand experiment with changing the text and changing the length condi-\ntion. Does it make an difference to your results if you change the variable\nnames, e.g., using [word for word in vocab if ...]?\n1.3  Computing with Language: Simple Statistics | 19\nLet\u2019s return to our task of finding words that characterize a text. Notice that the long\nwords in text4\n reflect its national focus\u2014 constitutionally, transcontinental\u2014whereas\nthose in text5 reflect its informal content: boooooooooooglyyyyyy and\nyuuuuuuuuuuuummmmmmmmmmmm. Have we succeeded in automatically extract-\ning words that typify a text? Well, these very long words are often hapaxes (i.e., unique)\nand perhaps it would be better to find frequently occurring long words. This seems\npromising since it eliminates frequent short words (e.g., the) and infrequent long words\n(e.g., antiphilosophists). Here are all words from the chat corpus that are longer than\nseven characters, that occur more than seven times:\n>>> fdist5 = FreqDist(text5)\n>>> sorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])\n['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',\n'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',\n'innocent', 'listening', 'remember', 'seriously', 'something', 'together',\n'tomorrow', 'watching']\n>>>\nNotice how we have used two conditions: len(w) > 7 ensures that the words are longer\nthan seven letters, and fdist5[w] > 7  ensures that these words occur more than seven\ntimes. At last we have managed to automatically identify the frequently occurring con-\ntent-bearing words of the text. It is a modest but important milestone: a tiny piece of\ncode, processing tens of thousands of words, produces some informative output.\nCollocations and Bigrams\nA collocation is a sequence of words that occur together unusually often. Thus red\nwine is a collocation, whereas the wine  is not. A characteristic of collocations is that\nthey are resistant to substitution with words that have similar senses; for example,\nmaroon wine sounds very odd.\nTo get a handle on collocations, we start off by extracting from a text a list of word\npairs, also known as bigrams. This is easily accomplished with the function bigrams():\n>>> bigrams(['more', 'is', 'said', 'than', 'done'])\n[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]\n>>>\nHere we see that the pair of words than-done is a bigram, and we write it in Python as\n('than', 'done') . Now, collocations are essentially just frequent bigrams, except that\nwe want to pay more attention to the cases that involve rare words. In particular, we\nwant to find bigrams that occur more often than we would expect based on the fre-\nquency of individual words. The collocations() function does this for us (we will see\nhow it works later):\n>>> text4.collocations()\nBuilding collocations list\nUnited States; fellow citizens; years ago; Federal Government; General\nGovernment; American people; Vice President; Almighty God; Fellow\ncitizens; Chief Magistrate; Chief Justice; God bless; Indian tribes;\npublic debt; foreign nations; political parties; State governments;\n20 | Chapter 1: \u2002Language Processing and Python\nNational Government; United Nations; public money\n>>> text8.collocations()\nBuilding collocations list\nmedium build; social drinker; quiet nights; long term; age open;\nfinancially secure; fun times; similar interests; Age open; poss\nrship; single mum; permanent relationship; slim build; seeks lady;\nLate 30s; Photo pls; Vibrant personality; European background; ASIAN\nLADY; country drives\n>>>\nThe collocations \nthat emerge are very specific to the genre of the texts. In order to find\nred wine as a collocation, we would need to process a much larger body of text.\nCounting Other Things\nCounting words is useful, but we can count other things too. For example, we can look\nat the distribution of word lengths in a text, by creating a FreqDist out of a long list of\nnumbers, where each number is the length of the corresponding word in the text:\n>>> [len(w) for w in text1] \n[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n>>> fdist = FreqDist([len(w) for w in text1])  \n>>> fdist  \n<FreqDist with 260819 outcomes>\n>>> fdist.keys()\n[3, 1, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]\n>>>\nWe start \nby deriving a list of the lengths of words in text1 \n , and the FreqDist then\ncounts \nthe number of times each of these occurs \n . The result \n  is a distribution\ncontaining a \nquarter of a million items, each of which is a number corresponding to a\nword token in the text. But there are only 20 distinct items being counted, the numbers\n1 through 20, because there are only 20 different word lengths. I.e., there are words\nconsisting of just 1 character, 2 characters, ..., 20 characters, but none with 21 or more\ncharacters. One might wonder how frequent the different lengths of words are (e.g.,\nhow many words of length 4 appear in the text, are there more words of length 5 than\nlength 4, etc.). We can do this as follows:\n>>> fdist.items()\n[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),\n(8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),\n(15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]\n>>> fdist.max()\n3\n>>> fdist[3]\n50223\n>>> fdist.freq(3)\n0.19255882431878046\n>>>\nFrom this we see that the most frequent word length is 3, and that words of length 3\naccount for roughly 50,000 (or 20%) of the words making up the book. Although we\nwill not pursue it here, further analysis of word length might help us understand\n1.3  Computing with Language: Simple Statistics | 21\ndifferences between authors, genres, or languages. Table 1-2 summarizes the functions\ndefined in frequency distributions.\nTable 1-2. Functions defined for NLTK\u2019s frequency distributions\nExample Description\nfdist = FreqDist(samples) Create a frequency distribution containing the given samples\nfdist.inc(sample) Increment the count for this sample\nfdist['monstrous'] Count of the number of times a given sample occurred\nfdist.freq('monstrous') Frequency of a given sample\nfdist.N() Total number of samples\nfdist.keys() The samples sorted in order of decreasing frequency\nfor sample in fdist: Iterate over the samples, in order of decreasing frequency\nfdist.max() Sample with the greatest count\nfdist.tabulate() Tabulate the frequency distribution\nfdist.plot() Graphical plot of the frequency distribution\nfdist.plot(cumulative=True) Cumulative plot of the frequency distribution\nfdist1 < fdist2 Test if samples in fdist1 occur less frequently than in fdist2\nOur discussion of frequency distributions has introduced some important Python con-\ncepts, and we will look at them systematically in Section 1.4.\n1.4  Back to Python: Making Decisions and Taking Control\nSo far, \nour little programs have had some interesting qualities: the ability to work with\nlanguage, and the potential to save human effort through automation. A key feature of\nprogramming is the ability of machines to make decisions on our behalf, executing\ninstructions when certain conditions are met, or repeatedly looping through text data\nuntil some condition is satisfied. This feature is known as control, and is the focus of\nthis section.\nConditionals\nPython supports a wide range of operators, such as < and >=, for testing the relationship\nbetween values. The full set of these relational operators are shown in Table 1-3.\nTable 1-3. Numerical comparison operators\nOperator Relationship\n< Less than\n<= Less than or equal to\n== Equal to (note this is two \u201c=\u201dsigns, not one)\n22 | Chapter 1: \u2002Language Processing and Python\nOperator Relationship\n!= Not equal to\n> Greater than\n>= Greater than or equal to\nWe can use these to select different words from a sentence of news text. Here are some\nexamples\u2014notice only \nthe operator is changed from one line to the next. They all use\nsent7, the first sentence from text7 ( Wall Street Journal ). As before, if you get an error\nsaying that sent7 is undefined, you need to first type: from nltk.book import *.\n>>> sent7\n['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n>>> [w for w in sent7 if len(w) < 4]\n[',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']\n>>> [w for w in sent7 if len(w) <= 4]\n[',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']\n>>> [w for w in sent7 if len(w) == 4]\n['will', 'join', 'Nov.']\n>>> [w for w in sent7 if len(w) != 4]\n['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',\n'as', 'a', 'nonexecutive', 'director', '29', '.']\n>>>\nThere is a common pattern to all of these examples: [w for w in text if  condition],\nwhere condition is a Python \u201ctest\u201d that yields either true or false. In the cases shown\nin the previous code example, the condition is always a numerical comparison. How-\never, we can also test various properties of words, using the functions listed in Table 1-4 .\nTable 1-4. Some word comparison operators\nFunction Meaning\ns.startswith(t) Test if s starts with t\ns.endswith(t) Test if s ends with t\nt in s Test if t is contained inside s\ns.islower() Test if all cased characters in s are lowercase\ns.isupper() Test if all cased characters in s are uppercase\ns.isalpha() Test if all characters in s are alphabetic\ns.isalnum() Test if all characters in s are alphanumeric\ns.isdigit() Test if all characters in s are digits\ns.istitle() Test if s is titlecased (all words in s have initial capitals)\nHere are some examples of these operators being used to select words from our texts:\nwords ending \nwith -ableness; words containing gnt; words having an initial capital; and\nwords consisting entirely of digits.\n1.4  Back to Python: Making Decisions and Taking Control | 23\n>>> sorted([w for w in set(text1) if w.endswith('ableness')])\n['comfortableness', 'honourableness', 'immutableness', 'indispensableness', ...]\n>>> sorted([term for term in set(text4) if 'gnt' in term])\n['Sovereignty', 'sovereignties', 'sovereignty']\n>>> sorted([item for item in set(text6) if item.istitle()])\n['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...]\n>>> sorted([item for item in set(sent7) if item.isdigit()])\n['29', '61']\n>>>\nWe can \nalso create more complex conditions. If c is a condition, then not c is also a\ncondition. If we have two conditions c1 and c2, then we can combine them to form a\nnew condition using conjunction and disjunction: c 1 and c 2, c1 or c 2.\nYour Turn: Run the following examples and try to explain what is going\non in each one. Next, try to make up some conditions of your own.\n>>> sorted([w for w in set(text7) if '-' in w and 'index' in w])\n>>> sorted([wd for wd in set(text3) if wd.istitle() and len(wd) > 10])\n>>> sorted([w for w in set(sent7) if not w.islower()])\n>>> sorted([t for t in set(text2) if 'cie' in t or 'cei' in t])\nOperating on Every Element\nIn Section 1.3 , we saw some examples of counting items other than words. Let\u2019s take\na closer look at the notation we used:\n>>> [len(w) for w in text1]\n[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n>>> [w.upper() for w in text1]\n['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...]\n>>>\nThese expressions have the form [f(w) for ...] or [w.f() for ...], where f is a\nfunction that operates on a word to compute its length, or to convert it to uppercase.\nFor now, you don\u2019t need to understand the difference between the notations f(w) and\nw.f(). Instead, simply learn this Python idiom which performs the same operation on\nevery element of a list. In the preceding examples, it goes through each word in\ntext1, assigning each one in turn to the variable w and performing the specified oper-\nation on the variable.\nThe notation just described is called a \u201clist comprehension.\u201d This is our\nfirst example \nof a Python idiom, a fixed notation that we use habitually\nwithout bothering to analyze each time. Mastering such idioms is an\nimportant part of becoming a fluent Python programmer.\nLet\u2019s return to the question of vocabulary size, and apply the same idiom here:\n>>> len(text1)\n260819\n24 | Chapter 1: \u2002Language Processing and Python\n>>> len(set(text1))\n19317\n>>> len(set([word.lower() for word in text1]))\n17231\n>>>\nNow that we are not double-counting words like This and this , which differ only in\ncapitalization, we\u2019ve wiped 2,000 off the vocabulary count! We can go a step further\nand eliminate numbers and punctuation from the vocabulary count by filtering out any\nnon-alphabetic items:\n>>> len(set([word.lower() for word in text1 if word.isalpha()]))\n16948\n>>>\nThis example is slightly complicated: it lowercases all the purely alphabetic items. Per-\nhaps it would have been simpler just to count the lowercase-only items, but this gives\nthe wrong answer (why?).\nDon\u2019t worry if you don\u2019t feel confident with list comprehensions yet, since you\u2019ll see\nmany more examples along with explanations in the following chapters.\nNested Code Blocks\nMost programming languages permit us to execute a block of code when a conditional\nexpression, or if statement, is satisfied. We already saw examples of conditional tests\nin code like [w for w in sent7 if len(w) < 4]. In the following program, we have\ncreated a variable called word containing the string value 'cat'. The if statement checks\nwhether the test len(word) < 5  is true. It is, so the body of the if statement is invoked\nand the print statement is executed, displaying a message to the user. Remember to\nindent the print statement by typing four spaces.\n>>> word = 'cat'\n>>> if len(word) < 5:\n...     print 'word length is less than 5'\n...   \nword length is less than 5\n>>>\nWhen we \nuse the Python interpreter we have to add an extra blank line \n  in order for\nit to detect that the nested block is complete.\nIf we \nchange the conditional test to len(word) >= 5 , to check that the length of word is\ngreater than or equal to 5, then the test will no longer be true. This time, the body of\nthe if statement will not be executed, and no message is shown to the user:\n>>> if len(word) >= 5:\n...   print 'word length is greater than or equal to 5'\n...\n>>>\n1.4  Back to Python: Making Decisions and Taking Control | 25\nAn if statement is known as a control structure  because it controls whether the code\nin the indented block will be run. Another control structure is the for loop. Try the\nfollowing, and remember to include the colon and the four spaces:\n>>> for word in ['Call', 'me', 'Ishmael', '.']:\n...     print word\n...\nCall\nme\nIshmael\n.\n>>>\nThis is \ncalled a loop because Python executes the code in circular fashion. It starts by\nperforming the assignment word = 'Call', effectively using the word variable to name\nthe first item of the list. Then, it displays the value of word to the user. Next, it goes\nback to the for statement, and performs the assignment word = 'me' before displaying\nthis new value to the user, and so on. It continues in this fashion until every item of the\nlist has been processed.\nLooping with Conditions\nNow we can combine the if and for statements. We will loop over every item of the\nlist, and print the item only if it ends with the letter l. We\u2019ll pick another name for the\nvariable to demonstrate that Python doesn\u2019t try to make sense of variable names.\n>>> sent1 = ['Call', 'me', 'Ishmael', '.']\n>>> for xyzzy in sent1:\n...     if xyzzy.endswith('l'):\n...         print xyzzy\n...\nCall\nIshmael\n>>>\nYou will notice that if and for statements have a colon at the end of the line, before\nthe indentation begins. In fact, all Python control structures end with a colon. The\ncolon indicates that the current statement relates to the indented block that follows.\nWe can also specify an action to be taken if the condition of the if statement is not\nmet. Here we see the elif (else if) statement, and the else statement. Notice that these\nalso have colons before the indented code.\n>>> for token in sent1:\n...     if token.islower():\n...         print token, 'is a lowercase word'\n...     elif token.istitle():\n...         print token, 'is a titlecase word'\n...     else:\n...         print token, 'is punctuation'\n...\nCall is a titlecase word\nme is a lowercase word\n26 | Chapter 1: \u2002Language Processing and Python\nIshmael is a titlecase word\n. is punctuation\n>>>\nAs you \ncan see, even with this small amount of Python knowledge, you can start to\nbuild multiline Python programs. It\u2019s important to develop such programs in pieces,\ntesting that each piece does what you expect before combining them into a program.\nThis is why the Python interactive interpreter is so invaluable, and why you should get\ncomfortable using it.\nFinally, let\u2019s combine the idioms we\u2019ve been exploring. First, we create a list of cie and\ncei words, then we loop over each item and print it. Notice the comma at the end of\nthe print statement, which tells Python to produce its output on a single line.\n>>> tricky = sorted([w for w in set(text2) if 'cie' in w or 'cei' in w])\n>>> for word in tricky:\n...     print word,\nancient ceiling conceit conceited conceive conscience\nconscientious conscientiously deceitful deceive ...\n>>>\n1.5  Automatic Natural Language Understanding\nWe have been exploring language bottom-up, with the help of texts and the Python\nprogramming language. However, we\u2019re also interested in exploiting our knowledge of\nlanguage and computation by building useful language technologies. We\u2019ll take the\nopportunity now to step back from the nitty-gritty of code in order to paint a bigger\npicture of natural language processing.\nAt a purely practical level, we all need help to navigate the universe of information\nlocked up in text on the Web. Search engines have been crucial to the growth and\npopularity of the Web, but have some shortcomings. It takes skill, knowledge, and\nsome luck, to extract answers to such questions as: What tourist sites can I visit between\nPhiladelphia and Pittsburgh on a limited budget?  What do experts say about digital SLR\ncameras? What predictions about the steel market were made by credible commentators\nin the past week? Getting a computer to answer them automatically involves a range of\nlanguage processing tasks, including information extraction, inference, and summari-\nzation, and would need to be carried out on a scale and with a level of robustness that\nis still beyond our current capabilities.\nOn a more philosophical level, a long-standing challenge within artificial intelligence\nhas been to build intelligent machines, and a major part of intelligent behavior is un-\nderstanding language. For many years this goal has been seen as too difficult. However,\nas NLP technologies become more mature, and robust methods for analyzing unre-\nstricted text become more widespread, the prospect of natural language understanding\nhas re-emerged as a plausible goal.\n1.5  Automatic Natural Language Understanding | 27\nIn this section we describe some language understanding technologies, to give you a\nsense of the interesting challenges that are waiting for you.\nWord Sense Disambiguation\nIn word sense \ndisambiguation  we want to work out which sense of a word was in-\ntended in a given context. Consider the ambiguous words serve and dish:\n(2) a. serve: help with food or drink; hold an office; put ball into play\nb.dish: plate; course of a meal; communications device\nIn a sentence containing the phrase: he served the dish , you can detect that both serve\nand dish are being used with their food meanings. It\u2019s unlikely that the topic of discus-\nsion shifted from sports to crockery in the space of three words. This would force you\nto invent bizarre images, like a tennis pro taking out his frustrations on a china tea-set\nlaid out beside the court. In other words, we automatically disambiguate words using\ncontext, exploiting the simple fact that nearby words have closely related meanings. As\nanother example of this contextual effect, consider the word by, which has several\nmeanings, for example, the book by Chesterton  (agentive\u2014Chesterton was the author\nof the book); the cup by the stove  (locative\u2014the stove is where the cup is); and submit\nby Friday  (temporal\u2014Friday is the time of the submitting). Observe in (3) that the\nmeaning of the italicized word helps us interpret the meaning of by.\n(3) a. The lost children were found by the searchers (agentive)\nb. The lost children were found by the mountain (locative)\nc. The lost children were found by the afternoon (temporal)\nPronoun Resolution\nA deeper kind of language understanding is to work out \u201cwho did what to whom,\u201d i.e.,\nto detect the subjects and objects of verbs. You learned to do this in elementary school,\nbut it\u2019s harder than you might think. In the sentence the thieves stole the paintings , it is\neasy to tell who performed the stealing action. Consider three possible following sen-\ntences in (4), and try to determine what was sold, caught, and found (one case is\nambiguous).\n(4) a. The thieves stole the paintings. They were subsequently sold.\nb. The thieves stole the paintings. They were subsequently caught.\nc. The thieves stole the paintings. They were subsequently found.\nAnswering this question involves finding the antecedent of the pronoun they, either\nthieves or paintings. Computational techniques for tackling this problem include ana-\nphora resolution \u2014identifying what a pronoun or noun phrase refers to\u2014and\n28 | Chapter 1: \u2002Language Processing and Python\nsemantic role labeling \u2014identifying how a noun phrase relates to the verb (as agent,\npatient, instrument, and so on).\nGenerating Language Output\nIf we can automatically solve such problems of language understanding, we will be able\nto move on to tasks that involve generating language output, such as question\nanswering and machine translation . In the first case, a machine should be able to\nanswer a user\u2019s questions relating to collection of texts:\n(5) a. Text: ... The thieves stole the paintings. They were subsequently sold. ...\nb.Human: Who or what was sold?\nc.Machine: The paintings.\nThe machine\u2019s answer demonstrates that it has correctly worked out that they refers to\npaintings and not to thieves. In the second case, the machine should be able to translate\nthe text into another language, accurately conveying the meaning of the original text.\nIn translating the example text into French, we are forced to choose the gender of the\npronoun in the second sentence: ils (masculine) if the thieves are sold, and elles (fem-\ninine) if the paintings are sold. Correct translation actually depends on correct under-\nstanding of the pronoun.\n(6) a. The thieves stole the paintings. They were subsequently found.\nb. Les voleurs ont vol\u00e9 les peintures. Ils ont \u00e9t\u00e9 trouv\u00e9s plus tard. (the thieves)\nc. Les voleurs ont vol\u00e9 les peintures. Elles ont \u00e9t\u00e9 trouv\u00e9es plus tard. (the\npaintings)\nIn all of these examples, working out the sense of a word, the subject of a verb, and the\nantecedent of a pronoun are steps in establishing the meaning of a sentence, things we\nwould expect a language understanding system to be able to do.\nMachine Translation\nFor a long time now, machine translation (MT) has been the holy grail of language\nunderstanding, ultimately seeking to provide high-quality, idiomatic translation be-\ntween any pair of languages. Its roots go back to the early days of the Cold War, when\nthe promise of automatic translation led to substantial government sponsorship, and\nwith it, the genesis of NLP itself.\nToday, practical translation systems exist for particular pairs of languages, and some\nare integrated into web search engines. However, these systems have some serious\nshortcomings. We can explore them with the help of NLTK\u2019s \u201cbabelizer\u201d (which is\nautomatically loaded when you import this chapter\u2019s materials using from nltk.book\nimport * ). This program submits a sentence for translation into a specified language,\n1.5  Automatic Natural Language Understanding | 29\nthen submits the resulting sentence for translation back into English. It stops after 12\niterations, or if it receives a translation that was produced already (indicating a loop):\n>>> babelize_shell()\nNLTK Babelizer: type 'help' for a list of commands.\nBabel> how long before the next flight to Alice Springs?\nBabel> german\nBabel> run\n0> how long before the next flight to Alice Springs?\n1> wie lang vor dem folgenden Flug zu Alice Springs?\n2> how long before the following flight to Alice jump?\n3> wie lang vor dem folgenden Flug zu Alice springen Sie?\n4> how long before the following flight to Alice do you jump?\n5> wie lang, bevor der folgende Flug zu Alice tun, Sie springen?\n6> how long, before the following flight to Alice does, do you jump?\n7> wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?\n8> how long before the following flight to Alice does, do you jump?\n9> wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?\n10> how long, before the following flight does to Alice, do do you jump?\n11> wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?\n12> how long before the following flight does leap to Alice, does you?\nObserve that \nthe system correctly translates Alice Springs  from English to German (in\nthe line starting 1>), but on the way back to English, this ends up as Alice jump\n(line 2). The preposition before is initially translated into the corresponding German\npreposition vor, but later into the conjunction bevor (line 5). After line 5 the sentences\nbecome non-sensical (but notice the various phrasings indicated by the commas, and\nthe change from jump to leap). The translation system did not recognize when a word\nwas part of a proper name, and it misinterpreted the grammatical structure. The gram-\nmatical problems are more obvious in the following example. Did John find the pig, or\ndid the pig find John?\n>>> babelize_shell()\nBabel> The pig that John found looked happy\nBabel> german\nBabel> run\n0> The pig that John found looked happy\n1> Das Schwein, das John fand, schaute gl?cklich\n2> The pig, which found John, looked happy\nMachine translation is difficult because a given word could have several possible trans-\nlations (depending on its meaning), and because word order must be changed in keep-\ning with the grammatical structure of the target language. Today these difficulties are\nbeing faced by collecting massive quantities of parallel texts from news and government\nwebsites that publish documents in two or more languages. Given a document in Ger-\nman and English, and possibly a bilingual dictionary, we can automatically pair up the\nsentences, a process called text alignment . Once we have a million or more sentence\npairs, we can detect corresponding words and phrases, and build a model that can be\nused for translating new text.\n30 | Chapter 1: \u2002Language Processing and Python\nSpoken Dialogue Systems\nIn the \nhistory of artificial intelligence, the chief measure of intelligence has been a lin-\nguistic one, namely the Turing Test : can a dialogue system, responding to a user\u2019s text\ninput, perform so naturally that we cannot distinguish it from a human-generated re-\nsponse? In contrast, today\u2019s commercial dialogue systems are very limited, but still\nperform useful functions in narrowly defined domains, as we see here:\nS: How may I help you?\nU: When is Saving Private Ryan playing?\nS: For what theater?\nU: The Paramount theater.\nS: Saving Private Ryan is not playing at the Paramount theater, but\nit\u2019s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.\nYou could not ask this system to provide driving instructions or details of nearby res-\ntaurants unless \nthe required information had already been stored and suitable question-\nanswer pairs had been incorporated into the language processing system.\nObserve that this system seems to understand the user\u2019s goals: the user asks when a\nmovie is showing and the system correctly determines from this that the user wants to\nsee the movie. This inference seems so obvious that you probably didn\u2019t notice it was\nmade, yet a natural language system needs to be endowed with this capability in order\nto interact naturally. Without it, when asked, Do you know when  Saving Private Ryan\nis playing?, a system might unhelpfully respond with a cold Yes. However, the devel-\nopers of commercial dialogue systems use contextual assumptions and business logic\nto ensure that the different ways in which a user might express requests or provide\ninformation are handled in a way that makes sense for the particular application. So,\nif you type When is ... , or I want to know when ... , or Can you tell me when ... , simple\nrules will always yield screening times. This is enough for the system to provide a useful\nservice.\nDialogue systems give us an opportunity to mention the commonly assumed pipeline\nfor NLP. Figure 1-5  shows the architecture of a simple dialogue system. Along the top\nof the diagram, moving from left to right, is a \u201cpipeline\u201d of some language understand-\ning components. These map from speech input via syntactic parsing to some kind of\nmeaning representation. Along the middle, moving from right to left, is the reverse\npipeline of components for converting concepts to speech. These components make\nup the dynamic aspects of the system. At the bottom of the diagram are some repre-\nsentative bodies of static information: the repositories of language-related data that the\nprocessing components draw on to do their work.\nYour Turn:  For an example of a primitive dialogue system, try having\na conversation with an NLTK chatbot. To see the available chatbots,\nrun nltk.chat.chatbots(). (Remember to import nltk first.)\n1.5  Automatic Natural Language Understanding | 31\nTextual Entailment\nThe challenge \nof language understanding has been brought into focus in recent years\nby a public \u201cshared task\u201d called Recognizing Textual Entailment (RTE). The basic\nscenario is simple. Suppose you want to find evidence to support the hypothesis: Sandra\nGoudie was defeated by Max Purnell, and that you have another short text that seems\nto be relevant, for example, Sandra Goudie was first elected to Parliament in the 2002\nelections, narrowly winning the seat of Coromandel by defeating Labour candidate Max\nPurnell and pushing incumbent Green MP Jeanette Fitzsimons into third place . Does the\ntext provide enough evidence for you to accept the hypothesis? In this particular case,\nthe answer will be \u201cNo.\u201d You can draw this conclusion easily, but it is very hard to\ncome up with automated methods for making the right decision. The RTE Challenges\nprovide data that allow competitors to develop their systems, but not enough data for\n\u201cbrute force\u201d machine learning techniques (a topic we will cover in Chapter 6 ). Con-\nsequently, some linguistic analysis is crucial. In the previous example, it is important\nfor the system to note that Sandra Goudie names the person being defeated in the\nhypothesis, not the person doing the defeating in the text. As another illustration of\nthe difficulty of the task, consider the following text-hypothesis pair:\n(7) a. Text: David Golinkin is the editor or author of 18 books, and over 150\nresponsa, articles, sermons and books\nb. Hypothesis: Golinkin has written 18 books\nFigure 1-5. Simple pipeline architecture for a spoken dialogue system: Spoken input (top left) is\nanalyzed, words \nare recognized, sentences are parsed and interpreted in context, application-specific\nactions take place (top right); a response is planned, realized as a syntactic structure, then to suitably\ninflected words, and finally to spoken output; different types of linguistic knowledge inform each stage\nof the process.\n32 | Chapter 1: \u2002Language Processing and Python\nIn order to determine whether the hypothesis is supported by the text, the system needs\nthe following \nbackground knowledge: (i) if someone is an author of a book, then he/\nshe has written that book; (ii) if someone is an editor of a book, then he/she has not\nwritten (all of) that book; (iii) if someone is editor or author of 18 books, then one\ncannot conclude that he/she is author of 18 books.\nLimitations of NLP\nDespite the research-led advances in tasks such as RTE, natural language systems that\nhave been deployed for real-world applications still cannot perform common-sense\nreasoning or draw on world knowledge in a general and robust manner. We can wait\nfor these difficult artificial intelligence problems to be solved, but in the meantime it is\nnecessary to live with some severe limitations on the reasoning and knowledge capa-\nbilities of natural language systems. Accordingly, right from the beginning, an impor-\ntant goal of NLP research has been to make progress on the difficult task of building\ntechnologies that \u201cunderstand language,\u201d using superficial yet powerful techniques\ninstead of unrestricted knowledge and reasoning capabilities. Indeed, this is one of the\ngoals of this book, and we hope to equip you with the knowledge and skills to build\nuseful NLP systems, and to contribute to the long-term aspiration of building intelligent\nmachines.\n1.6  Summary\n\u2022 Texts are represented in Python using lists: ['Monty', 'Python'] . We can use in-\ndexing, slicing, and the len() function on lists.\n\u2022 A word \u201ctoken\u201d is a particular appearance of a given word in a text; a word \u201ctype\u201d\nis the unique form of the word as a particular sequence of letters. We count word\ntokens using len(text) and word types using len(set(text)).\n\u2022 We obtain the vocabulary of a text t using sorted(set(t)).\n\u2022 We operate on each item of a text using [f(x) for x in text].\n\u2022 To derive the vocabulary, collapsing case distinctions and ignoring punctuation,\nwe can write set([w.lower() for w in text if w.isalpha()]).\n\u2022 We process each word in a text using a for statement, such as for w in t:  or for\nword in text:. This must be followed by the colon character and an indented block\nof code, to be executed each time through the loop.\n\u2022 We test a condition using an if statement: if len(word) < 5:. This must be fol-\nlowed by the colon character and an indented block of code, to be executed only\nif the condition is true.\n\u2022 A frequency distribution is a collection of items along with their frequency counts\n(e.g., the words of a text and their frequency of appearance).\n1.6  Summary | 33\n\u2022 A function is a block of code that has been assigned a name and can be reused.\nFunctions \nare defined using the def keyword, as in def mult(x, y); x and y are\nparameters of the function, and act as placeholders for actual data values.\n\u2022 A function is called by specifying its name followed by one or more arguments\ninside parentheses, like this: mult(3, 4), e.g., len(text1).\n1.7  Further Reading\nThis chapter \nhas introduced new concepts in programming, natural language process-\ning, and linguistics, all mixed in together. Many of them are consolidated in the fol-\nlowing chapters. However, you may also want to consult the online materials provided\nwith this chapter (at http://www.nltk.org/), including links to additional background\nmaterials, and links to online NLP systems. You may also like to read up on some\nlinguistics and NLP-related concepts in Wikipedia (e.g., collocations, the Turing Test,\nthe type-token distinction).\nYou should acquaint yourself with the Python documentation available at http://docs\n.python.org/, including the many tutorials and comprehensive reference materials\nlinked there. A Beginner\u2019s Guide to Python  is available at http://wiki.python.org/moin/\nBeginnersGuide. Miscellaneous questions about Python might be answered in the FAQ\nat http://www.python.org/doc/faq/general/.\nAs you delve into NLTK, you might want to subscribe to the mailing list where new\nreleases of the toolkit are announced. There is also an NLTK-Users mailing list, where\nusers help each other as they learn how to use Python and NLTK for language analysis\nwork. Details of these lists are available at http://www.nltk.org/.\nFor more information on the topics covered in Section 1.5, and on NLP more generally,\nyou might like to consult one of the following excellent books:\n\u2022 Indurkhya, Nitin and Fred Damerau (eds., 2010) Handbook of Natural Language\nProcessing (second edition), Chapman & Hall/CRC.\n\u2022 Jurafsky, Daniel and James Martin (2008) Speech and Language Processing  (second\nedition), Prentice Hall.\n\u2022 Mitkov, Ruslan (ed., 2002) The Oxford Handbook of Computational Linguistics .\nOxford University Press. (second edition expected in 2010).\nThe Association for Computational Linguistics is the international organization that\nrepresents the field of NLP. The ACL website  hosts many useful resources, including:\ninformation about international and regional conferences and workshops; the ACL\nWiki with links to hundreds of useful resources; and the ACL Anthology, which contains\nmost of the NLP research literature from the past 50 years, fully indexed and freely\ndownloadable.\n34 | Chapter 1: \u2002Language Processing and Python\nSome excellent introductory linguistics textbooks are: (Finegan, 2007), (O\u2019Grady et\nal., 2004), \n(OSU, 2007). You might like to consult LanguageLog, a popular linguistics\nblog with occasional posts that use the techniques described in this book.\n1.8  Exercises\n1.\u25cb Try using the Python interpreter as a calculator, and typing expressions like 12 /\n(4 + 1).\n2.\u25cb Given an alphabet of 26 letters, there are 26 to the power 10, or 26 ** 10, 10-\nletter strings we can form. That works out to 141167095653376L (the L at the end\njust indicates that this is Python\u2019s long-number format). How many hundred-letter\nstrings are possible?\n3.\u25cb The Python multiplication operation can be applied to lists. What happens when\nyou type ['Monty', 'Python'] * 20, or 3 * sent1?\n4.\u25cb Review Section 1.1  on computing with language. How many words are there in\ntext2? How many distinct words are there?\n5.\u25cb Compare the lexical diversity scores for humor and romance fiction in Ta-\nble 1-1. Which genre is more lexically diverse?\n6.\u25cb Produce a dispersion plot of the four main protagonists in Sense and Sensibility :\nElinor, Marianne, Edward, and Willoughby. What can you observe about the\ndifferent roles played by the males and females in this novel? Can you identify the\ncouples?\n7.\u25cb Find the collocations in text5.\n8.\u25cb Consider the following Python expression: len(set(text4)). State the purpose\nof this expression. Describe the two steps involved in performing this computation.\n9.\u25cb Review Section 1.2 on lists and strings.\na. Define a string and assign it to a variable, e.g., my_string = 'My String' (but\nput something more interesting in the string). Print the contents of this variable\nin two ways, first by simply typing the variable name and pressing Enter, then\nby using the print statement.\nb. Try adding the string to itself using my_string + my_string, or multiplying it\nby a number, e.g., my_string * 3. Notice that the strings are joined together\nwithout any spaces. How could you fix this?\n10.\u25cb Define a variable my_sent to be a list of words, using the syntax my_sent = [\"My\",\n\"sent\"] (but with your own words, or a favorite saying).\na. Use ' '.join(my_sent) to convert this into a string.\nb. Use split() to split the string back into the list form you had to start with.\n11.\u25cb Define several variables containing lists of words, e.g., phrase1, phrase2, and so\non. Join them together in various combinations (using the plus operator) to form\n1.8  Exercises | 35\nwhole sentences. What is the relationship between len(phrase1 + phrase2) and\nlen(phrase1) + len(phrase2)\n?\n12.\u25cb Consider the following two expressions, which have the same value. Which one\nwill typically be more relevant in NLP? Why?\na.\"Monty Python\"[6:12]\nb.[\"Monty\", \"Python\"][1]\n13.\u25cb We have seen how to represent a sentence as a list of words, where each word is\na sequence of characters. What does sent1[2][2] do? Why? Experiment with other\nindex values.\n14.\u25cb The first sentence of text3 is provided to you in the variable sent3. The index of\nthe in sent3 is 1, because sent3[1] gives us 'the'. What are the indexes of the two\nother occurrences of this word in sent3?\n15.\u25cb Review the discussion of conditionals in Section 1.4 . Find all words in the Chat\nCorpus (text5) starting with the letter b. Show them in alphabetical order.\n16.\u25cb Type the expression range(10) at the interpreter prompt. Now try range(10,\n20), range(10, 20, 2), and range(20, 10, -2). We will see a variety of uses for this\nbuilt-in function in later chapters.\n17.\u25d1 Use text9.index() to find the index of the word sunset. You\u2019ll need to insert this\nword as an argument between the parentheses. By a process of trial and error, find\nthe slice for the complete sentence that contains this word.\n18.\u25d1 Using list addition, and the set and sorted operations, compute the vocabulary\nof the sentences sent1 ... sent8.\n19.\u25d1 What is the difference between the following two lines? Which one will give a\nlarger value? Will this be the case for other texts?\n>>> sorted(set([w.lower() for w in text1]))\n>>> sorted([w.lower() for w in set(text1)])\n20.\u25d1 What is the difference between the following two tests: w.isupper() and not\nw.islower()?\n21.\u25d1 Write the slice expression that extracts the last two words of text2.\n22.\u25d1 Find all the four-letter words in the Chat Corpus ( text5). With the help of a\nfrequency distribution ( FreqDist), show these words in decreasing order of fre-\nquency.\n23.\u25d1 Review the discussion of looping with conditions in Section 1.4 . Use a combi-\nnation of for and if statements to loop over the words of the movie script for\nMonty Python and the Holy Grail  (text6) and print all the uppercase words, one\nper line.\n24.\u25d1 Write expressions for finding all words in text6 that meet the following condi-\ntions. The result should be in the form of a list of words: ['word1', 'word2', ...].\n36 | Chapter 1: \u2002Language Processing and Python\na. Ending in ize\nb. Containing the letter \nz\nc. Containing the sequence of letters pt\nd. All lowercase letters except for an initial capital (i.e., titlecase)\n25.\u25d1 Define sent to be the list of words ['she', 'sells', 'sea', 'shells', 'by',\n'the', 'sea', 'shore']. Now write code to perform the following tasks:\na. Print all words beginning with sh.\nb. Print all words longer than four characters\n26.\u25d1 What does the following Python code do? sum([len(w) for w in text1]) Can\nyou use it to work out the average word length of a text?\n27.\u25d1 Define a function called vocab_size(text) that has a single parameter for the\ntext, and which returns the vocabulary size of the text.\n28.\u25d1 Define a function percent(word, text) that calculates how often a given word\noccurs in a text and expresses the result as a percentage.\n29.\u25d1 We have been using sets to store vocabularies. Try the following Python expres-\nsion: set(sent3) < set(text1). Experiment with this using different arguments to\nset(). What does it do? Can you think of a practical application for this?\n1.8  Exercises | 37\n\nCHAPTER 2\nAccessing Text Corpora\nand Lexical Resources\nPractical work in Natural Language Processing typically uses large bodies of linguistic\ndata, or corpora. The goal of this chapter is to answer the following questions:\n1. What are \nsome useful text corpora and lexical resources, and how can we access\nthem with Python?\n2. Which Python constructs are most helpful for this work?\n3. How do we avoid repeating ourselves when writing Python code?\nThis chapter continues to present programming concepts by example, in the context\nof a linguistic processing task. We will wait until later before exploring each Python\nconstruct systematically. Don\u2019t worry if you see an example that contains something\nunfamiliar; simply try it out and see what it does, and\u2014if you\u2019re game\u2014modify it by\nsubstituting some part of the code with a different text or word. This way you will\nassociate a task with a programming idiom, and learn the hows and whys later.\n2.1  Accessing Text Corpora\nAs just mentioned, a text corpus is a large body of text. Many corpora are designed to\ncontain a careful balance of material in one or more genres. We examined some small\ntext collections in Chapter 1 , such as the speeches known as the US Presidential Inau-\ngural Addresses. This particular corpus actually contains dozens of individual texts\u2014\none per address\u2014but for convenience we glued them end-to-end and treated them as\na single text. Chapter 1  also used various predefined texts that we accessed by typing\nfrom book import *. However, since we want to be able to work with other texts, this\nsection examines a variety of text corpora. We\u2019ll see how to select individual texts, and\nhow to work with them.\n39\nGutenberg Corpus\nNLTK includes \na small selection of texts from the Project Gutenberg electronic text\narchive, which contains some 25,000 free electronic books, hosted at http://www.gu\ntenberg.org/. We begin by getting the Python interpreter to load the NLTK package,\nthen ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:\n>>> import nltk\n>>> nltk.corpus.gutenberg.fileids()\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n'shakespeare-macbeth.txt', 'whitman-leaves.txt']\nLet\u2019s pick out the first of these texts\u2014 Emma by Jane Austen\u2014and give it a short name,\nemma, then find out how many words it contains:\n>>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n>>> len(emma)\n192427\nIn Section 1.1 , we showed how you could carry out concordancing of a\ntext such as text1 with the command text1.concordance(). However,\nthis assumes that you are using one of the nine texts obtained as a result\nof doing from nltk.book import *. Now that you have started examining\ndata from nltk.corpus, as in the previous example, you have to employ\nthe following pair of statements to perform concordancing and other\ntasks from Section 1.1:\n>>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n>>> emma.concordance(\"surprize\")\nWhen we defined emma, we invoked the words() function of the gutenberg object in\nNLTK\u2019s corpus package. But since it is cumbersome to type such long names all the\ntime, Python provides another version of the import statement, as follows:\n>>> from nltk.corpus import gutenberg\n>>> gutenberg.fileids()\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n>>> emma = gutenberg.words('austen-emma.txt')\nLet\u2019s write a short program to display other information about each text, by looping\nover all the values of fileid corresponding to the gutenberg file identifiers listed earlier\nand then computing statistics for each text. For a compact output display, we will make\nsure that the numbers are all integers, using int().\n>>> for fileid in gutenberg.fileids():\n...     num_chars = len(gutenberg.raw(fileid)) \n...     num_words = len(gutenberg.words(fileid))\n...     num_sents = len(gutenberg.sents(fileid))\n40 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n...     num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n...     print int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), \n        fileid\n...\n4 21 26 austen-emma.txt\n4 23 16 austen-persuasion.txt\n4 24 22 austen-sense.txt\n4 33 79 bible-kjv.txt\n4 18 5 blake-poems.txt\n4 17 14 bryant-stories.txt\n4 17 12 burgess-busterbrown.txt\n4 16 12 carroll-alice.txt\n4 17 11 chesterton-ball.txt\n4 19 11 chesterton-brown.txt\n4 16 10 chesterton-thursday.txt\n4 18 24 edgeworth-parents.txt\n4 24 15 melville-moby_dick.txt\n4 52 10 milton-paradise.txt\n4 12 8 shakespeare-caesar.txt\n4 13 7 shakespeare-hamlet.txt\n4 13 6 shakespeare-macbeth.txt\n4 35 12 whitman-leaves.txt\nThis program \ndisplays three statistics for each text: average word length, average sen-\ntence length, and the number of times each vocabulary item appears in the text on\naverage (our lexical diversity score). Observe that average word length appears to be a\ngeneral property of English, since it has a recurrent value of 4. (In fact, the average word\nlength is really 3, not 4, since the num_chars variable counts space characters.) By con-\ntrast average sentence length and lexical diversity appear to be characteristics of par-\nticular authors.\nThe previous example also showed how we can access the \u201craw\u201d text of the book \n ,\nnot split \nup into tokens. The raw() function gives us the contents of the file without\nany linguistic processing. So, for example, len(gutenberg.raw('blake-poems.txt') tells\nus how many letters occur in the text, including the spaces between words. The\nsents() function divides the text up into its sentences, where each sentence is a list of\nwords:\n>>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n>>> macbeth_sentences\n[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',\n'1603', ']'], ['Actus', 'Primus', '.'], ...]\n>>> macbeth_sentences[1037]\n['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',\n'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']\n>>> longest_len = max([len(s) for s in macbeth_sentences])\n>>> [s for s in macbeth_sentences if len(s) == longest_len]\n[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',\n'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',\n'mercilesse', 'Macdonwald', ...], ...]\n2.1  Accessing Text Corpora | 41\nMost NLTK corpus readers include a variety of access methods apart\nfrom words(), raw(), and sents(). Richer linguistic content is available\nfrom some \ncorpora, such as part-of-speech tags, dialogue tags, syntactic\ntrees, and so forth; we will see these in later chapters.\nWeb and Chat Text\nAlthough Project Gutenberg contains thousands of books, it represents established\nliterature. It is important to consider less formal language as well. NLTK\u2019s small col-\nlection of web text includes content from a Firefox discussion forum, conversations\noverheard in New York, the movie script of Pirates of the Carribean , personal adver-\ntisements, and wine reviews:\n>>> from nltk.corpus import webtext\n>>> for fileid in webtext.fileids():\n...     print fileid, webtext.raw(fileid)[:65], '...'\n...\nfirefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se...\ngrail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...\noverheard.txt White guy: So, do you have any plans for this evening? Asian girl...\npirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr...\nsingles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...\nwine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...\nThere is also a corpus of instant messaging chat sessions, originally collected by the\nNaval Postgraduate School for research on automatic detection of Internet predators.\nThe corpus contains over 10,000 posts, anonymized by replacing usernames with\ngeneric names of the form \u201cUserNNN\u201d, and manually edited to remove any other\nidentifying information. The corpus is organized into 15 files, where each file contains\nseveral hundred posts collected on a given date, for an age-specific chatroom (teens,\n20s, 30s, 40s, plus a generic adults chatroom). The filename contains the date, chat-\nroom, and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered\nfrom the 20s chat room on 10/19/2006.\n>>> from nltk.corpus import nps_chat\n>>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n>>> chatroom[123]\n['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',\n'I', 'can', 'look', 'in', 'a', 'mirror', '.']\nBrown Corpus\nThe Brown Corpus was the first million-word electronic corpus of English, created in\n1961 at Brown University. This corpus contains text from 500 sources, and the sources\nhave been categorized by genre, such as news, editorial, and so on. Table 2-1  gives an\nexample of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n42 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nTable 2-1. Example document for each section of the Brown Corpus\nID File Genre Description\nA16 ca16 news Chicago Tribune: Society Reportage\nB02 cb02 editorial Christian Science Monitor: Editorials\nC17 cc17 reviews Time Magazine: Reviews\nD12 cd12 religion Underwood: Probing the Ethics of Realtors\nE36 ce36 hobbies Norling: Renting a Car in Europe\nF25 cf25 lore Boroff: Jewish Teenage Culture\nG22 cg22 belles_lettres Reiner: Coping with Runaway Technology\nH15 ch15 government US Office of Civil and Defence Mobilization: The Family Fallout Shelter\nJ17 cj19 learned Mosteller: Probability with Statistical Applications\nK04 ck04 fiction W.E.B. Du Bois: Worlds of Color\nL13 cl13 mystery Hitchens: Footsteps in the Night\nM01 cm01 science_fiction Heinlein: Stranger in a Strange Land\nN14 cn15 adventure Field: Rattlesnake Ridge\nP12 cp12 romance Callaghan: A Passion in Rome\nR06 cr06 humor Thurber: The Future, If Any, of Comedy\nWe can access the corpus as a list of words or a list of sentences (where each sentence\nis itself \njust a list of words). We can optionally specify particular categories or files to\nread:\n>>> from nltk.corpus import brown\n>>> brown.categories()\n['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n'science_fiction']\n>>> brown.words(categories='news')\n['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n>>> brown.words(fileids=['cg22'])\n['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n>>> brown.sents(categories=['news', 'editorial', 'reviews'])\n[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\nThe Brown Corpus is a convenient resource for studying systematic differences between\ngenres, a kind of linguistic inquiry known as stylistics. Let\u2019s compare genres in their\nusage of modal verbs. The first step is to produce the counts for a particular genre.\nRemember to import nltk before doing the following:\n>>> from nltk.corpus import brown\n>>> news_text = brown.words(categories='news')\n>>> fdist = nltk.FreqDist([w.lower() for w in news_text])\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> for m in modals:\n...     print m + ':', fdist[m],\n2.1  Accessing Text Corpora | 43\n...\ncan: 94 could: 87 may: 93 might: 38 must: 53 will: 389\nYour Turn:  Choose a different section of the Brown Corpus, and adapt\nthe preceding example to count a selection of wh words, such as what,\nwhen, where, who and why.\nNext, we need to obtain counts for each genre of interest. We\u2019ll use NLTK\u2019s support\nfor conditional frequency distributions. These are presented systematically in Sec-\ntion 2.2, where we also unpick the following code line by line. For the moment, you\ncan ignore the details and just concentrate on the output.\n>>> cfd = nltk.ConditionalFreqDist(\n...           (genre, word)\n...           for genre in brown.categories()\n...           for word in brown.words(categories=genre))\n>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> cfd.tabulate(conditions=genres, samples=modals)\n                 can could  may might must will\n           news   93   86   66   38   50  389\n       religion   82   59   78   12   54   71\n        hobbies  268   58  131   22   83  264\nscience_fiction   16   49    4   12    8   16\n        romance   74  193   11   51   45   43\n          humor   16   30    8    8    9   13\nObserve that the most frequent modal in the news genre is will, while the most frequent\nmodal in the romance genre is could. Would you have predicted this? The idea that\nword counts might distinguish genres will be taken up again in Chapter 6.\nReuters Corpus\nThe Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The\ndocuments have been classified into 90 topics, and grouped into two sets, called \u201ctrain-\ning\u201d and \u201ctest\u201d; thus, the text with fileid 'test/14826' is a document drawn from the\ntest set. This split is for training and testing algorithms that automatically detect the\ntopic of a document, as we will see in Chapter 6.\n>>> from nltk.corpus import reuters\n>>> reuters.fileids()\n['test/14826', 'test/14828', 'test/14829', 'test/14832', ...]\n>>> reuters.categories()\n['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\nUnlike the Brown Corpus, categories in the Reuters Corpus overlap with each other,\nsimply because a news story often covers multiple topics. We can ask for the topics\n44 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\ncovered by one or more documents, or for the documents included in one or more\ncategories. For \nconvenience, the corpus methods accept a single fileid or a list of fileids.\n>>> reuters.categories('training/9865')\n['barley', 'corn', 'grain', 'wheat']\n>>> reuters.categories(['training/9865', 'training/9880'])\n['barley', 'corn', 'grain', 'money-fx', 'wheat']\n>>> reuters.fileids('barley')\n['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n>>> reuters.fileids(['barley', 'corn'])\n['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',\n'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]\nSimilarly, we can specify the words or sentences we want in terms of files or categories.\nThe first handful of words in each of these texts are the titles, which by convention are\nstored as uppercase.\n>>> reuters.words('training/9865')[:14]\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',\n'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export']\n>>> reuters.words(['training/9865', 'training/9880'])\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n>>> reuters.words(categories='barley')\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n>>> reuters.words(categories=['barley', 'corn'])\n['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]\nInaugural Address Corpus\nIn Section 1.1, we looked at the Inaugural Address Corpus, but treated it as a single\ntext. The graph in Figure 1-2  used \u201cword offset\u201d as one of the axes; this is the numerical\nindex of the word in the corpus, counting from the first word of the first address.\nHowever, the corpus is actually a collection of 55 texts, one for each presidential ad-\ndress. An interesting property of this collection is its time dimension:\n>>> from nltk.corpus import inaugural\n>>> inaugural.fileids()\n['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n>>> [fileid[:4] for fileid in inaugural.fileids()]\n['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]\nNotice that the year of each text appears in its filename. To get the year out of the\nfilename, we extracted the first four characters, using fileid[:4].\nLet\u2019s look at how the words America and citizen are used over time. The following code\nconverts the words in the Inaugural corpus to lowercase using w.lower() \n , then checks\nwhether they \nstart with either of the \u201ctargets\u201d america or citizen using startswith()\n. Thus it will count words such as American\u2019s and Citizens. We\u2019ll learn about condi-\ntional frequency distributions in Section 2.2 ; for now, just consider the output, shown\nin Figure 2-1.\n2.1  Accessing Text Corpora | 45\n>>> cfd = nltk.ConditionalFreqDist(\n...           (target, file[:4])\n...           for fileid in inaugural.fileids()\n...           for w in inaugural.words(fileid)\n...           for target in ['america', 'citizen']\n...           if w.lower().startswith(target)) \n>>> cfd.plot()\nFigure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus\nthat begin \nwith america or citizen are counted; separate counts are kept for each address; these are\nplotted so that trends in usage over time can be observed; counts are not normalized for document\nlength.\nAnnotated Text Corpora\nMany text corpora contain linguistic annotations, representing part-of-speech tags,\nnamed entities, syntactic structures, semantic roles, and so forth. NLTK provides\nconvenient ways to access several of these corpora, and has data packages containing\ncorpora and corpus samples, freely downloadable for use in teaching and research.\nTable 2-2   lists some of the corpora. For information about downloading them, see\nhttp://www.nltk.org/data. For more examples of how to access NLTK corpora, please\nconsult the Corpus HOWTO at http://www.nltk.org/howto.\nTable 2-2. Some of the corpora and corpus samples distributed with NLTK\nCorpus Compiler Contents\nBrown Corpus Francis, Kucera 15 genres, 1.15M words, tagged, categorized\nCESS Treebanks CLiC-UB 1M words, tagged and parsed (Catalan, Spanish)\nChat-80 Data Files Pereira & Warren World Geographic Database\nCMU Pronouncing Dictionary CMU 127k entries\nCoNLL 2000 Chunking Data CoNLL 270k words, tagged and chunked\n46 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nCorpus Compiler Contents\nCoNLL 2002 Named Entity CoNLL 700k words, POS and named entity tagged (Dutch, Spanish)\nCoNLL 2007 Dependency Parsed Tree-\nbanks (selections)CoNLL 150k words, dependency parsed (Basque, Catalan)\nDependency Treebank Narad Dependency parsed version of Penn Treebank sample\nFloresta Treebank Diana Santos et al. 9k sentences, tagged and parsed (Portuguese)\nGazetteer Lists Various Lists of cities and countries\nGenesis Corpus Misc web sources 6 texts, 200k words, 6 languages\nGutenberg (selections) Hart, Newby, et al. 18 texts, 2M words\nInaugural Address Corpus CSpan U.S. Presidential Inaugural Addresses (1789\u2013present)\nIndian POS Tagged Corpus Kumaran et al. 60k words, tagged (Bangla, Hindi, Marathi, Telugu)\nMacMorpho Corpus NILC, USP, Brazil 1M words, tagged (Brazilian Portuguese)\nMovie Reviews Pang, Lee 2k movie reviews with sentiment polarity classification\nNames Corpus Kantrowitz, Ross 8k male and female names\nNIST 1999 Info Extr (selections) Garofolo 63k words, newswire and named entity SGML markup\nNPS Chat Corpus Forsyth, Martell 10k IM chat posts, POS and dialogue-act tagged\nPenn Treebank (selections) LDC 40k words, tagged and parsed\nPP Attachment Corpus Ratnaparkhi 28k prepositional phrases, tagged as noun or verb modifiers\nProposition Bank Palmer 113k propositions, 3,300 verb frames\nQuestion Classification Li, Roth 6k questions, categorized\nReuters Corpus Reuters 1.3M words, 10k news documents, categorized\nRoget\u2019s Thesaurus Project Gutenberg 200k words, formatted text\nRTE Textual Entailment Dagan et al. 8k sentence pairs, categorized\nSEMCOR Rus, Mihalcea 880k words, POS and sense tagged\nSenseval 2 Corpus Pedersen 600k words, POS and sense tagged\nShakespeare texts (selections) Bosak 8 books in XML format\nState of the Union Corpus CSpan 485k words, formatted text\nStopwords Corpus Porter et al. 2,400 stopwords for 11 languages\nSwadesh Corpus Wiktionary Comparative wordlists in 24 languages\nSwitchboard Corpus (selections) LDC 36 phone calls, transcribed, parsed\nTIMIT Corpus (selections) NIST/LDC Audio files and transcripts for 16 speakers\nUniv Decl of Human Rights United Nations 480k words, 300+ languages\nVerbNet 2.1 Palmer et al. 5k verbs, hierarchically organized, linked to WordNet\nWordlist Corpus OpenOffice.org et al. 960k words and 20k affixes for 8 languages\nWordNet 3.0 (English) Miller, Fellbaum 145k synonym sets\n2.1  Accessing Text Corpora | 47\nCorpora in Other Languages\nNLTK comes \nwith corpora for many languages, though in some cases you will need to\nlearn how to manipulate character encodings in Python before using these corpora (see\nSection 3.3).\n>>> nltk.corpus.cess_esp.words()\n['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]\n>>> nltk.corpus.floresta.words()\n['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n>>> nltk.corpus.indian.words('hindi.pos')\n['\\xe0\\xa4\\xaa\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa3',\n'\\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa4\\xbf\\xe0\\xa4\\xac\\xe0\\xa4\n\\x82\\xe0\\xa4\\xa7', ...]\n>>> nltk.corpus.udhr.fileids()\n['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]\n>>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]\n[u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]\nThe last of these corpora, udhr, contains the Universal Declaration of Human Rights\nin over 300 languages. The fileids for this corpus include information about the char-\nacter encoding used in the file, such as UTF8 or Latin1. Let\u2019s use a conditional frequency\ndistribution to examine the differences in word lengths for a selection of languages\nincluded in the udhr corpus. The output is shown in Figure 2-2  (run the program your-\nself to see a color plot). Note that True and False are Python\u2019s built-in Boolean values.\n>>> from nltk.corpus import udhr\n>>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n>>> cfd = nltk.ConditionalFreqDist(\n...           (lang, len(word))\n...           for lang in languages\n...           for word in udhr.words(lang + '-Latin1'))\n>>> cfd.plot(cumulative=True)\nYour Turn:  Pick a language of interest in udhr.fileids(), and define a\nvariable raw_text = udhr.raw(Language-Latin1). Now plot a frequency\ndistribution of the letters of the text using\nnltk.FreqDist(raw_text).plot().\nUnfortunately, for many languages, substantial corpora are not yet available. Often\nthere is insufficient government or industrial support for developing language resour-\nces, and individual efforts are piecemeal and hard to discover or reuse. Some languages\nhave no established writing system, or are endangered. (See Section 2.7  for suggestions\non how to locate language resources.)\n48 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nText Corpus Structure\nWe have \nseen a variety of corpus structures so far; these are summarized in Fig-\nure 2-3. The simplest kind lacks any structure: it is just a collection of texts. Often,\ntexts are grouped into categories that might correspond to genre, source, author, lan-\nguage, etc. Sometimes these categories overlap, notably in the case of topical categories,\nas a text can be relevant to more than one topic. Occasionally, text collections have\ntemporal structure, news collections being the most common example.\nNLTK\u2019s corpus readers support efficient access to a variety of corpora, and can be used\nto work with new corpora. Table 2-3 lists functionality provided by the corpus readers.\nFigure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of\nHuman Rights \nare processed; this graph shows that words having five or fewer letters account for\nabout 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\n2.1  Accessing Text Corpora | 49\nFigure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated\ntexts with \nno particular organization; some corpora are structured into categories, such as genre\n(Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other\ncorpora represent language use over time (Inaugural Address Corpus).\nTable 2-3. Basic corpus functionality defined in NLTK: More documentation can be found using\nhelp(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto.\nExample Description\nfileids() The files of the corpus\nfileids([categories]) The files of the corpus corresponding to these categories\ncategories() The categories of the corpus\ncategories([fileids]) The categories of the corpus corresponding to these files\nraw() The raw content of the corpus\nraw(fileids=[f1,f2,f3]) The raw content of the specified files\nraw(categories=[c1,c2]) The raw content of the specified categories\nwords() The words of the whole corpus\nwords(fileids=[f1,f2,f3]) The words of the specified fileids\nwords(categories=[c1,c2]) The words of the specified categories\nsents() The sentences of the specified categories\nsents(fileids=[f1,f2,f3]) The sentences of the specified fileids\nsents(categories=[c1,c2]) The sentences of the specified categories\nabspath(fileid) The location of the given file on disk\nencoding(fileid) The encoding of the file (if known)\nopen(fileid) Open a stream for reading the given corpus file\nroot() The path to the root of locally installed corpus\nreadme() The contents of the README file of the corpus\nWe illustrate the difference between some of the corpus access methods here:\n>>> raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n>>> raw[1:20]\n'The Adventures of B'\n>>> words = gutenberg.words(\"burgess-busterbrown.txt\")\n>>> words[1:20]\n50 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',\n'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',\n'Bear']\n>>> sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n>>> sents[1:20]\n[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as',\n'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched',\n'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', ...], ...]\nLoading Your Own Corpus\nIf you \nhave a your own collection of text files that you would like to access using the\nmethods discussed earlier, you can easily load them with the help of NLTK\u2019s Plain\ntextCorpusReader. Check the location of your files on your file system; in the following\nexample, we have taken this to be the directory /usr/share/dict. Whatever the location,\nset this to be the value of corpus_root \n . The second parameter of the PlaintextCor\npusReader initializer \n  can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern\nthat matches all fileids, like '[abc]/.*\\.txt' (see Section 3.4 for information about\nregular expressions).\n>>> from nltk.corpus import PlaintextCorpusReader\n>>> corpus_root = '/usr/share/dict' \n>>> wordlists = PlaintextCorpusReader(corpus_root, '.*') \n>>> wordlists.fileids()\n['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']\n>>> wordlists.words('connectives')\n['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]\nAs another \nexample, suppose you have your own local copy of Penn Treebank (release\n3), in C:\\corpora. We can use the BracketParseCorpusReader to access this corpus. We\nspecify the corpus_root to be the location of the parsed Wall Street Journal  component\nof the corpus \n , and give a file_pattern that matches the files contained within its\nsubfolders \n  (using forward slashes).\n>>> from nltk.corpus import BracketParseCorpusReader\n>>> corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\" \n>>> file_pattern = r\".*/wsj_.*\\.mrg\" \n>>> ptb = BracketParseCorpusReader(corpus_root, file_pattern)\n>>> ptb.fileids()\n['00/wsj_0001.mrg', '00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...]\n>>> len(ptb.sents())\n49208\n>>> ptb.sents(fileids='20/wsj_2013.mrg')[19]\n['The', '55-year-old', 'Mr.', 'Noriega', 'is', \"n't\", 'as', 'smooth', 'as', 'the',\n'shah', 'of', 'Iran', ',', 'as', 'well-born', 'as', 'Nicaragua', \"'s\", 'Anastasio',\n'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 'Philippines',\n'or', 'as', 'bloody', 'as', 'Haiti', \"'s\", 'Baby', Doc', 'Duvalier', '.']\n2.1  Accessing Text Corpora | 51\n2.2  Conditional Frequency Distributions\nWe introduced \nfrequency distributions in Section 1.3 . We saw that given some list\nmylist of words or other items, FreqDist(mylist) would compute the number of\noccurrences of each item in the list. Here we will generalize this idea.\nWhen the texts of a corpus are divided into several categories (by genre, topic, author,\netc.), we can maintain separate frequency distributions for each category. This will\nallow us to study systematic differences between the categories. In the previous section,\nwe achieved this using NLTK\u2019s ConditionalFreqDist data type. A conditional fre-\nquency distribution is a collection of frequency distributions, each one for a different\n\u201ccondition.\u201d The condition will often be the category of the text. Figure 2-4  depicts a\nfragment of a conditional frequency distribution having just two conditions, one for\nnews text and one for romance text.\nFigure 2-4. Counting words appearing in a text collection (a conditional frequency distribution).\nConditions and Events\nA frequency \ndistribution counts observable events, such as the appearance of words in\na text. A conditional frequency distribution needs to pair each event with a condition.\nSo instead of processing a sequence of words \n , we have to process a sequence of\npairs \n :\n>>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...] \n>>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...] \nEach pair has the form (condition, event). If we were processing the entire Brown\nCorpus by genre, there would be 15 conditions (one per genre) and 1,161,192 events\n(one per word).\nCounting Words by Genre\nIn Section 2.1 , we saw a conditional frequency distribution where the condition was\nthe section of the Brown Corpus, and for each condition we counted words. Whereas\nFreqDist() takes a simple list as input, ConditionalFreqDist() takes a list of pairs.\n52 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n>>> from nltk.corpus import brown\n>>> cfd = nltk.ConditionalFreqDist(\n...           (genre, word)\n...           for genre in brown.categories()\n...           for word in brown.words(categories=genre))\nLet\u2019s break \nthis down, and look at just two genres, news and romance. For each genre \n ,\nwe loop \nover every word in the genre \n , producing pairs consisting of the genre and\nthe word \n :\n>>> genre_word = [(genre, word) \n...               for genre in ['news', 'romance'] \n...               for word in brown.words(categories=genre)] \n>>> len(genre_word)\n170576\nSo, as \nwe can see in the following code, pairs at the beginning of the list genre_word will\nbe of the form ( 'news', word) \n , whereas those at the end will be of the form ( 'roman\nce', word) \n .\n>>> genre_word[:4]\n[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')] \n>>> genre_word[-4:]\n[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')] \nWe can now use this list of pairs to create a ConditionalFreqDist, and save it in a variable\ncfd. As usual, we can type the name of the variable to inspect it \n , and verify it has two\nconditions \n :\n>>> cfd = nltk.ConditionalFreqDist(genre_word)\n>>> cfd \n<ConditionalFreqDist with 2 conditions>\n>>> cfd.conditions()\n['news', 'romance'] \nLet\u2019s access the two conditions, and satisfy ourselves that each is just a frequency\ndistribution:\n>>> cfd['news']\n<FreqDist with 100554 outcomes>\n>>> cfd['romance']\n<FreqDist with 70022 outcomes>\n>>> list(cfd['romance'])\n[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had',\n'?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him',\n'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', ...]\n>>> cfd['romance']['could']\n193\nPlotting and Tabulating Distributions\nApart from \ncombining two or more frequency distributions, and being easy to initialize,\na ConditionalFreqDist provides some useful methods for tabulation and plotting.\n2.2  Conditional Frequency Distributions | 53\nThe plot in Figure 2-1  was based on a conditional frequency distribution reproduced\nin the following code. The condition is either of the words america or citizen \n , and\nthe counts \nbeing plotted are the number of times the word occurred in a particular\nspeech. It exploits the fact that the filename for each speech\u2014for example,\n1865-Lincoln.txt\u2014contains the year as the first four characters \n . This code generates\nthe pair ('\namerica', '1865') for every instance of a word whose lowercased form starts\nwith america\u2014such as Americans\u2014in the file 1865-Lincoln.txt.\n>>> from nltk.corpus import inaugural\n>>> cfd = nltk.ConditionalFreqDist(\n...           (target, fileid[:4]) \n...           for fileid in inaugural.fileids()\n...           for w in inaugural.words(fileid)\n...           for target in ['america', 'citizen'] \n...           if w.lower().startswith(target))\nThe plot \nin Figure 2-2  was also based on a conditional frequency distribution, repro-\nduced in the following code. This time, the condition is the name of the language, and\nthe counts being plotted are derived from word lengths \n . It exploits the fact that the\nfilename for each language is the language name followed by '-Latin1' (the character\nencoding).\n>>> from nltk.corpus import udhr\n>>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n>>> cfd = nltk.ConditionalFreqDist(\n...           (lang, len(word)) \n...           for lang in languages\n...           for word in udhr.words(lang + '-Latin1'))\nIn the plot() and tabulate() methods, \nwe can optionally specify which conditions to\ndisplay with a conditions= parameter. When we omit it, we get all the conditions.\nSimilarly, we can limit the samples to display with a samples= parameter. This makes\nit possible to load a large quantity of data into a conditional frequency distribution,\nand then to explore it by plotting or tabulating selected conditions and samples. It also\ngives us full control over the order of conditions and samples in any displays. For ex-\nample, we can tabulate the cumulative frequency data just for two languages, and for\nwords less than 10 characters long, as shown next. We interpret the last cell on the top\nrow to mean that 1,638 words of the English text have nine or fewer letters.\n>>> cfd.tabulate(conditions=['English', 'German_Deutsch'],\n...              samples=range(10), cumulative=True)\n                  0    1    2    3    4    5    6    7    8    9\n       English    0  185  525  883  997 1166 1283 1440 1558 1638\nGerman_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275\n54 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nYour Turn: Working with the news and romance genres from the\nBrown Corpus, find out which days of the week are most newsworthy,\nand which are most romantic. Define a variable called days containing\na list of days of the week, i.e., ['Monday', ...]. Now tabulate the counts\nfor these words using cfd.tabulate(samples=days). Now try the same\nthing using plot in place of tabulate. You may control the output order\nof days with the help of an extra parameter: condi\ntions=['Monday', ...].\nYou may have noticed that the multiline expressions we have been using with condi-\ntional frequency distributions look like list comprehensions, but without the brackets.\nIn general, when we use a list comprehension as a parameter to a function, like\nset([w.lower for w in t]), we are permitted to omit the square brackets and just write\nset(w.lower() for w in t). (See the discussion of \u201cgenerator expressions\u201d in Sec-\ntion 4.2 for more about this.)\nGenerating Random Text with Bigrams\nWe can use a conditional frequency distribution to create a table of bigrams (word\npairs, introduced in Section 1.3 ). The bigrams() function takes a list of words and builds\na list of consecutive word pairs:\n>>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n...   'and', 'the', 'earth', '.']\n>>> nltk.bigrams(sent)\n[('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),\n('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),\n('the', 'earth'), ('earth', '.')]\nIn Example 2-1, we treat each word as a condition, and for each one we effectively\ncreate a frequency distribution over the following words. The function gener\nate_model() contains a simple loop to generate text. When we call the function, we\nchoose a word (such as 'living') as our initial context. Then, once inside the loop, we\nprint the current value of the variable word, and reset word to be the most likely token\nin that context (using max()); next time through the loop, we use that word as our new\ncontext. As you can see by inspecting the output, this simple approach to text gener-\nation tends to get stuck in loops. Another method would be to randomly choose the\nnext word from among the available words.\nExample 2-1. Generating random text: This program obtains all bigrams from the text of the book\nof Genesis, then constructs a conditional frequency distribution to record which words are most likely\nto follow a given word; e.g., after the word living, the most likely word is creature; the\ngenerate_model()  function uses this data, and a seed word, to generate random text.\ndef generate_model(cfdist, word, num=15):\n    for i in range(num):\n        print word,\n        word = cfdist[word].max()\n2.2  Conditional Frequency Distributions | 55\ntext = nltk.corpus.genesis.words('english-kjv.txt')\nbigrams = nltk.bigrams(text)\ncfd = nltk.ConditionalFreqDist(bigrams) \n>>> print cfd['living']\n<FreqDist: 'creature': 7, 'thing': 4, 'substance': 2, ',': 1, '.': 1, 'soul': 1>\n>>> generate_model(cfd, 'living')\nliving creature that he said , and the land of the land of the land\nConditional frequency \ndistributions are a useful data structure for many NLP tasks.\nTheir commonly used methods are summarized in Table 2-4.\nTable 2-4. NLTK\u2019s conditional frequency distributions: Commonly used methods and idioms for\ndefining, accessing, and visualizing a conditional frequency distribution of counters\nExample Description\ncfdist = ConditionalFreqDist(pairs) Create a conditional frequency distribution from a list of pairs\ncfdist.conditions() Alphabetically sorted list of conditions\ncfdist[condition] The frequency distribution for this condition\ncfdist[condition][sample] Frequency for the given sample for this condition\ncfdist.tabulate() Tabulate the conditional frequency distribution\ncfdist.tabulate(samples, conditions) Tabulation limited to the specified samples and conditions\ncfdist.plot() Graphical plot of the conditional frequency distribution\ncfdist.plot(samples, conditions) Graphical plot limited to the specified samples and conditions\ncfdist1 < cfdist2 Test if samples in cfdist1 occur less frequently than in cfdist2\n2.3  More Python: Reusing Code\nBy this \ntime you\u2019ve probably typed and retyped a lot of code in the Python interactive\ninterpreter. If you mess up when retyping a complex example, you have to enter it again.\nUsing the arrow keys to access and modify previous commands is helpful but only goes\nso far. In this section, we see two important ways to reuse code: text editors and Python\nfunctions.\nCreating Programs with a Text Editor\nThe Python interactive interpreter performs your instructions as soon as you type them.\nOften, it is better to compose a multiline program using a text editor, then ask Python\nto run the whole program at once. Using IDLE, you can do this by going to the File\nmenu and opening a new window. Try this now, and enter the following one-line\nprogram:\nprint 'Monty Python'\n56 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nSave this program in a file called monty.py, then go to the Run menu and select the\ncommand Run Module. (We\u2019ll learn what modules are shortly.) The result in the main\nIDLE window should look like this:\n>>> ================================ RESTART ================================\n>>>\nMonty Python\n>>>\nYou can also type from monty import * and it will do the same thing.\nFrom now on, you have a choice of using the interactive interpreter or a text editor to\ncreate your programs. It is often convenient to test your ideas using the interpreter,\nrevising a line of code until it does what you expect. Once you\u2019re ready, you can paste\nthe code (minus any >>> or ... prompts) into the text editor, continue to expand it,\nand finally save the program in a file so that you don\u2019t have to type it in again later.\nGive the file a short but descriptive name, using all lowercase letters and separating\nwords with underscore, and using the .py filename extension, e.g., monty_python.py.\nImportant: Our inline code examples include the >>> and ... prompts\nas if we are interacting directly with the interpreter. As they get more\ncomplicated, you should instead type them into the editor, without the\nprompts, and run them from the editor as shown earlier. When we pro-\nvide longer programs in this book, we will leave out the prompts to\nremind you to type them into a file rather than using the interpreter.\nYou can see this already in Example 2-1 . Note that the example still\nincludes a couple of lines with the Python prompt; this is the interactive\npart of the task where you inspect some data and invoke a function.\nRemember that all code samples like Example 2-1  are downloadable\nfrom http://www.nltk.org/.\nFunctions\nSuppose that you work on analyzing text that involves different forms of the same word,\nand that part of your program needs to work out the plural form of a given singular\nnoun. Suppose it needs to do this work in two places, once when it is processing some\ntexts and again when it is processing user input.\nRather than repeating the same code several times over, it is more efficient and reliable\nto localize this work inside a function. A function is just a named block of code that\nperforms some well-defined task, as we saw in Section 1.1. A function is usually defined\nto take some inputs, using special variables known as parameters, and it may produce\na result, also known as a return value. We define a function using the keyword def\nfollowed by the function name and any input parameters, followed by the body of the\nfunction. Here\u2019s the function we saw in Section 1.1 (including the import statement\nthat makes division behave as expected):\n2.3  More Python: Reusing Code | 57\n>>> from __future__ import division\n>>> def lexical_diversity(text):\n...     return len(text) / len(set(text))\nWe use \nthe keyword return to indicate the value that is produced as output by the\nfunction. In this example, all the work of the function is done in the return statement.\nHere\u2019s an equivalent definition that does the same work using multiple lines of code.\nWe\u2019ll change the parameter name from text to my_text_data to remind you that this is\nan arbitrary choice:\n>>> def lexical_diversity(my_text_data):\n...     word_count = len(my_text_data)\n...     vocab_size = len(set(my_text_data))\n...     diversity_score = word_count / vocab_size\n...     return diversity_score\nNotice that we\u2019ve created some new variables inside the body of the function. These\nare local variables  and are not accessible outside the function. So now we have defined\na function with the name lexical_diversity. But just defining it won\u2019t produce any\noutput! Functions do nothing until they are \u201ccalled\u201d (or \u201cinvoked\u201d).\nLet\u2019s return to our earlier scenario, and actually define a simple function to work out\nEnglish plurals. The function plural() in Example 2-2  takes a singular noun and gen-\nerates a plural form, though it is not always correct. (We\u2019ll discuss functions at greater\nlength in Section 4.4.)\nExample 2-2. A Python function: This function tries to work out the plural form of any English noun;\nthe keyword def (define) is followed by the function name, then a parameter inside parentheses, and\na colon; the body of the function is the indented block of code; it tries to recognize patterns within the\nword and process the word accordingly; e.g., if the word ends with y, delete the y and add ies.\ndef plural(word):\n    if word.endswith('y'):\n        return word[:-1] + 'ies'\n    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n        return word + 'es'\n    elif word.endswith('an'):\n        return word[:-2] + 'en'\n    else:\n        return word + 's'\n>>> plural('fairy')\n'fairies'\n>>> plural('woman')\n'women'\nThe endswith() function is always associated with a string object (e.g., word in Exam-\nple 2-2 ). To call such functions, we give the name of the object, a period, and then the\nname of the function. These functions are usually known as methods.\n58 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nModules\nOver time \nyou will find that you create a variety of useful little text-processing functions,\nand you end up copying them from old programs to new ones. Which file contains the\nlatest version of the function you want to use? It makes life a lot easier if you can collect\nyour work into a single place, and access previously defined functions without making\ncopies.\nTo do this, save your function(s) in a file called (say) textproc.py. Now, you can access\nyour work simply by importing it from the file:\n>>> from textproc import plural\n>>> plural('wish')\nwishes\n>>> plural('fan')\nfen\nOur plural function obviously has an error, since the plural of fan is fans. Instead of\ntyping in a new version of the function, we can simply edit the existing one. Thus, at\nevery stage, there is only one version of our plural function, and no confusion about\nwhich one is being used.\nA collection of variable and function definitions in a file is called a Python module. A\ncollection of related modules is called a package. NLTK\u2019s code for processing the\nBrown Corpus is an example of a module, and its collection of code for processing all\nthe different corpora is an example of a package. NLTK itself is a set of packages,\nsometimes called a library.\nCaution!\nIf you \nare creating a file to contain some of your Python code, do not\nname your file nltk.py: it may get imported in place of the \u201creal\u201d NLTK\npackage. When it imports modules, Python first looks in the current\ndirectory (folder).\n2.4  Lexical Resources\nA lexicon, or lexical resource, is a collection of words and/or phrases along with asso-\nciated information, such as part-of-speech and sense definitions. Lexical resources are\nsecondary to texts, and are usually created and enriched with the help of texts. For\nexample, if we have defined a text my_text, then vocab = sorted(set(my_text))  builds\nthe vocabulary of my_text, whereas word_freq = FreqDist(my_text) counts the fre-\nquency of each word in the text. Both vocab and word_freq are simple lexical resources.\nSimilarly, a concordance like the one we saw in Section 1.1  gives us information about\nword usage that might help in the preparation of a dictionary. Standard terminology\nfor lexicons is illustrated in Figure 2-5 . A lexical entry  consists of a headword (also\nknown as a lemma) along with additional information, such as the part-of-speech and\n2.4  Lexical Resources | 59\nthe sense definition. Two distinct words having the same spelling are called\nhomonyms.\nThe simplest \nkind of lexicon is nothing more than a sorted list of words. Sophisticated\nlexicons include complex structure within and across the individual entries. In this\nsection, we\u2019ll look at some lexical resources included with NLTK.\nWordlist Corpora\nNLTK includes some corpora that are nothing more than wordlists. The Words Corpus\nis the /usr/dict/words file from Unix, used by some spellcheckers. We can use it to find\nunusual or misspelled words in a text corpus, as shown in Example 2-3.\nExample 2-3. Filtering a text: This program computes the vocabulary of a text, then removes all items\nthat occur in an existing wordlist, leaving just the uncommon or misspelled words.\ndef unusual_words(text):\n    text_vocab = set(w.lower() for w in text if w.isalpha())\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n    unusual = text_vocab.difference(english_vocab)\n    return sorted(unusual)\n>>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))\n['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',\n'adieus', 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham',\n'amiably', 'annamaria', 'annuities', 'apologising', 'arbour', 'archness', ...]\n>>> unusual_words(nltk.corpus.nps_chat.words())\n['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',\n'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk',\n'agaibn', 'agurlwithbigguns', 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]\nThere is also a corpus of stopwords, that is, high-frequency words such as the, to, and\nalso that we sometimes want to filter out of a document before further processing.\nStopwords usually have little lexical content, and their presence in a text fails to dis-\ntinguish it from other texts.\n>>> from nltk.corpus import stopwords\n>>> stopwords.words('english')\n['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across',\nFigure 2-5. Lexicon terminology: Lexical entries for two lemmas having the same spelling\n(homonyms), providing part-of-speech and gloss information.\n60 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow',\n'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', ...]\nLet\u2019s define \na function to compute what fraction of words in a text are not in the stop-\nwords list:\n>>> def content_fraction(text):\n...     stopwords = nltk.corpus.stopwords.words('english')\n...     content = [w for w in text if w.lower() not in stopwords]\n...     return len(content) / len(text)\n...\n>>> content_fraction(nltk.corpus.reuters.words())\n0.65997695393285261\nThus, with the help of stopwords, we filter out a third of the words of the text. Notice\nthat we\u2019ve combined two different kinds of corpus here, using a lexical resource to filter\nthe content of a text corpus.\nFigure 2-6. A word puzzle: A grid of randomly chosen letters with rules for creating words out of the\nletters; this puzzle is known as \u201cTarget.\u201d\nA wordlist \nis useful for solving word puzzles, such as the one in Figure 2-6. Our program\niterates through every word and, for each one, checks whether it meets the conditions.\nIt is easy to check obligatory letter \n  and length \n  constraints (and we\u2019ll only look for\nwords with \nsix or more letters here). It is trickier to check that candidate solutions only\nuse combinations of the supplied letters, especially since some of the supplied letters\nappear twice (here, the letter v). The FreqDist comparison method \n  permits us to\ncheck that \nthe frequency of each letter in the candidate word is less than or equal to the\nfrequency of the corresponding letter in the puzzle.\n>>> puzzle_letters = nltk.FreqDist('egivrvonl')\n>>> obligatory = 'r'\n>>> wordlist = nltk.corpus.words.words()\n>>> [w for w in wordlist if len(w) >= 6 \n...                      and obligatory in w \n...                      and nltk.FreqDist(w) <= puzzle_letters] \n['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',\n'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',\n'revolving', 'ringle', 'roving', 'violer', 'virole']\nOne more \nwordlist corpus is the Names Corpus, containing 8,000 first names catego-\nrized by gender. The male and female names are stored in separate files. Let\u2019s find names\nthat appear in both files, i.e., names that are ambiguous for gender:\n2.4  Lexical Resources | 61\n>>> names = nltk.corpus.names\n>>> names.fileids()\n['female.txt', 'male.txt']\n>>> male_names = names.words('male.txt')\n>>> female_names = names.words('female.txt')\n>>> [w for w in male_names if w in female_names]\n['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\nIt is \nwell known that names ending in the letter a are almost always female. We can see\nthis and some other patterns in the graph in Figure 2-7, produced by the following code.\nRemember that name[-1] is the last letter of name.\n>>> cfd = nltk.ConditionalFreqDist(\n...           (fileid, name[-1])\n...           for fileid in names.fileids()\n...           for name in names.words(fileid))\n>>> cfd.plot()\nFigure 2-7. Conditional frequency distribution: This plot shows the number of female and male names\nending with \neach letter of the alphabet; most names ending with a, e, or i are female; names ending\nin h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male.\n62 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nA Pronouncing Dictionary\nA slightly richer kind of lexical resource is a table (or spreadsheet), containing a word\nplus some \nproperties in each row. NLTK includes the CMU Pronouncing Dictionary\nfor U.S. English, which was designed for use by speech synthesizers.\n>>> entries = nltk.corpus.cmudict.entries()\n>>> len(entries)\n127012\n>>> for entry in entries[39943:39951]:\n...     print entry\n...\n('fir', ['F', 'ER1'])\n('fire', ['F', 'AY1', 'ER0'])\n('fire', ['F', 'AY1', 'R'])\n('firearm', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M'])\n('firearm', ['F', 'AY1', 'R', 'AA2', 'R', 'M'])\n('firearms', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'])\n('firearms', ['F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'])\n('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 'L'])\nFor each word, this lexicon provides a list of phonetic codes\u2014distinct labels for each\ncontrastive sound\u2014known as phones. Observe that fire has two pronunciations (in\nU.S. English): the one-syllable F AY1 R, and the two-syllable F AY1 ER0. The symbols\nin the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at\nhttp://en.wikipedia.org/wiki/Arpabet.\nEach entry consists of two parts, and we can process these individually using a more\ncomplex version of the for statement. Instead of writing for entry in entries:, we\nreplace entry with two variable names, word, pron \n . Now, each time through the loop,\nword is \nassigned the first part of the entry, and pron is assigned the second part of the\nentry:\n>>> for word, pron in entries: \n...     if len(pron) == 3: \n...         ph1, ph2, ph3 = pron \n...         if ph1 == 'P' and ph3 == 'T':\n...             print word, ph2,\n...\npait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1\npet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1\npott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1\nThe program \njust shown scans the lexicon looking for entries whose pronunciation\nconsists of three phones \n . If the condition is true, it assigns the contents of pron to\nthree \nnew variables: ph1, ph2, and ph3. Notice the unusual form of the statement that\ndoes that work \n .\nHere\u2019s another \nexample of the same for statement, this time used inside a list compre-\nhension. This program finds all words whose pronunciation ends with a syllable\nsounding like nicks. You could use this method to find rhyming words.\n2.4  Lexical Resources | 63\n>>> syllable = ['N', 'IH0', 'K', 'S']\n>>> [word for word, pron in entries if pron[-4:] == syllable]\n[\"atlantic's\", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',\n'chetniks', \"clinic's\", 'clinics', 'conics', 'cynics', 'diasonics', \"dominic's\",\n'ebonics', 'electronics', \"electronics'\", 'endotronics', \"endotronics'\", 'enix', ...]\nNotice that \nthe one pronunciation is spelled in several ways: nics, niks, nix, and even\nntic\u2019s with a silent t, for the word atlantic\u2019s. Let\u2019s look for some other mismatches\nbetween pronunciation and writing. Can you summarize the purpose of the following\nexamples and explain how they work?\n>>> [w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']\n['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']\n>>> sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))\n['gn', 'kn', 'mn', 'pn']\nThe phones contain digits to represent primary stress ( 1), secondary stress ( 2), and no\nstress (0). As our final example, we define a function to extract the stress digits and then\nscan our lexicon to find words having a particular stress pattern.\n>>> def stress(pron):\n...     return [char for phone in pron for char in phone if char.isdigit()]\n>>> [w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]\n['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',\n'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',\n'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]\n>>> [w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']]\n['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',\n'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',\n'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]\nA subtlety of this program is that our user-defined function stress() is\ninvoked inside \nthe condition of a list comprehension. There is also a\ndoubly nested for loop. There\u2019s a lot going on here, and you might want\nto return to this once you\u2019ve had more experience using list compre-\nhensions.\nWe can use a conditional frequency distribution to help us find minimally contrasting\nsets of words. Here we find all the p words consisting of three sounds \n , and group\nthem according to their first and last sounds \n .\n>>> p3 = [(pron[0]+'-'+pron[2], word) \n...       for (word, pron) in entries\n...       if pron[0] == 'P' and len(pron) == 3] \n>>> cfd = nltk.ConditionalFreqDist(p3)\n>>> for template in cfd.conditions():\n...     if len(cfd[template]) > 10:\n...         words = cfd[template].keys()\n...         wordlist = ' '.join(words)\n...         print template, wordlist[:70] + \"...\"\n...\nP-CH perch puche poche peach petsche poach pietsch putsch pautsch piche pet...\n64 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nP-K pik peek pic pique paque polk perc poke perk pac pock poch purk pak pa...\nP-L pil poehl pille pehl pol pall pohl pahl paul perl pale paille perle po...\nP-N paine payne pon pain pin pawn pinn pun pine paign pen pyne pane penn p...\nP-P pap paap pipp paup pape pup pep poop pop pipe paape popp pip peep pope...\nP-R paar poor par poore pear pare pour peer pore parr por pair porr pier...\nP-S pearse piece posts pasts peace perce pos pers pace puss pesce pass pur...\nP-T pot puett pit pete putt pat purt pet peart pott pett pait pert pote pa...\nP-Z pays p.s pao's pais paws p.'s pas pez paz pei's pose poise peas paiz p...\nRather than \niterating over the whole dictionary, we can also access it by looking up\nparticular words. We will use Python\u2019s dictionary data structure, which we will study\nsystematically in Section 5.3 . We look up a dictionary by specifying its name, followed\nby a key (such as the word 'fire') inside square brackets \n .\n>>> prondict = nltk.corpus.cmudict.dict()\n>>> prondict['fire'] \n[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]\n>>> prondict['blog'] \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'blog'\n>>> prondict['blog'] = [['B', 'L', 'AA1', 'G']] \n>>> prondict['blog']\n[['B', 'L', 'AA1', 'G']]\nIf we \ntry to look up a non-existent key \n , we get a KeyError. This is similar to what\nhappens when we index a list with an integer that is too large, producing an IndexEr\nror. The word blog is missing from the pronouncing dictionary, so we tweak our version\nby assigning a value for this key \n  (this has no effect on the NLTK corpus; next time\nwe access it, blog will still be absent).\nWe can \nuse any lexical resource to process a text, e.g., to filter out words having some\nlexical property (like nouns), or mapping every word of the text. For example, the\nfollowing text-to-speech function looks up each word of the text in the pronunciation\ndictionary:\n>>> text = ['natural', 'language', 'processing']\n>>> [ph for w in text for ph in prondict[w][0]]\n['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',\n'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']\nComparative Wordlists\nAnother example of a tabular lexicon is the comparative wordlist . NLTK includes\nso-called Swadesh wordlists , lists of about 200 common words in several languages.\nThe languages are identified using an ISO 639 two-letter code.\n>>> from nltk.corpus import swadesh\n>>> swadesh.fileids()\n['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk',\n'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']\n>>> swadesh.words('en')\n['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',\n2.4  Lexical Resources | 65\n'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some',\n'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', ...]\nWe can \naccess cognate words from multiple languages using the entries() method,\nspecifying a list of languages. With one further step we can convert this into a simple\ndictionary (we\u2019ll learn about dict() in Section 5.3).\n>>> fr2en = swadesh.entries(['fr', 'en'])\n>>> fr2en\n[('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ...]\n>>> translate = dict(fr2en)\n>>> translate['chien']\n'dog'\n>>> translate['jeter']\n'throw'\nWe can make our simple translator more useful by adding other source languages. Let\u2019s\nget the German-English and Spanish-English pairs, convert each to a dictionary using\ndict(), then update our original translate dictionary with these additional mappings:\n>>> de2en = swadesh.entries(['de', 'en'])    # German-English\n>>> es2en = swadesh.entries(['es', 'en'])    # Spanish-English\n>>> translate.update(dict(de2en))\n>>> translate.update(dict(es2en))\n>>> translate['Hund']\n'dog'\n>>> translate['perro']\n'dog'\nWe can compare words in various Germanic and Romance languages:\n>>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n>>> for i in [139, 140, 141, 142]:\n...     print swadesh.entries(languages)[i]\n...\n('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere')\n('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'canere')\n('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'ludere')\n('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'fluctuare')\nShoebox and Toolbox Lexicons\nPerhaps the single most popular tool used by linguists for managing data is Toolbox,\npreviously known as Shoebox since it replaces the field linguist\u2019s traditional shoebox\nfull of file cards. Toolbox is freely downloadable from http://www.sil.org/computing/\ntoolbox/.\nA Toolbox file consists of a collection of entries, where each entry is made up of one\nor more fields. Most fields are optional or repeatable, which means that this kind of\nlexical resource cannot be treated as a table or spreadsheet.\nHere is a dictionary for the Rotokas language. We see just the first entry, for the word\nkaa, meaning \u201cto gag\u201d:\n66 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n>>> from nltk.corpus import toolbox\n>>> toolbox.entries('rotokas.dic')\n[('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'),\n('dcsv', 'true'), ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),\n('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),\n('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),\n('xe', 'Apoka is gagging from food while talking.')]), ...]\nEntries consist of a series of attribute-value pairs, such as ('ps', 'V')  to indicate that\nthe part-of-speech \nis 'V' (verb), and ('ge', 'gag') to indicate that the gloss-into-\nEnglish is 'gag'. The last three pairs contain an example sentence in Rotokas and its\ntranslations into Tok Pisin and English.\nThe loose structure of Toolbox files makes it hard for us to do much more with them\nat this stage. XML provides a powerful way to process this kind of corpus, and we will\nreturn to this topic in Chapter 11.\nThe Rotokas language is spoken on the island of Bougainville, Papua\nNew Guinea. \nThis lexicon was contributed to NLTK by Stuart Robin-\nson. Rotokas is notable for having an inventory of just 12 phonemes\n(contrastive sounds); see http://en.wikipedia.org/wiki/Rotokas_language\n2.5  WordNet\nWordNet is a semantically oriented dictionary of English, similar to a traditional the-\nsaurus but with a richer structure. NLTK includes the English WordNet, with 155,287\nwords and 117,659 synonym sets. We\u2019ll begin by looking at synonyms and how they\nare accessed in WordNet.\nSenses and Synonyms\nConsider the sentence in (1a). If we replace the word motorcar in (1a) with automo-\nbile, to get (1b), the meaning of the sentence stays pretty much the same:\n(1) a. Benz is credited with the invention of the motorcar.\nb. Benz is credited with the invention of the automobile.\nSince everything else in the sentence has remained unchanged, we can conclude that\nthe words motorcar and automobile have the same meaning, i.e., they are synonyms.\nWe can explore these words with the help of WordNet:\n>>> from nltk.corpus import wordnet as wn\n>>> wn.synsets('motorcar')\n[Synset('car.n.01')]\nThus, motorcar has just one possible meaning and it is identified as car.n.01, the first\nnoun sense of car. The entity car.n.01 is called a synset, or \u201csynonym set,\u201d a collection\nof synonymous words (or \u201clemmas\u201d):\n2.5  WordNet | 67\n>>> wn.synset('car.n.01').lemma_names\n['car', 'auto', 'automobile', 'machine', 'motorcar']\nEach word \nof a synset can have several meanings, e.g., car can also signify a train car-\nriage, a gondola, or an elevator car. However, we are only interested in the single\nmeaning that is common to all words of this synset. Synsets also come with a prose\ndefinition and some example sentences:\n>>> wn.synset('car.n.01').definition\n'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n>>> wn.synset('car.n.01').examples\n['he needs a car to get to work']\nAlthough definitions help humans to understand the intended meaning of a synset, the\nwords of the synset are often more useful for our programs. To eliminate ambiguity,\nwe will identify these words as car.n.01.automobile, car.n.01.motorcar, and so on.\nThis pairing of a synset with a word is called a lemma. We can get all the lemmas for\na given synset \n , look up a particular lemma \n , get the synset corresponding to a lemma\n, and get the \u201cname\u201d of a lemma \n :\n>>> wn.synset('car.n.01').lemmas \n[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'),\nLemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n>>> wn.lemma('car.n.01.automobile') \nLemma('car.n.01.automobile')\n>>> wn.lemma('car.n.01.automobile').synset \nSynset('car.n.01')\n>>> wn.lemma('car.n.01.automobile').name \n'automobile'\nUnlike the \nwords automobile and motorcar, which are unambiguous and have one syn-\nset, the word car is ambiguous, having five synsets:\n>>> wn.synsets('car')\n[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'),\nSynset('cable_car.n.01')]\n>>> for synset in wn.synsets('car'):\n...     print synset.lemma_names\n...\n['car', 'auto', 'automobile', 'machine', 'motorcar']\n['car', 'railcar', 'railway_car', 'railroad_car']\n['car', 'gondola']\n['car', 'elevator_car']\n['cable_car', 'car']\nFor convenience, we can access all the lemmas involving the word car as follows:\n>>> wn.lemmas('car')\n[Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'),\nLemma('car.n.04.car'), Lemma('cable_car.n.01.car')]\n68 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nYour Turn: Write down all the senses of the word dish that you can\nthink of. Now, explore this word with the help of WordNet, using the\nsame operations shown earlier.\nThe WordNet Hierarchy\nWordNet synsets correspond to abstract concepts, and they don\u2019t always have corre-\nsponding words in English. These concepts are linked together in a hierarchy. Some\nconcepts are very general, such as Entity, State, Event; these are called unique begin-\nners or root synsets. Others, such as gas guzzler  and hatchback, are much more specific.\nA small portion of a concept hierarchy is illustrated in Figure 2-8.\nFigure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the\nhypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts.\nWordNet makes \nit easy to navigate between concepts. For example, given a concept\nlike motorcar, we can look at the concepts that are more specific\u2014the (immediate)\nhyponyms.\n>>> motorcar = wn.synset('car.n.01')\n>>> types_of_motorcar = motorcar.hyponyms()\n>>> types_of_motorcar[26]\nSynset('ambulance.n.01')\n>>> sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas])\n['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon',\n'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible',\n'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car',\n'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap',\n'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover',\n'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',\n2.5  WordNet | 69\n'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer',\n'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan',\n'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car',\n'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car',\n'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon',\n'wagon']\nWe can \nalso navigate up the hierarchy by visiting hypernyms. Some words have multiple\npaths, because they can be classified in more than one way. There are two paths between\ncar.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a\nvehicle and a container.\n>>> motorcar.hypernyms()\n[Synset('motor_vehicle.n.01')]\n>>> paths = motorcar.hypernym_paths()\n>>> len(paths)\n2\n>>> [synset.name for synset in paths[0]]\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01',\n'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n>>> [synset.name for synset in paths[1]]\n['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01',\n'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\nWe can get the most general hypernyms (or root hypernyms) of a synset as follows:\n>>> motorcar.root_hypernyms()\n[Synset('entity.n.01')]\nYour Turn:  Try out NLTK\u2019s convenient graphical WordNet browser:\nnltk.app.wordnet(). Explore the WordNet hierarchy by following the\nhypernym and hyponym links.\nMore Lexical Relations\nHypernyms and hyponyms are called lexical relations  because they relate one synset\nto another. These two relations navigate up and down the \u201cis-a\u201d hierarchy. Another\nimportant way to navigate the WordNet network is from items to their components\n(meronyms) or to the things they are contained in ( holonyms). For example, the parts\nof a tree are its trunk, crown, and so on; these are the part_meronyms(). The substance\na tree is made of includes heartwood and sapwood, i.e., the substance_meronyms(). A\ncollection of trees forms a forest, i.e., the member_holonyms():\n>>> wn.synset('tree.n.01').part_meronyms()\n[Synset('burl.n.02'), Synset('crown.n.07'), Synset('stump.n.01'),\nSynset('trunk.n.01'), Synset('limb.n.02')]\n>>> wn.synset('tree.n.01').substance_meronyms()\n[Synset('heartwood.n.01'), Synset('sapwood.n.01')]\n70 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\n>>> wn.synset('tree.n.01').member_holonyms()\n[Synset('forest.n.01')]\nTo see \njust how intricate things can get, consider the word mint, which has several\nclosely related senses. We can see that mint.n.04 is part of mint.n.02 and the substance\nfrom which mint.n.05 is made.\n>>> for synset in wn.synsets('mint', wn.NOUN):\n...     print synset.name + ':', synset.definition\n...\nbatch.n.02: (often followed by `of') a large number or amount or extent\nmint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and\n           small mauve flowers\nmint.n.03: any member of the mint family of plants\nmint.n.04: the leaves of a mint plant used fresh or candied\nmint.n.05: a candy that is flavored with a mint oil\nmint.n.06: a plant where money is coined by authority of the government\n>>> wn.synset('mint.n.04').part_holonyms()\n[Synset('mint.n.02')]\n>>> wn.synset('mint.n.04').substance_holonyms()\n[Synset('mint.n.05')]\nThere are also relationships between verbs. For example, the act of walking involves\nthe act of stepping, so walking entails stepping. Some verbs have multiple entailments:\n>>> wn.synset('walk.v.01').entailments()\n[Synset('step.v.01')]\n>>> wn.synset('eat.v.01').entailments()\n[Synset('swallow.v.01'), Synset('chew.v.01')]\n>>> wn.synset('tease.v.03').entailments()\n[Synset('arouse.v.07'), Synset('disappoint.v.01')]\nSome lexical relationships hold between lemmas, e.g., antonymy:\n>>> wn.lemma('supply.n.02.supply').antonyms()\n[Lemma('demand.n.02.demand')]\n>>> wn.lemma('rush.v.01.rush').antonyms()\n[Lemma('linger.v.04.linger')]\n>>> wn.lemma('horizontal.a.01.horizontal').antonyms()\n[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]\n>>> wn.lemma('staccato.r.01.staccato').antonyms()\n[Lemma('legato.r.01.legato')]\nYou can see the lexical relations, and the other methods defined on a synset, using\ndir(). For example, try dir(wn.synset('harmony.n.02')).\nSemantic Similarity\nWe have seen that synsets are linked by a complex network of lexical relations. Given\na particular synset, we can traverse the WordNet network to find synsets with related\nmeanings. Knowing which words are semantically related is useful for indexing a col-\nlection of texts, so that a search for a general term such as vehicle will match documents\ncontaining specific terms such as limousine.\n2.5  WordNet | 71\nRecall that each synset has one or more hypernym paths that link it to a root hypernym\nsuch as entity.n.01\n. Two synsets linked to the same root may have several hypernyms\nin common (see Figure 2-8 ). If two synsets share a very specific hypernym\u2014one that\nis low down in the hypernym hierarchy\u2014they must be closely related.\n>>> right = wn.synset('right_whale.n.01')\n>>> orca = wn.synset('orca.n.01')\n>>> minke = wn.synset('minke_whale.n.01')\n>>> tortoise = wn.synset('tortoise.n.01')\n>>> novel = wn.synset('novel.n.01')\n>>> right.lowest_common_hypernyms(minke)\n[Synset('baleen_whale.n.01')]\n>>> right.lowest_common_hypernyms(orca)\n[Synset('whale.n.02')]\n>>> right.lowest_common_hypernyms(tortoise)\n[Synset('vertebrate.n.01')]\n>>> right.lowest_common_hypernyms(novel)\n[Synset('entity.n.01')]\nOf course we know that whale is very specific (and baleen whale even more so), whereas\nvertebrate is more general and entity is completely general. We can quantify this concept\nof generality by looking up the depth of each synset:\n>>> wn.synset('baleen_whale.n.01').min_depth()\n14\n>>> wn.synset('whale.n.02').min_depth()\n13\n>>> wn.synset('vertebrate.n.01').min_depth()\n8\n>>> wn.synset('entity.n.01').min_depth()\n0\nSimilarity measures have been defined over the collection of WordNet synsets that\nincorporate this insight. For example, path_similarity assigns a score in the range\n0\u20131 based on the shortest path that connects the concepts in the hypernym hierarchy\n(-1 is returned in those cases where a path cannot be found). Comparing a synset with\nitself will return 1. Consider the following similarity scores, relating right whale  to minke\nwhale, orca, tortoise, and novel. Although the numbers won\u2019t mean much, they decrease\nas we move away from the semantic space of sea creatures to inanimate objects.\n>>> right.path_similarity(minke)\n0.25\n>>> right.path_similarity(orca)\n0.16666666666666666\n>>> right.path_similarity(tortoise)\n0.076923076923076927\n>>> right.path_similarity(novel)\n0.043478260869565216\n72 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nSeveral other similarity measures are available; you can type help(wn)\nfor more \ninformation. NLTK also includes VerbNet, a hierarchical verb\nlexicon linked to WordNet. It can be accessed with nltk.corpus.verb\nnet.\n2.6  Summary\n\u2022 A text corpus is a large, structured collection of texts. NLTK comes with many\ncorpora, e.g., the Brown Corpus, nltk.corpus.brown.\n\u2022 Some text corpora are categorized, e.g., by genre or topic; sometimes the categories\nof a corpus overlap each other.\n\u2022 A conditional frequency distribution is a collection of frequency distributions, each\none for a different condition. They can be used for counting word frequencies,\ngiven a context or a genre.\n\u2022 Python programs more than a few lines long should be entered using a text editor,\nsaved to a file with a .py extension, and accessed using an import statement.\n\u2022 Python functions permit you to associate a name with a particular block of code,\nand reuse that code as often as necessary.\n\u2022 Some functions, known as \u201cmethods,\u201d are associated with an object, and we give\nthe object name followed by a period followed by the method name, like this:\nx.funct(y), e.g., word.isalpha().\n\u2022 To find out about some variable v, type help(v) in the Python interactive interpreter\nto read the help entry for this kind of object.\n\u2022 WordNet is a semantically oriented dictionary of English, consisting of synonym\nsets\u2014or synsets\u2014and organized into a network.\n\u2022 Some functions are not available by default, but must be accessed using Python\u2019s\nimport statement.\n2.7  Further Reading\nExtra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. The corpus methods are summarized in the\nCorpus HOWTO, at http://www.nltk.org/howto, and documented extensively in the\nonline API documentation.\nSignificant sources of published corpora are the Linguistic Data Consortium  (LDC) and\nthe European Language Resources Agency (ELRA). Hundreds of annotated text and\nspeech corpora are available in dozens of languages. Non-commercial licenses permit\nthe data to be used in teaching and research. For some corpora, commercial licenses\nare also available (but for a higher fee).\n2.7  Further Reading | 73\nThese and many other language resources have been documented using OLAC Meta-\ndata, and \ncan be searched via the OLAC home page at http://www.language-archives\n.org/. Corpora List  (see http://gandalf.aksis.uib.no/corpora/sub.html) is a mailing list for\ndiscussions about corpora, and you can find resources by searching the list archives or\nposting to the list. The most complete inventory of the world\u2019s languages is Ethno-\nlogue, http://www.ethnologue.com/. Of 7,000 languages, only a few dozen have sub-\nstantial digital resources suitable for use in NLP.\nThis chapter has touched on the field of Corpus Linguistics. Other useful books in\nthis area include (Biber, Conrad, & Reppen, 1998), (McEnery, 2006), (Meyer, 2002),\n(Sampson & McCarthy, 2005), and (Scott & Tribble, 2006). Further readings in quan-\ntitative data analysis in linguistics are: (Baayen, 2008), (Gries, 2009), and (Woods,\nFletcher, & Hughes, 1986).\nThe original description of WordNet is (Fellbaum, 1998). Although WordNet was\noriginally developed for research in psycholinguistics, it is now widely used in NLP and\nInformation Retrieval. WordNets are being developed for many other languages, as\ndocumented at http://www.globalwordnet.org/. For a study of WordNet similarity\nmeasures, see (Budanitsky & Hirst, 2006).\nOther topics touched on in this chapter were phonetics and lexical semantics, and we\nrefer readers to Chapters 7 and 20 of (Jurafsky & Martin, 2008).\n2.8  Exercises\n1.\u25cb Create a variable phrase containing a list of words. Experiment with the opera-\ntions described in this chapter, including addition, multiplication, indexing, slic-\ning, and sorting.\n2.\u25cb Use the corpus module to explore austen-persuasion.txt. How many word\ntokens does this book have? How many word types?\n3.\u25cb Use the Brown Corpus reader nltk.corpus.brown.words() or the Web Text Cor-\npus reader nltk.corpus.webtext.words() to access some sample text in two differ-\nent genres.\n4.\u25cb Read in the texts of the State of the Union  addresses, using the state_union corpus\nreader. Count occurrences of men, women, and people in each document. What has\nhappened to the usage of these words over time?\n5.\u25cb Investigate the holonym-meronym relations for some nouns. Remember that\nthere are three kinds of holonym-meronym relation, so you need to use member_mer\nonyms(), part_meronyms(), substance_meronyms(), member_holonyms(),\npart_holonyms(), and substance_holonyms().\n6.\u25cb In the discussion of comparative wordlists, we created an object called trans\nlate, which you could look up using words in both German and Italian in order\n74 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\nto get corresponding words in English. What problem might arise with this ap-\nproach? Can you suggest a way to avoid this problem?\n7.\u25cb According to \nStrunk and White\u2019s Elements of Style , the word however, used at\nthe start of a sentence, means \u201cin whatever way\u201d or \u201cto whatever extent,\u201d and not\n\u201cnevertheless.\u201d They give this example of correct usage: However you advise him,\nhe will probably do as he thinks best.  (http://www.bartleby.com/141/strunk3.html)\nUse the concordance tool to study actual usage of this word in the various texts we\nhave been considering. See also the LanguageLog posting \u201cFossilized prejudices\nabout \u2018however\u2019\u201d at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913\n.html.\n8.\u25d1 Define a conditional frequency distribution over the Names Corpus that allows\nyou to see which initial letters are more frequent for males versus females (see\nFigure 2-7).\n9.\u25d1 Pick a pair of texts and study the differences between them, in terms of vocabu-\nlary, vocabulary richness, genre, etc. Can you find pairs of words that have quite\ndifferent meanings across the two texts, such as monstrous in Moby Dick  and in\nSense and Sensibility?\n10.\u25d1 Read the BBC News article: \u201cUK\u2019s Vicky Pollards \u2018left behind\u2019\u201d at http://news\n.bbc.co.uk/1/hi/education/6173441.stm. The article gives the following statistic\nabout teen language: \u201cthe top 20 words used, including yeah, no, but and like,\naccount for around a third of all words.\u201d How many word types account for a third\nof all word tokens, for a variety of text sources? What do you conclude about this\nstatistic? Read more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/\nlanguagelog/archives/003993.html.\n11.\u25d1 Investigate the table of modal distributions and look for other patterns. Try to\nexplain them in terms of your own impressionistic understanding of the different\ngenres. Can you find other closed classes of words that exhibit significant differ-\nences across different genres?\n12.\u25d1 The CMU Pronouncing Dictionary contains multiple pronunciations for certain\nwords. How many distinct words does it contain? What fraction of words in this\ndictionary have more than one possible pronunciation?\n13.\u25d1 What percentage of noun synsets have no hyponyms? You can get all noun syn-\nsets using wn.all_synsets('n').\n14.\u25d1 Define a function supergloss(s) that takes a synset s as its argument and returns\na string consisting of the concatenation of the definition of s, and the definitions\nof all the hypernyms and hyponyms of s.\n15.\u25d1 Write a program to find all words that occur at least three times in the Brown\nCorpus.\n16.\u25d1 Write a program to generate a table of lexical diversity scores (i.e., token/type\nratios), as we saw in Table 1-1 . Include the full set of Brown Corpus genres\n2.8  Exercises | 75\n(nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest\nnumber of tokens per type)? Is this what you would have expected?\n17.\u25d1 Write a function that finds the 50 most frequently occurring words of a text that\nare not stopwords.\n18.\u25d1 Write a program to print the 50 most frequent bigrams (pairs of adjacent words)\nof a text, omitting bigrams that contain stopwords.\n19.\u25d1 Write a program to create a table of word frequencies by genre, like the one given\nin Section 2.1 for modals. Choose your own words and try to find words whose\npresence (or absence) is typical of a genre. Discuss your findings.\n20.\u25d1 Write a function word_freq() that takes a word and the name of a section of the\nBrown Corpus as arguments, and computes the frequency of the word in that sec-\ntion of the corpus.\n21.\u25d1 Write a program to guess the number of syllables contained in a text, making\nuse of the CMU Pronouncing Dictionary.\n22.\u25d1 Define a function hedge(text) that processes a text and produces a new version\nwith the word 'like' between every third word.\n23.\u25cf Zipf\u2019s Law : Let f(w) be the frequency of a word w in free text. Suppose that all\nthe words of a text are ranked according to their frequency, with the most frequent\nword first. Zipf\u2019s Law states that the frequency of a word type is inversely\nproportional to its rank (i.e., f \u00d7 r = k, for some constant k). For example, the 50th\nmost common word type should occur three times as frequently as the 150th most\ncommon word type.\na. Write a function to process a large text and plot word frequency against word\nrank using pylab.plot. Do you confirm Zipf\u2019s law? (Hint: it helps to use a\nlogarithmic scale.) What is going on at the extreme ends of the plotted line?\nb. Generate random text, e.g., using random.choice(\"abcdefg \") , taking care to\ninclude the space character. You will need to import random first. Use the string\nconcatenation operator to accumulate characters into a (very) long string.\nThen tokenize this string, generate the Zipf plot as before, and compare the\ntwo plots. What do you make of Zipf\u2019s Law in the light of this?\n24.\u25cf Modify the text generation program in Example 2-1  further, to do the following\ntasks:\n76 | Chapter 2: \u2002Accessing Text Corpora and Lexical Resources\na. Store the n  most likely words in a list words, then randomly choose a word\nfrom the list using random.choice(). (You will need to import random first.)\nb. Select a particular genre, such as a section of the Brown Corpus or a Genesis\ntranslation, one of the Gutenberg texts, or one of the Web texts. Train the\nmodel on this corpus and get it to generate random text. You may have to\nexperiment with different start words. How intelligible is the text? Discuss the\nstrengths and weaknesses of this method of generating random text.\nc. Now train your system using two distinct genres and experiment with gener-\nating text in the hybrid genre. Discuss your observations.\n25.\u25cf Define a function find_language() that takes a string as its argument and returns\na list of languages that have that string as a word. Use the udhr corpus and limit\nyour searches to files in the Latin-1 encoding.\n26.\u25cf What is the branching factor of the noun hypernym hierarchy? I.e., for every\nnoun synset that has hyponyms\u2014or children in the hypernym hierarchy\u2014how\nmany do they have on average? You can get all noun synsets using wn.all_syn\nsets('n').\n27.\u25cf The polysemy of a word is the number of senses it has. Using WordNet, we can\ndetermine that the noun dog has seven senses with len(wn.synsets('dog', 'n')).\nCompute the average polysemy of nouns, verbs, adjectives, and adverbs according\nto WordNet.\n28.\u25cf Use one of the predefined similarity measures to score the similarity of each of\nthe following pairs of words. Rank the pairs in order of decreasing similarity. How\nclose is your ranking to the order given here, an order that was established exper-\nimentally by (Miller & Charles, 1998): car-automobile, gem-jewel, journey-voyage,\nboy-lad, coast-shore, asylum-madhouse, magician-wizard, midday-noon, furnace-\nstove, food-fruit, bird-cock, bird-crane, tool-implement, brother-monk, lad-\nbrother, crane-implement, journey-car, monk-oracle, cemetery-woodland, food-\nrooster, coast-hill, forest-graveyard, shore-woodland, monk-slave, coast-forest,\nlad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.\n2.8  Exercises | 77\n\nCHAPTER 3\nProcessing Raw Text\nThe most important source of texts is undoubtedly the Web. It\u2019s convenient to have\nexisting text \ncollections to explore, such as the corpora we saw in the previous chapters.\nHowever, you probably have your own text sources in mind, and need to learn how to\naccess them.\nThe goal of this chapter is to answer the following questions:\n1. How can we write programs to access text from local files and from the Web, in\norder to get hold of an unlimited range of language material?\n2. How can we split documents up into individual words and punctuation symbols,\nso we can carry out the same kinds of analysis we did with text corpora in earlier\nchapters?\n3. How can we write programs to produce formatted output and save it in a file?\nIn order to address these questions, we will be covering key concepts in NLP, including\ntokenization and stemming. Along the way you will consolidate your Python knowl-\nedge and learn about strings, files, and regular expressions. Since so much text on the\nWeb is in HTML format, we will also see how to dispense with markup.\nImportant: From this chapter onwards, our program samples will as-\nsume you begin your interactive session or your program with the fol-\nlowing import statements:\n>>> from __future__ import division\n>>> import nltk, re, pprint\n79\n3.1  Accessing Text from the Web and from Disk\nElectronic Books\nA small \nsample of texts from Project Gutenberg appears in the NLTK corpus collection.\nHowever, you may be interested in analyzing other texts from Project Gutenberg. You\ncan browse the catalog of 25,000 free online books at http://www.gutenberg.org/cata\nlog/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project\nGutenberg are in English, it includes material in over 50 other languages, including\nCatalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese, and Spanish\n(with more than 100 texts each).\nText number 2554 is an English translation of Crime and Punishment , and we can access\nit as follows.\n>>> from urllib import urlopen\n>>> url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n>>> raw = urlopen(url).read()\n>>> type(raw)\n<type 'str'>\n>>> len(raw)\n1176831\n>>> raw[:75]\n'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'\nThe read() process will take a few seconds as it downloads this large\nbook. \nIf you\u2019re using an Internet proxy that is not correctly detected by\nPython, you may need to specify the proxy manually as follows:\n>>> proxies = {'http': 'http://www.someproxy.com:3128'}\n>>> raw = urlopen(url, proxies=proxies).read()\nThe variable raw contains a string with 1,176,831 characters. (We can see that it is a\nstring, using type(raw).) This is the raw content of the book, including many details\nwe are not interested in, such as whitespace, line breaks, and blank lines. Notice the\n\\r and \\n in the opening line of the file, which is how Python displays the special carriage\nreturn and line-feed characters (the file must have been created on a Windows ma-\nchine). For our language processing, we want to break up the string into words and\npunctuation, as we saw in Chapter 1. This step is called tokenization, and it produces\nour familiar structure, a list of words and punctuation.\n>>> tokens = nltk.word_tokenize(raw)\n>>> type(tokens)\n<type 'list'>\n>>> len(tokens)\n255809\n>>> tokens[:10]\n['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n80 | Chapter 3: \u2002Processing Raw Text\nNotice that NLTK was needed for tokenization, but not for any of the earlier tasks of\nopening a URL and reading it into a string. If we now take the further step of creating\nan NLTK \ntext from this list, we can carry out all of the other linguistic processing we\nsaw in Chapter 1, along with the regular list operations, such as slicing:\n>>> text = nltk.Text(tokens)\n>>> type(text)\n<type 'nltk.text.Text'>\n>>> text[1020:1060]\n['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',\n'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',\n'which', 'he', 'lodged', 'in', 'S', '.', 'Place', 'and', 'walked', 'slowly',\n',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K', '.', 'bridge', '.']\n>>> text.collocations()\nKaterina Ivanovna; Pulcheria Alexandrovna; Avdotya Romanovna; Pyotr\nPetrovitch; Project Gutenberg; Marfa Petrovna; Rodion Romanovitch;\nSofya Semyonovna; Nikodim Fomitch; did not; Hay Market; Andrey\nSemyonovitch; old woman; Literary Archive; Dmitri Prokofitch; great\ndeal; United States; Praskovya Pavlovna; Porfiry Petrovitch; ear rings\nNotice that Project Gutenberg  appears as a collocation. This is because each text down-\nloaded from Project Gutenberg contains a header with the name of the text, the author,\nthe names of people who scanned and corrected the text, a license, and so on. Some-\ntimes this information appears in a footer at the end of the file. We cannot reliably\ndetect where the content begins and ends, and so have to resort to manual inspection\nof the file, to discover unique strings that mark the beginning and the end, before\ntrimming raw to be just the content and nothing else:\n>>> raw.find(\"PART I\")\n5303\n>>> raw.rfind(\"End of Project Gutenberg's Crime\")\n1157681\n>>> raw = raw[5303:1157681] \n>>> raw.find(\"PART I\")\n0\nThe find() and rfind() (\u201creverse \nfind\u201d) methods help us get the right index values to\nuse for slicing the string \n . We overwrite raw with this slice, so now it begins with\n\u201cPART I\u201d and goes up to (but not including) the phrase that marks the end of the\ncontent.\nThis was our first brush with the reality of the Web: texts found on the Web may contain\nunwanted material, and there may not be an automatic way to remove it. But with a\nsmall amount of extra work we can extract the material we need.\nDealing with HTML\nMuch of the text on the Web is in the form of HTML documents. You can use a web\nbrowser to save a page as text to a local file, then access this as described in the later\nsection on files. However, if you\u2019re going to do this often, it\u2019s easiest to get Python to\ndo the work directly. The first step is the same as before, using urlopen. For fun we\u2019ll\n3.1  Accessing Text from the Web and from Disk | 81\npick a BBC News story called \u201cBlondes to die out in 200 years,\u201d an urban legend passed\nalong by the BBC as established scientific fact:\n>>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n>>> html = urlopen(url).read()\n>>> html[:60]\n'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'\nYou can \ntype print html  to see the HTML content in all its glory, including meta tags,\nan image map, JavaScript, forms, and tables.\nGetting text out of HTML is a sufficiently common task that NLTK provides a helper\nfunction nltk.clean_html(), which takes an HTML string and returns raw text. We\ncan then tokenize this to get our familiar text structure:\n>>> raw = nltk.clean_html(html)\n>>> tokens = nltk.word_tokenize(raw)\n>>> tokens\n['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'\", 'to', 'die', 'out', ...]\nThis still contains unwanted material concerning site navigation and related stories.\nWith some trial and error you can find the start and end indexes of the content and\nselect the tokens of interest, and initialize a text as before.\n>>> tokens = tokens[96:399]\n>>> text = nltk.Text(tokens)\n>>> text.concordance('gene')\n they say too few people now carry the gene for blondes to last beyond the next tw\nt blonde hair is caused by a recessive gene . In order for a child to have blonde\nto have blonde hair , it must have the gene on both sides of the family in the gra\nthere is a disadvantage of having that gene or by chance . They don ' t disappear\nondes would disappear is if having the gene was a disadvantage and I do not think\nFor more sophisticated processing of HTML, use the Beautiful Soup\npackage, \navailable at http://www.crummy.com/software/BeautifulSoup/.\nProcessing Search Engine Results\nThe Web can be thought of as a huge corpus of unannotated text. Web search engines\nprovide an efficient means of searching this large quantity of text for relevant linguistic\nexamples. The main advantage of search engines is size: since you are searching such\na large set of documents, you are more likely to find any linguistic pattern you are\ninterested in. Furthermore, you can make use of very specific patterns, which would\nmatch only one or two examples on a smaller example, but which might match tens of\nthousands of examples when run on the Web. A second advantage of web search en-\ngines is that they are very easy to use. Thus, they provide a very convenient tool for\nquickly checking a theory, to see if it is reasonable. See Table 3-1 for an example.\n82 | Chapter 3: \u2002Processing Raw Text\nTable 3-1. Google hits for collocations: The number of hits for collocations involving the words\nabsolutely or definitely, followed \nby one of adore, love, like, or prefer. (Liberman, in LanguageLog,\n2005)\nGoogle hits adore love like prefer\nabsolutely 289,000 905,000 16,200 644\ndefinitely 1,460 51,000 158,000 62,600\nratio 198:1 18:1 1:10 1:97\nUnfortunately, search engines have some significant shortcomings. First, the allowable\nrange of \nsearch patterns is severely restricted. Unlike local corpora, where you write\nprograms to search for arbitrarily complex patterns, search engines generally only allow\nyou to search for individual words or strings of words, sometimes with wildcards. Sec-\nond, search engines give inconsistent results, and can give widely different figures when\nused at different times or in different geographical regions. When content has been\nduplicated across multiple sites, search results may be boosted. Finally, the markup in\nthe result returned by a search engine may change unpredictably, breaking any pattern-\nbased method of locating particular content (a problem which is ameliorated by the\nuse of search engine APIs).\nYour Turn:  Search the Web for \"the of\"  (inside quotes). Based on the\nlarge count, can we conclude that the of is a frequent collocation in\nEnglish?\nProcessing RSS Feeds\nThe blogosphere is an important source of text, in both formal and informal registers.\nWith the help of a third-party Python library called the Universal Feed Parser , freely\ndownloadable from http://feedparser.org/, we can access the content of a blog, as shown\nhere:\n>>> import feedparser\n>>> llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n>>> llog['feed']['title']\nu'Language Log'\n>>> len(llog.entries)\n15\n>>> post = llog.entries[2]\n>>> post.title\nu\"He's My BF\"\n>>> content = post.content[0].value\n>>> content[:70]\nu'<p>Today I was chatting with three of our visiting graduate students f'\n>>> nltk.word_tokenize(nltk.html_clean(content))\n>>> nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value))\n[u'Today', u'I', u'was', u'chatting', u'with', u'three', u'of', u'our', u'visiting',\nu'graduate', u'students', u'from', u'the', u'PRC', u'.', u'Thinking', u'that', u'I',\n3.1  Accessing Text from the Web and from Disk | 83\nu'was', u'being', u'au', u'courant', u',', u'I', u'mentioned', u'the', u'expression',\nu'DUI4XIANG4', u'\\u5c0d\\u8c61', u'(\"', u'boy', u'/', u'girl', u'friend', u'\"', ...]\nNote that \nthe resulting strings have a u prefix to indicate that they are Unicode strings\n(see Section 3.3 ). With some further work, we can write programs to create a small\ncorpus of blog posts, and use this as the basis for our NLP work.\nReading Local Files\nIn order to read a local file, we need to use Python\u2019s built-in open() function, followed\nby the read() method. Supposing you have a file document.txt, you can load its contents\nlike this:\n>>> f = open('document.txt')\n>>> raw = f.read()\nYour Turn: Create a file called document.txt using a text editor, and\ntype in a few lines of text, and save it as plain text. If you are using IDLE,\nselect the New Window command in the File menu, typing the required\ntext into this window, and then saving the file as document.txt inside\nthe directory that IDLE offers in the pop-up dialogue box. Next, in the\nPython interpreter, open the file using f = open('document.txt') , then\ninspect its contents using print f.read().\nVarious things might have gone wrong when you tried this. If the interpreter couldn\u2019t\nfind your file, you would have seen an error like this:\n>>> f = open('document.txt')\nTraceback (most recent call last):\nFile \"<pyshell#7>\", line 1, in -toplevel-\nf = open('document.txt')\nIOError: [Errno 2] No such file or directory: 'document.txt'\nTo check that the file that you are trying to open is really in the right directory, use\nIDLE\u2019s Open command in the File menu; this will display a list of all the files in the\ndirectory where IDLE is running. An alternative is to examine the current directory\nfrom within Python:\n>>> import os\n>>> os.listdir('.')\nAnother possible problem you might have encountered when accessing a text file is the\nnewline conventions, which are different for different operating systems. The built-in\nopen() function has a second parameter for controlling how the file is opened: open('do\ncument.txt', 'rU'). 'r' means to open the file for reading (the default), and 'U' stands\nfor \u201cUniversal\u201d, which lets us ignore the different conventions used for marking new-\nlines.\nAssuming that you can open the file, there are several methods for reading it. The\nread() method creates a string with the contents of the entire file:\n84 | Chapter 3: \u2002Processing Raw Text\n>>> f.read()\n'Time flies like an arrow.\\nFruit flies like a banana.\\n'\nRecall that \nthe '\\n' characters are newlines; this is equivalent to pressing Enter on a\nkeyboard and starting a new line.\nWe can also read a file one line at a time using a for loop:\n>>> f = open('document.txt', 'rU')\n>>> for line in f:\n...     print line.strip()\nTime flies like an arrow.\nFruit flies like a banana.\nHere we use the strip() method to remove the newline character at the end of the input\nline.\nNLTK\u2019s corpus files can also be accessed using these methods. We simply have to use\nnltk.data.find() to get the filename for any corpus item. Then we can open and read\nit in the way we just demonstrated:\n>>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n>>> raw = open(path, 'rU').read()\nExtracting Text from PDF, MSWord, and Other Binary Formats\nASCII text and HTML text are human-readable formats. Text often comes in binary\nformats\u2014such as PDF and MSWord\u2014that can only be opened using specialized soft-\nware. Third-party libraries such as pypdf and pywin32 provide access to these formats.\nExtracting text from multicolumn documents is particularly challenging. For one-off\nconversion of a few documents, it is simpler to open the document with a suitable\napplication, then save it as text to your local drive, and access it as described below. If\nthe document is already on the Web, you can enter its URL in Google\u2019s search box.\nThe search result often includes a link to an HTML version of the document, which\nyou can save as text.\nCapturing User Input\nSometimes we want to capture the text that a user inputs when she is interacting with\nour program. To prompt the user to type a line of input, call the Python function\nraw_input(). After saving the input to a variable, we can manipulate it just as we have\ndone for other strings.\n>>> s = raw_input(\"Enter some text: \")\nEnter some text: On an exceptionally hot evening early in July\n>>> print \"You typed\", len(nltk.word_tokenize(s)), \"words.\"\nYou typed 8 words.\n3.1  Accessing Text from the Web and from Disk | 85\nThe NLP Pipeline\nFigure 3-1  summarizes \nwhat we have covered in this section, including the process of\nbuilding a vocabulary that we saw in Chapter 1. (One step, normalization, will be\ndiscussed in Section 3.6.)\nFigure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the markup\nand select \na slice of characters; this is then tokenized and optionally converted into an nltk.Text\nobject; we can also lowercase all the words and extract the vocabulary.\nThere\u2019s a lot going on in this pipeline. To understand it properly, it helps to be clear\nabout the type of each variable that it mentions. We find out the type of any Python\nobject x using type(x); e.g., type(1) is <int> since 1 is an integer.\nWhen we load the contents of a URL or file, and when we strip out HTML markup,\nwe are dealing with strings, Python\u2019s <str> data type (we will learn more about strings\nin Section 3.2):\n>>> raw = open('document.txt').read()\n>>> type(raw)\n<type 'str'>\nWhen we tokenize a string we produce a list (of words), and this is Python\u2019s <list>\ntype. Normalizing and sorting lists produces other lists:\n>>> tokens = nltk.word_tokenize(raw)\n>>> type(tokens)\n<type 'list'>\n>>> words = [w.lower() for w in tokens]\n>>> type(words)\n<type 'list'>\n>>> vocab = sorted(set(words))\n>>> type(vocab)\n<type 'list'>\nThe type of an object determines what operations you can perform on it. So, for ex-\nample, we can append to a list but not to a string:\n86 | Chapter 3: \u2002Processing Raw Text\n>>> vocab.append('blog')\n>>> raw.append('blog')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'str' object has no attribute 'append'\nSimilarly, we \ncan concatenate strings with strings, and lists with lists, but we cannot\nconcatenate strings with lists:\n>>> query = 'Who knows?'\n>>> beatles = ['john', 'paul', 'george', 'ringo']\n>>> query + beatles\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: cannot concatenate 'str' and 'list' objects\nIn the next section, we examine strings more closely and further explore the relationship\nbetween strings and lists.\n3.2  Strings: Text Processing at the Lowest Level\nIt\u2019s time to study a fundamental data type that we\u2019ve been studiously avoiding so far.\nIn earlier chapters we focused on a text as a list of words. We didn\u2019t look too closely\nat words and how they are handled in the programming language. By using NLTK\u2019s\ncorpus interface we were able to ignore the files that these texts had come from. The\ncontents of a word, and of a file, are represented by programming languages as a fun-\ndamental data type known as a string. In this section, we explore strings in detail, and\nshow the connection between strings, words, texts, and files.\nBasic Operations with Strings\nStrings are specified using single quotes \n  or double quotes \n , as shown in the fol-\nlowing code \nexample. If a string contains a single quote, we must backslash-escape the\nquote \n  so Python knows a literal quote character is intended, or else put the string in\ndouble quotes \n . Otherwise, the quote inside the string \n  will be interpreted as a close\nquote, and the Python interpreter will report a syntax error:\n>>> monty = 'Monty Python' \n>>> monty\n'Monty Python'\n>>> circus = \"Monty Python's Flying Circus\" \n>>> circus\n\"Monty Python's Flying Circus\"\n>>> circus = 'Monty Python\\'s Flying Circus' \n>>> circus\n\"Monty Python's Flying Circus\"\n>>> circus = 'Monty Python's Flying Circus' \n  File \"<stdin>\", line 1\n    circus = 'Monty Python's Flying Circus'\n                           ^\nSyntaxError: invalid syntax\n3.2  Strings: Text Processing at the Lowest Level | 87\nSometimes strings go over several lines. Python provides us with various ways of en-\ntering them. \nIn the next example, a sequence of two strings is joined into a single string.\nWe need to use backslash \n  or parentheses \n  so that the interpreter knows that the\nstatement is not complete after the first line.\n>>> couplet = \"Shall I compare thee to a Summer's day?\"\\\n...           \"Thou are more lovely and more temperate:\" \n>>> print couplet\nShall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n>>> couplet = (\"Rough winds do shake the darling buds of May,\"\n...           \"And Summer's lease hath all too short a date:\") \n>>> print couplet\nRough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\nUnfortunately these \nmethods do not give us a newline between the two lines of the\nsonnet. Instead, we can use a triple-quoted string as follows:\n>>> couplet = \"\"\"Shall I compare thee to a Summer's day?\n... Thou are more lovely and more temperate:\"\"\"\n>>> print couplet\nShall I compare thee to a Summer's day?\nThou are more lovely and more temperate:\n>>> couplet = '''Rough winds do shake the darling buds of May,\n... And Summer's lease hath all too short a date:'''\n>>> print couplet\nRough winds do shake the darling buds of May,\nAnd Summer's lease hath all too short a date:\nNow that we can define strings, we can try some simple operations on them. First let\u2019s\nlook at the + operation, known as concatenation \n . It produces a new string that is a\ncopy of \nthe two original strings pasted together end-to-end. Notice that concatenation\ndoesn\u2019t do anything clever like insert a space between the words. We can even multiply\nstrings \n :\n>>> 'very' + 'very' + 'very' \n'veryveryvery'\n>>> 'very' * 3 \n'veryveryvery'\nYour Turn: Try running the following code, then try to use your un-\nderstanding of the string + and * operations to figure out how it works.\nBe careful to distinguish between the string ' ', which is a single white-\nspace character, and '', which is the empty string.\n>>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n>>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n>>> for line in b:\n...     print b\nWe\u2019ve seen that the addition and multiplication operations apply to strings, not just\nnumbers. However, note that we cannot use subtraction or division with strings:\n88 | Chapter 3: \u2002Processing Raw Text\n>>> 'very' - 'y'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n>>> 'very' / 2\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\nThese error \nmessages are another example of Python telling us that we have got our\ndata types in a muddle. In the first case, we are told that the operation of subtraction\n(i.e., -) cannot apply to objects of type str (strings), while in the second, we are told\nthat division cannot take str and int as its two operands.\nPrinting Strings\nSo far, when we have wanted to look at the contents of a variable or see the result of a\ncalculation, we have just typed the variable name into the interpreter. We can also see\nthe contents of a variable using the print statement:\n>>> print monty\nMonty Python\nNotice that there are no quotation marks this time. When we inspect a variable by\ntyping its name in the interpreter, the interpreter prints the Python representation of\nits value. Since it\u2019s a string, the result is quoted. However, when we tell the interpreter\nto print the contents of the variable, we don\u2019t see quotation characters, since there are\nnone inside the string.\nThe print statement allows us to display more than one item on a line in various ways,\nas shown here:\n>>> grail = 'Holy Grail'\n>>> print monty + grail\nMonty PythonHoly Grail\n>>> print monty, grail\nMonty Python Holy Grail\n>>> print monty, \"and the\", grail\nMonty Python and the Holy Grail\nAccessing Individual Characters\nAs we saw in Section 1.2  for lists, strings are indexed, starting from zero. When we\nindex a string, we get one of its characters (or letters). A single character is nothing\nspecial\u2014it\u2019s just a string of length 1.\n>>> monty[0]\n'M'\n>>> monty[3]\n't'\n>>> monty[5]\n' '\n3.2  Strings: Text Processing at the Lowest Level | 89\nAs with lists, if we try to access an index that is outside of the string, we get an error:\n>>> monty[20]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\nIndexError: string index out of range\nAgain as \nwith lists, we can use negative indexes for strings, where -1 is the index of the\nlast character \n . Positive and negative indexes give us two ways to refer to any position\nin a \nstring. In this case, when the string had a length of 12, indexes 5 and -7 both refer\nto the same character (a space). (Notice that 5 = len(monty) - 7.)\n>>> monty[-1] \n'n'\n>>> monty[5]\n' '\n>>> monty[-7]\n' '\nWe can \nwrite for loops to iterate over the characters in strings. This print statement\nends with a trailing comma, which is how we tell Python not to print a newline at the\nend.\n>>> sent = 'colorless green ideas sleep furiously'\n>>> for char in sent:\n...     print char,\n...\nc o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y\nWe can count individual characters as well. We should ignore the case distinction by\nnormalizing everything to lowercase, and filter out non-alphabetic characters:\n>>> from nltk.corpus import gutenberg\n>>> raw = gutenberg.raw('melville-moby_dick.txt')\n>>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n>>> fdist.keys()\n['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',\n'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\nThis gives us the letters of the alphabet, with the most frequently occurring letters listed\nfirst (this is quite complicated and we\u2019ll explain it more carefully later). You might like\nto visualize the distribution using fdist.plot(). The relative character frequencies of\na text can be used in automatically identifying the language of the text.\nAccessing Substrings\nA substring is any continuous section of a string that we want to pull out for further\nprocessing. We can easily access substrings using the same slice notation we used for\nlists (see Figure 3-2 ). For example, the following code accesses the substring starting\nat index 6, up to (but not including) index 10:\n>>> monty[6:10]\n'Pyth'\n90 | Chapter 3: \u2002Processing Raw Text\nHere we see the characters are 'P', 'y', 't', and 'h', which correspond to monty[6] ...\nmonty[9] but not monty[10]. This is because a slice starts at the first index but finishes\none before the end index.\nWe can also slice with negative indexes\u2014the same basic rule of starting from the start\nindex and stopping one before the end index applies; here we stop before the space\ncharacter.\n>>> monty[-12:-7]\n'Monty'\nAs with list slices, if we omit the first value, the substring begins at the start of the string.\nIf we omit the second value, the substring continues to the end of the string:\n>>> monty[:5]\n'Monty'\n>>> monty[6:]\n'Python'\nWe test if a string contains a particular substring using the in operator, as follows:\n>>> phrase = 'And now for something completely different'\n>>> if 'thing' in phrase:\n...     print 'found \"thing\"'\nfound \"thing\"\nWe can also find the position of a substring within a string, using find():\n>>> monty.find('Python')\n6\nYour Turn:  Make up a sentence and assign it to a variable, e.g., sent =\n'my sentence...'. Now write slice expressions to pull out individual\nwords. (This is obviously not a convenient way to process the words of\na text!)\nFigure 3-2. String slicing: The string Monty Python  is shown along with its positive and negative\nindexes; two substrings are selected using \u201cslice\u201d notation. The slice [m,n]  contains the characters\nfrom position m through n-1.\n3.2  Strings: Text Processing at the Lowest Level | 91\nMore Operations on Strings\nPython has \ncomprehensive support for processing strings. A summary, including some\noperations we haven\u2019t seen yet, is shown in Table 3-2. For more information on strings,\ntype help(str) at the Python prompt.\nTable 3-2. Useful string methods: Operations on strings in addition to the string tests shown in\nTable 1-4; all methods produce a new string or list\nMethod Functionality\ns.find(t) Index of first instance of string t inside s (-1 if not found)\ns.rfind(t) Index of last instance of string t inside s (-1 if not found)\ns.index(t) Like s.find(t), except it raises ValueError if not found\ns.rindex(t) Like s.rfind(t), except it raises ValueError if not found\ns.join(text) Combine the words of the text into a string using s as the glue\ns.split(t) Split s into a list wherever a t is found (whitespace by default)\ns.splitlines() Split s into a list of strings, one per line\ns.lower() A lowercased version of the string s\ns.upper() An uppercased version of the string s\ns.titlecase() A titlecased version of the string s\ns.strip() A copy of s without leading or trailing whitespace\ns.replace(t, u) Replace instances of t with u inside s\nThe Difference Between Lists and Strings\nStrings and lists are both kinds of sequence. We can pull them apart by indexing and\nslicing them, \nand we can join them together by concatenating them. However, we can-\nnot join strings and lists:\n>>> query = 'Who knows?'\n>>> beatles = ['John', 'Paul', 'George', 'Ringo']\n>>> query[2]\n'o'\n>>> beatles[2]\n'George'\n>>> query[:2]\n'Wh'\n>>> beatles[:2]\n['John', 'Paul']\n>>> query + \" I don't\"\n\"Who knows? I don't\"\n>>> beatles + 'Brian'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only concatenate list (not \"str\") to list\n>>> beatles + ['Brian']\n['John', 'Paul', 'George', 'Ringo', 'Brian']\n92 | Chapter 3: \u2002Processing Raw Text\nWhen we open a file for reading into a Python program, we get a string corresponding\nto the \ncontents of the whole file. If we use a for loop to process the elements of this\nstring, all we can pick out are the individual characters\u2014we don\u2019t get to choose the\ngranularity. By contrast, the elements of a list can be as big or small as we like: for\nexample, they could be paragraphs, sentences, phrases, words, characters. So lists have\nthe advantage that we can be flexible about the elements they contain, and corre-\nspondingly flexible about any downstream processing. Consequently, one of the first\nthings we are likely to do in a piece of NLP code is tokenize a string into a list of strings\n(Section 3.7 ). Conversely, when we want to write our results to a file, or to a terminal,\nwe will usually format them as a string (Section 3.9).\nLists and strings do not have exactly the same functionality. Lists have the added power\nthat you can change their elements:\n>>> beatles[0] = \"John Lennon\"\n>>> del beatles[-1]\n>>> beatles\n['John Lennon', 'Paul', 'George']\nOn the other hand, if we try to do that with a string\u2014changing the 0th character in\nquery to 'F'\u2014we get:\n>>> query[0] = 'F'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\nTypeError: object does not support item assignment\nThis is because strings are immutable: you can\u2019t change a string once you have created\nit. However, lists are mutable, and their contents can be modified at any time. As a\nresult, lists support operations that modify the original value rather than producing a\nnew value.\nYour Turn:  Consolidate your knowledge of strings by trying some of\nthe exercises on strings at the end of this chapter.\n3.3  Text Processing with Unicode\nOur programs will often need to deal with different languages, and different character\nsets. The concept of \u201cplain text\u201d is a fiction. If you live in the English-speaking world\nyou probably use ASCII, possibly without realizing it. If you live in Europe you might\nuse one of the extended Latin character sets, containing such characters as \u201c\u00f8\u201d for\nDanish and Norwegian, \u201c \u0151\u201d for Hungarian, \u201c\u00f1\u201d for Spanish and Breton, and \u201c \u0148\u201d for\nCzech and Slovak. In this section, we will give an overview of how to use Unicode for\nprocessing texts that use non-ASCII character sets.\n3.3  Text Processing with Unicode | 93\nWhat Is Unicode?\nUnicode supports \nover a million characters. Each character is assigned a number, called\na code point . In Python, code points are written in the form \\uXXXX, where XXXX\nis the number in four-digit hexadecimal form.\nWithin a program, we can manipulate Unicode strings just like normal strings. How-\never, when Unicode characters are stored in files or displayed on a terminal, they must\nbe encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a\nsingle byte per code point, so they can support only a small subset of Unicode, enough\nfor a single language. Other encodings (such as UTF-8) use multiple bytes and can\nrepresent the full range of Unicode characters.\nText in files will be in a particular encoding, so we need some mechanism for translating\nit into Unicode\u2014translation into Unicode is called decoding. Conversely, to write out\nUnicode to a file or a terminal, we first need to translate it into a suitable encoding\u2014\nthis translation out of Unicode is called encoding, and is illustrated in Figure 3-3.\nFigure 3-3. Unicode decoding and encoding.\nFrom a \nUnicode perspective, characters are abstract entities that can be realized as one\nor more glyphs. Only glyphs can appear on a screen or be printed on paper. A font is\na mapping from characters to glyphs.\nExtracting Encoded Text from Files\nLet\u2019s assume that we have a small text file, and that we know how it is encoded. For\nexample, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish\nWikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as\nLatin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for\nus.\n94 | Chapter 3: \u2002Processing Raw Text\n>>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\nThe Python codecs\n module provides functions to read encoded data into Unicode\nstrings, and to write out Unicode strings in encoded form. The codecs.open() function\ntakes an encoding parameter to specify the encoding of the file being read or written.\nSo let\u2019s import the codecs module, and call it with the encoding 'latin2' to open our\nPolish file as Unicode:\n>>> import codecs\n>>> f = codecs.open(path, encoding='latin2')\nFor a list of encoding parameters allowed by codecs, see http://docs.python.org/lib/\nstandard-encodings.html. Note that we can write Unicode-encoded data to a file using\nf = codecs.open(path, 'w', encoding='utf-8').\nText read from the file object f will be returned in Unicode. As we pointed out earlier,\nin order to view this text on a terminal, we need to encode it, using a suitable encoding.\nThe Python-specific encoding unicode_escape is a dummy encoding that converts all\nnon-ASCII characters into their \\uXXXX representations. Code points above the ASCII\n0\u2013127 range but below 256 are represented in the two-digit form \\xXX.\n>>> for line in f:\n...     line = line.strip()\n...     print line.encode('unicode_escape')\nPruska Biblioteka Pa\\u0144stwowa. Jej dawne zbiory znane pod nazw\\u0105\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\nNiemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\nodnalezione po 1945 r. na terytorium Polski. Trafi\\u0142y do Biblioteki\nJagiello\\u0144skiej w Krakowie, obejmuj\\u0105 ponad 500 tys. zabytkowych\narchiwali\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\nThe first line in this output illustrates a Unicode escape string preceded by the \\u escape\nstring, namely \\u0144. The relevant Unicode character will be displayed on the screen\nas the glyph \u0144. In the third line of the preceding example, we see \\xf3, which corre-\nsponds to the glyph \u00f3, and is within the 128\u2013255 range.\nIn Python, a Unicode string literal can be specified by preceding an ordinary string\nliteral with a u, as in u'hello'. Arbitrary Unicode characters are defined using the\n\\uXXXX escape sequence inside a Unicode string literal. We find the integer ordinal\nof a character using ord(). For example:\n>>> ord('a')\n97\nThe hexadecimal four-digit notation for 97 is 0061, so we can define a Unicode string\nliteral with the appropriate escape sequence:\n>>> a = u'\\u0061'\n>>> a\nu'a'\n>>> print a\na\n3.3  Text Processing with Unicode | 95\nNotice that the Python print statement is assuming a default encoding of the Unicode\ncharacter, namely ASCII. However, \u0144 is outside the ASCII range, so cannot be printed\nunless we specify an encoding. In the following example, we have specified that\nprint should use the repr() of the string, which outputs the UTF-8 escape sequences\n(of the form \\xXX) rather than trying to render the glyphs.\n>>> nacute = u'\\u0144'\n>>> nacute\nu'\\u0144'\n>>> nacute_utf = nacute.encode('utf8')\n>>> print repr(nacute_utf)\n'\\xc5\\x84'\nIf your operating system and locale are set up to render UTF-8 encoded characters, you\nought to be able to give the Python command print nacute_utf and see \u0144 on your\nscreen.\nThere are many factors determining what glyphs are rendered on your\nscreen. If \nyou are sure that you have the correct encoding, but your\nPython code is still failing to produce the glyphs you expected, you\nshould also check that you have the necessary fonts installed on your\nsystem.\nThe module unicodedata lets us inspect the properties of Unicode characters. In the\nfollowing example, we select all characters in the third line of our Polish text outside\nthe ASCII range and print their UTF-8 escaped value, followed by their code point\ninteger using the standard Unicode convention (i.e., prefixing the hex digits with U+),\nfollowed by their Unicode name.\n>>> import unicodedata\n>>> lines = codecs.open(path, encoding='latin2').readlines()\n>>> line = lines[2]\n>>> print line.encode('unicode_escape')\nNiemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\\n\n>>> for c in line:\n...     if ord(c) > 127:\n...         print '%r U+%04x %s' % (c.encode('utf8'), ord(c), unicodedata.name(c))\n'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\nIf you replace the %r (which yields the repr() value) by %s in the format string of the\npreceding code sample, and if your system supports UTF-8, you should see an output\nlike the following:\n\u00f3 U+00f3 LATIN SMALL LETTER O WITH ACUTE\n\u015b U+015b LATIN SMALL LETTER S WITH ACUTE\n\u015a U+015a LATIN CAPITAL LETTER S WITH ACUTE\n96 | Chapter 3: \u2002Processing Raw Text\n\u0105 U+0105 LATIN SMALL LETTER A WITH OGONEK\n\u0142 U+0142 LATIN SMALL LETTER L WITH STROKE\nAlternatively, you \nmay need to replace the encoding 'utf8' in the example by\n'latin2', again depending on the details of your system.\nThe next examples illustrate how Python string methods and the re module accept\nUnicode strings.\n>>> line.find(u'zosta\\u0142y')\n54\n>>> line = line.lower()\n>>> print line.encode('unicode_escape')\nniemc\\xf3w pod koniec ii wojny \\u015bwiatowej na dolny \\u015bl\\u0105sk, zosta\\u0142y\\n\n>>> import re\n>>> m = re.search(u'\\u015b\\w*', line)\n>>> m.group()\nu'\\u015bwiatowej'\nNLTK tokenizers allow Unicode strings as input, and correspondingly yield Unicode\nstrings as output.\n>>> nltk.word_tokenize(line)  \n[u'niemc\\xf3w', u'pod', u'koniec', u'ii', u'wojny', u'\\u015bwiatowej',\nu'na', u'dolny', u'\\u015bl\\u0105sk', u'zosta\\u0142y']\nUsing Your Local Encoding in Python\nIf you are used to working with characters in a particular local encoding, you probably\nwant to be able to use your standard methods for inputting and editing strings in a\nPython file. In order to do this, you need to include the string '# -*- coding: <coding>\n-*-' as the first or second line of your file. Note that <coding> has to be a string like\n'latin-1', 'big5', or 'utf-8' (see Figure 3-4).\nFigure 3-4 also illustrates how regular expressions can use encoded strings.\n3.4  Regular Expressions for Detecting Word Patterns\nMany linguistic processing tasks involve pattern matching. For example, we can find\nwords ending with ed using endswith('ed'). We saw a variety of such \u201cword tests\u201d in\nTable 1-4. Regular expressions give us a more powerful and flexible method for de-\nscribing the character patterns we are interested in.\nThere are many other published introductions to regular expressions,\norganized around \nthe syntax of regular expressions and applied to\nsearching text files. Instead of doing this again, we focus on the use of\nregular expressions at different stages of linguistic processing. As usual,\nwe\u2019ll adopt a problem-based approach and present new features only as\nthey are needed to solve practical problems. In our discussion we will\nmark regular expressions using chevrons like this: \u00abpatt\u00bb.\n3.4  Regular Expressions for Detecting Word Patterns | 97\nTo use regular expressions in Python, we need to import the re library using: import\nre. We also need a list of words to search; we\u2019ll use the Words Corpus again ( Sec-\ntion 2.4). We will preprocess it to remove any proper names.\n>>> import re\n>>> wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\nUsing Basic Metacharacters\nLet\u2019s find words ending with ed using the regular expression \u00ab ed$\u00bb. We will use the\nre.search(p, s) function to check whether the pattern p can be found somewhere\ninside the string s. We need to specify the characters of interest, and use the dollar sign,\nwhich has a special behavior in the context of regular expressions in that it matches the\nend of the word:\n>>> [w for w in wordlist if re.search('ed$', w)]\n['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]\nThe . wildcard symbol matches any single character. Suppose we have room in a\ncrossword puzzle for an eight-letter word, with j as its third letter and t as its sixth letter.\nIn place of each blank cell we use a period:\n>>> [w for w in wordlist if re.search('^..j..t..$', w)]\n['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]\nFigure 3-4. Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that\nan appropriate font is set in IDLE\u2019s preferences; here we have chosen Courier CE.\n98 | Chapter 3: \u2002Processing Raw Text\nYour Turn: The caret symbol ^ matches the start of a string, just like\nthe $ matches the end. What results do we get with the example just\nshown if we leave out both of these, and search for \u00ab..j..t..\u00bb?\nFinally, the ? symbol specifies that the previous character is optional. Thus \u00ab ^e-?mail\n$\u00bb will match both email and e-mail. We could count the total number of occurrences\nof this word (in either spelling) in a text using sum(1 for w in text if re.search('^e-?\nmail$', w)).\nRanges and Closures\nThe T9 system is used for entering text on mobile phones (see Figure 3-5). Two or more\nwords that are entered with the same sequence of keystrokes are known as\ntextonyms. For example, both hole and golf are entered by pressing the sequence 4653.\nWhat other words could be produced with the same sequence? Here we use the regular\nexpression \u00ab^[ghi][mno][jlk][def]$\u00bb:\n>>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]\n['gold', 'golf', 'hold', 'hole']\nThe first part of the expression, \u00ab^[ghi]\u00bb, matches the start of a word followed by g,\nh, or i. The next part of the expression, \u00ab [mno]\u00bb, constrains the second character to be m,\nn, or o. The third and fourth characters are also constrained. Only four words satisfy\nall these constraints. Note that the order of characters inside the square brackets is not\nsignificant, so we could have written \u00ab ^[hig][nom][ljk][fed]$\u00bb and matched the same\nwords.\nFigure 3-5. T9: Text on 9 keys.\nYour Turn:  Look for some \u201cfinger-twisters,\u201d by searching for words\nthat use only part of the number-pad. For example \u00ab ^[ghijklmno]+$\u00bb,\nor more concisely, \u00ab ^[g-o]+$\u00bb, will match words that only use keys 4,\n5, 6 in the center row, and \u00ab ^[a-fj-o]+$\u00bb will match words that use keys\n2, 3, 5, 6 in the top-right corner. What do - and + mean?\n3.4  Regular Expressions for Detecting Word Patterns | 99\nLet\u2019s explore the + symbol a bit further. Notice that it can be applied to individual\nletters, or to bracketed sets of letters:\n>>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n>>> [w for w in chat_words if re.search('^m+i+n+e+$', w)]\n['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',\n'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n>>> [w for w in chat_words if re.search('^[ha]+$', w)]\n['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',\n'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',\n'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]\nIt should be clear that + simply means \u201cone or more instances of the preceding item,\u201d\nwhich could be an individual character like m, a set like [fed], or a range like [d-f].\nNow let\u2019s replace + with *, which means \u201czero or more instances of the preceding item.\u201d\nThe regular expression \u00ab ^m*i*n*e*$\u00bb will match everything that we found using \u00ab ^m+i\n+n+e+$\u00bb, but also words where some of the letters don\u2019t appear at all, e.g., me, min, and\nmmmmm. Note that the + and * symbols are sometimes referred to as Kleene clo-\nsures, or simply closures.\nThe ^ operator has another function when it appears as the first character inside square\nbrackets. For example, \u00ab [^aeiouAEIOU]\u00bb matches any character other than a vowel. We\ncan search the NPS Chat Corpus for words that are made up entirely of non-vowel\ncharacters using \u00ab ^[^aeiouAEIOU]+$\u00bb to find items like these: :):):), grrr, cyb3r, and\nzzzzzzzz. Notice this includes non-alphabetic characters.\nHere are some more examples of regular expressions being used to find tokens that\nmatch a particular pattern, illustrating the use of some new symbols: \\, {}, (), and |.\n>>> wsj = sorted(set(nltk.corpus.treebank.words()))\n>>> [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',\n'0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',\n'1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]\n>>> [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n['C$', 'US$']\n>>> [w for w in wsj if re.search('^[0-9]{4}$', w)]\n['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]\n>>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]\n>>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',\n'savings-and-loan']\n>>> [w for w in wsj if re.search('(ed|ing)$', w)]\n['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]\nYour Turn:  Study the previous examples and try to work out what the \\,\n{}, (), and | notations mean before you read on.\n100 | Chapter 3: \u2002Processing Raw Text\nYou probably worked out that a backslash means that the following character is de-\nprived of \nits special powers and must literally match a specific character in the word.\nThus, while . is special, \\. only matches a period. The braced expressions, like {3,5},\nspecify the number of repeats of the previous item. The pipe character indicates a choice\nbetween the material on its left or its right. Parentheses indicate the scope of an oper-\nator, and they can be used together with the pipe (or disjunction) symbol like this:\n\u00abw(i|e|ai|oo)t\u00bb, matching wit, wet, wait, and woot. It is instructive to see what happens\nwhen you omit the parentheses from the last expression in the example, and search for\n\u00abed|ing$\u00bb.\nThe metacharacters we have seen are summarized in Table 3-3.\nTable 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures\nOperator Behavior\n. Wildcard, matches any character\n^abc Matches some pattern abc at the start of a string\nabc$ Matches some pattern abc at the end of a string\n[abc] Matches one of a set of characters\n[A-Z0-9] Matches one of a range of characters\ned|ing|s Matches one of the specified strings (disjunction)\n* Zero or more of previous item, e.g., a*, [a-z]* (also known as Kleene Closure)\n+ One or more of previous item, e.g., a+, [a-z]+\n? Zero or one of the previous item (i.e., optional), e.g., a?, [a-z]?\n{n} Exactly n repeats where n is a non-negative integer\n{n,} At least n repeats\n{,n} No more than n repeats\n{m,n} At least m and no more than n repeats\na(b|c)+ Parentheses that indicate the scope of the operators\nTo the Python interpreter, a regular expression is just like any other string. If the string\ncontains a \nbackslash followed by particular characters, it will interpret these specially.\nFor example, \\b would be interpreted as the backspace character. In general, when\nusing regular expressions containing backslash, we should instruct the interpreter not\nto look inside the string at all, but simply to pass it directly to the re library for pro-\ncessing. We do this by prefixing the string with the letter r, to indicate that it is a raw\nstring. For example, the raw string r'\\band\\b' contains two \\b symbols that are\ninterpreted by the re library as matching word boundaries instead of backspace char-\nacters. If you get into the habit of using r'...' for regular expressions\u2014as we will do\nfrom now on\u2014you will avoid having to think about these complications.\n3.4  Regular Expressions for Detecting Word Patterns | 101\n3.5  Useful Applications of Regular Expressions\nThe previous \nexamples all involved searching for words w that match some regular\nexpression regexp using re.search(regexp, w) . Apart from checking whether a regular\nexpression matches a word, we can use regular expressions to extract material from\nwords, or to modify words in specific ways.\nExtracting Word Pieces\nThe re.findall() (\u201cfind all\u201d) method finds all (non-overlapping) matches of the given\nregular expression. Let\u2019s find all the vowels in a word, then count them:\n>>> word = 'supercalifragilisticexpialidocious'\n>>> re.findall(r'[aeiou]', word)\n['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n>>> len(re.findall(r'[aeiou]', word))\n16\nLet\u2019s look for all sequences of two or more vowels in some text, and determine their\nrelative frequency:\n>>> wsj = sorted(set(nltk.corpus.treebank.words()))\n>>> fd = nltk.FreqDist(vs for word in wsj\n...                       for vs in re.findall(r'[aeiou]{2,}', word))\n>>> fd.items()\n[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),\n('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95),\n('ei', 86), ('oi', 65), ('oa', 59), ('eo', 39), ('iou', 27), ('eu', 18), ...]\nYour Turn: In the W3C Date Time Format, dates are represented like\nthis: 2009-12-31. \nReplace the ? in the following Python code with a\nregular expression, in order to convert the string '2009-12-31' to a list\nof integers [2009, 12, 31]:\n[int(n) for n in re.findall(?, '2009-12-31')]\nDoing More with Word Pieces\nOnce we can use re.findall() to extract material from words, there are interesting\nthings to do with the pieces, such as glue them back together or plot them.\nIt is sometimes noted that English text is highly redundant, and it is still easy to read\nwhen word-internal vowels are left out. For example, declaration becomes dclrtn, and\ninalienable becomes inlnble, retaining any initial or final vowel sequences. The regular\nexpression in our next example matches initial vowel sequences, final vowel sequences,\nand all consonants; everything else is ignored. This three-way disjunction is processed\nleft-to-right, and if one of the three parts matches the word, any later parts of the regular\nexpression are ignored. We use re.findall() to extract all the matching pieces, and\n''.join() to join them together (see Section 3.9 for more about the join operation).\n102 | Chapter 3: \u2002Processing Raw Text\n>>> regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n>>> def compress(word):\n...     pieces = re.findall(regexp, word)\n...     return ''.join(pieces)\n...\n>>> english_udhr = nltk.corpus.udhr.words('English-Latin1')\n>>> print nltk.tokenwrap(compress(w) for w in english_udhr[:75])\nUnvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\nof the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\nof frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\nrghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\nand the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\nNext, let\u2019s \ncombine regular expressions with conditional frequency distributions. Here\nwe will extract all consonant-vowel sequences from the words of Rotokas, such as ka\nand si. Since each of these is a pair, it can be used to initialize a conditional frequency\ndistribution. We then tabulate the frequency of each pair:\n>>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n>>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n>>> cfd = nltk.ConditionalFreqDist(cvs)\n>>> cfd.tabulate()\n     a    e    i    o    u\nk  418  148   94  420  173\np   83   31  105   34   51\nr  187   63   84   89   79\ns    0    0  100    2    1\nt   47    8    0  148   37\nv   93   27  105   48   49\nExamining the rows for s and t, we see they are in partial \u201ccomplementary distribution,\u201d\nwhich is evidence that they are not distinct phonemes in the language. Thus, we could\nconceivably drop s from the Rotokas alphabet and simply have a pronunciation rule\nthat the letter t is pronounced s when followed by i. (Note that the single entry having\nsu, namely kasuari, \u2018cassowary\u2019 is borrowed from English).\nIf we want to be able to inspect the words behind the numbers in that table, it would\nbe helpful to have an index, allowing us to quickly find the list of words that contains\na given consonant-vowel pair. For example, cv_index['su'] should give us all words\ncontaining su. Here\u2019s how we can do this:\n>>> cv_word_pairs = [(cv, w) for w in rotokas_words\n...                          for cv in re.findall(r'[ptksvr][aeiou]', w)]\n>>> cv_index = nltk.Index(cv_word_pairs)\n>>> cv_index['su']\n['kasuari']\n>>> cv_index['po']\n['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',\n'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]\nThis program processes each word w in turn, and for each one, finds every substring\nthat matches the regular expression \u00ab [ptksvr][aeiou]\u00bb. In the case of the word ka-\nsuari, it finds ka, su, and ri. Therefore, the cv_word_pairs list will contain ('ka', 'ka\n3.5  Useful Applications of Regular Expressions | 103\nsuari'), ('su', 'kasuari'), and (' ri', 'kasuari'). One further step, using\nnltk.Index(), converts this into a useful index.\nFinding Word Stems\nWhen we use a web search engine, we usually don\u2019t mind (or even notice) if the words\nin the document differ from our search terms in having different endings. A query for\nlaptops finds documents containing laptop and vice versa. Indeed, laptop and laptops\nare just two forms of the same dictionary word (or lemma). For some language pro-\ncessing tasks we want to ignore word endings, and just deal with word stems.\nThere are various ways we can pull out the stem of a word. Here\u2019s a simple-minded\napproach that just strips off anything that looks like a suffix:\n>>> def stem(word):\n...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n...         if word.endswith(suffix):\n...             return word[:-len(suffix)]\n...     return word\nAlthough we will ultimately use NLTK\u2019s built-in stemmers, it\u2019s interesting to see how\nwe can use regular expressions for this task. Our first step is to build up a disjunction\nof all the suffixes. We need to enclose it in parentheses in order to limit the scope of\nthe disjunction.\n>>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n['ing']\nHere, re.findall() just gave us the suffix even though the regular expression matched\nthe entire word. This is because the parentheses have a second function, to select sub-\nstrings to be extracted. If we want to use the parentheses to specify the scope of the\ndisjunction, but not to select the material to be output, we have to add ?:, which is just\none of many arcane subtleties of regular expressions. Here\u2019s the revised version.\n>>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n['processing']\nHowever, we\u2019d actually like to split the word into stem and suffix. So we should just\nparenthesize both parts of the regular expression:\n>>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n[('process', 'ing')]\nThis looks promising, but still has a problem. Let\u2019s look at a different word, processes:\n>>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n[('processe', 's')]\nThe regular expression incorrectly found an -s suffix instead of an -es suffix. This dem-\nonstrates another subtlety: the star operator is \u201cgreedy\u201d and so the .* part of the ex-\npression tries to consume as much of the input as possible. If we use the \u201cnon-greedy\u201d\nversion of the star operator, written *?, we get what we want:\n104 | Chapter 3: \u2002Processing Raw Text\n>>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n[('process', 'es')]\nThis works \neven when we allow an empty suffix, by making the content of the second\nparentheses optional:\n>>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')\n[('language', '')]\nThis approach still has many problems (can you spot them?), but we will move on to\ndefine a function to perform stemming, and apply it to a whole text:\n>>> def stem(word):\n...     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n...     stem, suffix = re.findall(regexp, word)[0]\n...     return stem\n...\n>>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n... is no basis for a system of government.  Supreme executive power derives from\n... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n>>> tokens = nltk.word_tokenize(raw)\n>>> [stem(t) for t in tokens]\n['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond',\n'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n'.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from',\n'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\nNotice that our regular expression removed the s from ponds but also from is and\nbasis. It produced some non-words, such as distribut and deriv, but these are acceptable\nstems in some applications.\nSearching Tokenized Text\nYou can use a special kind of regular expression for searching across multiple words in\na text (where a text is a list of tokens). For example, \"<a> <man>\"  finds all instances of\na man in the text. The angle brackets are used to mark token boundaries, and any\nwhitespace between the angle brackets is ignored (behaviors that are unique to NLTK\u2019s\nfindall() method for texts). In the following example, we include <.*> \n , which will\nmatch any single token, and enclose it in parentheses so only the matched word (e.g.,\nmonied) and \nnot the matched phrase (e.g., a monied man) is produced. The second\nexample finds three-word phrases ending with the word bro \n . The last example finds\nsequences of three or more words starting with the letter l \n .\n>>> from nltk.corpus import gutenberg, nps_chat\n>>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n>>> moby.findall(r\"<a> (<.*>) <man>\") \nmonied; nervous; dangerous; white; white; white; pious; queer; good;\nmature; white; Cape; great; wise; wise; butterless; white; fiendish;\npale; furious; better; certain; complete; dismasted; younger; brave;\nbrave; brave; brave\n>>> chat = nltk.Text(nps_chat.words())\n>>> chat.findall(r\"<.*> <.*> <bro>\") \nyou rule bro; telling you bro; u twizted bro\n3.5  Useful Applications of Regular Expressions | 105\n>>> chat.findall(r\"<l.*>{3,}\") \nlol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\nla la; lovely lol lol love; lol lol lol.; la la la; la la la\nYour Turn: Consolidate your understanding of regular expression pat-\nterns and substitutions using nltk.re_show(p, s), which annotates the\nstring s to show every place where pattern p was matched, and\nnltk.app.nemo(), which provides a graphical interface for exploring reg-\nular expressions. For more practice, try some of the exercises on regular\nexpressions at the end of this chapter.\nIt is easy to build search patterns when the linguistic phenomenon we\u2019re studying is\ntied to particular words. In some cases, a little creativity will go a long way. For instance,\nsearching a large text corpus for expressions of the form x and other ys  allows us to\ndiscover hypernyms (see Section 2.5):\n>>> from nltk.corpus import brown\n>>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n>>> hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")\nspeed and other activities; water and other liquids; tomb and other\nlandmarks; Statues and other monuments; pearls and other jewels;\ncharts and other items; roads and other features; figures and other\nobjects; military and other areas; demands and other factors;\nabstracts and other compilations; iron and other metals\nWith enough text, this approach would give us a useful store of information about the\ntaxonomy of objects, without the need for any manual labor. However, our search\nresults will usually contain false positives, i.e., cases that we would want to exclude.\nFor example, the result demands and other factors  suggests that demand is an instance\nof the type factor, but this sentence is actually about wage demands. Nevertheless, we\ncould construct our own ontology of English concepts by manually correcting the out-\nput of such searches.\nThis combination of automatic and manual processing is the most com-\nmon way \nfor new corpora to be constructed. We will return to this in\nChapter 11.\nSearching corpora also suffers from the problem of false negatives, i.e., omitting cases\nthat we would want to include. It is risky to conclude that some linguistic phenomenon\ndoesn\u2019t exist in a corpus just because we couldn\u2019t find any instances of a search pattern.\nPerhaps we just didn\u2019t think carefully enough about suitable patterns.\nYour Turn: Look for instances of the pattern as x as y  to discover in-\nformation about entities and their properties.\n106 | Chapter 3: \u2002Processing Raw Text\n3.6  Normalizing Text\nIn earlier \nprogram examples we have often converted text to lowercase before doing\nanything with its words, e.g., set(w.lower() for w in text). By using lower(), we have\nnormalized the text to lowercase so that the distinction between The and the is ignored.\nOften we want to go further than this and strip off any affixes, a task known as stem-\nming. A further step is to make sure that the resulting form is a known word in a\ndictionary, a task known as lemmatization. We discuss each of these in turn. First, we\nneed to define the data we will use in this section:\n>>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n... is no basis for a system of government.  Supreme executive power derives from\n... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n>>> tokens = nltk.word_tokenize(raw)\nStemmers\nNLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you\nshould use one of these in preference to crafting your own using regular expressions,\nsince NLTK\u2019s stemmers handle a wide range of irregular cases. The Porter and Lan-\ncaster stemmers follow their own rules for stripping affixes. Observe that the Porter\nstemmer correctly handles the word lying (mapping it to lie), whereas the Lancaster\nstemmer does not.\n>>> porter = nltk.PorterStemmer()\n>>> lancaster = nltk.LancasterStemmer()\n>>> [porter.stem(t) for t in tokens]\n['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond',\n'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n'.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',\n'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n>>> [lancaster.stem(t) for t in tokens]\n['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',\n'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',\n'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',\n'from', 'som', 'farc', 'aqu', 'ceremony', '.']\nStemming is not a well-defined process, and we typically pick the stemmer that best\nsuits the application we have in mind. The Porter Stemmer is a good choice if you are\nindexing some texts and want to support search using alternative forms of words (il-\nlustrated in Example 3-1 , which uses object-oriented programming techniques that are\noutside the scope of this book, string formatting techniques to be covered in Sec-\ntion 3.9, and the enumerate() function to be explained in Section 4.2).\nExample 3-1. Indexing a text using a stemmer.\nclass IndexedText(object):\n    def __init__(self, stemmer, text):\n        self._text = text\n        self._stemmer = stemmer\n3.6  Normalizing Text | 107\n        self._index = nltk.Index((self._stem(word), i)\n                                 for (i, word) in enumerate(text))\n    def concordance(self, word, width=40):\n        key = self._stem(word)\n        wc = width/4                # words of context\n        for i in self._index[key]:\n            lcontext = ' '.join(self._text[i-wc:i])\n            rcontext = ' '.join(self._text[i:i+wc])\n            ldisplay = '%*s'  % (width, lcontext[-width:])\n            rdisplay = '%-*s' % (width, rcontext[:width])\n            print ldisplay, rdisplay\n    def _stem(self, word):\n        return self._stemmer.stem(word).lower()\n>>> porter = nltk.PorterStemmer()\n>>> grail = nltk.corpus.webtext.words('grail.txt')\n>>> text = IndexedText(porter, grail)\n>>> text.concordance('lie')\nr king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !\ndoctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well\nere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\n   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\nh it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\nnot stop our fight ' til each one of you lies dead , and the Holy Grail returns t\nLemmatization\nThe WordNet \nlemmatizer removes affixes only if the resulting word is in its dictionary.\nThis additional checking process makes the lemmatizer slower than the stemmers just\nmentioned. Notice that it doesn\u2019t handle lying, but it converts women to woman.\n>>> wnl = nltk.WordNetLemmatizer()\n>>> [wnl.lemmatize(t) for t in tokens]\n['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',\n'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',\n'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',\n'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',\n'aquatic', 'ceremony', '.']\nThe WordNet lemmatizer is a good choice if you want to compile the vocabulary of\nsome texts and want a list of valid lemmas (or lexicon headwords).\nAnother normalization task involves identifying non-standard\nwords, including \nnumbers, abbreviations, and dates, and mapping any\nsuch tokens to a special vocabulary. For example, every decimal number\ncould be mapped to a single token 0.0, and every acronym could be\nmapped to AAA. This keeps the vocabulary small and improves the ac-\ncuracy of many language modeling tasks.\n108 | Chapter 3: \u2002Processing Raw Text\n3.7  Regular Expressions for Tokenizing Text\nTokenization is \nthe task of cutting a string into identifiable linguistic units that consti-\ntute a piece of language data. Although it is a fundamental task, we have been able to\ndelay it until now because many corpora are already tokenized, and because NLTK\nincludes some tokenizers. Now that you are familiar with regular expressions, you can\nlearn how to use them to tokenize text, and to have much more control over the process.\nSimple Approaches to Tokenization\nThe very simplest method for tokenizing text is to split on whitespace. Consider the\nfollowing text from Alice\u2019s Adventures in Wonderland:\n>>> raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\nWe could split this raw text on whitespace using raw.split(). To do the same using a\nregular expression, it is not enough to match any space characters in the string \n , since\nthis results \nin tokens that contain a \\n newline character; instead, we need to match\nany number of spaces, tabs, or newlines \n :\n>>> re.split(r' ', raw) \n[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',\n\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n>>> re.split(r'[ \\t\\n]+', raw) \n[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',\n\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\nThe regular expression \u00ab [ \\t\\n]+ \u00bb matches \none or more spaces, tabs ( \\t), or newlines\n(\\n). Other whitespace characters, such as carriage return and form feed, should really\nbe included too. Instead, we will use a built-in re abbreviation, \\s, which means any\nwhitespace character. The second statement in the preceding example can be rewritten\nas re.split(r'\\s+', raw).\nImportant: Remember to prefix regular expressions with the letter r\n(meaning \u201craw\u201d), which instructs the Python interpreter to treat the\nstring literally, rather than processing any backslashed characters it\ncontains.\nSplitting on whitespace gives us tokens like '(not' and 'herself,'. An alternative is to\nuse the fact that Python provides us with a character class \\w for word characters,\nequivalent to [a-zA-Z0-9_]. It also defines the complement of this class, \\W, i.e., all\n3.7  Regular Expressions for Tokenizing Text | 109\ncharacters other than letters, digits, or underscore. We can use \\W in a simple regular\nexpression to split the input on anything other than a word character:\n>>> re.split(r'\\W+', raw)\n['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',\n'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper',\n'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without',\n'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered',\n'']\nObserve that this gives us empty strings at the start and the end (to understand why,\ntry doing 'xx'.split('x')). With re.findall(r'\\w+', raw), we get the same tokens,\nbut without the empty strings, using a pattern that matches the words instead of the\nspaces. Now that we\u2019re matching the words, we\u2019re in a position to extend the regular\nexpression to cover a wider range of cases. The regular expression \u00ab \\w+|\\S\\w*\u00bb will first\ntry to match any sequence of word characters. If no match is found, it will try to match\nany non-whitespace character ( \\S is the complement of \\s) followed by further word\ncharacters. This means that punctuation is grouped with any following letters\n(e.g., \u2019s) but that sequences of two or more punctuation characters are separated.\n>>> re.findall(r'\\w+|\\S\\w*', raw)\n[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n'(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\",\n'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',\n'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that',\n'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\nLet\u2019s generalize the \\w+ in the preceding expression to permit word-internal hyphens\nand apostrophes: \u00ab \\w+([-']\\w+)*\u00bb. This expression means \\w+ followed by zero or more\ninstances of [-']\\w+; it would match hot-tempered and it\u2019s. (We need to include ?: in\nthis expression for reasons discussed earlier.) We\u2019ll also add a pattern to match quote\ncharacters so these are kept separate from the text they enclose.\n>>> print re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)\n[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n'(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I',\n\"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup',\n'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper',\n'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\nThe expression in this example also included \u00ab [-.(]+\u00bb, which causes the double hy-\nphen, ellipsis, and open parenthesis to be tokenized separately.\nTable 3-4  lists the regular expression character class symbols we have seen in this sec-\ntion, in addition to some other useful symbols.\nTable 3-4. Regular expression symbols\nSymbol Function\n\\b Word boundary (zero width)\n\\d Any decimal digit (equivalent to [0-9])\n110 | Chapter 3: \u2002Processing Raw Text\nSymbol Function\n\\D Any non-digit character (equivalent to [^0-9])\n\\s Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v]\n\\S Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v])\n\\w Any alphanumeric character (equivalent to [a-zA-Z0-9_])\n\\W Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n\\t The tab character\n\\n The newline character\nNLTK\u2019s Regular Expression Tokenizer\nThe function nltk.regexp_tokenize()\n is similar to re.findall() (as we\u2019ve been using\nit for tokenization). However, nltk.regexp_tokenize() is more efficient for this task,\nand avoids the need for special treatment of parentheses. For readability we break up\nthe regular expression over several lines and add a comment about each line. The special\n(?x) \u201cverbose flag\u201d tells Python to strip out the embedded whitespace and comments.\n>>> text = 'That U.S.A. poster-print costs $12.40...'\n>>> pattern = r'''(?x)    # set flag to allow verbose regexps\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n...   | \\w+(-\\w+)*        # words with optional internal hyphens\n...   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n...   | \\.\\.\\.            # ellipsis\n...   | [][.,;\"'?():-_`]  # these are separate tokens\n... '''\n>>> nltk.regexp_tokenize(text, pattern)\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\nWhen using the verbose flag, you can no longer use ' ' to match a space character; use\n\\s instead. The regexp_tokenize() function has an optional gaps parameter. When set\nto True, the regular expression specifies the gaps between tokens, as with re.split().\nWe can evaluate a tokenizer by comparing the resulting tokens with a\nwordlist, and then report any tokens that don\u2019t appear in the wordlist,\nusing set(tokens).difference(wordlist). You\u2019ll \nprobably want to\nlowercase all the tokens first.\nFurther Issues with Tokenization\nTokenization turns out to be a far more difficult task than you might have expected.\nNo single solution works well across the board, and we must decide what counts as a\ntoken depending on the application domain.\nWhen developing a tokenizer it helps to have access to raw text which has been man-\nually tokenized, in order to compare the output of your tokenizer with high-quality (or\n3.7  Regular Expressions for Tokenizing Text | 111\n\u201cgold-standard\u201d) tokens. The NLTK corpus collection includes a sample of Penn Tree-\nbank data, \nincluding the raw Wall Street Journal  text ( nltk.corpus.tree\nbank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\nA final issue for tokenization is the presence of contractions, such as didn\u2019t. If we are\nanalyzing the meaning of a sentence, it would probably be more useful to normalize\nthis form to two separate forms: did and n\u2019t (or not). We can do this work with the help\nof a lookup table.\n3.8  Segmentation\nThis section discusses more advanced concepts, which you may prefer to skip on the\nfirst time through this chapter.\nTokenization is an instance of a more general problem of segmentation. In this section,\nwe will look at two other instances of this problem, which use radically different tech-\nniques to the ones we have seen so far in this chapter.\nSentence Segmentation\nManipulating texts at the level of individual words often presupposes the ability to\ndivide a text into individual sentences. As we have seen, some corpora already provide\naccess at the sentence level. In the following example, we compute the average number\nof words per sentence in the Brown Corpus:\n>>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\n20.250994070456922\nIn other cases, the text is available only as a stream of characters. Before tokenizing the\ntext into words, we need to segment it into sentences. NLTK facilitates this by including\nthe Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example of its use in\nsegmenting the text of a novel. (Note that if the segmenter\u2019s internal data has been\nupdated by the time you read this, you will see different output.)\n>>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n>>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n>>> sents = sent_tokenizer.tokenize(text)\n>>> pprint.pprint(sents[171:181])\n['\"Nonsense!',\n '\" said Gregory, who was very rational when anyone else\\nattempted paradox.',\n '\"Why do all the clerks and navvies in the\\nrailway trains look so sad and tired,...',\n 'I will\\ntell you.',\n 'It is because they know that the train is going right.',\n 'It\\nis because they know that whatever place they have taken a ticket\\nfor that ...',\n 'It is because after they have\\npassed Sloane Square they know that the next stat...',\n 'Oh, their wild rapture!',\n 'oh,\\ntheir eyes like stars and their souls again in Eden, if the next\\nstation w...'\n '\"\\n\\n\"It is you who are unpoetical,\" replied the poet Syme.']\n112 | Chapter 3: \u2002Processing Raw Text\nNotice that this example is really a single sentence, reporting the speech of Mr. Lucian\nGregory. However, \nthe quoted speech contains several sentences, and these have been\nsplit into individual strings. This is reasonable behavior for most applications.\nSentence segmentation is difficult because a period is used to mark abbreviations, and\nsome periods simultaneously mark an abbreviation and terminate a sentence, as often\nhappens with acronyms like U.S.A.\nFor another approach to sentence segmentation, see Section 6.2.\nWord Segmentation\nFor some writing systems, tokenizing text is made more difficult by the fact that there\nis no visual representation of word boundaries. For example, in Chinese, the three-\ncharacter string: \u7231\u56fd\u4eba  (ai4 \u201clove\u201d [verb], guo3 \u201ccountry\u201d, ren2 \u201cperson\u201d) could be\ntokenized as \u7231\u56fd  / \u4eba, \u201ccountry-loving person,\u201d or as \u7231 / \u56fd\u4eba , \u201clove country-person.\u201d\nA similar problem arises in the processing of spoken language, where the hearer must\nsegment a continuous speech stream into individual words. A particularly challenging\nversion of this problem arises when we don\u2019t know the words in advance. This is the\nproblem faced by a language learner, such as a child hearing utterances from a parent.\nConsider the following artificial example, where word boundaries have been removed:\n(1) a. doyouseethekitty\nb. seethedoggy\nc. doyoulikethekitty\nd. likethedoggy\nOur first challenge is simply to represent the problem: we need to find a way to separate\ntext content from the segmentation. We can do this by annotating each character with\na boolean value to indicate whether or not a word-break appears after the character (an\nidea that will be used heavily for \u201cchunking\u201d in Chapter 7). Let\u2019s assume that the learner\nis given the utterance breaks, since these often correspond to extended pauses. Here is\na possible representation, including the initial and target segmentations:\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\nObserve that the segmentation strings consist of zeros and ones. They are one character\nshorter than the source text, since a text of length n can be broken up in only n\u20131 places.\nThe segment() function in Example 3-2  demonstrates that we can get back to the orig-\ninal segmented text from its representation.\n3.8  Segmentation | 113\nExample 3-2. Reconstruct segmented text from string representation: se g1 and seg2  represent the\ninitial and final segmentations of some hypothetical child-directed speech; the segment()  function can\nuse them to reproduce the segmented text.\ndef segment(text, segs):\n    words = []\n    last = 0\n    for i in range(len(segs)):\n        if segs[i] == '1':\n            words.append(text[last:i+1])\n            last = i+1\n    words.append(text[last:])\n    return words\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n>>> segment(text, seg1)\n['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n>>> segment(text, seg2)\n['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',\n 'like', 'the', kitty', 'like', 'the', 'doggy']\nNow the segmentation task becomes a search problem: find the bit string that causes\nthe text string to be correctly segmented into words. We assume the learner is acquiring\nwords and storing them in an internal lexicon. Given a suitable lexicon, it is possible\nto reconstruct the source text as a sequence of lexical items. Following (Brent & Cart-\nwright, 1995), we can define an objective function , a scoring function whose value\nwe will try to optimize, based on the size of the lexicon and the amount of information\nneeded to reconstruct the source text from the lexicon. We illustrate this in Figure 3-6 .\nFigure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text\n(on the \nleft), derive a lexicon and a derivation table that permit the source text to be reconstructed,\nthen total up the number of characters used by each lexical item (including a boundary marker) and\neach derivation, to serve as a score of the quality of the segmentation; smaller values of the score\nindicate a better segmentation.\nIt is a simple matter to implement this objective function, as shown in Example 3-3.\n114 | Chapter 3: \u2002Processing Raw Text\nExample 3-3. Computing the cost of storing the lexicon and reconstructing the source text.\ndef evaluate(text, segs):\n    words = segment(text, segs)\n    text_size = len(words)\n    lexicon_size = len(' '.join(list(set(words))))\n    return text_size + lexicon_size\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n>>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n>>> segment(text, seg3)\n['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',\n 'thekitt', 'y', 'like', 'thedogg', 'y']\n>>> evaluate(text, seg3)\n46\n>>> evaluate(text, seg2)\n47\n>>> evaluate(text, seg1)\n63\nThe final \nstep is to search for the pattern of zeros and ones that maximizes this objective\nfunction, shown in Example 3-4 . Notice that the best segmentation includes \u201cwords\u201d\nlike thekitty, since there\u2019s not enough evidence in the data to split this any further.\nExample 3-4. Non-deterministic search using simulated annealing: Begin searching with phrase\nsegmentations only; randomly perturb the zeros and ones proportional to the \u201ctemperature\u201d; with\neach iteration the temperature is lowered and the perturbation of boundaries is reduced.\nfrom random import randint\ndef flip(segs, pos):\n    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:] \ndef flip_n(segs, n):\n    for i in range(n):\n        segs = flip(segs, randint(0,len(segs)-1))\n    return segs\ndef anneal(text, segs, iterations, cooling_rate):\n    temperature = float(len(segs))\n    while temperature > 0.5:\n        best_segs, best = segs, evaluate(text, segs)\n        for i in range(iterations):\n            guess = flip_n(segs, int(round(temperature)))\n            score = evaluate(text, guess)\n            if score < best:\n                best, best_segs = score, guess\n        score, segs = best, best_segs\n        temperature = temperature / cooling_rate\n        print evaluate(text, segs), segment(text, segs)\n3.8  Segmentation | 115\n    print\n    return segs\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> anneal(text, seg1, 5000, 1.2)\n60 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']\n58 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']\n56 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']\n54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n53 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n51 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n'0000100100000001001000000010000100010000000100010000000'\nWith enough \ndata, it is possible to automatically segment text into words with a rea-\nsonable degree of accuracy. Such methods can be applied to tokenization for writing\nsystems that don\u2019t have any visual representation of word boundaries.\n3.9  Formatting: From Lists to Strings\nOften we write a program to report a single data item, such as a particular element in\na corpus that meets some complicated criterion, or a single summary statistic such as\na word-count or the performance of a tagger. More often, we write a program to produce\na structured result; for example, a tabulation of numbers or linguistic forms, or a re-\nformatting of the original data. When the results to be presented are linguistic, textual\noutput is usually the most natural choice. However, when the results are numerical, it\nmay be preferable to produce graphical output. In this section, you will learn about a\nvariety of ways to present program output.\nFrom Lists to Strings\nThe simplest kind of structured object we use for text processing is lists of words. When\nwe want to output these to a display or a file, we must convert these lists into strings.\nTo do this in Python we use the join() method, and specify the string to be used as the\n\u201cglue\u201d:\n>>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n>>> ' '.join(silly)\n'We called him Tortoise because he taught us .'\n>>> ';'.join(silly)\n'We;called;him;Tortoise;because;he;taught;us;.'\n>>> ''.join(silly)\n'WecalledhimTortoisebecausehetaughtus.'\nSo ' '.join(silly)  means: take all the items in silly and concatenate them as one big\nstring, using ' ' as a spacer between the items. I.e., join() is a method of the string\nthat you want to use as the glue. (Many people find this notation for join() counter-\nintuitive.) The join() method only works on a list of strings\u2014what we have been calling\na text\u2014a complex type that enjoys some privileges in Python.\n116 | Chapter 3: \u2002Processing Raw Text\nStrings and Formats\nWe have seen that there are two ways to display the contents of an object:\n>>> word = 'cat'\n>>> sentence = \"\"\"hello\n... world\"\"\"\n>>> print word\ncat\n>>> print sentence\nhello\nworld\n>>> word\n'cat'\n>>> sentence\n'hello\\nworld'\nThe print command \nyields Python\u2019s attempt to produce the most human-readable form\nof an object. The second method\u2014naming the variable at a prompt\u2014shows us a string\nthat can be used to recreate this object. It is important to keep in mind that both of\nthese are just strings, displayed for the benefit of you, the user. They do not give us any\nclue as to the actual internal representation of the object.\nThere are many other useful ways to display an object as a string of characters. This\nmay be for the benefit of a human reader, or because we want to export our data to a\nparticular file format for use in an external program.\nFormatted output typically contains a combination of variables and pre-specified\nstrings. For example, given a frequency distribution fdist, we could do:\n>>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n>>> for word in fdist:\n...     print word, '->', fdist[word], ';',\ndog -> 4 ; cat -> 3 ; snake -> 1 ;\nApart from the problem of unwanted whitespace, print statements that contain alter-\nnating variables and constants can be difficult to read and maintain. A better solution\nis to use string formatting expressions.\n>>> for word in fdist:\n...    print '%s->%d;' % (word, fdist[word]),\ndog->4; cat->3; snake->1;\nTo understand what is going on here, let\u2019s test out the string formatting expression on\nits own. (By now this will be your usual method of exploring new syntax.)\n>>> '%s->%d;' % ('cat', 3)\n'cat->3;'\n>>> '%s->%d;' % 'cat'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: not enough arguments for format string\n3.9  Formatting: From Lists to Strings | 117\nThe special symbols %s and %d are placeholders for strings and (decimal) integers. We\ncan embed \nthese inside a string, then use the % operator to combine them. Let\u2019s unpack\nthis code further, in order to see this behavior up close:\n>>> '%s->' % 'cat'\n'cat->'\n>>> '%d' % 3\n'3'\n>>> 'I want a %s right now' % 'coffee'\n'I want a coffee right now'\nWe can have a number of placeholders, but following the % operator we need to specify\na tuple with exactly the same number of values:\n>>> \"%s wants a %s %s\" % (\"Lee\", \"sandwich\", \"for lunch\")\n'Lee wants a sandwich for lunch'\nWe can also provide the values for the placeholders indirectly. Here\u2019s an example using\na for loop:\n>>> template = 'Lee wants a %s right now'\n>>> menu = ['sandwich', 'spam fritter', 'pancake']\n>>> for snack in menu:\n...     print template % snack\n...\nLee wants a sandwich right now\nLee wants a spam fritter right now\nLee wants a pancake right now\nThe %s and %d symbols are called conversion specifiers . They start with the % character\nand end with a conversion character such as s (for string) or d (for decimal integer) The\nstring containing conversion specifiers is called a format string. We combine a format\nstring with the % operator and a tuple of values to create a complete string formatting\nexpression.\nLining Things Up\nSo far our formatting strings generated output of arbitrary width on the page (or screen),\nsuch as %s and %d. We can specify a width as well, such as %6s, producing a string that\nis padded to width 6. It is right-justified by default \n , but we can include a minus sign\nto make \nit left-justified \n . In case we don\u2019t know in advance how wide a displayed\nvalue should \nbe, the width value can be replaced with a star in the formatting string,\nthen specified using a variable \n .\n>>> '%6s' % 'dog' \n'   dog'\n>>> '%-6s' % 'dog' \n'dog   '\n>>> width = 6\n>>> '%-*s' % (width, 'dog') \n'dog   '\n118 | Chapter 3: \u2002Processing Raw Text\nOther control characters are used for decimal integers and floating-point numbers.\nSince the \npercent character % has a special interpretation in formatting strings, we have\nto precede it with another % to get it in the output.\n>>> count, total = 3205, 9375\n>>> \"accuracy for %d words: %2.4f%%\" % (total, 100 * count / total)\n'accuracy for 9375 words: 34.1867%'\nAn important use of formatting strings is for tabulating data. Recall that in Sec-\ntion 2.1  we saw data being tabulated from a conditional frequency distribution. Let\u2019s\nperform the tabulation ourselves, exercising full control of headings and column\nwidths, as shown in Example 3-5. Note the clear separation between the language\nprocessing work, and the tabulation of results.\nExample 3-5. Frequency of modals in different sections of the Brown Corpus.\ndef tabulate(cfdist, words, categories):\n    print '%-16s' % 'Category',\n    for word in words:                                  # column headings\n        print '%6s' % word,\n    print\n    for category in categories:\n        print '%-16s' % category,                       # row heading\n        for word in words:                              # for each word\n            print '%6d' % cfdist[category][word],       # print table cell\n        print                                           # end the row\n>>> from nltk.corpus import brown\n>>> cfd = nltk.ConditionalFreqDist(\n...           (genre, word)\n...           for genre in brown.categories()\n...           for word in brown.words(categories=genre))\n>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> tabulate(cfd, modals, genres)\nCategory            can  could    may  might   must   will\nnews                 93     86     66     38     50    389\nreligion             82     59     78     12     54     71\nhobbies             268     58    131     22     83    264\nscience_fiction      16     49      4     12      8     16\nromance              74    193     11     51     45     43\nhumor                16     30      8      8      9     13\nRecall from the listing in Example 3-1  that we used a formatting string \"%*s\". This\nallows us to specify the width of a field using a variable.\n>>> '%*s' % (15, \"Monty Python\")\n'   Monty Python'\nWe could use this to automatically customize the column to be just wide enough to\naccommodate all the words, using width = max(len(w) for w in words). Remember\nthat the comma at the end of print statements adds an extra space, and this is sufficient\nto prevent the column headings from running into each other.\n3.9  Formatting: From Lists to Strings | 119\nWriting Results to a File\nWe have \nseen how to read text from files (Section 3.1). It is often useful to write output\nto files as well. The following code opens a file output.txt for writing, and saves the\nprogram output to the file.\n>>> output_file = open('output.txt', 'w')\n>>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n>>> for word in sorted(words):\n...     output_file.write(word + \"\\n\")\nYour Turn:  What is the effect of appending \\n to each string before we\nwrite it to the file? If you\u2019re using a Windows machine, you may want\nto use word + \"\\r\\n\" instead. What happens if we do\noutput_file.write(word)\nWhen we write non-text data to a file, we must convert it to a string first. We can do\nthis conversion using formatting strings, as we saw earlier. Let\u2019s write the total number\nof words to our file, before closing it.\n>>> len(words)\n2789\n>>> str(len(words))\n'2789'\n>>> output_file.write(str(len(words)) + \"\\n\")\n>>> output_file.close()\nCaution!\nYou should \navoid filenames that contain space characters, such as\noutput file.txt , or that are identical except for case distinctions, e.g.,\nOutput.txt and output.TXT.\nText Wrapping\nWhen the output of our program is text-like, instead of tabular, it will usually be nec-\nessary to wrap it so that it can be displayed conveniently. Consider the following output,\nwhich overflows its line, and which uses a complicated print statement:\n>>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n...           'more', 'is', 'said', 'than', 'done', '.']\n>>> for word in saying:\n...     print word, '(' + str(len(word)) + '),',\nAfter (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), \nWe can take care of line wrapping with the help of Python\u2019s textwrap module. For\nmaximum clarity we will separate each step onto its own line:\n>>> from textwrap import fill\n>>> format = '%s (%d),'\n120 | Chapter 3: \u2002Processing Raw Text\n>>> pieces = [format % (word, len(word)) for word in saying]\n>>> output = ' '.join(pieces)\n>>> wrapped = fill(output)\n>>> print wrapped\nAfter (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n(4), is (2), said (4), than (4), done (4), . (1),\nNotice that \nthere is a linebreak between more and its following number. If we wanted\nto avoid this, we could redefine the formatting string so that it contained no spaces\n(e.g., '%s_(%d),'), then instead of printing the value of wrapped, we could print wrap\nped.replace('_', ' ').\n3.10  Summary\n\u2022 In this book we view a text as a list of words. A \u201craw text\u201d is a potentially long\nstring containing words and whitespace formatting, and is how we typically store\nand visualize a text.\n\u2022 A string is specified in Python using single or double quotes: 'Monty Python',\n\"Monty Python\".\n\u2022 The characters of a string are accessed using indexes, counting from zero: 'Monty\nPython'[0] gives the value M. The length of a string is found using len().\n\u2022 Substrings are accessed using slice notation: 'Monty Python'[1:5]  gives the value\nonty. If the start index is omitted, the substring begins at the start of the string; if\nthe end index is omitted, the slice continues to the end of the string.\n\u2022 Strings can be split into lists: 'Monty Python'.split()  gives ['Monty', 'Python'] .\nLists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/\nPython'.\n\u2022 We can read text from a file f using text = open(f).read(). We can read text from\na URL u using text = urlopen(u).read(). We can iterate over the lines of a text file\nusing for line in open(f).\n\u2022 Texts found on the Web may contain unwanted material (such as headers, footers,\nand markup), that need to be removed before we do any linguistic processing.\n\u2022 Tokenization is the segmentation of a text into basic units\u2014or tokens\u2014such as\nwords and punctuation. Tokenization based on whitespace is inadequate for many\napplications because it bundles punctuation together with words. NLTK provides\nan off-the-shelf tokenizer nltk.word_tokenize().\n\u2022 Lemmatization is a process that maps the various forms of a word (such as ap-\npeared, appears) to the canonical or citation form of the word, also known as the\nlexeme or lemma (e.g., appear).\n\u2022 Regular expressions are a powerful and flexible method of specifying patterns.\nOnce we have imported the re module, we can use re.findall() to find all sub-\nstrings in a string that match a pattern.\n3.10  Summary | 121\n\u2022 If a regular expression string includes a backslash, you should tell Python not to\npreprocess the string, by using a raw string with an \nr prefix: r'regexp'.\n\u2022 When backslash is used before certain characters, e.g., \\n, this takes on a special\nmeaning (newline character); however, when backslash is used before regular ex-\npression wildcards and operators, e.g., \\., \\|, \\$, these characters lose their special\nmeaning and are matched literally.\n\u2022 A string formatting expression template % arg_tuple consists of a format string\ntemplate that contains conversion specifiers like %-6s and %0.2d.\n3.11  Further Reading\nExtra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. Remember to consult the Python reference ma-\nterials at http://docs.python.org/. (For example, this documentation covers \u201cuniversal\nnewline support,\u201d explaining how to work with the different newline conventions used\nby various operating systems.)\nFor more examples of processing words with NLTK, see the tokenization, stemming,\nand corpus HOWTOs at http://www.nltk.org/howto. Chapters 2 and 3 of (Jurafsky &\nMartin, 2008) contain more advanced material on regular expressions and morphology.\nFor more extensive discussion of text processing with Python, see (Mertz, 2003). For\ninformation about normalizing non-standard words, see (Sproat et al., 2001).\nThere are many references for regular expressions, both practical and theoretical. For\nan introductory tutorial to using regular expressions in Python, see Kuchling\u2019s Regular\nExpression HOWTO, http://www.amk.ca/python/howto/regex/. For a comprehensive\nand detailed manual in using regular expressions, covering their syntax in most major\nprogramming languages, including Python, see (Friedl, 2002). Other presentations in-\nclude Section 2.1 of (Jurafsky & Martin, 2008), and Chapter 3 of (Mertz, 2003).\nThere are many online resources for Unicode. Useful discussions of Python\u2019s facilities\nfor handling Unicode are:\n\u2022 PEP-100 http://www.python.org/dev/peps/pep-0100/\n\u2022 Jason Orendorff, Unicode for Programmers , http://www.jorendorff.com/articles/uni\ncode/\n\u2022 A. M. Kuchling, Unicode HOWTO, http://www.amk.ca/python/howto/unicode\n\u2022 Frederik Lundh, Python Unicode Objects , http://effbot.org/zone/unicode-objects\n.htm\n\u2022 Joel Spolsky, The Absolute Minimum Every Software Developer Absolutely, Posi-\ntively Must Know About Unicode and Character Sets (No Excuses!) , http://www.joe\nlonsoftware.com/articles/Unicode.html\n122 | Chapter 3: \u2002Processing Raw Text\nThe problem of tokenizing Chinese text is a major focus of SIGHAN, the ACL Special\nInterest Group \non Chinese Language Processing ( http://sighan.org/). Our method for\nsegmenting English text follows (Brent & Cartwright, 1995); this work falls in the area\nof language acquisition (Niyogi, 2006).\nCollocations are a special case of multiword expressions. A multiword expression  is\na small phrase whose meaning and other properties cannot be predicted from its words\nalone, e.g., part-of-speech (Baldwin & Kim, 2010).\nSimulated annealing is a heuristic for finding a good approximation to the optimum\nvalue of a function in a large, discrete search space, based on an analogy with annealing\nin metallurgy. The technique is described in many Artificial Intelligence texts.\nThe approach to discovering hyponyms in text using search patterns like x and other\nys is described by (Hearst, 1992).\n3.12  Exercises\n1.\u25cb Define a string s = 'colorless' . Write a Python statement that changes this to\n\u201ccolourless\u201d using only the slice and concatenation operations.\n2.\u25cb We can use the slice notation to remove morphological endings on words. For\nexample, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice\nnotation to remove the affixes from these words (we\u2019ve inserted a hyphen to indi-\ncate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-\nality, un-do, pre-heat.\n3.\u25cb We saw how we can generate an IndexError by indexing beyond the end of a\nstring. Is it possible to construct an index that goes too far to the left, before the\nstart of the string?\n4.\u25cb We can specify a \u201cstep\u201d size for the slice. The following returns every second\ncharacter within the slice: monty[6:11:2]. It also works in the reverse direction:\nmonty[10:5:-2]. Try these for yourself, and then experiment with different step\nvalues.\n5.\u25cb What happens if you ask the interpreter to evaluate monty[::-1]? Explain why\nthis is a reasonable result.\n6.\u25cb Describe the class of strings matched by the following regular expressions:\na.[a-zA-Z]+\nb.[A-Z][a-z]*\nc.p[aeiou]{,2}t\nd.\\d+(\\.\\d+)?\ne.([^aeiou][aeiou][^aeiou])*\nf.\\w+|[^\\w\\s]+\nTest your answers using nltk.re_show().\n3.12  Exercises | 123\n7.\u25cb Write regular expressions to match the following classes of strings:\na. A single determiner (assume that \na, an, and the are the only determiners)\nb. An arithmetic expression using integers, addition, and multiplication, such as\n2*3+8\n8.\u25cb Write a utility function that takes a URL as its argument, and returns the contents\nof the URL, with all HTML markup removed. Use urllib.urlopen to access the\ncontents of the URL, e.g.:\nraw_contents = urllib.urlopen('http://www.nltk.org/').read()\n9.\u25cb Save some text into a file corpus.txt. Define a function load(f) that reads from\nthe file named in its sole argument, and returns a string containing the text of the\nfile.\na. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\nkinds of punctuation in this text. Use one multiline regular expression inline\ncomments, using the verbose flag (?x).\nb. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\nkinds of expressions: monetary amounts; dates; names of people and\norganizations.\n10.\u25cb Rewrite the following loop as a list comprehension:\n>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n>>> result = []\n>>> for word in sent:\n...     word_len = (word, len(word))\n...     result.append(word_len)\n>>> result\n[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n11.\u25cb Define a string raw containing a sentence of your own choosing. Now, split raw\non some character other than space, such as 's'.\n12.\u25cb Write a for loop to print out the characters of a string, one per line.\n13.\u25cb What is the difference between calling split on a string with no argument and\none with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What\nhappens when the string being split contains tab characters, consecutive space\ncharacters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\nenter a tab character.)\n14.\u25cb Create a variable words containing a list of words. Experiment with\nwords.sort() and sorted(words). What is the difference?\n15.\u25cb Explore the difference between strings and integers by typing the following at a\nPython prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers\nusing int(\"3\") and str(3).\n16.\u25cb Earlier, we asked you to use a text editor to create a file called test.py, containing\nthe single line monty = 'Monty Python'. If you haven\u2019t already done this (or can\u2019t\nfind the file), go ahead and do it now. Next, start up a new session with the Python\n124 | Chapter 3: \u2002Processing Raw Text\ninterpreter, and enter the expression monty at the prompt. You will get an error\nfrom the interpreter. Now, try the following (note that you have to leave off\nthe .py part of the filename):\n>>> from test import msg\n>>> msg\nThis time, Python should return with a value. You can also try import test, in\nwhich case Python should be able to evaluate the expression test.monty at the\nprompt.\n17.\u25cb What happens when the formatting strings %6s and %-6s are used to display\nstrings that are longer than six characters?\n18.\u25d1 Read in some text from a corpus, tokenize it, and print the list of all wh-word\ntypes that occur. ( wh-words in English are used in questions, relative clauses, and\nexclamations: who, which, what, and so on.) Print them in order. Are any words\nduplicated in this list, because of the presence of case distinctions or punctuation?\n19.\u25d1 Create a file consisting of words and (made up) frequencies, where each line\nconsists of a word, the space character, and a positive integer, e.g., fuzzy 53 . Read\nthe file into a Python list using open(filename).readlines(). Next, break each line\ninto its two fields using split(), and convert the number into an integer using\nint(). The result should be a list of the form: [['fuzzy', 53], ...].\n20.\u25d1 Write code to access a favorite web page and extract some text from it. For\nexample, access a weather site and extract the forecast top temperature for your\ntown or city today.\n21.\u25d1 Write a function unknown() that takes a URL as its argument, and returns a list\nof unknown words that occur on that web page. In order to do this, extract all\nsubstrings consisting of lowercase letters (using re.findall()) and remove any\nitems from this set that occur in the Words Corpus ( nltk.corpus.words). Try to\ncategorize these words manually and discuss your findings.\n22.\u25d1 Examine the results of processing the URL http://news.bbc.co.uk/ using the reg-\nular expressions suggested above. You will see that there is still a fair amount of\nnon-textual data there, particularly JavaScript commands. You may also find that\nsentence breaks have not been properly preserved. Define further regular expres-\nsions that improve the extraction of text from this web page.\n23.\u25d1 Are you able to write a regular expression to tokenize text in such a way that the\nword don\u2019t is tokenized into do and n\u2019t? Explain why this regular expression won\u2019t\nwork: \u00abn't|\\w+\u00bb.\n24.\u25d1 Try to write code to convert text into hAck3r, using regular expressions and\nsubstitution, where e \u2192 3, i \u2192 1, o \u2192 0, l \u2192 |, s \u2192 5, . \u2192 5w33t!, ate \u2192 8. Normalize\nthe text to lowercase before converting it. Add more substitutions of your own.\nNow try to map s to two different values: $ for word-initial s, and 5 for word-\ninternal s.\n3.12  Exercises | 125\n25.\u25d1 Pig Latin is a simple transformation of English text. Each word of the text is\nconverted \nas follows: move any consonant (or consonant cluster) that appears at\nthe start of the word to the end, then append ay, e.g., string \u2192 ingstray, idle \u2192\nidleay (see http://en.wikipedia.org/wiki/Pig_Latin).\na. Write a function to convert a word to Pig Latin.\nb. Write code that converts text, instead of individual words.\nc. Extend it further to preserve capitalization, to keep qu together (so that\nquiet becomes ietquay, for example), and to detect when y is used as a con-\nsonant (e.g., yellow) versus a vowel (e.g., style).\n26.\u25d1 Download some text from a language that has vowel harmony (e.g., Hungarian),\nextract the vowel sequences of words, and create a vowel bigram table.\n27.\u25d1 Python\u2019s random module includes a function choice() which randomly chooses\nan item from a sequence; e.g., choice(\"aehh \")  will produce one of four possible\ncharacters, with the letter h being twice as frequent as the others. Write a generator\nexpression that produces a sequence of 500 randomly chosen letters drawn from\nthe string \"aehh \" , and put this expression inside a call to the ''.join() function,\nto concatenate them into one long string. You should get a result that looks like\nuncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split()\nand join() again to normalize the whitespace in this string.\n28.\u25d1 Consider the numeric expressions in the following sentence from the MedLine\nCorpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15%\nand 8.16 +/- 0.23%, respectively.  Should we say that the numeric expression 4.53\n+/- 0.15%  is three words? Or should we say that it\u2019s a single compound word? Or\nshould we say that it is actually nine words, since it\u2019s read \u201cfour point five three,\nplus or minus fifteen percent\u201d? Or should we say that it\u2019s not a \u201creal\u201d word at all,\nsince it wouldn\u2019t appear in any dictionary? Discuss these different possibilities. Can\nyou think of application domains that motivate at least two of these answers?\n29.\u25d1 Readability measures are used to score the reading difficulty of a text, for the\npurposes of selecting texts of appropriate difficulty for language learners. Let us\ndefine \u03bcw to be the average number of letters per word, and \u03bcs to be the average\nnumber of words per sentence, in a given text. The Automated Readability Index\n(ARI) of the text is defined to be: 4.71 \u03bcw + 0.5 \u03bcs - 21.43. Compute the ARI score\nfor various sections of the Brown Corpus, including section f (popular lore) and\nj (learned). Make use of the fact that nltk.corpus.brown.words() produces a se-\nquence of words, whereas nltk.corpus.brown.sents() produces a sequence of\nsentences.\n30.\u25d1 Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\non each word. Do the same thing with the Lancaster Stemmer, and see if you ob-\nserve any differences.\n31.\u25d1 Define the variable saying to contain the list [ 'After', 'all', 'is', 'said',\n'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list\n126 | Chapter 3: \u2002Processing Raw Text\nusing a for loop, and store the result in a new list lengths . Hint: begin by assigning\nthe empty list to lengths, using lengths = []. Then each time through the loop,\nuse append() to add another length value to the list.\n32.\u25d1 Define a variable silly to contain the string: 'newly formed bland ideas are\ninexpressible in an infuriating way'. (This happens to be the legitimate inter-\npretation that bilingual English-Spanish speakers can assign to Chomsky\u2019s famous\nnonsense phrase colorless green ideas sleep furiously , according to Wikipedia). Now\nwrite code to perform the following tasks:\na. Split silly into a list of strings, one per word, using Python\u2019s split() opera-\ntion, and save this to a variable called bland.\nb. Extract the second letter of each word in silly and join them into a string, to\nget 'eoldrnnnna'.\nc. Combine the words in bland back into a single string, using join(). Make sure\nthe words in the resulting string are separated with whitespace.\nd. Print the words of silly in alphabetical order, one per line.\n33.\u25d1 The index() function can be used to look up items in sequences. For example,\n'inexpressible'.index('e') tells us the index of the first position of the letter e.\na. What happens when you look up a substring, e.g., 'inexpressi\nble'.index('re')?\nb. Define a variable words containing a list of words. Now use words.index() to\nlook up the position of an individual word.\nc. Define a variable silly as in Exercise 32. Use the index() function in combi-\nnation with list slicing to build a list phrase consisting of all the words up to\n(but not including) in in silly.\n34.\u25d1 Write code to convert nationality adjectives such as Canadian and Australian to\ntheir corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/\nList_of_adjectival_forms_of_place_names).\n35.\u25d1 Read the LanguageLog post on phrases of the form as best as p can and as best p\ncan, where p is a pronoun. Investigate this phenomenon with the help of a corpus\nand the findall() method for searching tokenized text described in Section 3.5 .\nThe post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.\n36.\u25d1 Study the lolcat version of the book of Genesis, accessible as nltk.corpus.gene\nsis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://\nwww.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expres-\nsions to convert English words into corresponding lolspeak words.\n37.\u25d1 Read about the re.sub() function for string substitution using regular expres-\nsions, using help(re.sub) and by consulting the further readings for this chapter.\nUse re.sub in writing code to remove HTML tags from an HTML file, and to\nnormalize whitespace.\n3.12  Exercises | 127\n38.\u25cf An interesting challenge for tokenization is words that have been split across a\nlinebreak. E.g., if \nlong-term is split, then we have the string long-\\nterm.\na. Write a regular expression that identifies words that are hyphenated at a line-\nbreak. The expression will need to include the \\n character.\nb. Use re.sub() to remove the \\n character from these words.\nc. How might you identify words that should not remain hyphenated once the\nnewline is removed, e.g., 'encyclo-\\npedia'?\n39.\u25cf Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n40.\u25cf Obtain raw texts from two or more genres and compute their respective reading\ndifficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\nRural News and ABC Science News ( nltk.corpus.abc). Use Punkt to perform sen-\ntence segmentation.\n41.\u25cf Rewrite the following nested loop as a nested list comprehension:\n>>> words = ['attribution', 'confabulation', 'elocution',\n...          'sequoia', 'tenacious', 'unidirectional']\n>>> vsequences = set()\n>>> for word in words:\n...     vowels = []\n...     for char in word:\n...         if char in 'aeiou':\n...             vowels.append(char)\n...     vsequences.add(''.join(vowels))\n>>> sorted(vsequences)\n['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n42.\u25cf Use WordNet to create a semantic index for a text collection. Extend the con-\ncordance search program in Example 3-1 , indexing each word using the offset of\nits first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\nof its ancestors in the hypernym hierarchy).\n43.\u25cf With the help of a multilingual corpus such as the Universal Declaration of\nHuman Rights Corpus ( nltk.corpus.udhr), along with NLTK\u2019s frequency distri-\nbution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla\ntion), develop a system that guesses the language of a previously unseen text. For\nsimplicity, work with a single character encoding and just a few languages.\n44.\u25cf Write a program that processes a text and discovers cases where a word has been\nused with a novel sense. For each word, compute the WordNet similarity between\nall synsets of the word and all synsets of the words in its context. (Note that this\nis a crude approach; doing it well is a difficult, open research problem.)\n45.\u25cf Read the article on normalization of non-standard words (Sproat et al., 2001),\nand implement a similar system for text normalization.\n128 | Chapter 3: \u2002Processing Raw Text\nCHAPTER 4\nWriting Structured Programs\nBy now you will have a sense of the capabilities of the Python programming language\nfor processing \nnatural language. However, if you\u2019re new to Python or to programming,\nyou may still be wrestling with Python and not feel like you are in full control yet. In\nthis chapter we\u2019ll address the following questions:\n1. How can you write well-structured, readable programs that you and others will be\nable to reuse easily?\n2. How do the fundamental building blocks work, such as loops, functions, and\nassignment?\n3. What are some of the pitfalls with Python programming, and how can you avoid\nthem?\nAlong the way, you will consolidate your knowledge of fundamental programming\nconstructs, learn more about using features of the Python language in a natural and\nconcise way, and learn some useful techniques in visualizing natural language data. As\nbefore, this chapter contains many examples and exercises (and as before, some exer-\ncises introduce new material). Readers new to programming should work through them\ncarefully and consult other introductions to programming if necessary; experienced\nprogrammers can quickly skim this chapter.\nIn the other chapters of this book, we have organized the programming concepts as\ndictated by the needs of NLP. Here we revert to a more conventional approach, where\nthe material is more closely tied to the structure of the programming language. There\u2019s\nnot room for a complete presentation of the language, so we\u2019ll just focus on the language\nconstructs and idioms that are most important for NLP.\n129\n4.1  Back to the Basics\nAssignment\nAssignment would \nseem to be the most elementary programming concept, not deserv-\ning a separate discussion. However, there are some surprising subtleties here. Consider\nthe following code fragment:\n>>> foo = 'Monty'\n>>> bar = foo \n>>> foo = 'Python' \n>>> bar\n'Monty'\nThis behaves \nexactly as expected. When we write bar = foo in the code \n , the value\nof foo (the string 'Monty') is assigned to bar. That is, bar is a copy of foo, so when we\noverwrite foo with a new string 'Python' on line \n , the value of bar is not affected.\nHowever, assignment \nstatements do not always involve making copies in this way.\nAssignment always copies the value of an expression, but a value is not always what\nyou might expect it to be. In particular, the \u201cvalue\u201d of a structured object such as a list\nis actually just a reference to the object. In the following example, \n  assigns the refer-\nence of foo\n to the new variable bar. Now when we modify something inside foo on line\n, we can see that the contents of bar have also been changed.\n>>> foo = ['Monty', 'Python']\n>>> bar = foo \n>>> foo[1] = 'Bodkin' \n>>> bar\n['Monty', 'Bodkin']\nThe line bar = foo  \n does not copy the contents of the variable, only its \u201cobject refer-\nence.\u201d To \nunderstand what is going on here, we need to know how lists are stored in\nthe computer\u2019s memory. In Figure 4-1 , we see that a list foo is a reference to an object\nstored at location 3133 (which is itself a series of pointers to other locations holding\nstrings). When we assign bar = foo, it is just the object reference 3133 that gets copied.\nThis behavior extends to other aspects of the language, such as parameter passing\n(Section 4.4).\n130 | Chapter 4: \u2002Writing Structured Programs\nLet\u2019s experiment some more, by creating a variable empty holding the empty list, then\nusing it three times on the next line.\n>>> empty = []\n>>> nested = [empty, empty, empty]\n>>> nested\n[[], [], []]\n>>> nested[1].append('Python')\n>>> nested\n[['Python'], ['Python'], ['Python']]\nObserve that \nchanging one of the items inside our nested list of lists changed them all.\nThis is because each of the three elements is actually just a reference to one and the\nsame list in memory.\nYour Turn:  Use multiplication to create a list of lists: nested = [[]] *\n3. Now modify one of the elements of the list, and observe that all the\nelements are changed. Use Python\u2019s id() function to find out the nu-\nmerical identifier for any object, and verify that id(nested[0]),\nid(nested[1]), and id(nested[2]) are all the same.\nNow, notice that when we assign a new value to one of the elements of the list, it does\nnot propagate to the others:\n>>> nested = [[]] * 3\n>>> nested[1].append('Python')\n>>> nested[1] = ['Monty']\n>>> nested\n[['Python'], ['Monty'], ['Python']]\nWe began with a list containing three references to a single empty list object. Then we\nmodified that object by appending 'Python' to it, resulting in a list containing three\nreferences to a single list object ['Python']. Next, we overwrote one of those references\nwith a reference to a new object ['Monty']. This last step modified one of the three\nobject references inside the nested list. However, the ['Python'] object wasn\u2019t changed,\nFigure 4-1. List assignment and computer memory: Two list objects fo o and bar reference the same\nlocation in the computer\u2019s memory; updating foo will also modify bar, and vice versa.\n4.1  Back to the Basics | 131\nand is still referenced from two places in our nested list of lists. It is crucial to appreciate\nthis difference \nbetween modifying an object via an object reference and overwriting an\nobject reference.\nImportant: To copy the items from a list foo to a new list bar, you can\nwrite bar = foo[:] . This copies the object references inside the list. To\ncopy a structure without copying any object references, use copy.deep\ncopy().\nEquality\nPython provides two ways to check that a pair of items are the same. The is operator\ntests for object identity. We can use it to verify our earlier observations about objects.\nFirst, we create a list containing several copies of the same object, and demonstrate that\nthey are not only identical according to ==, but also that they are one and the same\nobject:\n>>> size = 5\n>>> python = ['Python']\n>>> snake_nest = [python] * size\n>>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\nTrue\n>>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\nTrue\nNow let\u2019s put a new python in this nest. We can easily show that the objects are not\nall identical:\n>>> import random\n>>> position = random.choice(range(size))\n>>> snake_nest[position] = ['Python']\n>>> snake_nest\n[['Python'], ['Python'], ['Python'], ['Python'], ['Python']]\n>>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\nTrue\n>>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\nFalse\nYou can do several pairwise tests to discover which position contains the interloper,\nbut the id() function makes detection is easier:\n>>> [id(snake) for snake in snake_nest]\n[513528, 533168, 513528, 513528, 513528]\nThis reveals that the second item of the list has a distinct identifier. If you try running\nthis code snippet yourself, expect to see different numbers in the resulting list, and\ndon\u2019t be surprised if the interloper is in a different position.\nHaving two kinds of equality might seem strange. However, it\u2019s really just the type-\ntoken distinction, familiar from natural language, here showing up in a programming\nlanguage.\n132 | Chapter 4: \u2002Writing Structured Programs\nConditionals\nIn the \ncondition part of an if statement, a non-empty string or list is evaluated as true,\nwhile an empty string or list evaluates as false.\n>>> mixed = ['cat', '', ['dog'], []]\n>>> for element in mixed:\n...     if element:\n...         print element\n...\ncat\n['dog']\nThat is, we don\u2019t need to say if len(element) > 0: in the condition.\nWhat\u2019s the \ndifference between using if...elif as opposed to using a couple of if\nstatements in a row? Well, consider the following situation:\n>>> animals = ['cat', 'dog']\n>>> if 'cat' in animals:\n...     print 1\n... elif 'dog' in animals:\n...     print 2\n...\n1\nSince the if clause of the statement is satisfied, Python never tries to evaluate the\nelif clause, so we never get to print out 2. By contrast, if we replaced the elif by an\nif, then we would print out both 1 and 2. So an elif clause potentially gives us more\ninformation than a bare if clause; when it evaluates to true, it tells us not only that the\ncondition is satisfied, but also that the condition of the main if clause was not satisfied.\nThe functions all() and any() can be applied to a list (or other sequence) to check\nwhether all or any items meet some condition:\n>>> sent = ['No', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '.']\n>>> all(len(w) > 4 for w in sent)\nFalse\n>>> any(len(w) > 4 for w in sent)\nTrue\n4.2  Sequences\nSo far, we have seen two kinds of sequence object: strings and lists. Another kind of\nsequence is called a tuple. Tuples are formed with the comma operator \n , and typically\nenclosed using \nparentheses. We\u2019ve actually seen them in the previous chapters, and\nsometimes referred to them as \u201cpairs,\u201d since there were always two members. However,\ntuples can have any number of members. Like lists and strings, tuples can be indexed\n and sliced \n , and have a length \n .\n>>> t = 'walk', 'fem', 3 \n>>> t\n('walk', 'fem', 3)\n4.2  Sequences | 133\n>>> t[0] \n'walk'\n>>> t[1:] \n('fem', 3)\n>>> len(t) \nCaution!\nTuples are \nconstructed using the comma operator. Parentheses are a\nmore general feature of Python syntax, designed for grouping. A tuple\ncontaining the single element 'snark' is defined by adding a trailing\ncomma, like this: 'snark',. The empty tuple is a special case, and is\ndefined using empty parentheses ().\nLet\u2019s compare strings, lists, and tuples directly, and do the indexing, slice, and length\noperation on each type:\n>>> raw = 'I turned off the spectroroute'\n>>> text = ['I', 'turned', 'off', 'the', 'spectroroute']\n>>> pair = (6, 'turned')\n>>> raw[2], text[3], pair[1]\n('t', 'the', 'turned')\n>>> raw[-3:], text[-3:], pair[-3:]\n('ute', ['off', 'the', 'spectroroute'], (6, 'turned'))\n>>> len(raw), len(text), len(pair)\n(29, 5, 2)\nNotice in this code sample that we computed multiple values on a single line, separated\nby commas. These comma-separated expressions are actually just tuples\u2014Python al-\nlows us to omit the parentheses around tuples if there is no ambiguity. When we print\na tuple, the parentheses are always displayed. By using tuples in this way, we are im-\nplicitly aggregating items together.\nYour Turn:  Define a set, e.g., using set(text), and see what happens\nwhen you convert it to a list or iterate over its members.\nOperating on Sequence Types\nWe can iterate over the items in a sequence s in a variety of useful ways, as shown in\nTable 4-1.\nTable 4-1. Various ways to iterate over sequences\nPython expression Comment\nfor item in s Iterate over the items of s\nfor item in sorted(s) Iterate over the items of s in order\nfor item in set(s) Iterate over unique elements of s\n134 | Chapter 4: \u2002Writing Structured Programs\nPython expression Comment\nfor item in reversed(s) Iterate over elements of s in reverse\nfor item in set(s).difference(t) Iterate over elements of s not in t\nfor item in random.shuffle(s) Iterate over elements of s in random order\nThe sequence functions illustrated in Table 4-1  can be combined in various ways; for\nexample, to get unique elements of s sorted in reverse, use reversed(sorted(set(s))).\nWe can convert between these sequence types. For example, tuple(s) converts any\nkind of sequence into a tuple, and list(s) converts any kind of sequence into a list.\nWe can convert a list of strings to a single string using the join() function, e.g.,\n':'.join(words).\nSome other objects, such as a FreqDist, can be converted into a sequence (using\nlist()) and support iteration:\n>>> raw = 'Red lorry, yellow lorry, red lorry, yellow lorry.'\n>>> text = nltk.word_tokenize(raw)\n>>> fdist = nltk.FreqDist(text)\n>>> list(fdist)\n['lorry', ',', 'yellow', '.', 'Red', 'red']\n>>> for key in fdist:\n...     print fdist[key],\n...\n4 3 2 1 1 1\nIn the next example, we use tuples to re-arrange the contents of our list. (We can omit\nthe parentheses because the comma has higher precedence than assignment.)\n>>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n>>> words[2], words[3], words[4] = words[3], words[4], words[2]\n>>> words\n['I', 'turned', 'the', 'spectroroute', 'off']\nThis is an idiomatic and readable way to move items inside a list. It is equivalent to the\nfollowing traditional way of doing such tasks that does not use tuples (notice that this\nmethod needs a temporary variable tmp).\n>>> tmp = words[2]\n>>> words[2] = words[3]\n>>> words[3] = words[4]\n>>> words[4] = tmp\nAs we have seen, Python has sequence functions such as sorted() and reversed() that\nrearrange the items of a sequence. There are also functions that modify the structure of\na sequence, which can be handy for language processing. Thus, zip() takes the items\nof two or more sequences and \u201czips\u201d them together into a single list of pairs. Given a\nsequence s, enumerate(s) returns pairs consisting of an index and the item at that index.\n>>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n>>> tags = ['noun', 'verb', 'prep', 'det', 'noun']\n>>> zip(words, tags)\n4.2  Sequences | 135\n[('I', 'noun'), ('turned', 'verb'), ('off', 'prep'),\n('the', 'det'), ('spectroroute', 'noun')]\n>>> list(enumerate(words))\n[(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]\nFor some \nNLP tasks it is necessary to cut up a sequence into two or more parts. For\ninstance, we might want to \u201ctrain\u201d a system on 90% of the data and test it on the\nremaining 10%. To do this we decide the location where we want to cut the data \n ,\nthen cut the sequence at that location \n .\n>>> text = nltk.corpus.nps_chat.words()\n>>> cut = int(0.9 * len(text)) \n>>> training_data, test_data = text[:cut], text[cut:] \n>>> text == training_data + test_data \nTrue\n>>> len(training_data) / len(test_data) \n9\nWe can \nverify that none of the original data is lost during this process, nor is it dupli-\ncated \n . We can also verify that the ratio of the sizes of the two pieces is what we\nintended \n .\nCombining Different Sequence Types\nLet\u2019s combine \nour knowledge of these three sequence types, together with list com-\nprehensions, to perform the task of sorting the words in a string by their length.\n>>> words = 'I turned off the spectroroute'.split() \n>>> wordlens = [(len(word), word) for word in words] \n>>> wordlens.sort() \n>>> ' '.join(w for (_, w) in wordlens) \n'I off the turned spectroroute'\nEach of \nthe preceding lines of code contains a significant feature. A simple string is\nactually an object with methods defined on it, such as split() \n . We use a list com-\nprehension to \nbuild a list of tuples \n , where each tuple consists of a number (the word\nlength) and the word, e.g., (3, 'the') . We use the sort() method \n  to sort the list in\nplace. Finally, \nwe discard the length information and join the words back into a single\nstring \n . (The underscore \n  is just a regular Python variable, but we can use underscore\nby convention to indicate that we will not use its value.)\nWe began \nby talking about the commonalities in these sequence types, but the previous\ncode illustrates important differences in their roles. First, strings appear at the beginning\nand the end: this is typical in the context where our program is reading in some text\nand producing output for us to read. Lists and tuples are used in the middle, but for\ndifferent purposes. A list is typically a sequence of objects all having the same type , of\narbitrary length . We often use lists to hold sequences of words. In contrast, a tuple is\ntypically a collection of objects of different types , of fixed length . We often use a tuple\nto hold a record, a collection of different fields relating to some entity. This distinction\nbetween the use of lists and tuples takes some getting used to, so here is another\nexample:\n136 | Chapter 4: \u2002Writing Structured Programs\n>>> lexicon = [\n...     ('the', 'det', ['Di:', 'D@']),\n...     ('off', 'prep', ['Qf', 'O:f'])\n... ]\nHere, a \nlexicon is represented as a list because it is a collection of objects of a single\ntype\u2014lexical entries\u2014of no predetermined length. An individual entry is represented\nas a tuple because it is a collection of objects with different interpretations, such as the\northographic form, the part-of-speech, and the pronunciations (represented in the\nSAMPA computer-readable phonetic alphabet; see http://www.phon.ucl.ac.uk/home/\nsampa/). Note that these pronunciations are stored using a list. (Why?)\nA good way to decide when to use tuples versus lists is to ask whether\nthe interpretation \nof an item depends on its position. For example, a\ntagged token combines two strings having different interpretations, and\nwe choose to interpret the first item as the token and the second item\nas the tag. Thus we use tuples like this: ('grail', 'noun'). A tuple of\nthe form ('noun', 'grail')  would be non-sensical since it would be a\nword noun tagged grail. In contrast, the elements of a text are all tokens,\nand position is not significant. Thus we use lists like this: ['venetian',\n'blind']. A list of the form ['blind', 'venetian'] would be equally\nvalid. The linguistic meaning of the words might be different, but the\ninterpretation of list items as tokens is unchanged.\nThe distinction between lists and tuples has been described in terms of usage. However,\nthere is a more fundamental difference: in Python, lists are mutable, whereas tuples\nare immutable. In other words, lists can be modified, whereas tuples cannot. Here are\nsome of the operations on lists that do in-place modification of the list:\n>>> lexicon.sort()\n>>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])\n>>> del lexicon[0]\nYour Turn:  Convert lexicon  to a tuple, using lexicon =\ntuple(lexicon), then try each of the operations, to confirm that none of\nthem is permitted on tuples.\nGenerator Expressions\nWe\u2019ve been making heavy use of list comprehensions, for compact and readable pro-\ncessing of texts. Here\u2019s an example where we tokenize and normalize a text:\n>>> text = '''\"When I use a word,\" Humpty Dumpty said in rather a scornful tone,\n... \"it means just what I choose it to mean - neither more nor less.\"'''\n>>> [w.lower() for w in nltk.word_tokenize(text)]\n['\"', 'when', 'i', 'use', 'a', 'word', ',', '\"', 'humpty', 'dumpty', 'said', ...]\n4.2  Sequences | 137\nSuppose we now want to process these words further. We can do this by inserting the\npreceding expression \ninside a call to some other function \n , but Python allows us to\nomit the brackets \n .\n>>> max([w.lower() for w in nltk.word_tokenize(text)]) \n'word'\n>>> max(w.lower() for w in nltk.word_tokenize(text)) \n'word'\nThe second \nline uses a generator expression . This is more than a notational conven-\nience: in many language processing situations, generator expressions will be more ef-\nficient. In \n , storage for the list object must be allocated before the value of max() is\ncomputed. \nIf the text is very large, this could be slow. In \n , the data is streamed to the\ncalling function. \nSince the calling function simply has to find the maximum value\u2014the\nword that comes latest in lexicographic sort order\u2014it can process the stream of data\nwithout having to store anything more than the maximum value seen so far.\n4.3  Questions of Style\nProgramming is as much an art as a science. The undisputed \u201cbible\u201d of programming,\na 2,500 page multivolume work by Donald Knuth, is called The Art of Computer Pro-\ngramming. Many books have been written on Literate Programming , recognizing that\nhumans, not just computers, must read and understand programs. Here we pick up on\nsome issues of programming style that have important ramifications for the readability\nof your code, including code layout, procedural versus declarative style, and the use of\nloop variables.\nPython Coding Style\nWhen writing programs you make many subtle choices about names, spacing, com-\nments, and so on. When you look at code written by other people, needless differences\nin style make it harder to interpret the code. Therefore, the designers of the Python\nlanguage have published a style guide for Python code, available at http://www.python\n.org/dev/peps/pep-0008/. The underlying value presented in the style guide is consis-\ntency, for the purpose of maximizing the readability of code. We briefly review some\nof its key recommendations here, and refer readers to the full guide for detailed dis-\ncussion with examples.\nCode layout should use four spaces per indentation level. You should make sure that\nwhen you write Python code in a file, you avoid tabs for indentation, since these can\nbe misinterpreted by different text editors and the indentation can be messed up. Lines\nshould be less than 80 characters long; if necessary, you can break a line inside paren-\ntheses, brackets, or braces, because Python is able to detect that the line continues over\nto the next line, as in the following examples:\n>>> cv_word_pairs = [(cv, w) for w in rotokas_words\n...                          for cv in re.findall('[ptksvr][aeiou]', w)]\n138 | Chapter 4: \u2002Writing Structured Programs\n>>> cfd = nltk.ConditionalFreqDist(\n...           (genre, word)\n...           for genre in brown.categories()\n...           for word in brown.words(categories=genre))\n \n>>> ha_words = ['aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha',\n...             'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'ha',\n...             'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha']\nIf you \nneed to break a line outside parentheses, brackets, or braces, you can often add\nextra parentheses, and you can always add a backslash at the end of the line that is\nbroken:\n>>> if (len(syllables) > 4 and len(syllables[2]) == 3 and\n...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]):\n...     process(syllables)\n>>> if len(syllables) > 4 and len(syllables[2]) == 3 and \\\n...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]:\n...     process(syllables)\nTyping spaces instead of tabs soon becomes a chore. Many program-\nming editors \nhave built-in support for Python, and can automatically\nindent code and highlight any syntax errors (including indentation er-\nrors). For a list of Python-aware editors, please see http://wiki.python\n.org/moin/PythonEditors.\nProcedural Versus Declarative Style\nWe have just seen how the same task can be performed in different ways, with impli-\ncations for efficiency. Another factor influencing program development is programming\nstyle. Consider the following program to compute the average length of words in the\nBrown Corpus:\n>>> tokens = nltk.corpus.brown.words(categories='news')\n>>> count = 0\n>>> total = 0\n>>> for token in tokens:\n...     count += 1\n...     total += len(token)\n>>> print total / count\n4.2765382469\nIn this program we use the variable count to keep track of the number of tokens seen,\nand total to store the combined length of all words. This is a low-level style, not far\nremoved from machine code, the primitive operations performed by the computer\u2019s\nCPU. The two variables are just like a CPU\u2019s registers, accumulating values at many\nintermediate stages, values that are meaningless until the end. We say that this program\nis written in a procedural style, dictating the machine operations step by step. Now\nconsider the following program that computes the same thing:\n4.3  Questions of Style | 139\n>>> total = sum(len(t) for t in tokens)\n>>> print total / len(tokens)\n4.2765382469\nThe first \nline uses a generator expression to sum the token lengths, while the second\nline computes the average as before. Each line of code performs a complete, meaningful\ntask, which can be understood in terms of high-level properties like: \u201c total is the sum\nof the lengths of the tokens.\u201d Implementation details are left to the Python interpreter.\nThe second program uses a built-in function, and constitutes programming at a more\nabstract level; the resulting code is more declarative. Let\u2019s look at an extreme example:\n>>> word_list = []\n>>> len_word_list = len(word_list)\n>>> i = 0\n>>> while i < len(tokens):\n...     j = 0\n...     while j < len_word_list and word_list[j] < tokens[i]:\n...         j += 1\n...     if j == 0 or tokens[i] != word_list[j]:\n...         word_list.insert(j, tokens[i])\n...         len_word_list += 1\n...     i += 1\nThe equivalent declarative version uses familiar built-in functions, and its purpose is\ninstantly recognizable:\n>>> word_list = sorted(set(tokens))\nAnother case where a loop counter seems to be necessary is for printing a counter with\neach line of output. Instead, we can use enumerate(), which processes a sequence s and\nproduces a tuple of the form (i, s[i]) for each item in s, starting with (0, s[0]). Here\nwe enumerate the keys of the frequency distribution, and capture the integer-string pair\nin the variables rank and word. We print rank+1 so that the counting appears to start\nfrom 1, as required when producing a list of ranked items.\n>>> fd = nltk.FreqDist(nltk.corpus.brown.words())\n>>> cumulative = 0.0\n>>> for rank, word in enumerate(fd):\n...     cumulative += fd[word] * 100 / fd.N()\n...     print \"%3d %6.2f%% %s\" % (rank+1, cumulative, word)\n...     if cumulative > 25:\n...         break\n...\n  1   5.40% the\n  2  10.42% ,\n  3  14.67% .\n  4  17.78% of\n  5  20.19% and\n  6  22.40% to\n  7  24.29% a\n  8  25.97% in\nIt\u2019s sometimes tempting to use loop variables to store a maximum or minimum value\nseen so far. Let\u2019s use this method to find the longest word in a text.\n140 | Chapter 4: \u2002Writing Structured Programs\n>>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')\n>>> longest = ''\n>>> for word in text:\n...     if len(word) > len(longest):\n...         longest = word\n>>> longest\n'unextinguishable'\nHowever, a \nmore transparent solution uses two list comprehensions, both having forms\nthat should be familiar by now:\n>>> maxlen = max(len(word) for word in text)\n>>> [word for word in text if len(word) == maxlen]\n['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']\nNote that our first solution found the first word having the longest length, while the\nsecond solution found all of the longest words (which is usually what we would want).\nAlthough there\u2019s a theoretical efficiency difference between the two solutions, the main\noverhead is reading the data into main memory; once it\u2019s there, a second pass through\nthe data is effectively instantaneous. We also need to balance our concerns about pro-\ngram efficiency with programmer efficiency. A fast but cryptic solution will be harder\nto understand and maintain.\nSome Legitimate Uses for Counters\nThere are cases where we still want to use loop variables in a list comprehension. For\nexample, we need to use a loop variable to extract successive overlapping n-grams from\na list:\n>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n>>> n = 3\n>>> [sent[i:i+n] for i in range(len(sent)-n+1)]\n[['The', 'dog', 'gave'],\n ['dog', 'gave', 'John'],\n ['gave', 'John', 'the'],\n ['John', 'the', 'newspaper']]\nIt is quite tricky to get the range of the loop variable right. Since this is a common\noperation in NLP, NLTK supports it with functions bigrams(text) and\ntrigrams(text), and a general-purpose ngrams(text, n).\nHere\u2019s an example of how we can use loop variables in building multidimensional\nstructures. For example, to build an array with m rows and n columns, where each cell\nis a set, we could use a nested list comprehension:\n>>> m, n = 3, 7\n>>> array = [[set() for i in range(n)] for j in range(m)]\n>>> array[2][5].add('Alice')\n>>> pprint.pprint(array)\n[[set([]), set([]), set([]), set([]), set([]), set([]), set([])],\n [set([]), set([]), set([]), set([]), set([]), set([]), set([])],\n [set([]), set([]), set([]), set([]), set([]), set(['Alice']), set([])]]\n4.3  Questions of Style | 141\nObserve that the loop variables i and j are not used anywhere in the resulting object;\nthey are just needed for a syntactically correct for statement. As another example of\nthis usage, observe that the expression ['very' for i in range(3)] produces a list\ncontaining three instances of 'very', with no integers in sight.\nNote that it would be incorrect to do this work using multiplication, for reasons con-\ncerning object copying that were discussed earlier in this section.\n>>> array = [[set()] * n] * m\n>>> array[2][5].add(7)\n>>> pprint.pprint(array)\n[[set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],\n [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],\n [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])]]\nIteration is an important programming device. It is tempting to adopt idioms from other\nlanguages. However, Python offers some elegant and highly readable alternatives, as\nwe have seen.\n4.4  Functions: The Foundation of Structured Programming\nFunctions provide an effective way to package and reuse program code, as already\nexplained in Section 2.3. For example, suppose we find that we often want to read text\nfrom an HTML file. This involves several steps: opening the file, reading it in, normal-\nizing whitespace, and stripping HTML markup. We can collect these steps into a func-\ntion, and give it a name such as get_text(), as shown in Example 4-1.\nExample 4-1. Read text from a file.\nimport re\ndef get_text(file):\n    \"\"\"Read text from a file, normalizing whitespace and stripping HTML markup.\"\"\"\n    text = open(file).read()\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r'<.*?>', ' ', text)\n    return text\nNow, any time we want to get cleaned-up text from an HTML file, we can just call\nget_text() with the name of the file as its only argument. It will return a string, and we\ncan assign this to a variable, e.g., contents = get_text(\"test.html\"). Each time we\nwant to use this series of steps, we only have to call the function.\nUsing functions has the benefit of saving space in our program. More importantly, our\nchoice of name for the function helps make the program readable. In the case of the\npreceding example, whenever our program needs to read cleaned-up text from a file\nwe don\u2019t have to clutter the program with four lines of code; we simply need to call\nget_text(). This naming helps to provide some \u201csemantic interpretation\u201d\u2014it helps a\nreader of our program to see what the program \u201cmeans.\u201d\n142 | Chapter 4: \u2002Writing Structured Programs\nNotice that this example function definition contains a string. The first string inside a\nfunction definition \nis called a docstring. Not only does it document the purpose of the\nfunction to someone reading the code, it is accessible to a programmer who has loaded\nthe code from a file:\n>>> help(get_text)\nHelp on function get_text:\nget_text(file)\n    Read text from a file, normalizing whitespace\n    and stripping HTML markup.\nWe have seen that functions help to make our work reusable and readable. They also\nhelp make it reliable. When we reuse code that has already been developed and tested,\nwe can be more confident that it handles a variety of cases correctly. We also remove\nthe risk of forgetting some important step or introducing a bug. The program that calls\nour function also has increased reliability. The author of that program is dealing with\na shorter program, and its components behave transparently.\nTo summarize, as its name suggests, a function captures functionality. It is a segment\nof code that can be given a meaningful name and which performs a well-defined task.\nFunctions allow us to abstract away from the details, to see a bigger picture, and to\nprogram more effectively.\nThe rest of this section takes a closer look at functions, exploring the mechanics and\ndiscussing ways to make your programs easier to read.\nFunction Inputs and Outputs\nWe pass information to functions using a function\u2019s parameters, the parenthesized list\nof variables and constants following the function\u2019s name in the function definition.\nHere\u2019s a complete example:\n>>> def repeat(msg, num):  \n...     return ' '.join([msg] * num)\n>>> monty = 'Monty Python'\n>>> repeat(monty, 3) \n'Monty Python Monty Python Monty Python'\nWe first \ndefine the function to take two parameters, msg and num \n . Then, we call the\nfunction and \npass it two arguments, monty and 3 \n; these arguments fill the \u201cplace-\nholders\u201d provided \nby the parameters and provide values for the occurrences of msg and\nnum in the function body.\nIt is not necessary to have any parameters, as we see in the following example:\n>>> def monty():\n...     return \"Monty Python\"\n>>> monty()\n'Monty Python'\n4.4  Functions: The Foundation of Structured Programming | 143\nA function usually communicates its results back to the calling program via the\nreturn statement, \nas we have just seen. To the calling program, it looks as if the function\ncall had been replaced with the function\u2019s result:\n>>> repeat(monty(), 3)\n'Monty Python Monty Python Monty Python'\n>>> repeat('Monty Python', 3)\n'Monty Python Monty Python Monty Python'\nA Python function is not required to have a return statement. Some functions do their\nwork as a side effect, printing a result, modifying a file, or updating the contents of a\nparameter to the function (such functions are called \u201cprocedures\u201d in some other\nprogramming languages).\nConsider the following three sort functions. The third one is dangerous because a pro-\ngrammer could use it without realizing that it had modified its input. In general, func-\ntions should modify the contents of a parameter ( my_sort1()), or return a value\n(my_sort2()), but not both (my_sort3()).\n>>> def my_sort1(mylist):      # good: modifies its argument, no return value\n...     mylist.sort()\n>>> def my_sort2(mylist):      # good: doesn't touch its argument, returns value\n...     return sorted(mylist)\n>>> def my_sort3(mylist):      # bad: modifies its argument and also returns it\n...     mylist.sort()\n...     return mylist\nParameter Passing\nBack in Section 4.1 , you saw that assignment works on values, but that the value of a\nstructured object is a reference to that object. The same is true for functions. Python\ninterprets function parameters as values (this is known as call-by-value). In the fol-\nlowing code, set_up() has two parameters, both of which are modified inside the func-\ntion. We begin by assigning an empty string to w and an empty dictionary to p. After\ncalling the function, w is unchanged, while p is changed:\n>>> def set_up(word, properties):\n...     word = 'lolcat'\n...     properties.append('noun')\n...     properties = 5\n...\n>>> w = ''\n>>> p = []\n>>> set_up(w, p)\n>>> w\n''\n>>> p\n['noun']\nNotice that w was not changed by the function. When we called set_up(w, p), the value\nof w (an empty string) was assigned to a new variable word. Inside the function, the value\n144 | Chapter 4: \u2002Writing Structured Programs\nof word was modified. However, that change did not propagate to w . This parameter\npassing is identical to the following sequence of assignments:\n>>> w = ''\n>>> word = w\n>>> word = 'lolcat'\n>>> w\n''\nLet\u2019s look at what happened with the list p. When we called set_up(w, p) , the value of\np (a reference to an empty list) was assigned to a new local variable properties, so both\nvariables now reference the same memory location. The function modifies\nproperties, and this change is also reflected in the value of p, as we saw. The function\nalso assigned a new value to properties (the number 5); this did not modify the contents\nat that memory location, but created a new local variable. This behavior is just as if we\nhad done the following sequence of assignments:\n>>> p = []\n>>> properties = p\n>>> properties.append['noun']\n>>> properties = 5\n>>> p\n['noun']\nThus, to understand Python\u2019s call-by-value parameter passing, it is enough to under-\nstand how assignment works. Remember that you can use the id() function and is\noperator to check your understanding of object identity after each statement.\nVariable Scope\nFunction definitions create a new local scope for variables. When you assign to a new\nvariable inside the body of a function, the name is defined only within that function.\nThe name is not visible outside the function, or in other functions. This behavior means\nyou can choose variable names without being concerned about collisions with names\nused in your other function definitions.\nWhen you refer to an existing name from within the body of a function, the Python\ninterpreter first tries to resolve the name with respect to the names that are local to the\nfunction. If nothing is found, the interpreter checks whether it is a global name within\nthe module. Finally, if that does not succeed, the interpreter checks whether the name\nis a Python built-in. This is the so-called LGB rule of name resolution: local, then\nglobal, then built-in.\nCaution!\nA function \ncan create a new global variable, using the global declaration.\nHowever, this practice should be avoided as much as possible. Defining\nglobal variables inside a function introduces dependencies on context\nand limits the portability (or reusability) of the function. In general you\nshould use parameters for function inputs and return values for function\noutputs.\n4.4  Functions: The Foundation of Structured Programming | 145\nChecking Parameter Types\nPython does \nnot force us to declare the type of a variable when we write a program,\nand this permits us to define functions that are flexible about the type of their argu-\nments. For example, a tagger might expect a sequence of words, but it wouldn\u2019t care\nwhether this sequence is expressed as a list, a tuple, or an iterator (a new sequence type\nthat we\u2019ll discuss later).\nHowever, often we want to write programs for later use by others, and want to program\nin a defensive style, providing useful warnings when functions have not been invoked\ncorrectly. The author of the following tag() function assumed that its argument would\nalways be a string.\n>>> def tag(word):\n...     if word in ['a', 'the', 'all']:\n...         return 'det'\n...     else:\n...         return 'noun'\n...\n>>> tag('the')\n'det'\n>>> tag('knight')\n'noun'\n>>> tag([\"'Tis\", 'but', 'a', 'scratch']) \n'noun'\nThe function \nreturns sensible values for the arguments 'the' and 'knight', but look\nwhat happens when it is passed a list \n \u2014it fails to complain, even though the result\nwhich \nit returns is clearly incorrect. The author of this function could take some extra\nsteps to ensure that the word parameter of the tag() function is a string. A naive ap-\nproach would be to check the type of the argument using if not type(word) is str,\nand if word is not a string, to simply return Python\u2019s special empty value, None. This is\na slight improvement, because the function is checking the type of the argument, and\ntrying to return a \u201cspecial\u201d diagnostic value for the wrong input. However, it is also\ndangerous because the calling program may not detect that None is intended as a \u201cspe-\ncial\u201d value, and this diagnostic return value may then be propagated to other parts of\nthe program with unpredictable consequences. This approach also fails if the word is\na Unicode string, which has type unicode, not str. Here\u2019s a better solution, using an\nassert statement together with Python\u2019s basestring type that generalizes over both\nunicode and str.\n>>> def tag(word):\n...     assert isinstance(word, basestring), \"argument to tag() must be a string\"\n...     if word in ['a', 'the', 'all']:\n...         return 'det'\n...     else:\n...         return 'noun'\nIf the assert statement fails, it will produce an error that cannot be ignored, since it\nhalts program execution. Additionally, the error message is easy to interpret. Adding\n146 | Chapter 4: \u2002Writing Structured Programs\nassertions to a program helps you find logical errors, and is a kind of defensive pro-\ngramming. A \nmore fundamental approach is to document the parameters to each\nfunction using docstrings, as described later in this section.\nFunctional Decomposition\nWell-structured programs usually make extensive use of functions. When a block of\nprogram code grows longer than 10\u201320 lines, it is a great help to readability if the code\nis broken up into one or more functions, each one having a clear purpose. This is\nanalogous to the way a good essay is divided into paragraphs, each expressing one main\nidea.\nFunctions provide an important kind of abstraction. They allow us to group multiple\nactions into a single, complex action, and associate a name with it. (Compare this with\nthe way we combine the actions of go and bring back into a single more complex action\nfetch.) When we use functions, the main program can be written at a higher level of\nabstraction, making its structure transparent, as in the following:\n>>> data = load_corpus()\n>>> results = analyze(data)\n>>> present(results)\nAppropriate use of functions makes programs more readable and maintainable. Addi-\ntionally, it becomes possible to reimplement a function\u2014replacing the function\u2019s body\nwith more efficient code\u2014without having to be concerned with the rest of the program.\nConsider the freq_words function in Example 4-2 . It updates the contents of a frequency\ndistribution that is passed in as a parameter, and it also prints a list of the n most\nfrequent words.\nExample 4-2. Poorly designed function to compute frequent words.\ndef freq_words(url, freqdist, n):\n    text = nltk.clean_url(url)\n    for word in nltk.word_tokenize(text):\n        freqdist.inc(word.lower())\n    print freqdist.keys()[:n]\n>>> constitution = \"http://www.archives.gov/national-archives-experience\" \\\n...                \"/charters/constitution_transcript.html\"\n>>> fd = nltk.FreqDist()\n>>> freq_words(constitution, fd, 20)\n['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n'declaration', 'impact', 'freedom', '-', 'making', 'independence']\nThis function has a number of problems. The function has two side effects: it modifies\nthe contents of its second parameter, and it prints a selection of the results it has com-\nputed. The function would be easier to understand and to reuse elsewhere if we initialize\nthe FreqDist() object inside the function (in the same place it is populated), and if we\nmoved the selection and display of results to the calling program. In Example 4-3  we\nrefactor this function, and simplify its interface by providing a single url parameter.\n4.4  Functions: The Foundation of Structured Programming | 147\nExample 4-3. Well-designed function to compute frequent words.\ndef freq_words(url):\n    freqdist = nltk.FreqDist()\n    text = nltk.clean_url(url)\n    for word in nltk.word_tokenize(text):\n        freqdist.inc(word.lower())\n    return freqdist\n>>> fd = freq_words(constitution)\n>>> print fd.keys()[:20]\n['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n'declaration', 'impact', 'freedom', '-', 'making', 'independence']\nNote that \nwe have now simplified the work of freq_words to the point that we can do\nits work with three lines of code:\n>>> words = nltk.word_tokenize(nltk.clean_url(constitution))\n>>> fd = nltk.FreqDist(word.lower() for word in words)\n>>> fd.keys()[:20]\n['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n'declaration', 'impact', 'freedom', '-', 'making', 'independence']\nDocumenting Functions\nIf we have done a good job at decomposing our program into functions, then it should\nbe easy to describe the purpose of each function in plain language, and provide this in\nthe docstring at the top of the function definition. This statement should not explain\nhow the functionality is implemented; in fact, it should be possible to reimplement the\nfunction using a different method without changing this statement.\nFor the simplest functions, a one-line docstring is usually adequate (see Example 4-1 ).\nYou should provide a triple-quoted string containing a complete sentence on a single\nline. For non-trivial functions, you should still provide a one-sentence summary on the\nfirst line, since many docstring processing tools index this string. This should be fol-\nlowed by a blank line, then a more detailed description of the functionality (see http://\nwww.python.org/dev/peps/pep-0257/ for more information on docstring conventions).\nDocstrings can include a doctest block , illustrating the use of the function and the\nexpected output. These can be tested automatically using Python\u2019s docutils module.\nDocstrings should document the type of each parameter to the function, and the return\ntype. At a minimum, that can be done in plain text. However, note that NLTK uses the\n\u201cepytext\u201d markup language to document parameters. This format can be automatically\nconverted into richly structured API documentation (see http://www.nltk.org/), and in-\ncludes special handling of certain \u201cfields,\u201d such as @param, which allow the inputs and\noutputs of functions to be clearly documented. Example 4-4  illustrates a complete\ndocstring.\n148 | Chapter 4: \u2002Writing Structured Programs\nExample 4-4. Illustration of a complete docstring, consisting of a one-line summary, a more detailed\nexplanation, a \ndoctest example, and epytext markup specifying the parameters, types, return type,\nand exceptions.\ndef accuracy(reference, test):\n    \"\"\"\n    Calculate the fraction of test items that equal the corresponding reference items.\n    Given a list of reference values and a corresponding list of test values,\n    return the fraction of corresponding values that are equal.\n    In particular, return the fraction of indexes\n    {0<i<=len(test)} such that C{test[i] == reference[i]}.\n    >>> accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])\n    0.5\n@param reference: An ordered list of reference values.\n@type reference: C{list}\n@param test: A list of values to compare against the corresponding\n    reference values.\n@type test: C{list}\n@rtype: C{float}\n@raise ValueError: If C{reference} and C{length} do not have the\n    same length.\n\"\"\"\nif len(reference) != len(test):\n    raise ValueError(\"Lists must have the same length.\")\nnum_correct = 0\nfor x, y in izip(reference, test):\n    if x == y:\n        num_correct += 1\nreturn float(num_correct) / len(reference)\n4.5  Doing More with Functions\nThis section discusses more advanced features, which you may prefer to skip on the\nfirst time through this chapter.\nFunctions As Arguments\nSo far the arguments we have passed into functions have been simple objects, such as\nstrings, or structured objects, such as lists. Python also lets us pass a function as an\nargument to another function. Now we can abstract out the operation, and apply a\ndifferent operation  on the same data . As the following examples show, we can pass the\nbuilt-in function len() or a user-defined function last_letter() as arguments to an-\nother function:\n>>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n>>> def extract_property(prop):\n...     return [prop(word) for word in sent]\n...\n4.5  Doing More with Functions | 149\n>>> extract_property(len)\n[4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]\n>>> def last_letter(word):\n...     return word[-1]\n>>> extract_property(last_letter)\n['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\nThe objects len\n and last_letter can be passed around like lists and dictionaries. Notice\nthat parentheses are used after a function name only if we are invoking the function;\nwhen we are simply treating the function as an object, these are omitted.\nPython provides us with one more way to define functions as arguments to other func-\ntions, so-called lambda expressions . Supposing there was no need to use the last_let\nter() function in multiple places, and thus no need to give it a name. Let\u2019s suppose we\ncan equivalently write the following:\n>>> extract_property(lambda w: w[-1])\n['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\nOur next example illustrates passing a function to the sorted() function. When we call\nthe latter with a single argument (the list to be sorted), it uses the built-in comparison\nfunction cmp(). However, we can supply our own sort function, e.g., to sort by de-\ncreasing length.\n>>> sorted(sent)\n[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n'take', 'the', 'the', 'themselves', 'will']\n>>> sorted(sent, cmp)\n[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n'take', 'the', 'the', 'themselves', 'will']\n>>> sorted(sent, lambda x, y: cmp(len(y), len(x)))\n['themselves', 'sounds', 'sense', 'Take', 'care', 'will', 'take', 'care',\n'the', 'and', 'the', 'of', 'of', ',', '.']\nAccumulative Functions\nThese functions start by initializing some storage, and iterate over input to build it up,\nbefore returning some final object (a large structure or aggregated result). A standard\nway to do this is to initialize an empty list, accumulate the material, then return the\nlist, as shown in function search1() in Example 4-5.\nExample 4-5. Accumulating output into a list.\ndef search1(substring, words):\n    result = []\n    for word in words:\n        if substring in word:\n            result.append(word)\n    return result\ndef search2(substring, words):\n    for word in words:\n        if substring in word:\n            yield word\n150 | Chapter 4: \u2002Writing Structured Programs\nprint \"search1:\"\nfor item in search1('zz', nltk.corpus.brown.words()):\n    print item\nprint \"search2:\"\nfor item in search2('zz', nltk.corpus.brown.words()):\n    print item\nThe function search2()\n is a generator. The first time this function is called, it gets as\nfar as the yield statement and pauses. The calling program gets the first word and does\nany necessary processing. Once the calling program is ready for another word, execu-\ntion of the function is continued from where it stopped, until the next time it encounters\na yield statement. This approach is typically more efficient, as the function only gen-\nerates the data as it is required by the calling program, and does not need to allocate\nadditional memory to store the output (see the earlier discussion of generator expres-\nsions).\nHere\u2019s a more sophisticated example of a generator which produces all permutations\nof a list of words. In order to force the permutations() function to generate all its output,\nwe wrap it with a call to list() \n .\n>>> def permutations(seq):\n...     if len(seq) <= 1:\n...         yield seq\n...     else:\n...         for perm in permutations(seq[1:]):\n...             for i in range(len(perm)+1):\n...                 yield perm[:i] + seq[0:1] + perm[i:]\n...\n>>> list(permutations(['police', 'fish', 'buffalo'])) \n[['police', 'fish', 'buffalo'], ['fish', 'police', 'buffalo'],\n ['fish', 'buffalo', 'police'], ['police', 'buffalo', 'fish'],\n ['buffalo', 'police', 'fish'], ['buffalo', 'fish', 'police']]\nThe permutations function uses a technique called recursion, discussed\nlater in Section 4.7 . The ability to generate permutations of a set of words\nis useful for creating data to test a grammar (Chapter 8).\nHigher-Order Functions\nPython provides some higher-order functions that are standard features of functional\nprogramming languages such as Haskell. We illustrate them here, alongside the equiv-\nalent expression using list comprehensions.\nLet\u2019s start by defining a function is_content_word() which checks whether a word is\nfrom the open class of content words. We use this function as the first parameter of\nfilter(), which applies the function to each item in the sequence contained in its\nsecond parameter, and retains only the items for which the function returns True.\n4.5  Doing More with Functions | 151\n>>> def is_content_word(word):\n...     return word.lower() not in ['a', 'of', 'the', 'and', 'will', ',', '.']\n>>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n>>> filter(is_content_word, sent)\n['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\n>>> [w for w in sent if is_content_word(w)]\n['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\nAnother higher-order \nfunction is map(), which applies a function to every item in a\nsequence. It is a general version of the extract_property() function we saw earlier in\nthis section. Here is a simple way to find the average length of a sentence in the news\nsection of the Brown Corpus, followed by an equivalent version with list comprehen-\nsion calculation:\n>>> lengths = map(len, nltk.corpus.brown.sents(categories='news'))\n>>> sum(lengths) / len(lengths)\n21.7508111616\n>>> lengths = [len(w) for w in nltk.corpus.brown.sents(categories='news'))]\n>>> sum(lengths) / len(lengths)\n21.7508111616\nIn the previous examples, we specified a user-defined function is_content_word() and\na built-in function len(). We can also provide a lambda expression. Here\u2019s a pair of\nequivalent examples that count the number of vowels in each word.\n>>> map(lambda w: len(filter(lambda c: c.lower() in \"aeiou\", w)), sent)\n[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\n>>> [len([c for c in w if c.lower() in \"aeiou\"]) for w in sent]\n[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\nThe solutions based on list comprehensions are usually more readable than the solu-\ntions based on higher-order functions, and we have favored the former approach\nthroughout this book.\nNamed Arguments\nWhen there are a lot of parameters it is easy to get confused about the correct order.\nInstead we can refer to parameters by name, and even assign them a default value just\nin case one was not provided by the calling program. Now the parameters can be speci-\nfied in any order, and can be omitted.\n>>> def repeat(msg='<empty>', num=1):\n...     return msg * num\n>>> repeat(num=3)\n'<empty><empty><empty>'\n>>> repeat(msg='Alice')\n'Alice'\n>>> repeat(num=5, msg='Alice')\n'AliceAliceAliceAliceAlice'\nThese are called keyword arguments . If we mix these two kinds of parameters, then\nwe must ensure that the unnamed parameters precede the named ones. It has to be this\n152 | Chapter 4: \u2002Writing Structured Programs\nway, since unnamed parameters are defined by position. We can define a function that\ntakes an \narbitrary number of unnamed and named parameters, and access them via an\nin-place list of arguments *args and an in-place dictionary of keyword arguments\n**kwargs.\n>>> def generic(*args, **kwargs):\n...     print args\n...     print kwargs\n...\n>>> generic(1, \"African swallow\", monty=\"python\")\n(1, 'African swallow')\n{'monty': 'python'}\nWhen *args appears as a function parameter, it actually corresponds to all the unnamed\nparameters of the function. As another illustration of this aspect of Python syntax,\nconsider the zip() function, which operates on a variable number of arguments. We\u2019ll\nuse the variable name *song to demonstrate that there\u2019s nothing special about the name\n*args.\n>>> song = [['four', 'calling', 'birds'],\n...         ['three', 'French', 'hens'],\n...         ['two', 'turtle', 'doves']]\n>>> zip(song[0], song[1], song[2])\n[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\n>>> zip(*song)\n[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\nIt should be clear from this example that typing *song is just a convenient shorthand,\nand equivalent to typing out song[0], song[1], song[2].\nHere\u2019s another example of the use of keyword arguments in a function definition, along\nwith three equivalent ways to call the function:\n>>> def freq_words(file, min=1, num=10):\n...     text = open(file).read()\n...     tokens = nltk.word_tokenize(text)\n...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)\n...     return freqdist.keys()[:num]\n>>> fw = freq_words('ch01.rst', 4, 10)\n>>> fw = freq_words('ch01.rst', min=4, num=10)\n>>> fw = freq_words('ch01.rst', num=10, min=4)\nA side effect of having named arguments is that they permit optionality. Thus we can\nleave out any arguments where we are happy with the default value:\nfreq_words('ch01.rst', min=4) , freq_words('ch01.rst', 4) . Another common use of\noptional arguments is to permit a flag. Here\u2019s a revised version of the same function\nthat reports its progress if a verbose flag is set:\n>>> def freq_words(file, min=1, num=10, verbose=False):\n...     freqdist = FreqDist()\n...     if trace: print \"Opening\", file\n...     text = open(file).read()\n...     if trace: print \"Read in %d characters\" % len(file)\n...     for word in nltk.word_tokenize(text):\n4.5  Doing More with Functions | 153\n...         if len(word) >= min:\n...             freqdist.inc(word)\n...             if trace and freqdist.N() % 100 == 0: print \".\"\n...     if trace: print\n...     return freqdist.keys()[:num]\nCaution!\nTake care \nnot to use a mutable object as the default value of a parameter.\nA series of calls to the function will use the same object, sometimes with\nbizarre results, as we will see in the discussion of debugging later.\n4.6  Program Development\nProgramming is a skill that is acquired over several years of experience with a variety\nof programming languages and tasks. Key high-level abilities are algorithm design  and\nits manifestation in structured programming . Key low-level abilities include familiarity\nwith the syntactic constructs of the language, and knowledge of a variety of diagnostic\nmethods for trouble-shooting a program which does not exhibit the expected behavior.\nThis section describes the internal structure of a program module and how to organize\na multi-module program. Then it describes various kinds of error that arise during\nprogram development, what you can do to fix them and, better still, to avoid them in\nthe first place.\nStructure of a Python Module\nThe purpose of a program module is to bring logically related definitions and functions\ntogether in order to facilitate reuse and abstraction. Python modules are nothing more\nthan individual .py files. For example, if you were working with a particular corpus\nformat, the functions to read and write the format could be kept together. Constants\nused by both formats, such as field separators, or a EXTN = \".inf\"  filename extension,\ncould be shared. If the format was updated, you would know that only one file needed\nto be changed. Similarly, a module could contain code for creating and manipulating\na particular data structure such as syntax trees, or code for performing a particular\nprocessing task such as plotting corpus statistics.\nWhen you start writing Python modules, it helps to have some examples to emulate.\nYou can locate the code for any NLTK module on your system using the __file__\nvariable:\n>>> nltk.metrics.distance.__file__\n'/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'\nThis returns the location of the compiled .pyc file for the module, and you\u2019ll probably\nsee a different location on your machine. The file that you will need to open is the\ncorresponding .py source file, and this will be in the same directory as the .pyc file.\n154 | Chapter 4: \u2002Writing Structured Programs\nAlternatively, you can view the latest version of this module on the Web at http://code\n.google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py.\nLike every \nother NLTK module, distance.py begins with a group of comment lines giving\na one-line title of the module and identifying the authors. (Since the code is distributed,\nit also includes the URL where the code is available, a copyright statement, and license\ninformation.) Next is the module-level docstring, a triple-quoted multiline string con-\ntaining information about the module that will be printed when someone types\nhelp(nltk.metrics.distance).\n# Natural Language Toolkit: Distance Metrics\n#\n# Copyright (C) 2001-2009 NLTK Project\n# Author: Edward Loper <edloper@gradient.cis.upenn.edu>\n#         Steven Bird <sb@csse.unimelb.edu.au>\n#         Tom Lippincott <tom@cs.columbia.edu>\n# URL: <http://www.nltk.org/>\n# For license information, see LICENSE.TXT\n#\n\"\"\"\nDistance Metrics.\nCompute the distance between two items (usually strings).\nAs metrics, they must satisfy the following three requirements:\n1. d(a, a) = 0\n2. d(a, b) >= 0\n3. d(a, c) <= d(a, b) + d(b, c)\n\"\"\"\nAfter this comes all the import statements required for the module, then any global\nvariables, followed by a series of function definitions that make up most of the module.\nOther modules define \u201cclasses,\u201d the main building blocks of object-oriented program-\nming, which falls outside the scope of this book. (Most NLTK modules also include a\ndemo() function, which can be used to see examples of the module in use.)\nSome module variables and functions are only used within the module.\nThese should \nhave names beginning with an underscore, e.g.,\n_helper(), since this will hide the name. If another module imports this\none, using the idiom: from module import *, these names will not be\nimported. You can optionally list the externally accessible names of a\nmodule using a special built-in variable like this: __all__ = ['edit_dis\ntance', 'jaccard_distance'].\nMultimodule Programs\nSome programs bring together a diverse range of tasks, such as loading data from a\ncorpus, performing some analysis tasks on the data, then visualizing it. We may already\n4.6  Program Development | 155\nhave stable modules that take care of loading data and producing visualizations. Our\nwork might \ninvolve coding up the analysis task, and just invoking functions from the\nexisting modules. This scenario is depicted in Figure 4-2.\nFigure 4-2. Structure of a multimodule program: The main program my_program.py imports\nfunctions \nfrom two other modules; unique analysis tasks are localized to the main program, while\ncommon loading and visualization tasks are kept apart to facilitate reuse and abstraction.\nBy dividing our work into several modules and using import statements to access func-\ntions defined elsewhere, we can keep the individual modules simple and easy to main-\ntain. This approach will also result in a growing collection of modules, and make it\npossible for us to build sophisticated systems involving a hierarchy of modules. De-\nsigning such systems well is a complex software engineering task, and beyond the scope\nof this book.\nSources of Error\nMastery of programming depends on having a variety of problem-solving skills to draw\nupon when the program doesn\u2019t work as expected. Something as trivial as a misplaced\nsymbol might cause the program to behave very differently. We call these \u201cbugs\u201d be-\ncause they are tiny in comparison to the damage they can cause. They creep into our\ncode unnoticed, and it\u2019s only much later when we\u2019re running the program on some\nnew data that their presence is detected. Sometimes, fixing one bug only reveals an-\nother, and we get the distinct impression that the bug is on the move. The only reas-\nsurance we have is that bugs are spontaneous and not the fault of the programmer.\n156 | Chapter 4: \u2002Writing Structured Programs\nFlippancy aside, debugging code is hard because there are so many ways for it to be\nfaulty. Our \nunderstanding of the input data, the algorithm, or even the programming\nlanguage, may be at fault. Let\u2019s look at examples of each of these.\nFirst, the input data may contain some unexpected characters. For example, WordNet\nsynset names have the form tree.n.01, with three components separated using periods.\nThe NLTK WordNet module initially decomposed these names using split('.').\nHowever, this method broke when someone tried to look up the word PhD, which has\nthe synset name ph.d..n.01, containing four periods instead of the expected two. The\nsolution was to use rsplit('.', 2) to do at most two splits, using the rightmost in-\nstances of the period, and leaving the ph.d. string intact. Although several people had\ntested the module before it was released, it was some weeks before someone detected\nthe problem (see http://code.google.com/p/nltk/issues/detail?id=297).\nSecond, a supplied function might not behave as expected. For example, while testing\nNLTK\u2019s interface to WordNet, one of the authors noticed that no synsets had any\nantonyms defined, even though the underlying database provided a large quantity of\nantonym information. What looked like a bug in the WordNet interface turned out to\nbe a misunderstanding about WordNet itself: antonyms are defined for lemmas, not\nfor synsets. The only \u201cbug\u201d was a misunderstanding of the interface (see http://code\n.google.com/p/nltk/issues/detail?id=98).\nThird, our understanding of Python\u2019s semantics may be at fault. It is easy to make the\nwrong assumption about the relative scope of two operators. For example, \"%s.%s.\n%02d\" % \"ph.d.\", \"n\", 1 produces a runtime error TypeError: not enough arguments\nfor format string. This is because the percent operator has higher precedence than\nthe comma operator. The fix is to add parentheses in order to force the required scope.\nAs another example, suppose we are defining a function to collect all tokens of a text\nhaving a given length. The function has parameters for the text and the word length,\nand an extra parameter that allows the initial value of the result to be given as a\nparameter:\n>>> def find_words(text, wordlength, result=[]):\n...     for word in text:\n...         if len(word) == wordlength:\n...             result.append(word)\n...     return result\n>>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n['omg', 'teh', 'teh', 'mat']\n>>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 2, ['ur']) \n['ur', 'on']\n>>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n['omg', 'teh', 'teh', 'mat', 'omg', 'teh', 'teh', 'mat']\nThe first \ntime we call find_words() \n , we get all three-letter words as expected. The\nsecond time we specify an initial value for the result, a one-element list ['ur'], and as\nexpected, the \nresult has this word along with the other two-letter word in our text.\nNow, the next time we call find_words() \n  we use the same parameters as in \n , but\nwe get \na different result! Each time we call find_words() with no third parameter, the\n4.6  Program Development | 157\nresult will simply extend the result of the previous call, rather than start with the empty\nresult list \nas specified in the function definition. The program\u2019s behavior is not as ex-\npected because we incorrectly assumed that the default value was created at the time\nthe function was invoked. However, it is created just once, at the time the Python\ninterpreter loads the function. This one list object is used whenever no explicit value\nis provided to the function.\nDebugging Techniques\nSince most code errors result from the programmer making incorrect assumptions, the\nfirst thing to do when you detect a bug is to check your assumptions . Localize the prob-\nlem by adding print statements to the program, showing the value of important vari-\nables, and showing how far the program has progressed.\nIf the program produced an \u201cexception\u201d\u2014a runtime error\u2014the interpreter will print\na stack trace , pinpointing the location of program execution at the time of the error.\nIf the program depends on input data, try to reduce this to the smallest size while still\nproducing the error.\nOnce you have localized the problem to a particular function or to a line of code, you\nneed to work out what is going wrong. It is often helpful to recreate the situation using\nthe interactive command line. Define some variables, and then copy-paste the offending\nline of code into the session and see what happens. Check your understanding of the\ncode by reading some documentation and examining other code samples that purport\nto do the same thing that you are trying to do. Try explaining your code to someone\nelse, in case she can see where things are going wrong.\nPython provides a debugger which allows you to monitor the execution of your pro-\ngram, specify line numbers where execution will stop (i.e., breakpoints), and step\nthrough sections of code and inspect the value of variables. You can invoke the debug-\nger on your code as follows:\n>>> import pdb\n>>> import mymodule\n>>> pdb.run('mymodule.myfunction()')\nIt will present you with a prompt (Pdb) where you can type instructions to the debugger.\nType help to see the full list of commands. Typing step (or just s) will execute the\ncurrent line and stop. If the current line calls a function, it will enter the function and\nstop at the first line. Typing next (or just n) is similar, but it stops execution at the next\nline in the current function. The break (or b) command can be used to create or list\nbreakpoints. Type continue (or c) to continue execution as far as the next breakpoint.\nType the name of any variable to inspect its value.\nWe can use the Python debugger to locate the problem in our find_words() function.\nRemember that the problem arose the second time the function was called. We\u2019ll start\nby calling the function without using the debugger \n , using the smallest possible input.\nThe second time, we\u2019ll call it with the debugger \n .\n158 | Chapter 4: \u2002Writing Structured Programs\n>>> import pdb\n>>> find_words(['cat'], 3) \n['cat']\n>>> pdb.run(\"find_words(['dog'], 3)\") \n> <string>(1)<module>()\n(Pdb) step\n--Call--\n> <stdin>(1)find_words()\n(Pdb) args\ntext = ['dog']\nwordlength = 3\nresult = ['cat']\nHere we \ntyped just two commands into the debugger: step took us inside the function,\nand args showed the values of its arguments (or parameters). We see immediately that\nresult has an initial value of ['cat'], and not the empty list as expected. The debugger\nhas helped us to localize the problem, prompting us to check our understanding of\nPython functions.\nDefensive Programming\nIn order to avoid some of the pain of debugging, it helps to adopt some defensive\nprogramming habits. Instead of writing a 20-line program and then testing it, build the\nprogram bottom-up out of small pieces that are known to work. Each time you combine\nthese pieces to make a larger unit, test it carefully to see that it works as expected.\nConsider adding assert statements to your code, specifying properties of a variable,\ne.g., assert(isinstance(text, list)) . If the value of the text variable later becomes a\nstring when your code is used in some larger context, this will raise an\nAssertionError and you will get immediate notification of the problem.\nOnce you think you\u2019ve found the bug, view your solution as a hypothesis. Try to predict\nthe effect of your bugfix before re-running the program. If the bug isn\u2019t fixed, don\u2019t fall\ninto the trap of blindly changing the code in the hope that it will magically start working\nagain. Instead, for each change, try to articulate a hypothesis about what is wrong and\nwhy the change will fix the problem. Then undo the change if the problem was not\nresolved.\nAs you develop your program, extend its functionality, and fix any bugs, it helps to\nmaintain a suite of test cases. This is called regression testing , since it is meant to\ndetect situations where the code \u201cregresses\u201d\u2014where a change to the code has an un-\nintended side effect of breaking something that used to work. Python provides a simple\nregression-testing framework in the form of the doctest module. This module searches\na file of code or documentation for blocks of text that look like an interactive Python\nsession, of the form you have already seen many times in this book. It executes the\nPython commands it finds, and tests that their output matches the output supplied in\nthe original file. Whenever there is a mismatch, it reports the expected and actual val-\nues. For details, please consult the doctest documentation at\n4.6  Program Development | 159\nhttp://docs.python.org/library/doctest.html. Apart from its value for regression testing,\nthe doctest module is useful for ensuring that your software documentation stays in\nsync with your code.\nPerhaps the most important defensive programming strategy is to set out your code\nclearly, choose meaningful variable and function names, and simplify the code wher-\never possible by decomposing it into functions and modules with well-documented\ninterfaces.\n4.7  Algorithm Design\nThis section discusses more advanced concepts, which you may prefer to skip on the\nfirst time through this chapter.\nA major part of algorithmic problem solving is selecting or adapting an appropriate\nalgorithm for the problem at hand. Sometimes there are several alternatives, and choos-\ning the best one depends on knowledge about how each alternative performs as the size\nof the data grows. Whole books are written on this topic, and we only have space to\nintroduce some key concepts and elaborate on the approaches that are most prevalent\nin natural language processing.\nThe best-known strategy is known as divide-and-conquer. We attack a problem of\nsize n by dividing it into two problems of size n/2, solve these problems, and combine\ntheir results into a solution of the original problem. For example, suppose that we had\na pile of cards with a single word written on each card. We could sort this pile by\nsplitting it in half and giving it to two other people to sort (they could do the same in\nturn). Then, when two sorted piles come back, it is an easy task to merge them into a\nsingle sorted pile. See Figure 4-3 for an illustration of this process.\nAnother example is the process of looking up a word in a dictionary. We open the book\nsomewhere around the middle and compare our word with the current page. If it\u2019s\nearlier in the dictionary, we repeat the process on the first half; if it\u2019s later, we use the\nsecond half. This search method is called binary search since it splits the problem in\nhalf at every step.\nIn another approach to algorithm design, we attack a problem by transforming it into\nan instance of a problem we already know how to solve. For example, in order to detect\nduplicate entries in a list, we can pre-sort the list, then scan through it once to check\nwhether any adjacent pairs of elements are identical.\nRecursion\nThe earlier examples of sorting and searching have a striking property: to solve a prob-\nlem of size n, we have to break it in half and then work on one or more problems of\nsize n/2. A common way to implement such methods uses recursion. We define a\nfunction f, which simplifies the problem, and calls itself to solve one or more easier\n160 | Chapter 4: \u2002Writing Structured Programs\ninstances of the same problem. It then combines the results into a solution for the\noriginal problem.\nFor example, \nsuppose we have a set of n words, and want to calculate how many dif-\nferent ways they can be combined to make a sequence of words. If we have only one\nword (n=1), there is just one way to make it into a sequence. If we have a set of two\nwords, there are two ways to put them into a sequence. For three words there are six\npossibilities. In general, for n words, there are n \u00d7 n-1 \u00d7 \u2026 \u00d7 2 \u00d7 1 ways (i.e., the factorial\nof n). We can code this up as follows:\n>>> def factorial1(n):\n...     result = 1\n...     for i in range(n):\n...         result *= (i+1)\n...     return result\nHowever, there is also a recursive algorithm for solving this problem, based on the\nfollowing observation. Suppose we have a way to construct all orderings for n-1 distinct\nwords. Then for each such ordering, there are n places where we can insert a new word:\nat the start, the end, or any of the n-2 boundaries between the words. Thus we simply\nmultiply the number of solutions found for n-1 by the value of n. We also need the\nbase case , to say that if we have a single word, there\u2019s just one ordering. We can code\nthis up as follows:\n>>> def factorial2(n):\n...     if n == 1:\n...         return 1\n...     else:\n...         return n * factorial2(n-1)\nFigure 4-3. Sorting by divide-and-conquer: To sort an array, we split it in half and sort each half\n(recursively); we \nmerge each sorted half back into a whole list (again recursively); this algorithm is\nknown as \u201cMerge Sort.\u201d\n4.7  Algorithm Design | 161\nThese two algorithms solve the same problem. One uses iteration while the other uses\nrecursion. We \ncan use recursion to navigate a deeply nested object, such as the Word-\nNet hypernym hierarchy. Let\u2019s count the size of the hypernym hierarchy rooted at a\ngiven synset s. We\u2019ll do this by finding the size of each hyponym of s, then adding these\ntogether (we will also add 1 for the synset itself). The following function size1() does\nthis work; notice that the body of the function includes a recursive call to size1():\n>>> def size1(s):\n...     return 1 + sum(size1(child) for child in s.hyponyms())\nWe can also design an iterative solution to this problem which processes the hierarchy\nin layers. The first layer is the synset itself \n , then all the hyponyms of the synset, then\nall the \nhyponyms of the hyponyms. Each time through the loop it computes the next\nlayer by finding the hyponyms of everything in the last layer \n . It also maintains a total\nof the number of synsets encountered so far \n .\n>>> def size2(s):\n...     layer = [s] \n...     total = 0\n...     while layer:\n...         total += len(layer) \n...         layer = [h for c in layer for h in c.hyponyms()] \n...     return total\nNot only \nis the iterative solution much longer, it is harder to interpret. It forces us to\nthink procedurally, and keep track of what is happening with the layer and total\nvariables through time. Let\u2019s satisfy ourselves that both solutions give the same result.\nWe\u2019ll use a new form of the import statement, allowing us to abbreviate the name\nwordnet to wn:\n>>> from nltk.corpus import wordnet as wn\n>>> dog = wn.synset('dog.n.01')\n>>> size1(dog)\n190\n>>> size2(dog)\n190\nAs a final example of recursion, let\u2019s use it to construct a deeply nested object. A letter\ntrie is a data structure that can be used for indexing a lexicon, one letter at a time. (The\nname is based on the word retrieval.) For example, if trie contained a letter trie, then\ntrie['c'] would be a smaller trie which held all words starting with c. Example 4-6\ndemonstrates the recursive process of building a trie, using Python dictionaries ( Sec-\ntion 5.3 ). To insert the word chien (French for dog), we split off the c and recursively\ninsert hien into the sub-trie trie['c']. The recursion continues until there are no letters\nremaining in the word, when we store the intended value (in this case, the word dog).\n162 | Chapter 4: \u2002Writing Structured Programs\nExample 4-6. Building a letter trie: A recursive function that builds a nested dictionary structure; each\nlevel of \nnesting contains all words with a given prefix, and a sub-trie containing all possible\ncontinuations.\ndef insert(trie, key, value):\n    if key:\n        first, rest = key[0], key[1:]\n        if first not in trie:\n            trie[first] = {}\n        insert(trie[first], rest, value)\n    else:\n        trie['value'] = value\n>>> trie = nltk.defaultdict(dict)\n>>> insert(trie, 'chat', 'cat')\n>>> insert(trie, 'chien', 'dog')\n>>> insert(trie, 'chair', 'flesh')\n>>> insert(trie, 'chic', 'stylish')\n>>> trie = dict(trie)               # for nicer printing\n>>> trie['c']['h']['a']['t']['value']\n'cat'\n>>> pprint.pprint(trie)\n{'c': {'h': {'a': {'t': {'value': 'cat'}},\n                  {'i': {'r': {'value': 'flesh'}}},\n             'i': {'e': {'n': {'value': 'dog'}}}\n                  {'c': {'value': 'stylish'}}}}}\nCaution!\nDespite the \nsimplicity of recursive programming, it comes with a cost.\nEach time a function is called, some state information needs to be push-\ned on a stack, so that once the function has completed, execution can\ncontinue from where it left off. For this reason, iterative solutions are\noften more efficient than recursive solutions.\nSpace-Time Trade-offs\nWe can sometimes significantly speed up the execution of a program by building an\nauxiliary data structure, such as an index. The listing in Example 4-7 implements a\nsimple text retrieval system for the Movie Reviews Corpus. By indexing the document\ncollection, it provides much faster lookup.\nExample 4-7. A simple text retrieval system.\ndef raw(file):\n    contents = open(file).read()\n    contents = re.sub(r'<.*?>', ' ', contents)\n    contents = re.sub('\\s+', ' ', contents)\n    return contents\ndef snippet(doc, term): # buggy\n    text = ' '*30 + raw(doc) + ' '*30\n    pos = text.index(term)\n    return text[pos-30:pos+30]\n4.7  Algorithm Design | 163\nprint \"Building Index...\"\nfiles = nltk.corpus.movie_reviews.abspaths()\nidx = nltk.Index((w, f) for f in files for w in raw(f).split())\nquery = ''\nwhile query != \"quit\":\n    query = raw_input(\"query> \")\n    if query in idx:\n        for doc in idx[query]:\n            print snippet(doc, query)\n    else:\n        print \"Not found\"\nA more \nsubtle example of a space-time trade-off involves replacing the tokens of a\ncorpus with integer identifiers. We create a vocabulary for the corpus, a list in which\neach word is stored once, then invert this list so that we can look up any word to find\nits identifier. Each document is preprocessed, so that a list of words becomes a list of\nintegers. Any language models can now work with integers. See the listing in Exam-\nple 4-8 for an example of how to do this for a tagged corpus.\nExample 4-8. Preprocess tagged corpus data, converting all words and tags to integers.\ndef preprocess(tagged_corpus):\n    words = set()\n    tags = set()\n    for sent in tagged_corpus:\n        for word, tag in sent:\n            words.add(word)\n            tags.add(tag)\n    wm = dict((w,i) for (i,w) in enumerate(words))\n    tm = dict((t,i) for (i,t) in enumerate(tags))\n    return [[(wm[w], tm[t]) for (w,t) in sent] for sent in tagged_corpus]\nAnother example of a space-time trade-off is maintaining a vocabulary list. If you need\nto process an input text to check that all words are in an existing vocabulary, the vo-\ncabulary should be stored as a set, not a list. The elements of a set are automatically\nindexed, so testing membership of a large set will be much faster than testing mem-\nbership of the corresponding list.\nWe can test this claim using the timeit module. The Timer class has two parameters: a\nstatement that is executed multiple times, and setup code that is executed once at the\nbeginning. We will simulate a vocabulary of 100,000 items using a list \n  or set \n  of\nintegers. The \ntest statement will generate a random item that has a 50% chance of being\nin the vocabulary \n .\n164 | Chapter 4: \u2002Writing Structured Programs\n>>> from timeit import Timer\n>>> vocab_size = 100000\n>>> setup_list = \"import random; vocab = range(%d)\" % vocab_size \n>>> setup_set = \"import random; vocab = set(range(%d))\" % vocab_size \n>>> statement = \"random.randint(0, %d) in vocab\" % vocab_size * 2 \n>>> print Timer(statement, setup_list).timeit(1000)\n2.78092288971\n>>> print Timer(statement, setup_set).timeit(1000)\n0.0037260055542\nPerforming 1,000 \nlist membership tests takes a total of 2.8 seconds, whereas the equiv-\nalent tests on a set take a mere 0.0037 seconds, or three orders of magnitude faster!\nDynamic Programming\nDynamic programming is a general technique for designing algorithms which is widely\nused in natural language processing. The term \u201cprogramming\u201d is used in a different\nsense to what you might expect, to mean planning or scheduling. Dynamic program-\nming is used when a problem contains overlapping subproblems. Instead of computing\nsolutions to these subproblems repeatedly, we simply store them in a lookup table. In\nthe remainder of this section, we will introduce dynamic programming, but in a rather\ndifferent context to syntactic parsing.\nPingala was an Indian author who lived around the 5th century B.C., and wrote a\ntreatise on Sanskrit prosody called the Chandas Shastra . Virahanka extended this work\naround the 6th century A.D., studying the number of ways of combining short and long\nsyllables to create a meter of length n. Short syllables, marked S, take up one unit of\nlength, while long syllables, marked L, take two. Pingala found, for example, that there\nare five ways to construct a meter of length 4: V4 = {LL, SSL, SLS, LSS, SSSS}. Observe\nthat we can split V4 into two subsets, those starting with L and those starting with S,\nas shown in (1).\n(1)V4 =\n  LL, LSS\n    i.e. L prefixed to each item of V 2 = {L, SS}\n  SSL, SLS, SSSS\n    i.e. S prefixed to each item of V 3 = {SL, LS, SSS}\nWith this observation, we can write a little recursive function called virahanka1() to\ncompute these meters, shown in Example 4-9 . Notice that, in order to compute V4 we\nfirst compute V3 and V2. But to compute V3, we need to first compute V2 and V1. This\ncall structure is depicted in (2).\n4.7  Algorithm Design | 165\nExample 4-9. Four ways to compute Sanskrit meter: (i) iterative, (ii) bottom-up dynamic\nprogramming, (iii) top-down dynamic programming, and (iv) built-in memoization.\ndef virahanka1(n):\n    if n == 0:\n        return [\"\"]\n    elif n == 1:\n        return [\"S\"]\n    else:\n        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\n        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\n        return s + l\ndef virahanka2(n):\n    lookup = [[\"\"], [\"S\"]]\n    for i in range(n-1):\n        s = [\"S\" + prosody for prosody in lookup[i+1]]\n        l = [\"L\" + prosody for prosody in lookup[i]]\n        lookup.append(s + l)\n    return lookup[n]\ndef virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\n    if n not in lookup:\n        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\n        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\n        lookup[n] = s + l\n    return lookup[n]\nfrom nltk import memoize\n@memoize\ndef virahanka4(n):\n    if n == 0:\n        return [\"\"]\n    elif n == 1:\n        return [\"S\"]\n    else:\n        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\n        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\n        return s + l\n>>> virahanka1(4)\n['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n>>> virahanka2(4)\n['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n>>> virahanka3(4)\n['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n>>> virahanka4(4)\n['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n166 | Chapter 4: \u2002Writing Structured Programs\n(2)\nAs you can see, V2 is computed twice. This might not seem like a significant problem,\nbut it turns out to be rather wasteful as n gets large: to compute V20 using this recursive\ntechnique, we would compute V2 4,181 times; and for V40 we would compute V2\n63,245,986 times! A much better alternative is to store the value of V2 in a table and\nlook it up whenever we need it. The same goes for other values, such as V3 and so on.\nFunction virahanka2() implements a dynamic programming approach to the problem.\nIt works by filling up a table (called lookup) with solutions to all smaller instances of\nthe problem, stopping as soon as we reach the value we\u2019re interested in. At this point\nwe read off the value and return it. Crucially, each subproblem is only ever solved once.\nNotice that the approach taken in virahanka2() is to solve smaller problems on the way\nto solving larger problems. Accordingly, this is known as the bottom-up approach to\ndynamic programming. Unfortunately it turns out to be quite wasteful for some ap-\nplications, since it may compute solutions to sub-problems that are never required for\nsolving the main problem. This wasted computation can be avoided using the top-\ndown approach to dynamic programming, which is illustrated in the function vira\nhanka3() in Example 4-9 . Unlike the bottom-up approach, this approach is recursive.\nIt avoids the huge wastage of virahanka1() by checking whether it has previously stored\nthe result. If not, it computes the result recursively and stores it in the table. The last\nstep is to return the stored result. The final method, in virahanka4(), is to use a Python\n\u201cdecorator\u201d called memoize, which takes care of the housekeeping work done by\nvirahanka3() without cluttering up the program. This \u201cmemoization\u201d process stores\nthe result of each previous call to the function along with the parameters that were\nused. If the function is subsequently called with the same parameters, it returns the\nstored result instead of recalculating it. (This aspect of Python syntax is beyond the\nscope of this book.)\nThis concludes our brief introduction to dynamic programming. We will encounter it\nagain in Section 8.4.\n4.8  A Sample of Python Libraries\nPython has hundreds of third-party libraries, specialized software packages that extend\nthe functionality of Python. NLTK is one such library. To realize the full power of\nPython programming, you should become familiar with several other libraries. Most\nof these will need to be manually installed on your computer.\n4.8  A Sample of Python Libraries | 167\nMatplotlib\nPython has \nsome libraries that are useful for visualizing language data. The Matplotlib\npackage supports sophisticated plotting functions with a MATLAB-style interface, and\nis available from http://matplotlib.sourceforge.net/.\nSo far we have focused on textual presentation and the use of formatted print statements\nto get output lined up in columns. It is often very useful to display numerical data in\ngraphical form, since this often makes it easier to detect patterns. For example, in\nExample 3-5, we saw a table of numbers showing the frequency of particular modal\nverbs in the Brown Corpus, classified by genre. The program in Example 4-10  presents\nthe same information in graphical format. The output is shown in Figure 4-4  (a color\nfigure in the graphical display).\nExample 4-10. Frequency of modals in different sections of the Brown Corpus.\ncolors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black\ndef bar_chart(categories, words, counts):\n    \"Plot a bar chart showing counts for each word by category\"\n    import pylab\n    ind = pylab.arange(len(words))\n    width = 1 / (len(categories) + 1)\n    bar_groups = []\n    for c in range(len(categories)):\n        bars = pylab.bar(ind+c*width, counts[categories[c]], width,\n                         color=colors[c % len(colors)])\n        bar_groups.append(bars)\n    pylab.xticks(ind+width, words)\n    pylab.legend([b[0] for b in bar_groups], categories, loc='upper left')\n    pylab.ylabel('Frequency')\n    pylab.title('Frequency of Six Modal Verbs by Genre')\n    pylab.show()\n>>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> cfdist = nltk.ConditionalFreqDist(\n...              (genre, word)\n...              for genre in genres\n...              for word in nltk.corpus.brown.words(categories=genre)\n...              if word in modals)\n...\n>>> counts = {}\n>>> for genre in genres:\n...     counts[genre] = [cfdist[genre][word] for word in modals]\n>>> bar_chart(genres, modals, counts)\nFrom the bar chart it is immediately obvious that may and must have almost identical\nrelative frequencies. The same goes for could and might.\nIt is also possible to generate such data visualizations on the fly. For example, a web\npage with form input could permit visitors to specify search parameters, submit the\nform, and see a dynamically generated visualization. To do this we have to specify the\n168 | Chapter 4: \u2002Writing Structured Programs\nAgg backend for matplotlib , which is a library for producing raster (pixel) images \n .\nNext, we \nuse all the same PyLab methods as before, but instead of displaying the result\non a graphical terminal using pylab.show(), we save it to a file using pylab.savefig()\n. We specify the filename and dpi, then print HTML markup that directs the web\nbrowser to load the file.\n>>> import matplotlib\n>>> matplotlib.use('Agg') \n>>> pylab.savefig('modals.png') \n>>> print 'Content-Type: text/html'\n>>> print\n>>> print '<html><body>'\n>>> print '<img src=\"modals.png\"/>'\n>>> print '</body></html>'\nFigure 4-4. Bar chart showing frequency of modals in different sections of Brown Corpus: This\nvisualization was produced by the program in Example 4-10.\nNetworkX\nThe NetworkX \npackage is for defining and manipulating structures consisting of nodes\nand edges, known as graphs. It is available from https://networkx.lanl.gov/. NetworkX\n4.8  A Sample of Python Libraries | 169\ncan be used in conjunction with Matplotlib to visualize networks, such as WordNet\n(the semantic \nnetwork we introduced in Section 2.5 ). The program in Example 4-11\ninitializes an empty graph \n  and then traverses the WordNet hypernym hierarchy\nadding edges \nto the graph \n . Notice that the traversal is recursive \n , applying the\nprogramming technique \ndiscussed in Section 4.7. The resulting display is shown in\nFigure 4-5.\nExample 4-11. Using the NetworkX and Matplotlib libraries.\nimport networkx as nx\nimport matplotlib\nfrom nltk.corpus import wordnet as wn\ndef traverse(graph, start, node):\n    graph.depth[node.name] = node.shortest_path_distance(start)\n    for child in node.hyponyms():\n        graph.add_edge(node.name, child.name) \n        traverse(graph, start, child) \ndef hyponym_graph(start):\n    G = nx.Graph() \n    G.depth = {}\n    traverse(G, start, start)\n    return G\ndef graph_draw(graph):\n    nx.draw_graphviz(graph,\n         node_size = [16 * graph.degree(n) for n in graph],\n         node_color = [graph.depth[n] for n in graph],\n         with_labels = False)\n    matplotlib.pyplot.show()\n>>> dog = wn.synset('dog.n.01')\n>>> graph = hyponym_graph(dog)\n>>> graph_draw(graph)\ncsv\nLanguage analysis \nwork often involves data tabulations, containing information about\nlexical items, the participants in an empirical study, or the linguistic features extracted\nfrom a corpus. Here\u2019s a fragment of a simple lexicon, in CSV format:\nsleep, sli:p, v.i, a condition of body and mind ...\nwalk, wo:k, v.intr, progress by lifting and setting down each foot ...\nwake, weik, intrans, cease to sleep\nWe can use Python\u2019s CSV library to read and write files stored in this format. For\nexample, we can open a CSV file called lexicon.csv \n  and iterate over its rows \n :\n>>> import csv\n>>> input_file = open(\"lexicon.csv\", \"rb\") \n>>> for row in csv.reader(input_file): \n...     print row\n['sleep', 'sli:p', 'v.i', 'a condition of body and mind ...']\n170 | Chapter 4: \u2002Writing Structured Programs\n['walk', 'wo:k', 'v.intr', 'progress by lifting and setting down each foot ...']\n['wake', 'weik', 'intrans', 'cease to sleep']\nEach row \nis just a list of strings. If any fields contain numerical data, they will appear\nas strings, and will have to be converted using int() or float().\nFigure 4-5. Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy\nis displayed, \nstarting with dog.n.01 (the darkest node in the middle); node size is based on the number\nof children of the node, and color is based on the distance of the node from dog.n.01; this visualization\nwas produced by the program in Example 4-11.\nNumPy\nThe NumPy package provides substantial support for numerical processing in Python.\nNumPy has a multidimensional array object, which is easy to initialize and access:\n>>> from numpy import array\n>>> cube = array([ [[0,0,0], [1,1,1], [2,2,2]],\n...                [[3,3,3], [4,4,4], [5,5,5]],\n...                [[6,6,6], [7,7,7], [8,8,8]] ])\n>>> cube[1,1,1]\n4\n>>> cube[2].transpose()\narray([[6, 7, 8],\n       [6, 7, 8],\n       [6, 7, 8]])\n>>> cube[2,1:]\narray([[7, 7, 7],\n       [8, 8, 8]])\nNumPy includes linear algebra functions. Here we perform singular value decomposi-\ntion on a matrix, an operation used in latent semantic analysis  to help identify implicit\nconcepts in a document collection:\n4.8  A Sample of Python Libraries | 171\n>>> from numpy import linalg\n>>> a=array([[4,0], [3,-5]])\n>>> u,s,vt = linalg.svd(a)\n>>> u\narray([[-0.4472136 , -0.89442719],\n       [-0.89442719,  0.4472136 ]])\n>>> s\narray([ 6.32455532,  3.16227766])\n>>> vt\narray([[-0.70710678,  0.70710678],\n       [-0.70710678, -0.70710678]])\nNLTK\u2019s clustering \npackage nltk.cluster makes extensive use of NumPy arrays, and\nincludes support for k-means clustering, Gaussian EM clustering, group average\nagglomerative clustering, and dendogram plots. For details, type help(nltk.cluster).\nOther Python Libraries\nThere are many other Python libraries, and you can search for them with the help of\nthe Python Package Index at http://pypi.python.org/. Many libraries provide an interface\nto external software, such as relational databases (e.g., mysql-python) and large docu-\nment collections (e.g., PyLucene). Many other libraries give access to file formats such\nas PDF, MSWord, and XML ( pypdf, pywin32, xml.etree), RSS feeds (e.g., feedparser),\nand electronic mail (e.g., imaplib, email).\n4.9  Summary\n\u2022 Python\u2019s assignment and parameter passing use object references; e.g., if a is a list\nand we assign b = a, then any operation on a will modify b, and vice versa.\n\u2022 The is operation tests whether two objects are identical internal objects, whereas\n== tests whether two objects are equivalent. This distinction parallels the type-\ntoken distinction.\n\u2022 Strings, lists, and tuples are different kinds of sequence object, supporting common\noperations such as indexing, slicing, len(), sorted(), and membership testing using\nin.\n\u2022 We can write text to a file by opening the file for writing\nofile = open('output.txt', 'w'\nthen adding content to the file ofile.write(\"Monty Python\"), and finally closing\nthe file ofile.close().\n\u2022 A declarative programming style usually produces more compact, readable code;\nmanually incremented loop variables are usually unnecessary. When a sequence\nmust be enumerated, use enumerate().\n\u2022 Functions are an essential programming abstraction: key concepts to understand\nare parameter passing, variable scope, and docstrings.\n172 | Chapter 4: \u2002Writing Structured Programs\n\u2022 A function serves as a namespace: names defined inside a function are not visible\noutside that function, unless those names are declared to be global.\n\u2022\nModules permit logically related material to be localized in a file. A module serves\nas a namespace: names defined in a module\u2014such as variables and functions\u2014\nare not visible to other modules, unless those names are imported.\n\u2022 Dynamic programming is an algorithm design technique used widely in NLP that\nstores the results of previous computations in order to avoid unnecessary\nrecomputation.\n4.10  Further Reading\nThis chapter has touched on many topics in programming, some specific to Python,\nand some quite general. We\u2019ve just scratched the surface, and you may want to read\nmore about these topics, starting with the further materials for this chapter available\nat http://www.nltk.org/.\nThe Python website provides extensive documentation. It is important to understand\nthe built-in functions and standard types, described at http://docs.python.org/library/\nfunctions.html and http://docs.python.org/library/stdtypes.html. We have learned about\ngenerators and their importance for efficiency; for information about iterators, a closely\nrelated topic, see http://docs.python.org/library/itertools.html. Consult your favorite Py-\nthon book for more information on such topics. An excellent resource for using Python\nfor multimedia processing, including working with sound files, is (Guzdial, 2005).\nWhen using the online Python documentation, be aware that your installed version\nmight be different from the version of the documentation you are reading. You can\neasily check what version you have, with import sys; sys.version. Version-specific\ndocumentation is available at http://www.python.org/doc/versions/.\nAlgorithm design is a rich field within computer science. Some good starting points are\n(Harel, 2004), (Levitin, 2004), and (Knuth, 2006). Useful guidance on the practice of\nsoftware development is provided in (Hunt & Thomas, 2000) and (McConnell, 2004).\n4.11  Exercises\n1.\u25cb Find out more about sequence objects using Python\u2019s help facility. In the inter-\npreter, type help(str), help(list), and help(tuple). This will give you a full list of\nthe functions supported by each type. Some functions have special names flanked\nwith underscores; as the help documentation shows, each such function corre-\nsponds to something more familiar. For example x.__getitem__(y) is just a long-\nwinded way of saying x[y].\n2.\u25cb Identify three operations that can be performed on both tuples and lists. Identify\nthree list operations that cannot be performed on tuples. Name a context where\nusing a list instead of a tuple generates a Python error.\n4.11  Exercises | 173\n3.\u25cb Find out how to create a tuple consisting of a single item. There are at least two\nways to do this.\n4.\u25cb Create \na list words = ['is', 'NLP', 'fun', '?']. Use a series of assignment\nstatements (e.g., words[1] = words[2] ) and a temporary variable tmp to transform\nthis list into the list ['NLP', 'is', 'fun', '!']. Now do the same transformation\nusing tuple assignment.\n5.\u25cb Read about the built-in comparison function cmp, by typing help(cmp). How does\nit differ in behavior from the comparison operators?\n6.\u25cb Does the method for creating a sliding window of n-grams behave correctly for\nthe two limiting cases: n = 1 and n = len(sent)?\n7.\u25cb We pointed out that when empty strings and empty lists occur in the condition\npart of an if clause, they evaluate to False. In this case, they are said to be occurring\nin a Boolean context. Experiment with different kinds of non-Boolean expressions\nin Boolean contexts, and see whether they evaluate as True or False.\n8.\u25cb Use the inequality operators to compare strings, e.g., 'Monty' < 'Python'. What\nhappens when you do 'Z' < 'a' ? Try pairs of strings that have a common prefix,\ne.g., 'Monty' < 'Montague'. Read up on \u201clexicographical sort\u201d in order to under-\nstand what is going on here. Try comparing structured objects, e.g., ('Monty', 1)\n< ('Monty', 2). Does this behave as expected?\n9.\u25cb Write code that removes whitespace at the beginning and end of a string, and\nnormalizes whitespace between words to be a single-space character.\na. Do this task using split() and join().\nb. Do this task using regular expression substitutions.\n10.\u25cb Write a program to sort words by length. Define a helper function cmp_len which\nuses the cmp comparison function on word lengths.\n11.\u25d1 Create a list of words and store it in a variable sent1. Now assign sent2 =\nsent1. Modify one of the items in sent1 and verify that sent2 has changed.\na. Now try the same exercise, but instead assign sent2 = sent1[:]. Modify\nsent1 again and see what happens to sent2. Explain.\nb. Now define text1 to be a list of lists of strings (e.g., to represent a text consisting\nof multiple sentences). Now assign text2 = text1[:], assign a new value to\none of the words, e.g., text1[1][1] = 'Monty' . Check what this did to text2.\nExplain.\nc. Load Python\u2019s deepcopy() function (i.e., from copy import deepcopy), consult\nits documentation, and test that it makes a fresh copy of any object.\n12.\u25d1 Initialize an n-by-m list of lists of empty strings using list multiplication, e.g.,\nword_table = [[''] * n] * m . What happens when you set one of its values, e.g.,\nword_table[1][2] = \"hello\"? Explain why this happens. Now write an expression\nusing range() to construct a list of lists, and show that it does not have this problem.\n174 | Chapter 4: \u2002Writing Structured Programs\n13.\u25d1 Write code to initialize a two-dimensional array of sets called word_vowels and\nprocess a list of words, adding each word to word_vowels[l][v] where l is the length\nof the word and v is the number of vowels it contains.\n14.\u25d1 Write a function novel10(text) that prints any word that appeared in the last\n10% of a text that had not been encountered earlier.\n15.\u25d1 Write a program that takes a sentence expressed as a single string, splits it, and\ncounts up the words. Get it to print out each word and the word\u2019s frequency, one\nper line, in alphabetical order.\n16.\u25d1 Read up on Gematria, a method for assigning numbers to words, and for mapping\nbetween words having the same number to discover the hidden meaning of texts\n(http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).\na. Write a function gematria() that sums the numerical values of the letters of a\nword, according to the letter values in letter_vals:\n>>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\nb. Process a corpus (e.g., nltk.corpus.state_union) and for each document,\ncount how many of its words have the number 666.\nc. Write a function decode() to process a text, randomly replacing words with\ntheir Gematria equivalents, in order to discover the \u201chidden meaning\u201d of the\ntext.\n17.\u25d1 Write a function shorten(text, n) to process a text, omitting the n most fre-\nquently occurring words of the text. How readable is it?\n18.\u25d1 Write code to print out an index for a lexicon, allowing someone to look up\nwords according to their meanings (or their pronunciations; whatever properties\nare contained in the lexical entries).\n19.\u25d1 Write a list comprehension that sorts a list of WordNet synsets for proximity to\na given synset. For example, given the synsets minke_whale.n.01, orca.n.01,\nnovel.n.01, and tortoise.n.01, sort them according to their path_distance() from\nright_whale.n.01.\n20.\u25d1 Write a function that takes a list of words (containing duplicates) and returns a\nlist of words (with no duplicates) sorted by decreasing frequency. E.g., if the input\nlist contained 10 instances of the word table and 9 instances of the word chair,\nthen table would appear before chair in the output list.\n21.\u25d1 Write a function that takes a text and a vocabulary as its arguments and returns\nthe set of words that appear in the text but not in the vocabulary. Both arguments\ncan be represented as lists of strings. Can you do this in a single line, using set.dif\nference()?\n22.\u25d1 Import the itemgetter() function from the operator module in Python\u2019s standard\nlibrary (i.e., from operator import itemgetter). Create a list words containing sev-\n4.11  Exercises | 175\neral words. Now try calling: sorted(words, key=itemgetter(1)), and sor\nted(words, key=itemgetter(-1))\n. Explain what itemgetter() is doing.\n23.\u25d1 Write a recursive function lookup(trie, key)  that looks up a key in a trie, and\nreturns the value it finds. Extend the function to return a word when it is uniquely\ndetermined by its prefix (e.g., vanguard is the only word that starts with vang-, so\nlookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')).\n24.\u25d1 Read up on \u201ckeyword linkage\u201d (Chapter 5 of (Scott & Tribble, 2006)). Extract\nkeywords from NLTK\u2019s Shakespeare Corpus and using the NetworkX package,\nplot keyword linkage networks.\n25.\u25d1 Read about string edit distance and the Levenshtein Algorithm. Try the imple-\nmentation provided in nltk.edit_dist(). In what way is this using dynamic pro-\ngramming? Does it use the bottom-up or top-down approach? (See also http://\nnorvig.com/spell-correct.html.)\n26.\u25d1 The Catalan numbers arise in many applications of combinatorial mathematics,\nincluding the counting of parse trees ( Section 8.6 ). The series can be defined as\nfollows: C 0 = 1, and C n+1 = \u03a3 0..n (CiCn-i).\na. Write a recursive function to compute nth Catalan number C n.\nb. Now write another function that does this computation using dynamic pro-\ngramming.\nc. Use the timeit module to compare the performance of these functions as n\nincreases.\n27.\u25cf Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship\nidentification.\n28.\u25cf Study gender-specific lexical choice, and see if you can reproduce some of the\nresults of http://www.clintoneast.com/articles/words.php.\n29.\u25cf Write a recursive function that pretty prints a trie in alphabetically sorted order,\nfor example:\nchair: 'flesh'\n---t: 'cat'\n--ic: 'stylish'\n---en: 'dog'\n30.\u25cf With the help of the trie data structure, write a recursive function that processes\ntext, locating the uniqueness point in each word, and discarding the remainder of\neach word. How much compression does this give? How readable is the resulting\ntext?\n31.\u25cf Obtain some raw text, in the form of a single, long string. Use Python\u2019s text\nwrap module to break it up into multiple lines. Now write code to add extra spaces\nbetween words, in order to justify the output. Each line must have the same width,\nand spaces must be approximately evenly distributed across each line. No line can\nbegin or end with a space.\n176 | Chapter 4: \u2002Writing Structured Programs\n32.\u25cf Develop a simple extractive summarization tool, that prints the sentences of a\ndocument which contain the highest total word frequency. Use FreqDist() to count\nword frequencies, and use sum to sum the frequencies of the words in each sentence.\nRank the sentences according to their score. Finally, print the n highest-scoring\nsentences in document order. Carefully review the design of your program,\nespecially your approach to this double sorting. Make sure the program is written\nas clearly as possible.\n33.\u25cf Develop your own NgramTagger class that inherits from NLTK\u2019s class, and which\nencapsulates the method of collapsing the vocabulary of the tagged training and\ntesting data that was described in Chapter 5 . Make sure that the unigram and\ndefault backoff taggers have access to the full vocabulary.\n34.\u25cf Read the following article on semantic orientation of adjectives. Use the Net-\nworkX package to visualize a network of adjectives with edges to indicate same\nversus different semantic orientation (see http://www.aclweb.org/anthology/P97\n-1023).\n35.\u25cf Design an algorithm to find the \u201cstatistically improbable phrases\u201d of a document\ncollection (see http://www.amazon.com/gp/search-inside/sipshelp.html).\n36.\u25cf Write a program to implement a brute-force algorithm for discovering word\nsquares, a kind of n \u00d7 n: crossword in which the entry in the nth row is the same\nas the entry in the nth column. For discussion, see http://itre.cis.upenn.edu/~myl/\nlanguagelog/archives/002679.html.\n4.11  Exercises | 177\n\nCHAPTER 5\nCategorizing and Tagging Words\nBack in elementary school you learned the difference between nouns, verbs, adjectives,\nand adverbs. \nThese \u201cword classes\u201d are not just the idle invention of grammarians, but\nare useful categories for many language processing tasks. As we will see, they arise from\nsimple analysis of the distribution of words in text. The goal of this chapter is to answer\nthe following questions:\n1. What are lexical categories, and how are they used in natural language processing?\n2. What is a good Python data structure for storing words and their categories?\n3. How can we automatically tag each word of a text with its word class?\nAlong the way, we\u2019ll cover some fundamental techniques in NLP, including sequence\nlabeling, n-gram models, backoff, and evaluation. These techniques are useful in many\nareas, and tagging gives us a simple context in which to present them. We will also see\nhow tagging is the second step in the typical NLP pipeline, following tokenization.\nThe process of classifying words into their parts-of-speech and labeling them accord-\ningly is known as part-of-speech tagging, POS tagging, or simply tagging. Parts-\nof-speech are also known as word classes  or lexical categories . The collection of tags\nused for a particular task is known as a tagset. Our emphasis in this chapter is on\nexploiting tags, and tagging text automatically.\n5.1  Using a Tagger\nA part-of-speech tagger, or POS tagger , processes a sequence of words, and attaches\na part of speech tag to each word (don\u2019t forget to import nltk):\n>>> text = nltk.word_tokenize(\"And now for something completely different\")\n>>> nltk.pos_tag(text)\n[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),\n('completely', 'RB'), ('different', 'JJ')]\nHere we see that and is CC, a coordinating conjunction; now and completely are RB, or\nadverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.\n179\nNLTK provides documentation for each tag, which can be queried using\nthe tag, \ne.g., nltk.help.upenn_tagset('RB'), or a regular expression,\ne.g., nltk.help.upenn_brown_tagset('NN.*'). Some corpora have RE-\nADME files with tagset documentation; see nltk.name.readme(), sub-\nstituting in the name of the corpus.\nLet\u2019s look at another example, this time including some homonyms:\n>>> text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n>>> nltk.pos_tag(text)\n[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'),\n('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\nNotice that refuse and permit both appear as a present tense verb ( VBP) and a noun\n(NN). E.g., refUSE is a verb meaning \u201cdeny,\u201d while REFuse is a noun meaning \u201ctrash\u201d\n(i.e., they are not homophones). Thus, we need to know which word is being used in\norder to pronounce the text correctly. (For this reason, text-to-speech systems usually\nperform POS tagging.)\nYour Turn: Many words, like ski and race, can be used as nouns or\nverbs with no difference in pronunciation. Can you think of others?\nHint: think of a commonplace object and try to put the word to before\nit to see if it can also be a verb, or think of an action and try to put the\nbefore it to see if it can also be a noun. Now make up a sentence with\nboth uses of this word, and run the POS tagger on this sentence.\nLexical categories like \u201cnoun\u201d and part-of-speech tags like NN seem to have their uses,\nbut the details will be obscure to many readers. You might wonder what justification\nthere is for introducing this extra level of information. Many of these categories arise\nfrom superficial analysis of the distribution of words in text. Consider the following\nanalysis involving woman (a noun), bought (a verb), over (a preposition), and the (a\ndeterminer). The text.similar() method takes a word w, finds all contexts w1w w2,\nthen finds all words w' that appear in the same context, i.e. w 1w'w 2.\n>>> text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n>>> text.similar('woman')\nBuilding word-context index...\nman time day year car moment world family house country child boy\nstate job way war girl place room word\n>>> text.similar('bought')\nmade said put done seen had found left given heard brought got been\nwas set told took in felt that\n>>> text.similar('over')\nin on to of and for with from at by that into as up out down through\nis all about\n>>> text.similar('the')\na his this their its her an that our any all one these my in your no\nsome other and\n180 | Chapter 5: \u2002Categorizing and Tagging Words\nObserve that searching for woman finds nouns; searching for bought mostly finds verbs;\nsearching for over generally finds prepositions; searching for the finds several deter-\nminers. A tagger can correctly identify the tags on these words in the context of a\nsentence, e.g., The woman bought over $150,000 worth of clothes.\nA tagger can also model our knowledge of unknown words; for example, we can guess\nthat scrobbling is probably a verb, with the root scrobble, and likely to occur in contexts\nlike he was scrobbling.\n5.2  Tagged Corpora\nRepresenting Tagged Tokens\nBy convention in NLTK, a tagged token is represented using a tuple consisting of the\ntoken and the tag. We can create one of these special tuples from the standard string\nrepresentation of a tagged token, using the function str2tuple():\n>>> tagged_token = nltk.tag.str2tuple('fly/NN')\n>>> tagged_token\n('fly', 'NN')\n>>> tagged_token[0]\n'fly'\n>>> tagged_token[1]\n'NN'\nWe can construct a list of tagged tokens directly from a string. The first step is to\ntokenize the string to access the individual word/tag strings, and then to convert each\nof these into a tuple (using str2tuple()).\n>>> sent = '''\n... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n... interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n... '''\n>>> [nltk.tag.str2tuple(t) for t in sent.split()]\n[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'),\n('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ... ('.', '.')]\nReading Tagged Corpora\nSeveral of the corpora included with NLTK have been tagged for their part-of-speech.\nHere\u2019s an example of what you might see if you opened a file from the Brown Corpus\nwith a text editor:\nThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at inves-\ntigation/nn of/in Atlanta\u2019s/np$ recent/jj primary/nn election/nn produced/vbd / no/at\nevidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n5.2  Tagged Corpora | 181\nOther corpora use a variety of formats for storing part-of-speech tags. NLTK\u2019s corpus\nreaders provide \na uniform interface so that you don\u2019t have to be concerned with the\ndifferent file formats. In contrast with the file extract just shown, the corpus reader for\nthe Brown Corpus represents the data as shown next. Note that part-of-speech tags\nhave been converted to uppercase; this has become standard practice since the Brown\nCorpus was published.\n>>> nltk.corpus.brown.tagged_words()\n[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ...]\n>>> nltk.corpus.brown.tagged_words(simplify_tags=True)\n[('The', 'DET'), ('Fulton', 'N'), ('County', 'N'), ...]\nWhenever a corpus contains tagged text, the NLTK corpus interface will have a\ntagged_words() method. Here are some more examples, again using the output format\nillustrated for the Brown Corpus:\n>>> print nltk.corpus.nps_chat.tagged_words()\n[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]\n>>> nltk.corpus.conll2000.tagged_words()\n[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ...]\n>>> nltk.corpus.treebank.tagged_words()\n[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\nNot all corpora employ the same set of tags; see the tagset help functionality and the\nreadme() methods mentioned earlier for documentation. Initially we want to avoid the\ncomplications of these tagsets, so we use a built-in mapping to a simplified tagset:\n>>> nltk.corpus.brown.tagged_words(simplify_tags=True)\n[('The', 'DET'), ('Fulton', 'NP'), ('County', 'N'), ...]\n>>> nltk.corpus.treebank.tagged_words(simplify_tags=True)\n[('Pierre', 'NP'), ('Vinken', 'NP'), (',', ','), ...]\nTagged corpora for several other languages are distributed with NLTK, including Chi-\nnese, Hindi, Portuguese, Spanish, Dutch, and Catalan. These usually contain non-\nASCII text, and Python always displays this in hexadecimal when printing a larger\nstructure such as a list.\n>>> nltk.corpus.sinica_treebank.tagged_words()\n[('\\xe4\\xb8\\x80', 'Neu'), ('\\xe5\\x8f\\x8b\\xe6\\x83\\x85', 'Nad'), ...]\n>>> nltk.corpus.indian.tagged_words()\n[('\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf\\xe0\\xa6\\xb7\\xe0\\xa7\\x87\\xe0\\xa6\\xb0', 'NN'),\n('\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0\\xa6\\xa4\\xe0\\xa6\\xbe\\xe0\\xa6\\xa8', 'NN'),\n...]\n>>> nltk.corpus.mac_morpho.tagged_words()\n[('Jersei', 'N'), ('atinge', 'V'), ('m\\xe9dia', 'N'), ...]\n>>> nltk.corpus.conll2002.tagged_words()\n[('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]\n>>> nltk.corpus.cess_cat.tagged_words()\n[('El', 'da0ms0'), ('Tribunal_Suprem', 'np0000o'), ...]\nIf your environment is set up correctly, with appropriate editors and fonts, you should\nbe able to display individual strings in a human-readable way. For example, Fig-\nure 5-1 shows data accessed using nltk.corpus.indian.\n182 | Chapter 5: \u2002Categorizing and Tagging Words\nIf the corpus is also segmented into sentences, it will have a tagged_sents() method\nthat \ndivides up the tagged words into sentences rather than presenting them as one big\nlist. This will be useful when we come to developing automatic taggers, as they are\ntrained and tested on lists of sentences, not words.\nA Simplified Part-of-Speech Tagset\nTagged corpora use many different conventions for tagging words. To help us get star-\nted, we will be looking at a simplified tagset (shown in Table 5-1).\nTable 5-1. Simplified part-of-speech tagset\nTag Meaning Examples\nADJ adjective new, good, high, special, big, local\nADV adverb really, already, still, early, now\nCNJ conjunction and, or, but, if, while, although\nDET determiner the, a, some, most, every, no\nEX existential there, there\u2019s\nFW foreign word dolce, ersatz, esprit, quo, maitre\nMOD modal verb will, can, would, may, must, should\nN noun year, home, costs, time, education\nNP proper noun Alison, Africa, April, Washington\nNUM number twenty-four, fourth, 1991, 14:24\nPRO pronoun he, their, her, its, my, I, us\nP preposition on, of, at, with, by, into, under\nTO the word to to\nUH interjection ah, bang, ha, whee, hmpf, oops\nV verb is, has, get, do, make, see, run\nVD past tense said, took, told, made, asked\nVG present participle making, going, playing, working\nVN past participle given, taken, begun, sung\nWH wh determiner who, which, when, what, where, how\n5.2  Tagged Corpora | 183\nFigure 5-1. POS tagged data from four Indian languages: Bangla, Hindi, Marathi, and Telugu.\nLet\u2019s see which of these tags are the most common in the news category of the Brown\nCorpus:\n>>> from nltk.corpus import brown\n>>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n>>> tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n>>> tag_fd.keys()\n['N', 'P', 'DET', 'NP', 'V', 'ADJ', ',', '.', 'CNJ', 'PRO', 'ADV', 'VD', ...]\nYour Turn: Plot the frequency distribution just shown using\ntag_fd.plot(cumulative=True). What percentage of words are tagged\nusing the first five tags of the above list?\nWe can use these tags to do powerful searches using a graphical POS-concordance tool\nnltk.app.concordance(). Use it to search for any combination of words and POS tags,\ne.g., N N N N, hit/VD, hit/VN, or the ADJ man.\nNouns\nNouns generally refer to people, places, things, or concepts, e.g., woman, Scotland,\nbook, intelligence . Nouns can appear after determiners and adjectives, and can be the\nsubject or object of the verb, as shown in Table 5-2.\nTable 5-2. Syntactic patterns involving some nouns\nWord After a determiner Subject of the verb\nwoman the woman who I saw yesterday ... the woman sat down\nScotland the Scotland I remember as a child ... Scotland has five million people\nbook the book I bought yesterday ... this book recounts the colonization of Australia\nintelligence the intelligence displayed by the child ... Mary\u2019s intelligence impressed her teachers\nThe simplified noun tags are N for common nouns like book, and NP for proper nouns\nlike Scotland.\n184 | Chapter 5: \u2002Categorizing and Tagging Words\nLet\u2019s inspect some tagged text to see what parts-of-speech occur before a noun, with\nthe most \nfrequent ones first. To begin with, we construct a list of bigrams whose mem-\nbers are themselves word-tag pairs, such as (('The', 'DET'), ('Fulton', 'NP')) and\n(('Fulton', 'NP'), ('County', 'N')). Then we construct a FreqDist from the tag parts\nof the bigrams.\n>>> word_tag_pairs = nltk.bigrams(brown_news_tagged)\n>>> list(nltk.FreqDist(a[1] for (a, b) in word_tag_pairs if b[1] == 'N'))\n['DET', 'ADJ', 'N', 'P', 'NP', 'NUM', 'V', 'PRO', 'CNJ', '.', ',', 'VG', 'VN', ...]\nThis confirms our assertion that nouns occur after determiners and adjectives, includ-\ning numeral adjectives (tagged as NUM).\nVerbs\nVerbs are words that describe events and actions, e.g., fall and eat, as shown in Ta-\nble 5-3. In the context of a sentence, verbs typically express a relation involving the\nreferents of one or more noun phrases.\nTable 5-3. Syntactic patterns involving some verbs\nWord Simple With modifiers and adjuncts (italicized)\nfall Rome fell Dot com stocks suddenly fell like a stone\neat Mice eat cheese John ate the pizza with gusto\nWhat are the most common verbs in news text? Let\u2019s sort all the verbs by frequency:\n>>> wsj = nltk.corpus.treebank.tagged_words(simplify_tags=True)\n>>> word_tag_fd = nltk.FreqDist(wsj)\n>>> [word + \"/\" + tag for (word, tag) in word_tag_fd if tag.startswith('V')]\n['is/V', 'said/VD', 'was/VD', 'are/V', 'be/V', 'has/V', 'have/V', 'says/V',\n'were/VD', 'had/VD', 'been/VN', \"'s/V\", 'do/V', 'say/V', 'make/V', 'did/VD',\n'rose/VD', 'does/V', 'expected/VN', 'buy/V', 'take/V', 'get/V', 'sell/V',\n'help/V', 'added/VD', 'including/VG', 'according/VG', 'made/VN', 'pay/V', ...]\nNote that \nthe items being counted in the frequency distribution are word-tag pairs.\nSince words and tags are paired, we can treat the word as a condition and the tag as an\nevent, and initialize a conditional frequency distribution with a list of condition-event\npairs. This lets us see a frequency-ordered list of tags given a word:\n>>> cfd1 = nltk.ConditionalFreqDist(wsj)\n>>> cfd1['yield'].keys()\n['V', 'N']\n>>> cfd1['cut'].keys()\n['V', 'VD', 'N', 'VN']\nWe can reverse the order of the pairs, so that the tags are the conditions, and the words\nare the events. Now we can see likely words for a given tag:\n5.2  Tagged Corpora | 185\n>>> cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)\n>>> cfd2['VN'].keys()\n['been', 'expected', 'made', 'compared', 'based', 'priced', 'used', 'sold',\n'named', 'designed', 'held', 'fined', 'taken', 'paid', 'traded', 'said', ...]\nTo clarify \nthe distinction between VD (past tense) and VN (past participle), let\u2019s find\nwords that can be both VD and VN, and see some surrounding text:\n>>> [w for w in cfd1.conditions() if 'VD' in cfd1[w] and 'VN' in cfd1[w]]\n['Asked', 'accelerated', 'accepted', 'accused', 'acquired', 'added', 'adopted', ...]\n>>> idx1 = wsj.index(('kicked', 'VD'))\n>>> wsj[idx1-4:idx1+1]\n[('While', 'P'), ('program', 'N'), ('trades', 'N'), ('swiftly', 'ADV'),\n('kicked', 'VD')]\n>>> idx2 = wsj.index(('kicked', 'VN'))\n>>> wsj[idx2-4:idx2+1]\n[('head', 'N'), ('of', 'P'), ('state', 'N'), ('has', 'V'), ('kicked', 'VN')]\nIn this case, we see that the past participle of kicked is preceded by a form of the auxiliary\nverb have. Is this generally true?\nYour Turn: Given the list of past participles specified by\ncfd2['VN'].keys(), try to collect a list of all the word-tag pairs that im-\nmediately precede items in that list.\nAdjectives and Adverbs\nTwo other important word classes are adjectives and adverbs. Adjectives describe\nnouns, and can be used as modifiers (e.g., large in the large pizza ), or as predicates (e.g.,\nthe pizza is large ). English adjectives can have internal structure (e.g., fall+ing in the\nfalling stocks ). Adverbs modify verbs to specify the time, manner, place, or direction of\nthe event described by the verb (e.g., quickly in the stocks fell quickly). Adverbs may\nalso modify adjectives (e.g., really in Mary\u2019s teacher was really nice).\nEnglish has several categories of closed class words in addition to prepositions, such\nas articles (also often called determiners) (e.g., the, a), modals (e.g., should, may),\nand personal pronouns (e.g., she, they). Each dictionary and grammar classifies these\nwords differently.\nYour Turn:  If you are uncertain about some of these parts-of-speech,\nstudy them using nltk.app.concordance(), or watch some of the School-\nhouse Rock! grammar videos available at YouTube, or consult Sec-\ntion 5.9.\n186 | Chapter 5: \u2002Categorizing and Tagging Words\nUnsimplified Tags\nLet\u2019s find \nthe most frequent nouns of each noun part-of-speech type. The program in\nExample 5-1  finds all tags starting with NN, and provides a few example words for each\none. You will see that there are many variants of NN; the most important contain $ for\npossessive nouns, S for plural nouns (since plural nouns typically end in s), and P for\nproper nouns. In addition, most of the tags have suffix modifiers: -NC for citations,\n-HL for words in headlines, and -TL for titles (a feature of Brown tags).\nExample 5-1. Program to find the most frequent noun tags.\ndef findtags(tag_prefix, tagged_text):\n    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n                                  if tag.startswith(tag_prefix))\n    return dict((tag, cfd[tag].keys()[:5]) for tag in cfd.conditions())\n>>> tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))\n>>> for tag in sorted(tagdict):\n...     print tag, tagdict[tag]\n...\nNN ['year', 'time', 'state', 'week', 'man']\nNN$ [\"year's\", \"world's\", \"state's\", \"nation's\", \"company's\"]\nNN$-HL [\"Golf's\", \"Navy's\"]\nNN$-TL [\"President's\", \"University's\", \"League's\", \"Gallery's\", \"Army's\"]\nNN-HL ['cut', 'Salary', 'condition', 'Question', 'business']\nNN-NC ['eva', 'ova', 'aya']\nNN-TL ['President', 'House', 'State', 'University', 'City']\nNN-TL-HL ['Fort', 'City', 'Commissioner', 'Grove', 'House']\nNNS ['years', 'members', 'people', 'sales', 'men']\nNNS$ [\"children's\", \"women's\", \"men's\", \"janitors'\", \"taxpayers'\"]\nNNS$-HL [\"Dealers'\", \"Idols'\"]\nNNS$-TL [\"Women's\", \"States'\", \"Giants'\", \"Officers'\", \"Bombers'\"]\nNNS-HL ['years', 'idols', 'Creations', 'thanks', 'centers']\nNNS-TL ['States', 'Nations', 'Masters', 'Rules', 'Communists']\nNNS-TL-HL ['Nations']\nWhen we \ncome to constructing part-of-speech taggers later in this chapter, we will use\nthe unsimplified tags.\nExploring Tagged Corpora\nLet\u2019s briefly return to the kinds of exploration of corpora we saw in previous chapters,\nthis time exploiting POS tags.\nSuppose we\u2019re studying the word often and want to see how it is used in text. We could\nask to see the words that follow often:\n>>> brown_learned_text = brown.words(categories='learned')\n>>> sorted(set(b for (a, b) in nltk.ibigrams(brown_learned_text) if a == 'often'))\n[',', '.', 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming',\n'became', 'become', 'been', 'began', 'call', 'called', 'carefully', 'chose', ...]\nHowever, it\u2019s probably more instructive use the tagged_words() method to look at the\npart-of-speech tag of the following words:\n5.2  Tagged Corpora | 187\n>>> brown_lrnd_tagged = brown.tagged_words(categories='learned', simplify_tags=True)\n>>> tags = [b[1] for (a, b) in nltk.ibigrams(brown_lrnd_tagged) if a[0] == 'often']\n>>> fd = nltk.FreqDist(tags)\n>>> fd.tabulate()\n  VN    V   VD  DET  ADJ  ADV    P  CNJ    ,   TO   VG   WH  VBZ    .\n  15   12    8    5    5    4    4    3    3    1    1    1    1    1\nNotice that the most high-frequency parts-of-speech following often are verbs. Nouns\nnever appear in this position (in this particular corpus).\nNext, let\u2019s \nlook at some larger context, and find words involving particular sequences\nof tags and words (in this case \"<Verb> to <Verb>\"). In Example 5-2 , we consider each\nthree-word window in the sentence \n , and check whether they meet our criterion \n .\nIf the tags match, we print the corresponding words \n .\nExample 5-2. Searching for three-word phrases using POS tags.\nfrom nltk.corpus import brown\ndef process(sentence):\n    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence): \n        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')): \n            print w1, w2, w3 \n>>> for tagged_sent in brown.tagged_sents():\n...     process(tagged_sent)\n...\ncombined to achieve\ncontinue to place\nserve to protect\nwanted to wait\nallowed to place\nexpected to become\n...\nFinally, let\u2019s \nlook for words that are highly ambiguous as to their part-of-speech tag.\nUnderstanding why such words are tagged as they are in each context can help us clarify\nthe distinctions between the tags.\n>>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n>>> data = nltk.ConditionalFreqDist((word.lower(), tag)\n...                                 for (word, tag) in brown_news_tagged)\n>>> for word in data.conditions():\n...     if len(data[word]) > 3:\n...         tags = data[word].keys()\n...         print word, ' '.join(tags)\n...\nbest ADJ ADV NP V\nbetter ADJ ADV V DET\nclose ADV ADJ V N\ncut V N VN VD\neven ADV DET ADJ V\ngrant NP N V -\nhit V VD VN N\nlay ADJ V NP VD\nleft VD ADJ N VN\n188 | Chapter 5: \u2002Categorizing and Tagging Words\nlike CNJ V ADJ P -\nnear P ADV ADJ DET\nopen ADJ V N ADV\npast N ADJ DET P\npresent ADJ ADV V N\nread V VN VD NP\nright ADJ N DET ADV\nsecond NUM ADV DET N\nset VN V VD N -\nthat CNJ V WH DET\nYour Turn:  Open the POS concordance tool nltk.app.concordance()\nand load the complete Brown Corpus (simplified tagset). Now pick\nsome of the words listed at the end of the previous code example and\nsee how the tag of the word correlates with the context of the word. E.g.,\nsearch for near to see all forms mixed together, near/ADJ to see it used\nas an adjective, near N to see just those cases where a noun follows, and\nso forth.\n5.3  Mapping Words to Properties Using Python Dictionaries\nAs we have seen, a tagged word of the form (word, tag) is an association between a\nword and a part-of-speech tag. Once we start doing part-of-speech tagging, we will be\ncreating programs that assign a tag to a word, the tag which is most likely in a given\ncontext. We can think of this process as mapping from words to tags. The most natural\nway to store mappings in Python uses the so-called dictionary data type (also known\nas an associative array  or hash array  in other programming languages). In this sec-\ntion, we look at dictionaries and see how they can represent a variety of language in-\nformation, including parts-of-speech.\nIndexing Lists Versus Dictionaries\nA text, as we have seen, is treated in Python as a list of words. An important property\nof lists is that we can \u201clook up\u201d a particular item by giving its index, e.g., text1[100].\nNotice how we specify a number and get back a word. We can think of a list as a simple\nkind of table, as shown in Figure 5-2.\nFigure 5-2. List lookup: We access the contents of a Python list with the help of an integer index.\n5.3  Mapping Words to Properties Using Python Dictionaries | 189\nContrast this situation with frequency distributions ( Section 1.3 ), where we specify a\nword and get back a number, e.g., fdist['monstrous'], which tells us the number of\ntimes a given word has occurred in a text. Lookup using words is familiar to anyone\nwho has used a dictionary. Some more examples are shown in Figure 5-3.\nFigure 5-3. Dictionary lookup: we access the entry of a dictionary using a key such as someone\u2019s name,\na web \ndomain, or an English word; other names for dictionary are map, hashmap, hash, and\nassociative array.\nIn the case of a phonebook, we look up an entry using a name and get back a number.\nWhen we type a domain name in a web browser, the computer looks this up to get\nback an IP address. A word frequency table allows us to look up a word and find its\nfrequency in a text collection. In all these cases, we are mapping from names to num-\nbers, rather than the other way around as with a list. In general, we would like to be\nable to map between arbitrary types of information. Table 5-4 lists a variety of linguistic\nobjects, along with what they map.\nTable 5-4. Linguistic objects as mappings from keys to values\nLinguistic object Maps from Maps to\nDocument Index Word List of pages (where word is found)\nThesaurus Word sense List of synonyms\nDictionary Headword Entry (part-of-speech, sense definitions, etymology)\nComparative Wordlist Gloss term Cognates (list of words, one per language)\nMorph Analyzer Surface form Morphological analysis (list of component morphemes)\nMost often, we are mapping from a \u201cword\u201d to some structured object. For example, a\ndocument index \nmaps from a word (which we can represent as a string) to a list of pages\n(represented as a list of integers). In this section, we will see how to represent such\nmappings in Python.\nDictionaries in Python\nPython provides a dictionary data type that can be used for mapping between arbitrary\ntypes. It is like a conventional dictionary, in that it gives you an efficient way to look\nthings up. However, as we see from Table 5-4, it has a much wider range of uses.\n190 | Chapter 5: \u2002Categorizing and Tagging Words\nTo illustrate, we define pos to be an empty dictionary and then add four entries to it,\nspecifying the part-of-speech of some words. We add entries to a dictionary using the\nfamiliar square bracket notation:\n>>> pos = {}\n>>> pos\n{}\n>>> pos['colorless'] = 'ADJ' \n>>> pos\n{'colorless': 'ADJ'}\n>>> pos['ideas'] = 'N'\n>>> pos['sleep'] = 'V'\n>>> pos['furiously'] = 'ADV'\n>>> pos \n{'furiously': 'ADV', 'ideas': 'N', 'colorless': 'ADJ', 'sleep': 'V'}\nSo, for \nexample, \n  says that the part-of-speech of colorless is adjective, or more spe-\ncifically, that the key 'colorless' is assigned the value 'ADJ' in dictionary pos. When\nwe inspect the value of pos \n  we see a set of key-value pairs. Once we have populated\nthe dictionary in this way, we can employ the keys to retrieve values:\n>>> pos['ideas']\n'N'\n>>> pos['colorless']\n'ADJ'\nOf course, we might accidentally use a key that hasn\u2019t been assigned a value.\n>>> pos['green']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\nKeyError: 'green'\nThis raises \nan important question. Unlike lists and strings, where we can use len() to\nwork out which integers will be legal indexes, how do we work out the legal keys for a\ndictionary? If the dictionary is not too big, we can simply inspect its contents by eval-\nuating the variable pos. As we saw earlier in line \n , this gives us the key-value pairs.\nNotice that \nthey are not in the same order they were originally entered; this is because\ndictionaries are not sequences but mappings (see Figure 5-3 ), and the keys are not\ninherently ordered.\nAlternatively, to just find the keys, we can either convert the dictionary to a list \n  or\nuse the \ndictionary in a context where a list is expected, as the parameter of sorted()\n or in a for loop \n .\n>>> list(pos) \n['ideas', 'furiously', 'colorless', 'sleep']\n>>> sorted(pos) \n['colorless', 'furiously', 'ideas', 'sleep']\n>>> [w for w in pos if w.endswith('s')] \n['colorless', 'ideas']\n5.3  Mapping Words to Properties Using Python Dictionaries | 191\nWhen you type list(pos), you might see a different order to the one\nshown here. If you want to see the keys in order, just sort them.\nAs well as iterating over all keys in the dictionary with a for loop, we can use the for\nloop as we did for printing lists:\n>>> for word in sorted(pos):\n...     print word + \":\", pos[word]\n...\ncolorless: ADJ\nfuriously: ADV\nsleep: V\nideas: N\nFinally, the dictionary methods keys(), values(), and items() allow us to access the\nkeys, values, and key-value pairs as separate lists. We can even sort tuples \n , which\norders them \naccording to their first element (and if the first elements are the same, it\nuses their second elements).\n>>> pos.keys()\n['colorless', 'furiously', 'sleep', 'ideas']\n>>> pos.values()\n['ADJ', 'ADV', 'V', 'N']\n>>> pos.items()\n[('colorless', 'ADJ'), ('furiously', 'ADV'), ('sleep', 'V'), ('ideas', 'N')]\n>>> for key, val in sorted(pos.items()): \n...     print key + \":\", val\n...\ncolorless: ADJ\nfuriously: ADV\nideas: N\nsleep: V\nWe want \nto be sure that when we look something up in a dictionary, we get only one\nvalue for each key. Now suppose we try to use a dictionary to store the fact that the\nword sleep can be used as both a verb and a noun:\n>>> pos['sleep'] = 'V'\n>>> pos['sleep']\n'V'\n>>> pos['sleep'] = 'N'\n>>> pos['sleep']\n'N'\nInitially, pos['sleep'] is given the value 'V'. But this is immediately overwritten with\nthe new value, 'N'. In other words, there can be only one entry in the dictionary for\n'sleep'. However, there is a way of storing multiple values in that entry: we use a list\nvalue, e.g., pos['sleep'] = ['N', 'V']. In fact, this is what we saw in Section 2.4  for\nthe CMU Pronouncing Dictionary, which stores multiple pronunciations for a single\nword.\n192 | Chapter 5: \u2002Categorizing and Tagging Words\nDefining Dictionaries\nWe can \nuse the same key-value pair format to create a dictionary. There are a couple\nof ways to do this, and we will normally use the first:\n>>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n>>> pos = dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')\nNote that dictionary keys must be immutable types, such as strings and tuples. If we\ntry to define a dictionary using a mutable key, we get a TypeError:\n>>> pos = {['ideas', 'blogs', 'adventures']: 'N'}\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: list objects are unhashable\nDefault Dictionaries\nIf we try to access a key that is not in a dictionary, we get an error. However, it\u2019s often\nuseful if a dictionary can automatically create an entry for this new key and give it a\ndefault value, such as zero or the empty list. Since Python 2.5, a special kind of dic-\ntionary called a defaultdict has been available. (It is provided as nltk.defaultdict for\nthe benefit of readers who are using Python 2.4.) In order to use it, we have to supply\na parameter which can be used to create the default value, e.g., int, float, str, list,\ndict, tuple.\n>>> frequency = nltk.defaultdict(int)\n>>> frequency['colorless'] = 4\n>>> frequency['ideas']\n0\n>>> pos = nltk.defaultdict(list)\n>>> pos['sleep'] = ['N', 'V']\n>>> pos['ideas']\n[]\nThese default values are actually functions that convert other objects to\nthe specified \ntype (e.g., int(\"2\"), list(\"2\")). When they are called with\nno parameter\u2014say, int(), list()\u2014they return 0 and [] respectively.\nThe preceding examples specified the default value of a dictionary entry to be the default\nvalue of a particular data type. However, we can specify any default value we like, simply\nby providing the name of a function that can be called with no arguments to create the\nrequired value. Let\u2019s return to our part-of-speech example, and create a dictionary\nwhose default value for any entry is 'N' \n . When we access a non-existent entry \n , it\nis automatically added to the dictionary \n .\n>>> pos = nltk.defaultdict(lambda: 'N') \n>>> pos['colorless'] = 'ADJ'\n>>> pos['blog'] \n'N'\n5.3  Mapping Words to Properties Using Python Dictionaries | 193\n>>> pos.items()\n[('blog', 'N'), ('colorless', 'ADJ')] \nThis example used a lambda expression , introduced in Section 4.4. This\nlambda expression specifies no parameters, so we call it using paren-\ntheses with no arguments. Thus, the following definitions of f and g are\nequivalent:\n>>> f = lambda: 'N'\n>>> f()\n'N'\n>>> def g():\n...     return 'N'\n>>> g()\n'N'\nLet\u2019s see \nhow default dictionaries could be used in a more substantial language pro-\ncessing task. Many language processing tasks\u2014including tagging\u2014struggle to cor-\nrectly process the hapaxes of a text. They can perform better with a fixed vocabulary\nand a guarantee that no new words will appear. We can preprocess a text to replace\nlow-frequency words with a special \u201cout of vocabulary\u201d token, UNK, with the help of a\ndefault dictionary. (Can you work out how to do this without reading on?)\nWe need to create a default dictionary that maps each word to its replacement. The\nmost frequent n words will be mapped to themselves. Everything else will be mapped\nto UNK.\n>>> alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n>>> vocab = nltk.FreqDist(alice)\n>>> v1000 = list(vocab)[:1000]\n>>> mapping = nltk.defaultdict(lambda: 'UNK')\n>>> for v in v1000:\n...     mapping[v] = v\n...\n>>> alice2 = [mapping[v] for v in alice]\n>>> alice2[:100]\n['UNK', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'UNK', 'UNK',\n'UNK', 'UNK', 'CHAPTER', 'I', '.', 'UNK', 'the', 'Rabbit', '-', 'UNK', 'Alice',\n'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her',\n'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do',\n':', 'once', 'or', 'twice', 'she', 'had', 'UNK', 'into', 'the', 'book', 'her',\n'sister', 'was', 'UNK', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'UNK',\n'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\",\n'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\", ...]\n>>> len(set(alice2))\n1001\nIncrementally Updating a Dictionary\nWe can employ dictionaries to count occurrences, emulating the method for tallying\nwords shown in Figure 1-3. We begin by initializing an empty defaultdict, then process\neach part-of-speech tag in the text. If the tag hasn\u2019t been seen before, it will have a zero\n194 | Chapter 5: \u2002Categorizing and Tagging Words\ncount by default. Each time we encounter a tag, we increment its count using the +=\noperator (see Example 5-3).\nExample 5-3. Incrementally updating a dictionary, and sorting by value.\n>>> counts = nltk.defaultdict(int)\n>>> from nltk.corpus import brown\n>>> for (word, tag) in brown.tagged_words(categories='news'):\n...     counts[tag] += 1\n...\n>>> counts['N']\n22226\n>>> list(counts)\n['FW', 'DET', 'WH', \"''\", 'VBZ', 'VB+PPO', \"'\", ')', 'ADJ', 'PRO', '*', '-', ...]\n>>> from operator import itemgetter\n>>> sorted(counts.items(), key=itemgetter(1), reverse=True)\n[('N', 22226), ('P', 10845), ('DET', 10648), ('NP', 8336), ('V', 7313), ...]\n>>> [t for t, c in sorted(counts.items(), key=itemgetter(1), reverse=True)]\n['N', 'P', 'DET', 'NP', 'V', 'ADJ', ',', '.', 'CNJ', 'PRO', 'ADV', 'VD', ...]\nThe listing \nin Example 5-3  illustrates an important idiom for sorting a dictionary by its\nvalues, to show words in decreasing order of frequency. The first parameter of\nsorted() is the items to sort, which is a list of tuples consisting of a POS tag and a\nfrequency. The second parameter specifies the sort key using a function itemget\nter(). In general, itemgetter(n) returns a function that can be called on some other\nsequence object to obtain the nth element:\n>>> pair = ('NP', 8336)\n>>> pair[1]\n8336\n>>> itemgetter(1)(pair)\n8336\nThe last parameter of sorted() specifies that the items should be returned in reverse\norder, i.e., decreasing values of frequency.\nThere\u2019s a second useful programming idiom at the beginning of Example 5-3 , where\nwe initialize a defaultdict and then use a for loop to update its values. Here\u2019s a sche-\nmatic version:\n>>> my_dictionary = nltk.defaultdict(function to create default value)\n>>> for item in sequence:\n...      my_dictionary[item_key] is updated with information about item\nHere\u2019s another instance of this pattern, where we index words according to their last\ntwo letters:\n>>> last_letters = nltk.defaultdict(list)\n>>> words = nltk.corpus.words.words('en')\n>>> for word in words:\n...     key = word[-2:]\n...     last_letters[key].append(word)\n...\n5.3  Mapping Words to Properties Using Python Dictionaries | 195\n>>> last_letters['ly']\n['abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately',\n'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', ...]\n>>> last_letters['zy']\n['blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'Chazy', ...]\nThe following \nexample uses the same pattern to create an anagram dictionary. (You\nmight experiment with the third line to get an idea of why this program works.)\n>>> anagrams = nltk.defaultdict(list)\n>>> for word in words:\n...     key = ''.join(sorted(word))\n...     anagrams[key].append(word)\n...\n>>> anagrams['aeilnrt']\n['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\nSince accumulating words like this is such a common task, NLTK provides a more\nconvenient way of creating a defaultdict(list), in the form of nltk.Index():\n>>> anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)\n>>> anagrams['aeilnrt']\n['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\nnltk.Index is a defaultdict(list) with extra support for initialization.\nSimilarly, nltk.FreqDist is \nessentially a defaultdict(int) with extra\nsupport for initialization (along with sorting and plotting methods).\nComplex Keys and Values\nWe can use default dictionaries with complex keys and values. Let\u2019s study the range of\npossible tags for a word, given the word itself and the tag of the previous word. We will\nsee how this information can be used by a POS tagger.\n>>> pos = nltk.defaultdict(lambda: nltk.defaultdict(int))\n>>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n>>> for ((w1, t1), (w2, t2)) in nltk.ibigrams(brown_news_tagged): \n...     pos[(t1, w2)][t2] += 1 \n...\n>>> pos[('DET', 'right')] \ndefaultdict(<type 'int'>, {'ADV': 3, 'ADJ': 9, 'N': 3})\nThis example \nuses a dictionary whose default value for an entry is a dictionary (whose\ndefault value is int(), i.e., zero). Notice how we iterated over the bigrams of the tagged\ncorpus, processing a pair of word-tag pairs for each iteration \n . Each time through the\nloop we \nupdated our pos dictionary\u2019s entry for (t1, w2) , a tag and its following word\n. When we look up an item in pos we must specify a compound key \n , and we get\nback a \ndictionary object. A POS tagger could use such information to decide that the\nword right, when preceded by a determiner, should be tagged as ADJ.\n196 | Chapter 5: \u2002Categorizing and Tagging Words\nInverting a Dictionary\nDictionaries support \nefficient lookup, so long as you want to get the value for any key.\nIf d is a dictionary and k is a key, we type d[k] and immediately obtain the value. Finding\na key given a value is slower and more cumbersome:\n>>> counts = nltk.defaultdict(int)\n>>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):\n...     counts[word] += 1\n...\n>>> [key for (key, value) in counts.items() if value == 32]\n['brought', 'Him', 'virtue', 'Against', 'There', 'thine', 'King', 'mortal',\n'every', 'been']\nIf we expect to do this kind of \u201creverse lookup\u201d often, it helps to construct a dictionary\nthat maps values to keys. In the case that no two keys have the same value, this is an\neasy thing to do. We just get all the key-value pairs in the dictionary, and create a new\ndictionary of value-key pairs. The next example also illustrates another way of initial-\nizing a dictionary pos with key-value pairs.\n>>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n>>> pos2 = dict((value, key) for (key, value) in pos.items())\n>>> pos2['N']\n'ideas'\nLet\u2019s first make our part-of-speech dictionary a bit more realistic and add some more\nwords to pos using the dictionary update() method, to create the situation where mul-\ntiple keys have the same value. Then the technique just shown for reverse lookup will\nno longer work (why not?). Instead, we have to use append() to accumulate the words\nfor each part-of-speech, as follows:\n>>> pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'})\n>>> pos2 = nltk.defaultdict(list)\n>>> for key, value in pos.items():\n...     pos2[value].append(key)\n...\n>>> pos2['ADV']\n['peacefully', 'furiously']\nNow we have inverted the pos dictionary, and can look up any part-of-speech and find\nall words having that part-of-speech. We can do the same thing even more simply using\nNLTK\u2019s support for indexing, as follows:\n>>> pos2 = nltk.Index((value, key) for (key, value) in pos.items())\n>>> pos2['ADV']\n['peacefully', 'furiously']\nA summary of Python\u2019s dictionary methods is given in Table 5-5.\n5.3  Mapping Words to Properties Using Python Dictionaries | 197\nTable 5-5. Python\u2019s dictionary methods: A summary of commonly used methods and idioms involving\ndictionaries\nExample Description\nd = {} Create an empty dictionary and assign it to d\nd[key] = value Assign a value to a given dictionary key\nd.keys() The list of keys of the dictionary\nlist(d) The list of keys of the dictionary\nsorted(d) The keys of the dictionary, sorted\nkey in d Test whether a particular key is in the dictionary\nfor key in d Iterate over the keys of the dictionary\nd.values() The list of values in the dictionary\ndict([(k1,v1), (k2,v2), ...]) Create a dictionary from a list of key-value pairs\nd1.update(d2) Add all items from d2 to d1\ndefaultdict(int) A dictionary whose default value is zero\n5.4  Automatic Tagging\nIn the \nrest of this chapter we will explore various ways to automatically add part-of-\nspeech tags to text. We will see that the tag of a word depends on the word and its\ncontext within a sentence. For this reason, we will be working with data at the level of\n(tagged) sentences rather than words. We\u2019ll begin by loading the data we will be using.\n>>> from nltk.corpus import brown\n>>> brown_tagged_sents = brown.tagged_sents(categories='news')\n>>> brown_sents = brown.sents(categories='news')\nThe Default Tagger\nThe simplest possible tagger assigns the same tag to each token. This may seem to be\na rather banal step, but it establishes an important baseline for tagger performance. In\norder to get the best result, we tag each word with the most likely tag. Let\u2019s find out\nwhich tag is most likely (now using the unsimplified tagset):\n>>> tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n>>> nltk.FreqDist(tags).max()\n'NN'\nNow we can create a tagger that tags everything as NN.\n>>> raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n>>> tokens = nltk.word_tokenize(raw)\n>>> default_tagger = nltk.DefaultTagger('NN')\n>>> default_tagger.tag(tokens)\n[('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'),\n('eggs', 'NN'), ('and', 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'),\n198 | Chapter 5: \u2002Categorizing and Tagging Words\n('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'),\n('I', 'NN'), ('am', 'NN'), ('!', 'NN')]\nUnsurprisingly, this \nmethod performs rather poorly. On a typical corpus, it will tag\nonly about an eighth of the tokens correctly, as we see here:\n>>> default_tagger.evaluate(brown_tagged_sents)\n0.13089484257215028\nDefault taggers assign their tag to every single word, even words that have never been\nencountered before. As it happens, once we have processed several thousand words of\nEnglish text, most new words will be nouns. As we will see, this means that default\ntaggers can help to improve the robustness of a language processing system. We will\nreturn to them shortly.\nThe Regular Expression Tagger\nThe regular expression tagger assigns tags to tokens on the basis of matching patterns.\nFor instance, we might guess that any word ending in ed is the past participle of a verb,\nand any word ending with \u2019s is a possessive noun. We can express these as a list of\nregular expressions:\n>>> patterns = [\n...     (r'.*ing$', 'VBG'),               # gerunds\n...     (r'.*ed$', 'VBD'),                # simple past\n...     (r'.*es$', 'VBZ'),                # 3rd singular present\n...     (r'.*ould$', 'MD'),               # modals\n...     (r'.*\\'s$', 'NN$'),               # possessive nouns\n...     (r'.*s$', 'NNS'),                 # plural nouns\n...     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n...     (r'.*', 'NN')                     # nouns (default)\n... ]\nNote that these are processed in order, and the first one that matches is applied. Now\nwe can set up a tagger and use it to tag a sentence. After this step, it is correct about a\nfifth of the time.\n>>> regexp_tagger = nltk.RegexpTagger(patterns)\n>>> regexp_tagger.tag(brown_sents[3])\n[('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'),\n('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'),\n(\"''\", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'),\n('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ...]\n>>> regexp_tagger.evaluate(brown_tagged_sents)\n0.20326391789486245\nThe final regular expression \u00ab .*\u00bb is a catch-all that tags everything as a noun. This is\nequivalent to the default tagger (only much less efficient). Instead of respecifying this\nas part of the regular expression tagger, is there a way to combine this tagger with the\ndefault tagger? We will see how to do this shortly.\n5.4  Automatic Tagging | 199\nYour Turn:  See if you can come up with patterns to improve the per-\nformance of the regular expression tagger just shown. (Note that Sec-\ntion 6.1 describes a way to partially automate such work.)\nThe Lookup Tagger\nA lot of high-frequency words do not have the NN tag. Let\u2019s find the hundred most\nfrequent words and store their most likely tag. We can then use this information as the\nmodel for a \u201clookup tagger\u201d (an NLTK UnigramTagger):\n>>> fd = nltk.FreqDist(brown.words(categories='news'))\n>>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n>>> most_freq_words = fd.keys()[:100]\n>>> likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n>>> baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n>>> baseline_tagger.evaluate(brown_tagged_sents)\n0.45578495136941344\nIt should come as no surprise by now that simply knowing the tags for the 100 most\nfrequent words enables us to tag a large fraction of tokens correctly (nearly half, in fact).\nLet\u2019s see what it does on some untagged input text:\n>>> sent = brown.sents(categories='news')[3]\n>>> baseline_tagger.tag(sent)\n[('``', '``'), ('Only', None), ('a', 'AT'), ('relative', None),\n('handful', None), ('of', 'IN'), ('such', None), ('reports', None),\n('was', 'BEDZ'), ('received', None), (\"''\", \"''\"), (',', ','),\n('the', 'AT'), ('jury', None), ('said', 'VBD'), (',', ','),\n('``', '``'), ('considering', None), ('the', 'AT'), ('widespread', None),\n('interest', None), ('in', 'IN'), ('the', 'AT'), ('election', None),\n(',', ','), ('the', 'AT'), ('number', None), ('of', 'IN'),\n('voters', None), ('and', 'CC'), ('the', 'AT'), ('size', None),\n('of', 'IN'), ('this', 'DT'), ('city', None), (\"''\", \"''\"), ('.', '.')]\nMany words have been assigned a tag of None, because they were not among the 100\nmost frequent words. In these cases we would like to assign the default tag of NN. In\nother words, we want to use the lookup table first, and if it is unable to assign a tag,\nthen use the default tagger, a process known as backoff ( Section 5.5 ). We do this by\nspecifying one tagger as a parameter to the other, as shown next. Now the lookup tagger\nwill only store word-tag pairs for words other than nouns, and whenever it cannot\nassign a tag to a word, it will invoke the default tagger.\n>>> baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n...                                      backoff=nltk.DefaultTagger('NN'))\nLet\u2019s put all this together and write a program to create and evaluate lookup taggers\nhaving a range of sizes (Example 5-4).\n200 | Chapter 5: \u2002Categorizing and Tagging Words\nExample 5-4. Lookup tagger performance with varying model size.\ndef performance(cfd, wordlist):\n    lt = dict((word, cfd[word].max()) for word in wordlist)\n    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\ndef display():\n    import pylab\n    words_by_freq = list(nltk.FreqDist(brown.words(categories='news')))\n    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n    sizes = 2 ** pylab.arange(15)\n    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n    pylab.plot(sizes, perfs, '-bo')\n    pylab.title('Lookup Tagger Performance with Varying Model Size')\n    pylab.xlabel('Model Size')\n    pylab.ylabel('Performance')\n    pylab.show()\n>>> display()                                  \nObserve in Figure \n5-4 that performance initially increases rapidly as the model size\ngrows, eventually reaching a plateau, when large increases in model size yield little\nimprovement in performance. (This example used the pylab plotting package, dis-\ncussed in Section 4.8.)\nEvaluation\nIn the previous examples, you will have noticed an emphasis on accuracy scores. In\nfact, evaluating the performance of such tools is a central theme in NLP. Recall the\nprocessing pipeline in Figure 1-5 ; any errors in the output of one module are greatly\nmultiplied in the downstream modules.\nWe evaluate the performance of a tagger relative to the tags a human expert would\nassign. Since we usually don\u2019t have access to an expert and impartial human judge, we\nmake do instead with gold standard  test data. This is a corpus which has been man-\nually annotated and accepted as a standard against which the guesses of an automatic\nsystem are assessed. The tagger is regarded as being correct if the tag it guesses for a\ngiven word is the same as the gold standard tag.\nOf course, the humans who designed and carried out the original gold standard anno-\ntation were only human. Further analysis might show mistakes in the gold standard,\nor may eventually lead to a revised tagset and more elaborate guidelines. Nevertheless,\nthe gold standard is by definition \u201ccorrect\u201d as far as the evaluation of an automatic\ntagger is concerned.\n5.4  Automatic Tagging | 201\nDeveloping an annotated corpus is a major undertaking. Apart from the\ndata, it \ngenerates sophisticated tools, documentation, and practices for\nensuring high-quality annotation. The tagsets and other coding schemes\ninevitably depend on some theoretical position that is not shared by all.\nHowever, corpus creators often go to great lengths to make their work\nas theory-neutral as possible in order to maximize the usefulness of their\nwork. We will discuss the challenges of creating a corpus in Chapter 11 .\n5.5  N-Gram Tagging\nUnigram Tagging\nUnigram taggers are based on a simple statistical algorithm: for each token, assign the\ntag that is most likely for that particular token. For example, it will assign the tag JJ to\nany occurrence of the word frequent, since frequent is used as an adjective (e.g., a fre-\nquent word ) more often than it is used as a verb (e.g., I frequent this cafe). A unigram\ntagger behaves just like a lookup tagger ( Section 5.4), except there is a more convenient\nFigure 5-4. Lookup tagger\n202 | Chapter 5: \u2002Categorizing and Tagging Words\ntechnique for setting it up, called training. In the following code sample, we train a\nunigram tagger, use it to tag a sentence, and then evaluate:\n>>> from nltk.corpus import brown\n>>> brown_tagged_sents = brown.tagged_sents(categories='news')\n>>> brown_sents = brown.sents(categories='news')\n>>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n>>> unigram_tagger.tag(brown_sents[2007])\n[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'),\n(',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'),\n('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'),\n('direct', 'JJ'), ('.', '.')]\n>>> unigram_tagger.evaluate(brown_tagged_sents)\n0.9349006503968017\nWe train a UnigramTagger by specifying tagged sentence data as a parameter when we\ninitialize the tagger. The training process involves inspecting the tag of each word and\nstoring the most likely tag for any word in a dictionary that is stored inside the tagger.\nSeparating the Training and Testing Data\nNow that we are training a tagger on some data, we must be careful not to test it on\nthe same data, as we did in the previous example. A tagger that simply memorized its\ntraining data and made no attempt to construct a general model would get a perfect\nscore, but would be useless for tagging new text. Instead, we should split the data,\ntraining on 90% and testing on the remaining 10%:\n>>> size = int(len(brown_tagged_sents) * 0.9)\n>>> size\n4160\n>>> train_sents = brown_tagged_sents[:size]\n>>> test_sents = brown_tagged_sents[size:]\n>>> unigram_tagger = nltk.UnigramTagger(train_sents)\n>>> unigram_tagger.evaluate(test_sents)\n0.81202033290142528\nAlthough the score is worse, we now have a better picture of the usefulness of this\ntagger, i.e., its performance on previously unseen text.\nGeneral N-Gram Tagging\nWhen we perform a language processing task based on unigrams, we are using one\nitem of context. In the case of tagging, we consider only the current token, in isolation\nfrom any larger context. Given such a model, the best we can do is tag each word with\nits a priori most likely tag. This means we would tag a word such as wind with the same\ntag, regardless of whether it appears in the context the wind or to wind.\nAn n-gram tagger  is a generalization of a unigram tagger whose context is the current\nword together with the part-of-speech tags of the n-1 preceding tokens, as shown in\nFigure 5-5 . The tag to be chosen, tn, is circled, and the context is shaded in grey. In the\nexample of an n-gram tagger shown in Figure 5-5, we have n=3; that is, we consider\n5.5  N-Gram Tagging | 203\nthe tags of the two preceding words in addition to the current word. An n-gram tagger\npicks the tag that is most likely in the given context.\nA 1-gram tagger is another term for a unigram tagger: i.e., the context\nused to \ntag a token is just the text of the token itself. 2-gram taggers are\nalso called bigram taggers , and 3-gram taggers are called trigram taggers .\nThe NgramTagger class uses a tagged training corpus to determine which part-of-speech\ntag is most likely for each context. Here we see a special case of an n-gram tagger,\nnamely a bigram tagger. First we train it, then use it to tag untagged sentences:\n>>> bigram_tagger = nltk.BigramTagger(train_sents)\n>>> bigram_tagger.tag(brown_sents[2007])\n[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'),\n('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'),\n('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'),\n('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n>>> unseen_sent = brown_sents[4203]\n>>> bigram_tagger.tag(unseen_sent)\n[('The', 'AT'), ('population', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Congo', 'NP'),\n('is', 'BEZ'), ('13.5', None), ('million', None), (',', None), ('divided', None),\n('into', None), ('at', None), ('least', None), ('seven', None), ('major', None),\n('``', None), ('culture', None), ('clusters', None), (\"''\", None), ('and', None),\n('innumerable', None), ('tribes', None), ('speaking', None), ('400', None),\n('separate', None), ('dialects', None), ('.', None)]\nNotice that the bigram tagger manages to tag every word in a sentence it saw during\ntraining, but does badly on an unseen sentence. As soon as it encounters a new word\n(i.e., 13.5), it is unable to assign a tag. It cannot tag the following word (i.e., million),\neven if it was seen during training, simply because it never saw it during training with\na None tag on the previous word. Consequently, the tagger fails to tag the rest of the\nsentence. Its overall accuracy score is very low:\n>>> bigram_tagger.evaluate(test_sents)\n0.10276088906608193\nFigure 5-5. Tagger context.\n204 | Chapter 5: \u2002Categorizing and Tagging Words\nAs n gets larger, the specificity of the contexts increases, as does the chance that the\ndata \nwe wish to tag contains contexts that were not present in the training data. This\nis known as the sparse data  problem, and is quite pervasive in NLP. As a consequence,\nthere is a trade-off between the accuracy and the coverage of our results (and this is\nrelated to the precision/recall trade-off in information retrieval).\nCaution!\nN-gram taggers \nshould not consider context that crosses a sentence\nboundary. Accordingly, NLTK taggers are designed to work with lists\nof sentences, where each sentence is a list of words. At the start of a\nsentence, t n-1 and preceding tags are set to None.\nCombining Taggers\nOne way to address the trade-off between accuracy and coverage is to use the more\naccurate algorithms when we can, but to fall back on algorithms with wider coverage\nwhen necessary. For example, we could combine the results of a bigram tagger, a\nunigram tagger, and a default tagger, as follows:\n1. Try tagging the token with the bigram tagger.\n2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.\n3. If the unigram tagger is also unable to find a tag, use a default tagger.\nMost NLTK taggers permit a backoff tagger to be specified. The backoff tagger may\nitself have a backoff tagger:\n>>> t0 = nltk.DefaultTagger('NN')\n>>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n>>> t2 = nltk.BigramTagger(train_sents, backoff=t1)\n>>> t2.evaluate(test_sents)\n0.84491179108940495\nYour Turn: Extend the preceding example by defining a TrigramTag\nger called t3, which backs off to t2.\nNote that we specify the backoff tagger when the tagger is initialized so that training\ncan take advantage of the backoff tagger. Thus, if the bigram tagger would assign the\nsame tag as its unigram backoff tagger in a certain context, the bigram tagger discards\nthe training instance. This keeps the bigram tagger model as small as possible. We can\nfurther specify that a tagger needs to see more than one instance of a context in order\nto retain it. For example, nltk.BigramTagger(sents, cutoff=2, backoff=t1) will dis-\ncard contexts that have only been seen once or twice.\n5.5  N-Gram Tagging | 205\nTagging Unknown Words\nOur approach \nto tagging unknown words still uses backoff to a regular expression\ntagger or a default tagger. These are unable to make use of context. Thus, if our tagger\nencountered the word blog, not seen during training, it would assign it the same tag,\nregardless of whether this word appeared in the context the blog  or to blog . How can\nwe do better with these unknown words, or out-of-vocabulary items?\nA useful method to tag unknown words based on context is to limit the vocabulary of\na tagger to the most frequent n words, and to replace every other word with a special\nword UNK using the method shown in Section 5.3 . During training, a unigram tagger\nwill probably learn that UNK is usually a noun. However, the n-gram taggers will detect\ncontexts in which it has some other tag. For example, if the preceding word is to (tagged\nTO), then UNK will probably be tagged as a verb.\nStoring Taggers\nTraining a tagger on a large corpus may take a significant time. Instead of training a\ntagger every time we need one, it is convenient to save a trained tagger in a file for later\nreuse. Let\u2019s save our tagger t2 to a file t2.pkl:\n>>> from cPickle import dump\n>>> output = open('t2.pkl', 'wb')\n>>> dump(t2, output, -1)\n>>> output.close()\nNow, in a separate Python process, we can load our saved tagger:\n>>> from cPickle import load\n>>> input = open('t2.pkl', 'rb')\n>>> tagger = load(input)\n>>> input.close()\nNow let\u2019s check that it can be used for tagging:\n>>> text = \"\"\"The board's action shows what free enterprise\n...     is up against in our complex maze of regulatory laws .\"\"\"\n>>> tokens = text.split()\n>>> tagger.tag(tokens)\n[('The', 'AT'), (\"board's\", 'NN$'), ('action', 'NN'), ('shows', 'NNS'),\n('what', 'WDT'), ('free', 'JJ'), ('enterprise', 'NN'), ('is', 'BEZ'),\n('up', 'RP'), ('against', 'IN'), ('in', 'IN'), ('our', 'PP$'), ('complex', 'JJ'),\n('maze', 'NN'), ('of', 'IN'), ('regulatory', 'NN'), ('laws', 'NNS'), ('.', '.')]\nPerformance Limitations\nWhat is the upper limit to the performance of an n-gram tagger? Consider the case of\na trigram tagger. How many cases of part-of-speech ambiguity does it encounter? We\ncan determine the answer to this question empirically:\n206 | Chapter 5: \u2002Categorizing and Tagging Words\n>>> cfd = nltk.ConditionalFreqDist(\n...            ((x[1], y[1], z[0]), z[1])\n...            for sent in brown_tagged_sents\n...            for x, y, z in nltk.trigrams(sent))\n>>> ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n>>> sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()\n0.049297702068029296\nThus, 1 \nout of 20 trigrams is ambiguous. Given the current word and the previous two\ntags, in 5% of cases there is more than one tag that could be legitimately assigned to\nthe current word according to the training data. Assuming we always pick the most\nlikely tag in such ambiguous contexts, we can derive a lower bound on the performance\nof a trigram tagger.\nAnother way to investigate the performance of a tagger is to study its mistakes. Some\ntags may be harder than others to assign, and it might be possible to treat them specially\nby pre- or post-processing the data. A convenient way to look at tagging errors is the\nconfusion matrix . It charts expected tags (the gold standard) against actual tags gen-\nerated by a tagger:\n>>> test_tags = [tag for sent in brown.sents(categories='editorial')\n...                  for (word, tag) in t2.tag(sent)]\n>>> gold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]\n>>> print nltk.ConfusionMatrix(gold, test)                \nBased on such analysis we may decide to modify the tagset. Perhaps a distinction be-\ntween tags that is difficult to make can be dropped, since it is not important in the\ncontext of some larger processing task.\nAnother way to analyze the performance bound on a tagger comes from the less than\n100% agreement between human annotators.\nIn general, observe that the tagging process collapses distinctions: e.g., lexical identity\nis usually lost when all personal pronouns are tagged PRP. At the same time, the tagging\nprocess introduces new distinctions and removes ambiguities: e.g., deal tagged as VB or\nNN. This characteristic of collapsing certain distinctions and introducing new distinc-\ntions is an important feature of tagging which facilitates classification and prediction.\nWhen we introduce finer distinctions in a tagset, an n-gram tagger gets more detailed\ninformation about the left-context when it is deciding what tag to assign to a particular\nword. However, the tagger simultaneously has to do more work to classify the current\ntoken, simply because there are more tags to choose from. Conversely, with fewer dis-\ntinctions (as with the simplified tagset), the tagger has less information about context,\nand it has a smaller range of choices in classifying the current token.\nWe have seen that ambiguity in the training data leads to an upper limit in tagger\nperformance. Sometimes more context will resolve the ambiguity. In other cases, how-\never, as noted by (Abney, 1996), the ambiguity can be resolved only with reference to\nsyntax or to world knowledge. Despite these imperfections, part-of-speech tagging has\nplayed a central role in the rise of statistical approaches to natural language processing.\nIn the early 1990s, the surprising accuracy of statistical taggers was a striking\n5.5  N-Gram Tagging | 207\ndemonstration that it was possible to solve one small part of the language understand-\ning problem, namely part-of-speech disambiguation, without reference to deeper sour-\nces of linguistic knowledge. Can this idea be pushed further? In Chapter 7 , we will see\nthat it can.\nTagging Across Sentence Boundaries\nAn n-gram tagger uses recent tags to guide the choice of tag for the current word. When\ntagging the first word of a sentence, a trigram tagger will be using the part-of-speech\ntag of the previous two tokens, which will normally be the last word of the previous\nsentence and the sentence-ending punctuation. However, the lexical category that\nclosed the previous sentence has no bearing on the one that begins the next sentence.\nTo deal with this situation, we can train, run, and evaluate taggers using lists of tagged\nsentences, as shown in Example 5-5.\nExample 5-5. N-gram tagging at the sentence level.\nbrown_tagged_sents = brown.tagged_sents(categories='news')\nbrown_sents = brown.sents(categories='news')\nsize = int(len(brown_tagged_sents) * 0.9)\ntrain_sents = brown_tagged_sents[:size]\ntest_sents = brown_tagged_sents[size:]\nt0 = nltk.DefaultTagger('NN')\nt1 = nltk.UnigramTagger(train_sents, backoff=t0)\nt2 = nltk.BigramTagger(train_sents, backoff=t1)\n>>> t2.evaluate(test_sents)\n0.84491179108940495\n5.6  Transformation-Based Tagging\nA potential issue with n-gram taggers is the size of their n-gram table (or language\nmodel). If tagging is to be employed in a variety of language technologies deployed on\nmobile computing devices, it is important to strike a balance between model size and\ntagger performance. An n-gram tagger with backoff may store trigram and bigram ta-\nbles, which are large, sparse arrays that may have hundreds of millions of entries.\nA second issue concerns context. The only information an n-gram tagger considers\nfrom prior context is tags, even though words themselves might be a useful source of\ninformation. It is simply impractical for n-gram models to be conditioned on the iden-\ntities of words in the context. In this section, we examine Brill tagging, an inductive\ntagging method which performs very well using models that are only a tiny fraction of\nthe size of n-gram taggers.\nBrill tagging is a kind of transformation-based learning , named after its inventor. The\ngeneral idea is very simple: guess the tag of each word, then go back and fix the mistakes.\n208 | Chapter 5: \u2002Categorizing and Tagging Words\nIn this way, a Brill tagger successively transforms a bad tagging of a text into a better\none. As \nwith n-gram tagging, this is a supervised learning  method, since we need an-\nnotated training data to figure out whether the tagger\u2019s guess is a mistake or not. How-\never, unlike n-gram tagging, it does not count observations but compiles a list of trans-\nformational correction rules.\nThe process of Brill tagging is usually explained by analogy with painting. Suppose we\nwere painting a tree, with all its details of boughs, branches, twigs, and leaves, against\na uniform sky-blue background. Instead of painting the tree first and then trying to\npaint blue in the gaps, it is simpler to paint the whole canvas blue, then \u201ccorrect\u201d the\ntree section by over-painting the blue background. In the same fashion, we might paint\nthe trunk a uniform brown before going back to over-paint further details with even\nfiner brushes. Brill tagging uses the same idea: begin with broad brush strokes, and\nthen fix up the details, with successively finer changes. Let\u2019s look at an example in-\nvolving the following sentence:\n(1) The President said he will ask Congress to increase grants to states for voca-\ntional rehabilitation.\nWe will examine the operation of two rules: (a) replace NN with VB when the previous\nword is TO; (b) replace TO with IN when the next tag is NNS. Table 5-6 illustrates this\nprocess, first tagging with the unigram tagger, then applying the rules to fix the errors.\nTable 5-6. Steps in Brill tagging\nPhrase to increase grants to states for vocational rehabilitation\nUnigram TO NN NNS TO NNS IN JJ NN\nRule 1  VB       \nRule 2    IN     \nOutput TO VB NNS IN NNS IN JJ NN\nGold TO VB NNS IN NNS IN JJ NN\nIn this table, we see two rules. All such rules are generated from a template of the\nfollowing form: \n\u201creplace T1 with T2 in the context C.\u201d Typical contexts are the identity\nor the tag of the preceding or following word, or the appearance of a specific tag within\ntwo to three words of the current word. During its training phase, the tagger guesses\nvalues for T1, T2, and C, to create thousands of candidate rules. Each rule is scored\naccording to its net benefit: the number of incorrect tags that it corrects, less the number\nof correct tags it incorrectly modifies.\nBrill taggers have another interesting property: the rules are linguistically interpretable.\nCompare this with the n-gram taggers, which employ a potentially massive table of n-\ngrams. We cannot learn much from direct inspection of such a table, in comparison to\nthe rules learned by the Brill tagger. Example 5-6 demonstrates NLTK\u2019s Brill tagger.\n5.6  Transformation-Based Tagging | 209\nExample 5-6. Brill tagger demonstration: The tagger has a collection of templates of the form X \u2192 Y\nif the preceding word is Z; the variables in these templates are instantiated to particular words and\ntags to create \u201crules\u201d; the score for a rule is the number of broken examples it corrects minus the\nnumber of correct cases it breaks; apart from training a tagger, the demonstration displays residual\nerrors.\n>>> nltk.tag.brill.demo()\nTraining Brill tagger on 80 sentences...\nFinding initial useful rules...\n    Found 6555 useful rules.\n           B      |\n   S   F   r   O  |        Score = Fixed - Broken\n   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n   e   d   n   r  |  e\n------------------+-------------------------------------------------------\n  12  13   1   4  | NN -> VB if the tag of the preceding word is 'TO'\n   8   9   1  23  | NN -> VBD if the tag of the following word is 'DT'\n   8   8   0   9  | NN -> VBD if the tag of the preceding word is 'NNS'\n   6   9   3  16  | NN -> NNP if the tag of words i-2...i-1 is '-NONE-'\n   5   8   3   6  | NN -> NNP if the tag of the following word is 'NNP'\n   5   6   1   0  | NN -> NNP if the text of words i-2...i-1 is 'like'\n   5   5   0   3  | NN -> VBN if the text of the following word is '*-1'\n   ...\n>>> print(open(\"errors.out\").read())\n             left context |    word/test->gold     | right context\n--------------------------+------------------------+--------------------------\n                          |      Then/NN->RB       | ,/, in/IN the/DT guests/N\n, in/IN the/DT guests/NNS |       '/VBD->POS       | honor/NN ,/, the/DT speed\n'/POS honor/NN ,/, the/DT |    speedway/JJ->NN     | hauled/VBD out/RP four/CD\nNN ,/, the/DT speedway/NN |     hauled/NN->VBD     | out/RP four/CD drivers/NN\nDT speedway/NN hauled/VBD |      out/NNP->RP       | four/CD drivers/NNS ,/, c\ndway/NN hauled/VBD out/RP |      four/NNP->CD      | drivers/NNS ,/, crews/NNS\nhauled/VBD out/RP four/CD |    drivers/NNP->NNS    | ,/, crews/NNS and/CC even\nP four/CD drivers/NNS ,/, |     crews/NN->NNS      | and/CC even/RB the/DT off\nNNS and/CC even/RB the/DT |    official/NNP->JJ    | Indianapolis/NNP 500/CD a\n                          |     After/VBD->IN      | the/DT race/NN ,/, Fortun\nter/IN the/DT race/NN ,/, |    Fortune/IN->NNP     | 500/CD executives/NNS dro\ns/NNS drooled/VBD like/IN |  schoolboys/NNP->NNS   | over/IN the/DT cars/NNS a\nolboys/NNS over/IN the/DT |      cars/NN->NNS      | and/CC drivers/NNS ./.\n5.7  How to Determine the Category of a Word\nNow that we have examined word classes in detail, we turn to a more basic question:\nhow do we decide what category a word belongs to in the first place? In general, linguists\nuse morphological, syntactic, and semantic clues to determine the category of a word.\n210 | Chapter 5: \u2002Categorizing and Tagging Words\nMorphological Clues\nThe internal \nstructure of a word may give useful clues as to the word\u2019s category. For\nexample, -ness is a suffix that combines with an adjective to produce a noun, e.g., happy\n\u2192 happiness, ill \u2192 illness. So if we encounter a word that ends in -ness, this is very likely\nto be a noun. Similarly, -ment is a suffix that combines with some verbs to produce a\nnoun, e.g., govern \u2192 government and establish \u2192 establishment.\nEnglish verbs can also be morphologically complex. For instance, the present par-\nticiple of a verb ends in -ing, and expresses the idea of ongoing, incomplete action (e.g.,\nfalling, eating). The -ing suffix also appears on nouns derived from verbs, e.g., the falling\nof the leaves (this is known as the gerund).\nSyntactic Clues\nAnother source of information is the typical contexts in which a word can occur. For\nexample, assume that we have already determined the category of nouns. Then we\nmight say that a syntactic criterion for an adjective in English is that it can occur im-\nmediately before a noun, or immediately following the words be or very. According to\nthese tests, near should be categorized as an adjective:\n(2) a. the near window\nb. The end is (very) near.\nSemantic Clues\nFinally, the meaning of a word is a useful clue as to its lexical category. For example,\nthe best-known definition of a noun is semantic: \u201cthe name of a person, place, or thing.\u201d\nWithin modern linguistics, semantic criteria for word classes are treated with suspicion,\nmainly because they are hard to formalize. Nevertheless, semantic criteria underpin\nmany of our intuitions about word classes, and enable us to make a good guess about\nthe categorization of words in languages with which we are unfamiliar. For example,\nif all we know about the Dutch word verjaardag is that it means the same as the English\nword birthday, then we can guess that verjaardag is a noun in Dutch. However, some\ncare is needed: although we might translate zij is vandaag jarig  as it\u2019s her birthday to-\nday, the word jarig is in fact an adjective in Dutch, and has no exact equivalent in\nEnglish.\nNew Words\nAll languages acquire new lexical items. A list of words recently added to the Oxford\nDictionary of English includes cyberslacker, fatoush, blamestorm, SARS, cantopop,\nbupkis, noughties, muggle, and robata. Notice that all these new words are nouns, and\nthis is reflected in calling nouns an open class . By contrast, prepositions are regarded\nas a closed class. That is, there is a limited set of words belonging to the class (e.g.,\nabove, along, at, below, beside, between, during, for, from, in, near, on, outside, over,\n5.7  How to Determine the Category of a Word | 211\npast, through, towards, under, up, with), and membership of the set only changes very\ngradually over time.\nMorphology in Part-of-Speech Tagsets\nCommon tagsets \noften capture some morphosyntactic information, that is, informa-\ntion about the kind of morphological markings that words receive by virtue of their\nsyntactic role. Consider, for example, the selection of distinct grammatical forms of the\nword go illustrated in the following sentences:\n(3) a. Go away!\nb. He sometimes goes to the cafe.\nc. All the cakes have gone.\nd. We went on the excursion.\nEach of these forms\u2014 go, goes, gone, and went\u2014is morphologically distinct from the\nothers. Consider the form goes. This occurs in a restricted set of grammatical contexts,\nand requires a third person singular subject. Thus, the following sentences are\nungrammatical.\n(4) a. *They sometimes goes to the cafe.\nb. *I sometimes goes to the cafe.\nBy contrast, gone is the past participle form; it is required after have (and cannot be\nreplaced in this context by goes), and cannot occur as the main verb of a clause.\n(5) a. *All the cakes have goes.\nb. *He sometimes gone to the cafe.\nWe can easily imagine a tagset in which the four distinct grammatical forms just dis-\ncussed were all tagged as VB. Although this would be adequate for some purposes, a\nmore fine-grained tagset provides useful information about these forms that can help\nother processors that try to detect patterns in tag sequences. The Brown tagset captures\nthese distinctions, as summarized in Table 5-7.\nTable 5-7. Some morphosyntactic distinctions in the Brown tagset\nForm Category Tag\ngo base VB\ngoes third singular present VBZ\ngone past participle VBN\ngoing gerund VBG\nwent simple past VBD\n212 | Chapter 5: \u2002Categorizing and Tagging Words\nIn addition to this set of verb tags, the various forms of the verb to be  have special tags:\nbe/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED, and was/BEDZ (plus extra\ntags for negative forms of the verb). All told, this fine-grained tagging of verbs means\nthat an automatic tagger that uses this tagset is effectively carrying out a limited amount\nof morphological analysis.\nMost part-of-speech tagsets make use of the same basic categories, such as noun, verb,\nadjective, and preposition. However, tagsets differ both in how finely they divide words\ninto categories, and in how they define their categories. For example, is might be tagged\nsimply as a verb in one tagset, but as a distinct form of the lexeme be  in another tagset\n(as in the Brown Corpus). This variation in tagsets is unavoidable, since part-of-speech\ntags are used in different ways for different tasks. In other words, there is no one \u201cright\nway\u201d to assign tags, only more or less useful ways depending on one\u2019s goals.\n5.8  Summary\n\u2022 Words can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\nThese classes are known as lexical categories or parts-of-speech. Parts-of-speech\nare assigned short labels, or tags, such as NN and VB.\n\u2022 The process of automatically assigning parts-of-speech to words in text is called\npart-of-speech tagging, POS tagging, or just tagging.\n\u2022 Automatic tagging is an important step in the NLP pipeline, and is useful in a variety\nof situations, including predicting the behavior of previously unseen words, ana-\nlyzing word usage in corpora, and text-to-speech systems.\n\u2022 Some linguistic corpora, such as the Brown Corpus, have been POS tagged.\n\u2022 A variety of tagging methods are possible, e.g., default tagger, regular expression\ntagger, unigram tagger, and n-gram taggers. These can be combined using a tech-\nnique known as backoff.\n\u2022 Taggers can be trained and evaluated using tagged corpora.\n\u2022 Backoff is a method for combining models: when a more specialized model (such\nas a bigram tagger) cannot assign a tag in a given context, we back off to a more\ngeneral model (such as a unigram tagger).\n\u2022 Part-of-speech tagging is an important, early example of a sequence classification\ntask in NLP: a classification decision at any one point in the sequence makes use\nof words and tags in the local context.\n\u2022 A dictionary is used to map between arbitrary types of information, such as a string\nand a number: freq['cat'] = 12. We create dictionaries using the brace notation:\npos = {}, pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}.\n\u2022 N-gram taggers can be defined for large values of n, but once n is larger than 3, we\nusually encounter the sparse data problem; even with a large quantity of training\ndata, we see only a tiny fraction of possible contexts.\n5.8  Summary | 213\n\u2022 Transformation-based tagging involves learning a series of repair rules of the form\n\u201cchange tag s to tag t in context c,\u201d where each rule fixes mistakes and possibly\nintroduces a (smaller) number of errors.\n5.9  Further Reading\nExtra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. For more examples of tagging with NLTK, please\nsee the Tagging HOWTO at http://www.nltk.org/howto. Chapters 4 and 5 of (Jurafsky\n& Martin, 2008) contain more advanced material on n-grams and part-of-speech tag-\nging. Other approaches to tagging involve machine learning methods ( Chapter 6 ). In\nChapter 7, we will see a generalization of tagging called chunking in which a contiguous\nsequence of words is assigned a single tag.\nFor tagset documentation, see nltk.help.upenn_tagset() and nltk.help.brown_tag\nset(). Lexical categories are introduced in linguistics textbooks, including those listed\nin Chapter 1 of this book.\nThere are many other kinds of tagging. Words can be tagged with directives to a speech\nsynthesizer, indicating which words should be emphasized. Words can be tagged with\nsense numbers, indicating which sense of the word was used. Words can also be tagged\nwith morphological features. Examples of each of these kinds of tags are shown in the\nfollowing list. For space reasons, we only show the tag for a single word. Note also that\nthe first two examples use XML-style tags, where elements in angle brackets enclose\nthe word that is tagged.\nSpeech Synthesis Markup Language (W3C SSML)\nThat is a <emphasis>big</emphasis> car!\nSemCor: Brown Corpus tagged with WordNet senses\nSpace in any <wf pos=\"NN\" lemma=\"form\" wnsn=\"4\">form</wf> is completely meas\nured by the three dimensions.  (Wordnet form/nn sense 4: \u201cshape, form, config-\nuration, contour, conformation\u201d)\nMorphological tagging, from the Turin University Italian Treebank\nE' italiano , come progetto e realizzazione , il primo (PRIMO ADJ ORDIN M\nSING) porto turistico dell' Albania .\nNote that tagging is also performed at higher levels. Here is an example of dialogue act\ntagging, from the NPS Chat Corpus (Forsyth & Martell, 2007) included with NLTK.\nEach turn of the dialogue is categorized as to its communicative function:\nStatement  User117 Dude..., I wanted some of that\nynQuestion User120 m I missing something?\nBye        User117 I'm gonna go fix food, I'll be back later.\nSystem     User122 JOIN\nSystem     User2   slaps User122 around a bit with a large trout.\nStatement  User121 18/m pm me if u tryin to chat\n214 | Chapter 5: \u2002Categorizing and Tagging Words\n5.10  Exercises\n1.\u25cb Search the \nWeb for \u201cspoof newspaper headlines,\u201d to find such gems as: British\nLeft Waffles on Falkland Islands , and Juvenile Court to Try Shooting Defendant .\nManually tag these headlines to see whether knowledge of the part-of-speech tags\nremoves the ambiguity.\n2.\u25cb Working with someone else, take turns picking a word that can be either a noun\nor a verb (e.g., contest); the opponent has to predict which one is likely to be the\nmost frequent in the Brown Corpus. Check the opponent\u2019s prediction, and tally\nthe score over several turns.\n3.\u25cb Tokenize and tag the following sentence: They wind back the clock, while we\nchase after the wind . What different pronunciations and parts-of-speech are\ninvolved?\n4.\u25cb Review the mappings in Table 5-4 . Discuss any other examples of mappings you\ncan think of. What type of information do they map from and to?\n5.\u25cb Using the Python interpreter in interactive mode, experiment with the dictionary\nexamples in this chapter. Create a dictionary d, and add some entries. What hap-\npens whether you try to access a non-existent entry, e.g., d['xyz']?\n6.\u25cb Try deleting an element from a dictionary d, using the syntax del d['abc']. Check\nthat the item was deleted.\n7.\u25cb Create two dictionaries, d1 and d2, and add some entries to each. Now issue the\ncommand d1.update(d2). What did this do? What might it be useful for?\n8.\u25cb Create a dictionary e, to represent a single lexical entry for some word of your\nchoice. Define keys such as headword, part-of-speech, sense, and example, and as-\nsign them suitable values.\n9.\u25cb Satisfy yourself that there are restrictions on the distribution of go and went, in\nthe sense that they cannot be freely interchanged in the kinds of contexts illustrated\nin (3), Section 5.7.\n10.\u25cb Train a unigram tagger and run it on some new text. Observe that some words\nare not assigned a tag. Why not?\n11.\u25cb Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger\nand run it on some new text. Experiment with different settings for the affix length\nand the minimum word length. Discuss your findings.\n12.\u25cb Train a bigram tagger with no backoff tagger, and run it on some of the training\ndata. Next, run it on some new data. What happens to the performance of the\ntagger? Why?\n13.\u25cb We can use a dictionary to specify the values to be substituted into a formatting\nstring. Read Python\u2019s library documentation for formatting strings ( http://docs.py\n5.10  Exercises | 215\nthon.org/lib/typesseq-strings.html) and use this method to display today\u2019s date in\ntwo different formats.\n14.\u25d1 Use sorted() and set() to get a sorted list of tags used in the Brown Corpus,\nremoving duplicates.\n15.\u25d1 Write programs to process the Brown Corpus and find answers to the following\nquestions:\na. Which nouns are more common in their plural form, rather than their singular\nform? (Only consider regular plurals, formed with the -s suffix.)\nb. Which word has the greatest number of distinct tags? What are they, and what\ndo they represent?\nc. List tags in order of decreasing frequency. What do the 20 most frequent tags\nrepresent?\nd. Which tags are nouns most commonly found after? What do these tags\nrepresent?\n16.\u25d1 Explore the following issues that arise in connection with the lookup tagger:\na. What happens to the tagger performance for the various model sizes when a\nbackoff tagger is omitted?\nb. Consider the curve in Figure 5-4 ; suggest a good size for a lookup tagger that\nbalances memory and performance. Can you come up with scenarios where it\nwould be preferable to minimize memory usage, or to maximize performance\nwith no regard for memory usage?\n17.\u25d1 What is the upper limit of performance for a lookup tagger, assuming no limit\nto the size of its table? (Hint: write a program to work out what percentage of tokens\nof a word are assigned the most likely tag for that word, on average.)\n18.\u25d1 Generate some statistics for tagged data to answer the following questions:\na. What proportion of word types are always assigned the same part-of-speech\ntag?\nb. How many words are ambiguous, in the sense that they appear with at least\ntwo tags?\nc. What percentage of word tokens in the Brown Corpus involve these ambiguous\nwords?\n19.\u25d1 The evaluate() method works out how accurately the tagger performs on this\ntext. For example, if the supplied tagged text was [('the', 'DT'), ('dog',\n'NN')] and the tagger produced the output [('the', 'NN'), ('dog', 'NN')], then\nthe score would be 0.5. Let\u2019s try to figure out how the evaluation method works:\na. A tagger t takes a list of words as input, and produces a list of tagged words\nas output. However, t.evaluate() is given correctly tagged text as its only\nparameter. What must it do with this input before performing the tagging?\n216 | Chapter 5: \u2002Categorizing and Tagging Words\nb. Once the tagger has created newly tagged text, how might the evaluate()\nmethod \ngo about comparing it with the original tagged text and computing\nthe accuracy score?\nc. Now examine the source code to see how the method is implemented. Inspect\nnltk.tag.api.__file__ to discover the location of the source code, and open\nthis file using an editor (be sure to use the api.py file and not the compiled\napi.pyc binary file).\n20.\u25d1 Write code to search the Brown Corpus for particular words and phrases ac-\ncording to tags, to answer the following questions:\na. Produce an alphabetically sorted list of the distinct words tagged as MD.\nb. Identify words that can be plural nouns or third person singular verbs (e.g.,\ndeals, flies).\nc. Identify three-word prepositional phrases of the form IN + DET + NN (e.g.,\nin the lab).\nd. What is the ratio of masculine to feminine pronouns?\n21.\u25d1 In Table 3-1 , we saw a table involving frequency counts for the verbs adore, love,\nlike, and prefer, and preceding qualifiers such as really. Investigate the full range\nof qualifiers (Brown tag QL) that appear before these four verbs.\n22.\u25d1 We defined the regexp_tagger that can be used as a fall-back tagger for unknown\nwords. This tagger only checks for cardinal numbers. By testing for particular prefix\nor suffix strings, it should be possible to guess other tags. For example, we could\ntag any word that ends with -s as a plural noun. Define a regular expression tagger\n(using RegexpTagger()) that tests for at least five other patterns in the spelling of\nwords. (Use inline documentation to explain the rules.)\n23.\u25d1 Consider the regular expression tagger developed in the exercises in the previous\nsection. Evaluate the tagger using its accuracy() method, and try to come up with\nways to improve its performance. Discuss your findings. How does objective eval-\nuation help in the development process?\n24.\u25d1 How serious is the sparse data problem? Investigate the performance of n-gram\ntaggers as n increases from 1 to 6. Tabulate the accuracy score. Estimate the training\ndata required for these taggers, assuming a vocabulary size of 105 and a tagset size\nof 102.\n25.\u25d1 Obtain some tagged data for another language, and train and evaluate a variety\nof taggers on it. If the language is morphologically complex, or if there are any\northographic clues (e.g., capitalization) to word classes, consider developing a reg-\nular expression tagger for it (ordered after the unigram tagger, and before the de-\nfault tagger). How does the accuracy of your tagger(s) compare with the same\ntaggers run on English data? Discuss any issues you encounter in applying these\nmethods to the language.\n5.10  Exercises | 217\n26.\u25d1 Example 5-4  plotted a curve showing change in the performance of a lookup\ntagger as the model size was increased. Plot the performance curve for a unigram\ntagger, as the amount of training data is varied.\n27.\u25d1 Inspect the confusion matrix for the bigram tagger t2 defined in Section 5.5, and\nidentify one or more sets of tags to collapse. Define a dictionary to do the mapping,\nand evaluate the tagger on the simplified data.\n28.\u25d1 Experiment with taggers using the simplified tagset (or make one of your own\nby discarding all but the first character of each tag name). Such a tagger has fewer\ndistinctions to make, but much less information on which to base its work. Discuss\nyour findings.\n29.\u25d1 Recall the example of a bigram tagger which encountered a word it hadn\u2019t seen\nduring training, and tagged the rest of the sentence as None. It is possible for a\nbigram tagger to fail partway through a sentence even if it contains no unseen words\n(even if the sentence was used during training). In what circumstance can this\nhappen? Can you write a program to find some examples of this?\n30.\u25d1 Preprocess the Brown News data by replacing low-frequency words with UNK,\nbut leaving the tags untouched. Now train and evaluate a bigram tagger on this\ndata. How much does this help? What is the contribution of the unigram tagger\nand default tagger now?\n31.\u25d1 Modify the program in Example 5-4  to use a logarithmic scale on the x-axis, by\nreplacing pylab.plot() with pylab.semilogx(). What do you notice about the\nshape of the resulting plot? Does the gradient tell you anything?\n32.\u25d1 Consult the documentation for the Brill tagger demo function, using\nhelp(nltk.tag.brill.demo). Experiment with the tagger by setting different values\nfor the parameters. Is there any trade-off between training time (corpus size) and\nperformance?\n33.\u25d1 Write code that builds a dictionary of dictionaries of sets. Use it to store the set\nof POS tags that can follow a given word having a given POS tag, i.e., word i \u2192 tagi \u2192\ntagi+1.\n34.\u25cf There are 264 distinct words in the Brown Corpus having exactly three possible\ntags.\na. Print a table with the integers 1..10 in one column, and the number of distinct\nwords in the corpus having 1..10 distinct tags in the other column.\nb. For the word with the greatest number of distinct tags, print out sentences\nfrom the corpus containing the word, one for each possible tag.\n35.\u25cf Write a program to classify contexts involving the word must according to the\ntag of the following word. Can this be used to discriminate between the epistemic\nand deontic uses of must?\n36.\u25cf Create a regular expression tagger and various unigram and n-gram taggers,\nincorporating backoff, and train them on part of the Brown Corpus.\n218 | Chapter 5: \u2002Categorizing and Tagging Words\na. Create three different combinations of the taggers. Test the accuracy of each\ncombined tagger. Which combination works best?\nb.\nTry varying the size of the training corpus. How does it affect your results?\n37.\u25cf Our approach for tagging an unknown word has been to consider the letters of\nthe word (using RegexpTagger()), or to ignore the word altogether and tag it as a\nnoun (using nltk.DefaultTagger()). These methods will not do well for texts hav-\ning new words that are not nouns. Consider the sentence I like to blog on Kim\u2019s\nblog. If blog is a new word, then looking at the previous tag ( TO versus NP$) would\nprobably be helpful, i.e., we need a default tagger that is sensitive to the preceding\ntag.\na. Create a new kind of unigram tagger that looks at the tag of the previous word,\nand ignores the current word. (The best way to do this is to modify the source\ncode for UnigramTagger(), which presumes knowledge of object-oriented pro-\ngramming in Python.)\nb. Add this tagger to the sequence of backoff taggers (including ordinary trigram\nand bigram taggers that look at words), right before the usual default tagger.\nc. Evaluate the contribution of this new unigram tagger.\n38.\u25cf Consider the code in Section 5.5 , which determines the upper bound for accuracy\nof a trigram tagger. Review Abney\u2019s discussion concerning the impossibility of\nexact tagging (Abney, 2006). Explain why correct tagging of these examples re-\nquires access to other kinds of information than just words and tags. How might\nyou estimate the scale of this problem?\n39.\u25cf Use some of the estimation techniques in nltk.probability, such as Lidstone or\nLaplace estimation, to develop a statistical tagger that does a better job than n-\ngram backoff taggers in cases where contexts encountered during testing were not\nseen during training.\n40.\u25cf Inspect the diagnostic files created by the Brill tagger rules.out and\nerrors.out. Obtain the demonstration code by accessing the source code (at http:\n//www.nltk.org/code) and create your own version of the Brill tagger. Delete some\nof the rule templates, based on what you learned from inspecting rules.out. Add\nsome new rule templates which employ contexts that might help to correct the\nerrors you saw in errors.out.\n41.\u25cf Develop an n-gram backoff tagger that permits \u201canti-n-grams\u201d such as [\"the\",\n\"the\"] to be specified when a tagger is initialized. An anti-n-gram is assigned a\ncount of zero and is used to prevent backoff for this n-gram (e.g., to avoid esti-\nmating P(the | the) as just P(the)).\n42.\u25cf Investigate three different ways to define the split between training and testing\ndata when developing a tagger using the Brown Corpus: genre ( category), source\n(fileid), and sentence. Compare their relative performance and discuss which\nmethod is the most legitimate. (You might use n-fold cross validation, discussed\nin Section 6.3, to improve the accuracy of the evaluations.)\n5.10  Exercises | 219\n\nCHAPTER 6\nLearning to Classify Text\nDetecting patterns is a central part of Natural Language Processing. Words ending in\n-ed tend \nto be past tense verbs ( Chapter 5 ). Frequent use of will is indicative of news\ntext ( Chapter 3 ). These observable patterns\u2014word structure and word frequency\u2014\nhappen to correlate with particular aspects of meaning, such as tense and topic. But\nhow did we know where to start looking, which aspects of form to associate with which\naspects of meaning?\nThe goal of this chapter is to answer the following questions:\n1. How can we identify particular features of language data that are salient for clas-\nsifying it?\n2. How can we construct models of language that can be used to perform language\nprocessing tasks automatically?\n3. What can we learn about language from these models?\nAlong the way we will study some important machine learning techniques, including\ndecision trees, naive Bayes classifiers, and maximum entropy classifiers. We will gloss\nover the mathematical and statistical underpinnings of these techniques, focusing in-\nstead on how and when to use them (see Section 6.9  for more technical background).\nBefore looking at these methods, we first need to appreciate the broad scope of this\ntopic.\n6.1  Supervised Classification\nClassification is the task of choosing the correct class label  for a given input. In basic\nclassification tasks, each input is considered in isolation from all other inputs, and the\nset of labels is defined in advance. Some examples of classification tasks are:\n221\n\u2022 Deciding whether an email is spam or not.\n\u2022 Deciding what \nthe topic of a news article is, from a fixed list of topic areas such as\n\u201csports,\u201d \u201ctechnology,\u201d and \u201cpolitics.\u201d\n\u2022 Deciding whether a given occurrence of the word bank is used to refer to a river\nbank, a financial institution, the act of tilting to the side, or the act of depositing\nsomething in a financial institution.\nThe basic classification task has a number of interesting variants. For example, in multi-\nclass classification, each instance may be assigned multiple labels; in open-class clas-\nsification, the set of labels is not defined in advance; and in sequence classification, a\nlist of inputs are jointly classified.\nA classifier is called supervised if it is built based on training corpora containing the\ncorrect label for each input. The framework used by supervised classification is shown\nin Figure 6-1.\nFigure 6-1. Supervised classification. (a) During training, a feature extractor is used to convert each\ninput value \nto a feature set. These feature sets, which capture the basic information about each input\nthat should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are\nfed into the machine learning algorithm to generate a model. (b) During prediction, the same feature\nextractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model,\nwhich generates predicted labels.\nIn the rest of this section, we will look at how classifiers can be employed to solve a\nwide variety of tasks. Our discussion is not intended to be comprehensive, but to give\na representative sample of tasks that can be performed with the help of text classifiers.\nGender Identification\nIn Section 2.4, we saw that male and female names have some distinctive characteristics.\nNames ending in a, e, and i are likely to be female, while names ending in k, o, r, s, and\nt are likely to be male. Let\u2019s build a classifier to model these differences more precisely.\n222 | Chapter 6: \u2002Learning to Classify Text\nThe first step in creating a classifier is deciding what features of the input are relevant,\nand how to encode those features. For this example, we\u2019ll start by just looking at the\nfinal letter of a given name. The following feature extractor function builds a dic-\ntionary containing relevant information about a given name:\n>>> def gender_features(word):\n...     return {'last_letter': word[-1]}\n>>> gender_features('Shrek')\n{'last_letter': 'k'}\nThe dictionary that is returned by this function is called a feature set  and maps from\nfeatures\u2019 names to their values. Feature names are case-sensitive strings that typically\nprovide a short human-readable description of the feature. Feature values are values\nwith simple types, such as Booleans, numbers, and strings.\nMost classification methods require that features be encoded using sim-\nple value \ntypes, such as Booleans, numbers, and strings. But note that\njust because a feature has a simple type, this does not necessarily mean\nthat the feature\u2019s value is simple to express or compute; indeed, it is\neven possible to use very complex and informative values, such as the\noutput of a second supervised classifier, as features.\nNow that we\u2019ve defined a feature extractor, we need to prepare a list of examples and\ncorresponding class labels:\n>>> from nltk.corpus import names\n>>> import random\n>>> names = ([(name, 'male') for name in names.words('male.txt')] +\n...          [(name, 'female') for name in names.words('female.txt')])\n>>> random.shuffle(names)\nNext, we use the feature extractor to process the names data, and divide the resulting\nlist of feature sets into a training set  and a test set . The training set is used to train a\nnew \u201cnaive Bayes\u201d classifier.\n>>> featuresets = [(gender_features(n), g) for (n,g) in names]\n>>> train_set, test_set = featuresets[500:], featuresets[:500]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\nWe will learn more about the naive Bayes classifier later in the chapter. For now, let\u2019s\njust test it out on some names that did not appear in its training data:\n>>> classifier.classify(gender_features('Neo'))\n'male'\n>>> classifier.classify(gender_features('Trinity'))\n'female'\nObserve that these character names from The Matrix  are correctly classified. Although\nthis science fiction movie is set in 2199, it still conforms with our expectations about\nnames and genders. We can systematically evaluate the classifier on a much larger\nquantity of unseen data:\n6.1  Supervised Classification | 223\n>>> print nltk.classify.accuracy(classifier, test_set)\n0.758\nFinally, we \ncan examine the classifier to determine which features it found most effec-\ntive for distinguishing the names\u2019 genders:\n>>> classifier.show_most_informative_features(5)\nMost Informative Features\n             last_letter = 'a'            female : male   =     38.3 : 1.0\n             last_letter = 'k'              male : female =     31.4 : 1.0\n             last_letter = 'f'              male : female =     15.3 : 1.0\n             last_letter = 'p'              male : female =     10.6 : 1.0\n             last_letter = 'w'              male : female =     10.6 : 1.0\nThis listing shows that the names in the training set that end in a are female 38 times\nmore often than they are male, but names that end in k are male 31 times more often\nthan they are female. These ratios are known as likelihood ratios , and can be useful\nfor comparing different feature-outcome relationships.\nYour Turn:  Modify the gender_features() function to provide the clas-\nsifier with features encoding the length of the name, its first letter, and\nany other features that seem like they might be informative. Retrain the\nclassifier with these new features, and test its accuracy.\nWhen working with large corpora, constructing a single list that contains the features\nof every instance can use up a large amount of memory. In these cases, use the function\nnltk.classify.apply_features, which returns an object that acts like a list but does not\nstore all the feature sets in memory:\n>>> from nltk.classify import apply_features\n>>> train_set = apply_features(gender_features, names[500:])\n>>> test_set = apply_features(gender_features, names[:500])\nChoosing the Right Features\nSelecting relevant features and deciding how to encode them for a learning method can\nhave an enormous impact on the learning method\u2019s ability to extract a good model.\nMuch of the interesting work in building a classifier is deciding what features might be\nrelevant, and how we can represent them. Although it\u2019s often possible to get decent\nperformance by using a fairly simple and obvious set of features, there are usually sig-\nnificant gains to be had by using carefully constructed features based on a thorough\nunderstanding of the task at hand.\nTypically, feature extractors are built through a process of trial-and-error, guided by\nintuitions about what information is relevant to the problem. It\u2019s common to start with\na \u201ckitchen sink\u201d approach, including all the features that you can think of, and then\nchecking to see which features actually are helpful. We take this approach for name\ngender features in Example 6-1.\n224 | Chapter 6: \u2002Learning to Classify Text\nExample 6-1. A feature extractor that overfits gender features. The featuresets returned by this feature\nextractor contain \na large number of specific features, leading to overfitting for the relatively small\nNames Corpus.\ndef gender_features2(name):\n    features = {}\n    features[\"firstletter\"] = name[0].lower()\n    features[\"lastletter\"] = name[\u20131].lower()\n    for letter in 'abcdefghijklmnopqrstuvwxyz':\n        features[\"count(%s)\" % letter] = name.lower().count(letter)\n        features[\"has(%s)\" % letter] = (letter in name.lower())\n    return features\n>>> gender_features2('John') \n{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}\nHowever, there are usually limits to the number of features that you should use with a\ngiven learning algorithm\u2014if you provide too many features, then the algorithm will\nhave a higher chance of relying on idiosyncrasies of your training data that don\u2019t gen-\neralize well to new examples. This problem is known as overfitting, and can be espe-\ncially problematic when working with small training sets. For example, if we train a\nnaive Bayes classifier using the feature extractor shown in Example 6-1 , it will overfit\nthe relatively small training set, resulting in a system whose accuracy is about 1% lower\nthan the accuracy of a classifier that only pays attention to the final letter of each name:\n>>> featuresets = [(gender_features2(n), g) for (n,g) in names]\n>>> train_set, test_set = featuresets[500:], featuresets[:500]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> print nltk.classify.accuracy(classifier, test_set)\n0.748\nOnce an initial set of features has been chosen, a very productive method for refining\nthe feature set is error analysis. First, we select a development set, containing the\ncorpus data for creating the model. This development set is then subdivided into the\ntraining set and the dev-test set.\n>>> train_names = names[1500:]\n>>> devtest_names = names[500:1500]\n>>> test_names = names[:500]\nThe training set is used to train the model, and the dev-test set is used to perform error\nanalysis. The test set serves in our final evaluation of the system. For reasons discussed\nlater, it is important that we employ a separate dev-test set for error analysis, rather\nthan just using the test set. The division of the corpus data into different subsets is\nshown in Figure 6-2.\nHaving divided the corpus into appropriate datasets, we train a model using the training\nset \n , and then run it on the dev-test set \n .\n>>> train_set = [(gender_features(n), g) for (n,g) in train_names]\n>>> devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n>>> test_set = [(gender_features(n), g) for (n,g) in test_names]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set) \n6.1  Supervised Classification | 225\n>>> print nltk.classify.accuracy(classifier, devtest_set) \n0.765\nFigure 6-2. Organization of corpus data for training supervised classifiers. The corpus data is divided\ninto two \nsets: the development set and the test set. The development set is often further subdivided into\na training set and a dev-test set.\nUsing the dev-test set, we can generate a list of the errors that the classifier makes when\npredicting name genders:\n>>> errors = []\n>>> for (name, tag) in devtest_names:\n...     guess = classifier.classify(gender_features(name))\n...     if guess != tag:\n...         errors.append( (tag, guess, name) )\nWe can then examine individual error cases where the model predicted the wrong label,\nand try to determine what additional pieces of information would allow it to make the\nright decision (or which existing pieces of information are tricking it into making the\nwrong decision). The feature set can then be adjusted accordingly. The names classifier\nthat we have built generates about 100 errors on the dev-test corpus:\n>>> for (tag, guess, name) in sorted(errors): # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE \n...     print 'correct=%-8s guess=%-8s name=%-30s' % \n(tag, guess, name)\n       ...\ncorrect=female   guess=male     name=Cindelyn\n       ...\ncorrect=female   guess=male     name=Katheryn\ncorrect=female   guess=male     name=Kathryn\n       ...\ncorrect=male     guess=female   name=Aldrich\n       ...\ncorrect=male     guess=female   name=Mitch\n       ...\ncorrect=male     guess=female   name=Rich\n       ...\n226 | Chapter 6: \u2002Learning to Classify Text\nLooking through this list of errors makes it clear that some suffixes that are more than\none letter \ncan be indicative of name genders. For example, names ending in yn appear\nto be predominantly female, despite the fact that names ending in n tend to be male;\nand names ending in ch are usually male, even though names that end in h tend to be\nfemale. We therefore adjust our feature extractor to include features for two-letter\nsuffixes:\n>>> def gender_features(word):\n...     return {'suffix1': word[-1:],\n...             'suffix2': word[-2:]}\nRebuilding the classifier with the new feature extractor, we see that the performance\non the dev-test dataset improves by almost three percentage points (from 76.5% to\n78.2%):\n>>> train_set = [(gender_features(n), g) for (n,g) in train_names]\n>>> devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> print nltk.classify.accuracy(classifier, devtest_set)\n0.782\nThis error analysis procedure can then be repeated, checking for patterns in the errors\nthat are made by the newly improved classifier. Each time the error analysis procedure\nis repeated, we should select a different dev-test/training split, to ensure that the clas-\nsifier does not start to reflect idiosyncrasies in the dev-test set.\nBut once we\u2019ve used the dev-test set to help us develop the model, we can no longer\ntrust that it will give us an accurate idea of how well the model would perform on new\ndata. It is therefore important to keep the test set separate, and unused, until our model\ndevelopment is complete. At that point, we can use the test set to evaluate how well\nour model will perform on new input values.\nDocument Classification\nIn Section 2.1 , we saw several examples of corpora where documents have been labeled\nwith categories. Using these corpora, we can build classifiers that will automatically\ntag new documents with appropriate category labels. First, we construct a list of docu-\nments, labeled with the appropriate categories. For this example, we\u2019ve chosen the\nMovie Reviews Corpus, which categorizes each review as positive or negative.\n>>> from nltk.corpus import movie_reviews\n>>> documents = [(list(movie_reviews.words(fileid)), category)\n...              for category in movie_reviews.categories()\n...              for fileid in movie_reviews.fileids(category)]\n>>> random.shuffle(documents)\nNext, we define a feature extractor for documents, so the classifier will know which\naspects of the data it should pay attention to (see Example 6-2). For document topic\nidentification, we can define a feature for each word, indicating whether the document\ncontains that word. To limit the number of features that the classifier needs to process,\nwe begin by constructing a list of the 2,000 most frequent words in the overall\n6.1  Supervised Classification | 227\ncorpus \n . We can then define a feature extractor \n  that simply checks whether each\nof these words is present in a given document.\nExample 6-2. \nA feature extractor for document classification, whose features indicate whether or not\nindividual words are present in a given document.\nall_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\nword_features = all_words.keys()[:2000] \ndef document_features(document): \n    document_words = set(document) \n    features = {}\n    for word in word_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features\n>>> print document_features(movie_reviews.words('pos/cv957_8737.txt')) \n{'contains(waste)': False, 'contains(lot)': False, ...}\nWe compute the set of all words in a document in \n , rather than just\nchecking if word in document\n, because checking whether a word occurs\nin a set is much faster than checking whether it occurs in a list (see\nSection 4.7).\nNow that we\u2019ve defined our feature extractor, we can use it to train a classifier to label\nnew movie reviews (Example 6-3 ). To check how reliable the resulting classifier is, we\ncompute its accuracy on the test set \n . And once again, we can use show_most_infor\nmative_features() to \nfind out which features the classifier found to be most\ninformative \n .\nExample 6-3. Training and testing a classifier for document classification.\nfeaturesets = [(document_features(d), c) for (d,c) in documents]\ntrain_set, test_set = featuresets[100:], featuresets[:100]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> print nltk.classify.accuracy(classifier, test_set) \n0.81\n>>> classifier.show_most_informative_features(5) \nMost Informative Features\n   contains(outstanding) = True               pos : neg   =     11.1 : 1.0\n        contains(seagal) = True               neg : pos   =      7.7 : 1.0\n   contains(wonderfully) = True               pos : neg   =      6.8 : 1.0\n         contains(damon) = True               pos : neg   =      5.9 : 1.0\n        contains(wasted) = True               neg : pos   =      5.8 : 1.0\nApparently in this corpus, a review that mentions Seagal is almost 8 times more likely\nto be \nnegative than positive, while a review that mentions Damon is about 6 times more\nlikely to be positive.\n228 | Chapter 6: \u2002Learning to Classify Text\nPart-of-Speech Tagging\nIn Chapter 5 , we \nbuilt a regular expression tagger that chooses a part-of-speech tag for\na word by looking at the internal makeup of the word. However, this regular expression\ntagger had to be handcrafted. Instead, we can train a classifier to work out which suf-\nfixes are most informative. Let\u2019s begin by finding the most common suffixes:\n>>> from nltk.corpus import brown\n>>> suffix_fdist = nltk.FreqDist()\n>>> for word in brown.words():\n...     word = word.lower()\n...     suffix_fdist.inc(word[-1:])\n...     suffix_fdist.inc(word[-2:])\n...     suffix_fdist.inc(word[-3:])\n>>> common_suffixes = suffix_fdist.keys()[:100]\n>>> print common_suffixes \n['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the',\n 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l',\n 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or',\n 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', ...]\nNext, we\u2019ll define a feature extractor function that checks a given word for these\nsuffixes:\n>>> def pos_features(word):\n...     features = {}\n...     for suffix in common_suffixes:\n...         features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n...     return features\nFeature extraction functions behave like tinted glasses, highlighting some of the prop-\nerties (colors) in our data and making it impossible to see other properties. The classifier\nwill rely exclusively on these highlighted properties when determining how to label\ninputs. In this case, the classifier will make its decisions based only on information\nabout which of the common suffixes (if any) a given word has.\nNow that we\u2019ve defined our feature extractor, we can use it to train a new \u201cdecision\ntree\u201d classifier (to be discussed in Section 6.4):\n>>> tagged_words = brown.tagged_words(categories='news')\n>>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n>>> size = int(len(featuresets) * 0.1)\n>>> train_set, test_set = featuresets[size:], featuresets[:size]\n>>> classifier = nltk.DecisionTreeClassifier.train(train_set)\n>>> nltk.classify.accuracy(classifier, test_set)\n0.62705121829935351\n>>> classifier.classify(pos_features('cats'))\n'NNS'\nOne nice feature of decision tree models is that they are often fairly easy to interpret.\nWe can even instruct NLTK to print them out as pseudocode:\n6.1  Supervised Classification | 229\n>>> print classifier.pseudocode(depth=4)\nif endswith(,) == True: return ','\nif endswith(,) == False:\n  if endswith(the) == True: return 'AT'\n  if endswith(the) == False:\n    if endswith(s) == True:\n      if endswith(is) == True: return 'BEZ'\n      if endswith(is) == False: return 'VBZ'\n    if endswith(s) == False:\n      if endswith(.) == True: return '.'\n      if endswith(.) == False: return 'NN'\nHere, we \ncan see that the classifier begins by checking whether a word ends with a\ncomma\u2014if so, then it will receive the special tag \",\". Next, the classifier checks whether\nthe word ends in \"the\", in which case it\u2019s almost certainly a determiner. This \u201csuffix\u201d\ngets used early by the decision tree because the word the is so common. Continuing\non, the classifier checks if the word ends in s. If so, then it\u2019s most likely to receive the\nverb tag VBZ (unless it\u2019s the word is, which has the special tag BEZ), and if not, then it\u2019s\nmost likely a noun (unless it\u2019s the punctuation mark \u201c.\u201d). The actual classifier contains\nfurther nested if-then statements below the ones shown here, but the depth=4 argument\njust displays the top portion of the decision tree.\nExploiting Context\nBy augmenting the feature extraction function, we could modify this part-of-speech\ntagger to leverage a variety of other word-internal features, such as the length of the\nword, the number of syllables it contains, or its prefix. However, as long as the feature\nextractor just looks at the target word, we have no way to add features that depend on\nthe context in which the word appears. But contextual features often provide powerful\nclues about the correct tag\u2014for example, when tagging the word fly, knowing that the\nprevious word is a will allow us to determine that it is functioning as a noun, not a verb.\nIn order to accommodate features that depend on a word\u2019s context, we must revise the\npattern that we used to define our feature extractor. Instead of just passing in the word\nto be tagged, we will pass in a complete (untagged) sentence, along with the index of\nthe target word. This approach is demonstrated in Example 6-4 , which employs a con-\ntext-dependent feature extractor to define a part-of-speech tag classifier.\n230 | Chapter 6: \u2002Learning to Classify Text\nExample 6-4. A part-of-speech classifier whose feature detector examines the context in which a word\nappears in \norder to determine which part-of-speech tag should be assigned. In particular, the identity\nof the previous word is included as a feature.\ndef pos_features(sentence, i): \n    features = {\"suffix(1)\": sentence[i][-1:],\n                \"suffix(2)\": sentence[i][-2:],\n                \"suffix(3)\": sentence[i][-3:]}\n    if i == 0:\n        features[\"prev-word\"] = \"<START>\"\n    else:\n        features[\"prev-word\"] = sentence[i-1]\n    return features\n>>> pos_features(brown.sents()[0], 8)\n{'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'}\n>>> tagged_sents = brown.tagged_sents(categories='news')\n>>> featuresets = []\n>>> for tagged_sent in tagged_sents:\n...     untagged_sent = nltk.tag.untag(tagged_sent)\n...     for i, (word, tag) in enumerate(tagged_sent):\n...         featuresets.append( \n(pos_features(untagged_sent, i), tag) )\n>>> size = int(len(featuresets) * 0.1)\n>>> train_set, test_set = featuresets[size:], featuresets[:size]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> nltk.classify.accuracy(classifier, test_set)\n0.78915962207856782\nIt\u2019s clear \nthat exploiting contextual features improves the performance of our part-of-\nspeech tagger. For example, the classifier learns that a word is likely to be a noun if it\ncomes immediately after the word large or the word gubernatorial. However, it is unable\nto learn the generalization that a word is probably a noun if it follows an adjective,\nbecause it doesn\u2019t have access to the previous word\u2019s part-of-speech tag. In general,\nsimple classifiers always treat each input as independent from all other inputs. In many\ncontexts, this makes perfect sense. For example, decisions about whether names tend\nto be male or female can be made on a case-by-case basis. However, there are often\ncases, such as part-of-speech tagging, where we are interested in solving classification\nproblems that are closely related to one another.\nSequence Classification\nIn order to capture the dependencies between related classification tasks, we can use\njoint classifier models, which choose an appropriate labeling for a collection of related\ninputs. In the case of part-of-speech tagging, a variety of different sequence\nclassifier models can be used to jointly choose part-of-speech tags for all the words in\na given sentence.\n6.1  Supervised Classification | 231\nOne sequence classification strategy, known as consecutive classification  or greedy\nsequence classification , \nis to find the most likely class label for the first input, then\nto use that answer to help find the best label for the next input. The process can then\nbe repeated until all of the inputs have been labeled. This is the approach that was taken\nby the bigram tagger from Section 5.5 , which began by choosing a part-of-speech tag\nfor the first word in the sentence, and then chose the tag for each subsequent word\nbased on the word itself and the predicted tag for the previous word.\nThis strategy is demonstrated in Example 6-5 . First, we must augment our feature\nextractor function to take a history argument, which provides a list of the tags that\nwe\u2019ve predicted for the sentence so far \n . Each tag in history corresponds with a word\nin sentence. But note that history will only contain tags for words we\u2019ve already clas-\nsified, that is, words to the left of the target word. Thus, although it is possible to look\nat some features of words to the right of the target word, it is not possible to look at\nthe tags for those words (since we haven\u2019t generated them yet).\nHaving defined a feature extractor, we can proceed to build our sequence\nclassifier \n . During training, we use the annotated tags to provide the appropriate\nhistory to the feature extractor, but when tagging new sentences, we generate the his-\ntory list based on the output of the tagger itself.\nExample 6-5. Part-of-speech tagging with a consecutive classifier.\ndef pos_features(sentence, i, history): \n    features = {\"suffix(1)\": sentence[i][-1:],\n                \"suffix(2)\": sentence[i][-2:],\n                \"suffix(3)\": sentence[i][-3:]}\n    if i == 0:\n        features[\"prev-word\"] = \"<START>\"\n        features[\"prev-tag\"] = \"<START>\"\n    else:\n        features[\"prev-word\"] = sentence[i-1]\n        features[\"prev-tag\"] = history[i-1]\n    return features\nclass ConsecutivePosTagger(nltk.TaggerI): \n    def __init__(self, train_sents):\n        train_set = []\n        for tagged_sent in train_sents:\n            untagged_sent = nltk.tag.untag(tagged_sent)\n            history = []\n            for i, (word, tag) in enumerate(tagged_sent):\n                featureset = pos_features(untagged_sent, i, history)\n                train_set.append( (featureset, tag) )\n                history.append(tag)\n        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n \n    def tag(self, sentence):\n        history = []\n        for i, word in enumerate(sentence):\n            featureset = pos_features(sentence, i, history)\n232 | Chapter 6: \u2002Learning to Classify Text\n            tag = self.classifier.classify(featureset)\n            history.append(tag)\n        return zip(sentence, history)\n>>> tagged_sents = brown.tagged_sents(categories='news')\n>>> size = int(len(tagged_sents) * 0.1)\n>>> train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n>>> tagger = ConsecutivePosTagger(train_sents)\n>>> print tagger.evaluate(test_sents)\n0.79796012981\nOther Methods for Sequence Classification\nOne shortcoming \nof this approach is that we commit to every decision that we make.\nFor example, if we decide to label a word as a noun, but later find evidence that it should\nhave been a verb, there\u2019s no way to go back and fix our mistake. One solution to this\nproblem is to adopt a transformational strategy instead. Transformational joint classi-\nfiers work by creating an initial assignment of labels for the inputs, and then iteratively\nrefining that assignment in an attempt to repair inconsistencies between related inputs.\nThe Brill tagger, described in Section 5.6, is a good example of this strategy.\nAnother solution is to assign scores to all of the possible sequences of part-of-speech\ntags, and to choose the sequence whose overall score is highest. This is the approach\ntaken by Hidden Markov Models . Hidden Markov Models are similar to consecutive\nclassifiers in that they look at both the inputs and the history of predicted tags. How-\never, rather than simply finding the single best tag for a given word, they generate a\nprobability distribution over tags. These probabilities are then combined to calculate\nprobability scores for tag sequences, and the tag sequence with the highest probability\nis chosen. Unfortunately, the number of possible tag sequences is quite large. Given a\ntag set with 30 tags, there are about 600 trillion (3010) ways to label a 10-word sentence.\nIn order to avoid considering all these possible sequences separately, Hidden Markov\nModels require that the feature extractor only look at the most recent tag (or the most\nrecent n tags, where n is fairly small). Given that restriction, it is possible to use dynamic\nprogramming ( Section 4.7 ) to efficiently find the most likely tag sequence. In particular,\nfor each consecutive word index i, a score is computed for each possible current and\nprevious tag. This same basic approach is taken by two more advanced models, called\nMaximum Entropy Markov Models  and Linear-Chain Conditional Random\nField Models; but different algorithms are used to find scores for tag sequences.\n6.2  Further Examples of Supervised Classification\nSentence Segmentation\nSentence segmentation can be viewed as a classification task for punctuation: whenever\nwe encounter a symbol that could possibly end a sentence, such as a period or a question\nmark, we have to decide whether it terminates the preceding sentence.\n6.2  Further Examples of Supervised Classification | 233\nThe first step is to obtain some data that has already been segmented into sentences\nand convert it into a form that is suitable for extracting features:\n>>> sents = nltk.corpus.treebank_raw.sents()\n>>> tokens = []\n>>> boundaries = set()\n>>> offset = 0\n>>> for sent in nltk.corpus.treebank_raw.sents():\n...     tokens.extend(sent)\n...     offset += len(sent)\n...     boundaries.add(offset-1)\nHere, tokens is \na merged list of tokens from the individual sentences, and boundaries\nis a set containing the indexes of all sentence-boundary tokens. Next, we need to specify\nthe features of the data that will be used in order to decide whether punctuation indi-\ncates a sentence boundary:\n>>> def punct_features(tokens, i):\n...     return {'next-word-capitalized': tokens[i+1][0].isupper(),\n...             'prevword': tokens[i-1].lower(),\n...             'punct': tokens[i],\n...             'prev-word-is-one-char': len(tokens[i-1]) == 1}\nBased on this feature extractor, we can create a list of labeled featuresets by selecting\nall the punctuation tokens, and tagging whether they are boundary tokens or not:\n>>> featuresets = [(punct_features(tokens, i), (i in boundaries))\n...                for i in range(1, len(tokens)-1)\n...                if tokens[i] in '.?!']\nUsing these featuresets, we can train and evaluate a punctuation classifier:\n>>> size = int(len(featuresets) * 0.1)\n>>> train_set, test_set = featuresets[size:], featuresets[:size]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> nltk.classify.accuracy(classifier, test_set)\n0.97419354838709682\nTo use this classifier to perform sentence segmentation, we simply check each punc-\ntuation mark to see whether it\u2019s labeled as a boundary, and divide the list of words at\nthe boundary marks. The listing in Example 6-6 shows how this can be done.\nExample 6-6. Classification-based sentence segmenter.\ndef segment_sentences(words):\n    start = 0\n    sents = []\n    for i, word in words:\n        if word in '.?!' and classifier.classify(words, i) == True:\n            sents.append(words[start:i+1])\n            start = i+1\n    if start < len(words):\n        sents.append(words[start:])\n234 | Chapter 6: \u2002Learning to Classify Text\nIdentifying Dialogue Act Types\nWhen processing \ndialogue, it can be useful to think of utterances as a type of action\nperformed by the speaker. This interpretation is most straightforward for performative\nstatements such as I forgive you  or I bet you can\u2019t climb that hill . But greetings, questions,\nanswers, assertions, and clarifications can all be thought of as types of speech-based\nactions. Recognizing the dialogue acts  underlying the utterances in a dialogue can be\nan important first step in understanding the conversation.\nThe NPS Chat Corpus, which was demonstrated in Section 2.1, consists of over 10,000\nposts from instant messaging sessions. These posts have all been labeled with one of\n15 dialogue act types, such as \u201cStatement,\u201d \u201cEmotion,\u201d \u201cynQuestion,\u201d and \u201cContin-\nuer.\u201d We can therefore use this data to build a classifier that can identify the dialogue\nact types for new instant messaging posts. The first step is to extract the basic messaging\ndata. We will call xml_posts() to get a data structure representing the XML annotation\nfor each post:\n>>> posts = nltk.corpus.nps_chat.xml_posts()[:10000]\nNext, we\u2019ll define a simple feature extractor that checks what words the post contains:\n>>> def dialogue_act_features(post):\n...     features = {}\n...     for word in nltk.word_tokenize(post):\n...         features['contains(%s)' % word.lower()] = True\n...     return features\nFinally, we construct the training and testing data by applying the feature extractor to\neach post (using post.get('class') to get a post\u2019s dialogue act type), and create a new\nclassifier:\n>>> featuresets = [(dialogue_act_features(post.text), post.get('class'))\n...                for post in posts]\n>>> size = int(len(featuresets) * 0.1)\n>>> train_set, test_set = featuresets[size:], featuresets[:size]\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> print nltk.classify.accuracy(classifier, test_set)\n0.66\nRecognizing Textual Entailment\nRecognizing textual entailment (RTE) is the task of determining whether a given piece\nof text T entails another text called the \u201chypothesis\u201d (as already discussed in Sec-\ntion 1.5 ). To date, there have been four RTE Challenges, where shared development\nand test data is made available to competing teams. Here are a couple of examples of\ntext/hypothesis pairs from the Challenge 3 development dataset. The label True indi-\ncates that the entailment holds, and False indicates that it fails to hold.\n6.2  Further Examples of Supervised Classification | 235\nChallenge 3, Pair 34 (True)\nT: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation\nOrganisation \n(SCO), the fledgling association that binds Russia, China and four\nformer Soviet republics of central Asia together to fight terrorism.\nH: China is a member of SCO.\nChallenge 3, Pair 81 (False)\nT: According to NC Articles of Organization, the members of LLC company are\nH. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.\nH\n: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.\nIt should be emphasized that the relationship between text and hypothesis is not in-\ntended to be logical entailment, but rather whether a human would conclude that the\ntext provides reasonable evidence for taking the hypothesis to be true.\nWe can \ntreat RTE as a classification task, in which we try to predict the True/False label\nfor each pair. Although it seems likely that successful approaches to this task will in-\nvolve a combination of parsing, semantics, and real-world knowledge, many early at-\ntempts at RTE achieved reasonably good results with shallow analysis, based on sim-\nilarity between the text and hypothesis at the word level. In the ideal case, we would\nexpect that if there is an entailment, then all the information expressed by the hypoth-\nesis should also be present in the text. Conversely, if there is information found in the\nhypothesis that is absent from the text, then there will be no entailment.\nIn our RTE feature detector ( Example 6-7), we let words (i.e., word types) serve as\nproxies for information, and our features count the degree of word overlap, and the\ndegree to which there are words in the hypothesis but not in the text (captured by the\nmethod hyp_extra()). Not all words are equally important\u2014named entity mentions,\nsuch as the names of people, organizations, and places, are likely to be more significant,\nwhich motivates us to extract distinct information for words and nes (named entities).\nIn addition, some high-frequency function words are filtered out as \u201cstopwords.\u201d\nExample 6-7. \u201cRecognizing Text Entailment\u201d feature extractor: The RTEFeatureExtractor  class\nbuilds a bag of words for both the text and the hypothesis after throwing away some stopwords, then\ncalculates overlap and difference.\ndef rte_features(rtepair):\n    extractor = nltk.RTEFeatureExtractor(rtepair)\n    features = {}\n    features['word_overlap'] = len(extractor.overlap('word'))\n    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n    features['ne_overlap'] = len(extractor.overlap('ne'))\n    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n    return features\nTo illustrate the content of these features, we examine some attributes of the text/\nhypothesis Pair 34 shown earlier:\n236 | Chapter 6: \u2002Learning to Classify Text\n>>> rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n>>> extractor = nltk.RTEFeatureExtractor(rtepair)\n>>> print extractor.text_words\nset(['Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at',\n'operation', 'SCO', ...])\n>>> print extractor.hyp_words\nset(['member', 'SCO', 'China'])\n>>> print extractor.overlap('word')\nset([])\n>>> print extractor.overlap('ne')\nset(['SCO', 'China'])\n>>> print extractor.hyp_extra('word')\nset(['member'])\nThese features \nindicate that all important words in the hypothesis are contained in the\ntext, and thus there is some evidence for labeling this as True.\nThe module nltk.classify.rte_classify reaches just over 58% accuracy on the com-\nbined RTE test data using methods like these. Although this figure is not very\nimpressive, it requires significant effort, and more linguistic processing, to achieve\nmuch better results.\nScaling Up to Large Datasets\nPython provides an excellent environment for performing basic text processing and\nfeature extraction. However, it is not able to perform the numerically intensive calcu-\nlations required by machine learning methods nearly as quickly as lower-level languages\nsuch as C. Thus, if you attempt to use the pure-Python machine learning implemen-\ntations (such as nltk.NaiveBayesClassifier) on large datasets, you may find that the\nlearning algorithm takes an unreasonable amount of time and memory to complete.\nIf you plan to train classifiers with large amounts of training data or a large number of\nfeatures, we recommend that you explore NLTK\u2019s facilities for interfacing with external\nmachine learning packages. Once these packages have been installed, NLTK can trans-\nparently invoke them (via system calls) to train classifier models significantly faster than\nthe pure-Python classifier implementations. See the NLTK web page for a list of rec-\nommended machine learning packages that are supported by NLTK.\n6.3  Evaluation\nIn order to decide whether a classification model is accurately capturing a pattern, we\nmust evaluate that model. The result of this evaluation is important for deciding how\ntrustworthy the model is, and for what purposes we can use it. Evaluation can also be\nan effective tool for guiding us in making future improvements to the model.\nThe Test Set\nMost evaluation techniques calculate a score for a model by comparing the labels that\nit generates for the inputs in a test set  (or evaluation set ) with the correct labels for\n6.3  Evaluation | 237\nthose inputs. This test set typically has the same format as the training set. However,\nit is \nvery important that the test set be distinct from the training corpus: if we simply\nreused the training set as the test set, then a model that simply memorized its input,\nwithout learning how to generalize to new examples, would receive misleadingly high\nscores.\nWhen building the test set, there is often a trade-off between the amount of data avail-\nable for testing and the amount available for training. For classification tasks that have\na small number of well-balanced labels and a diverse test set, a meaningful evaluation\ncan be performed with as few as 100 evaluation instances. But if a classification task\nhas a large number of labels or includes very infrequent labels, then the size of the test\nset should be chosen to ensure that the least frequent label occurs at least 50 times.\nAdditionally, if the test set contains many closely related instances\u2014such as instances\ndrawn from a single document\u2014then the size of the test set should be increased to\nensure that this lack of diversity does not skew the evaluation results. When large\namounts of annotated data are available, it is common to err on the side of safety by\nusing 10% of the overall data for evaluation.\nAnother consideration when choosing the test set is the degree of similarity between\ninstances in the test set and those in the development set. The more similar these two\ndatasets are, the less confident we can be that evaluation results will generalize to other\ndatasets. For example, consider the part-of-speech tagging task. At one extreme, we\ncould create the training set and test set by randomly assigning sentences from a data\nsource that reflects a single genre, such as news:\n>>> import random\n>>> from nltk.corpus import brown\n>>> tagged_sents = list(brown.tagged_sents(categories='news'))\n>>> random.shuffle(tagged_sents)\n>>> size = int(len(tagged_sents) * 0.1)\n>>> train_set, test_set = tagged_sents[size:], tagged_sents[:size]\nIn this case, our test set will be very similar to our training set. The training set and test\nset are taken from the same genre, and so we cannot be confident that evaluation results\nwould generalize to other genres. What\u2019s worse, because of the call to\nrandom.shuffle(), the test set contains sentences that are taken from the same docu-\nments that were used for training. If there is any consistent pattern within a document\n(say, if a given word appears with a particular part-of-speech tag especially frequently),\nthen that difference will be reflected in both the development set and the test set. A\nsomewhat better approach is to ensure that the training set and test set are taken from\ndifferent documents:\n>>> file_ids = brown.fileids(categories='news')\n>>> size = int(len(file_ids) * 0.1)\n>>> train_set = brown.tagged_sents(file_ids[size:])\n>>> test_set = brown.tagged_sents(file_ids[:size])\nIf we want to perform a more stringent evaluation, we can draw the test set from docu-\nments that are less closely related to those in the training set:\n238 | Chapter 6: \u2002Learning to Classify Text\n>>> train_set = brown.tagged_sents(categories='news')\n>>> test_set = brown.tagged_sents(categories='fiction')\nIf we \nbuild a classifier that performs well on this test set, then we can be confident that\nit has the power to generalize well beyond the data on which it was trained.\nAccuracy\nThe simplest metric that can be used to evaluate a classifier, accuracy, measures the\npercentage of inputs in the test set that the classifier correctly labeled. For example, a\nname gender classifier that predicts the correct name 60 times in a test set containing\n80 names would have an accuracy of 60/80 = 75%. The function nltk.classify.accu\nracy() will calculate the accuracy of a classifier model on a given test set:\n>>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n>>> print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test_set) \n0.75\nWhen interpreting the accuracy score of a classifier, it is important to consider the\nfrequencies of the individual class labels in the test set. For example, consider a classifier\nthat determines the correct word sense for each occurrence of the word bank. If we\nevaluate this classifier on financial newswire text, then we may find that the financial-\ninstitution sense appears 19 times out of 20. In that case, an accuracy of 95% would\nhardly be impressive, since we could achieve that accuracy with a model that always\nreturns the financial-institution sense. However, if we instead evaluate the classifier\non a more balanced corpus, where the most frequent word sense has a frequency of\n40%, then a 95% accuracy score would be a much more positive result. (A similar issue\narises when measuring inter-annotator agreement in Section 11.2.)\nPrecision and Recall\nAnother instance where accuracy scores can be misleading is in \u201csearch\u201d tasks, such as\ninformation retrieval, where we are attempting to find documents that are relevant to\na particular task. Since the number of irrelevant documents far outweighs the number\nof relevant documents, the accuracy score for a model that labels every document as\nirrelevant would be very close to 100%.\nIt is therefore conventional to employ a different set of measures for search tasks, based\non the number of items in each of the four categories shown in Figure 6-3:\n\u2022True positives are relevant items that we correctly identified as relevant.\n\u2022True negatives are irrelevant items that we correctly identified as irrelevant.\n\u2022False positives (or Type I errors) are irrelevant items that we incorrectly identi-\nfied as relevant.\n\u2022False negatives  (or Type II errors ) are relevant items that we incorrectly identi-\nfied as irrelevant.\n6.3  Evaluation | 239\nGiven these four numbers, we can define the following metrics:\n\u2022Precision, which \nindicates how many of the items that we identified were relevant,\nis TP/(TP+FP).\n\u2022Recall, which indicates how many of the relevant items that we identified, is\nTP/(TP+FN).\n\u2022 The F-Measure (or F-Score), which combines the precision and recall to give a\nsingle score, is defined to be the harmonic mean of the precision and recall\n(2 \u00d7 Precision \u00d7 Recall)/(Precision+Recall).\nConfusion Matrices\nWhen performing classification tasks with three or more labels, it can be informative\nto subdivide the errors made by the model based on which types of mistake it made. A\nconfusion matrix is a table where each cell [ i,j] indicates how often label j was pre-\ndicted when the correct label was i. Thus, the diagonal entries (i.e., cells [ i,j]) indicate\nlabels that were correctly predicted, and the off-diagonal entries indicate errors. In the\nfollowing example, we generate a confusion matrix for the unigram tagger developed\nin Section 5.4:\nFigure 6-3. True and false positives and negatives.\n240 | Chapter 6: \u2002Learning to Classify Text\n>>> def tag_list(tagged_sents):\n...     return [tag for sent in tagged_sents for (word, tag) in sent]\n>>> def apply_tagger(tagger, corpus):\n...     return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\n>>> gold = tag_list(brown.tagged_sents(categories='editorial'))\n>>> test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial'))) \n>>> cm = nltk.ConfusionMatrix(gold, test)\n    |                                         N                      |\n    |      N      I      A      J             N             V      N |\n    |      N      N      T      J      .      S      ,      B      P |\n----+----------------------------------------------------------------+\n NN | <11.8%>  0.0%      .   0.2%      .   0.0%      .   0.3%   0.0% |\n IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n AT |      .      .  <8.6%>     .      .      .      .      .      . |\n JJ |   1.6%      .      .  <4.0%>     .      .      .   0.0%   0.0% |\n  . |      .      .      .      .  <4.8%>     .      .      .      . |\n NS |   1.5%      .      .      .      .  <3.2%>     .      .   0.0% |\n  , |      .      .      .      .      .      .  <4.4%>     .      . |\n  B |   0.9%      .      .   0.0%      .      .      .  <2.4%>     . |\n NP |   1.0%      .      .   0.0%      .      .      .      .  <1.9%>|\n----+----------------------------------------------------------------+\n(row = reference; col = test)\nThe confusion \nmatrix indicates that common errors include a substitution of NN for\nJJ (for 1.6% of words), and of NN for NNS (for 1.5% of words). Note that periods (.)\nindicate cells whose value is 0, and that the diagonal entries\u2014which correspond to\ncorrect classifications\u2014are marked with angle brackets.\nCross-Validation\nIn order to evaluate our models, we must reserve a portion of the annotated data for\nthe test set. As we already mentioned, if the test set is too small, our evaluation may\nnot be accurate. However, making the test set larger usually means making the training\nset smaller, which can have a significant impact on performance if a limited amount of\nannotated data is available.\nOne solution to this problem is to perform multiple evaluations on different test sets,\nthen to combine the scores from those evaluations, a technique known as cross-\nvalidation. In particular, we subdivide the original corpus into N subsets called\nfolds. For each of these folds, we train a model using all of the data except the data in\nthat fold, and then test that model on the fold. Even though the individual folds might\nbe too small to give accurate evaluation scores on their own, the combined evaluation\nscore is based on a large amount of data and is therefore quite reliable.\nA second, and equally important, advantage of using cross-validation is that it allows\nus to examine how widely the performance varies across different training sets. If we\nget very similar scores for all N training sets, then we can be fairly confident that the\nscore is accurate. On the other hand, if scores vary widely across the N training sets,\nthen we should probably be skeptical about the accuracy of the evaluation score.\n6.3  Evaluation | 241\n6.4  Decision Trees\nIn the \nnext three sections, we\u2019ll take a closer look at three machine learning methods\nthat can be used to automatically build classification models: decision trees, naive Bayes\nclassifiers, and Maximum Entropy classifiers. As we\u2019ve seen, it\u2019s possible to treat these\nlearning methods as black boxes, simply training models and using them for prediction\nwithout understanding how they work. But there\u2019s a lot to be learned from taking a\ncloser look at how these learning methods select models based on the data in a training\nset. An understanding of these methods can help guide our selection of appropriate\nfeatures, and especially our decisions about how those features should be encoded.\nAnd an understanding of the generated models can allow us to extract information\nabout which features are most informative, and how those features relate to one an-\nother.\nA decision tree is a simple flowchart that selects labels for input values. This flowchart\nconsists of decision nodes , which check feature values, and leaf nodes , which assign\nlabels. To choose the label for an input value, we begin at the flowchart\u2019s initial decision\nnode, known as its root node . This node contains a condition that checks one of the\ninput value\u2019s features, and selects a branch based on that feature\u2019s value. Following the\nbranch that describes our input value, we arrive at a new decision node, with a new\ncondition on the input value\u2019s features. We continue following the branch selected by\neach node\u2019s condition, until we arrive at a leaf node which provides a label for the input\nvalue. Figure 6-4 shows an example decision tree model for the name gender task.\nOnce we have a decision tree, it is straightforward to use it to assign labels to new input\nvalues. What\u2019s less straightforward is how we can build a decision tree that models a\ngiven training set. But before we look at the learning algorithm for building decision\ntrees, we\u2019ll consider a simpler task: picking the best \u201cdecision stump\u201d for a corpus. A\nFigure 6-4. Decision Tree model for the name gender task. Note that tree diagrams are conventionally\ndrawn \u201cupside down,\u201d with the root at the top, and the leaves at the bottom.\n242 | Chapter 6: \u2002Learning to Classify Text\ndecision stump  is a decision tree with a single node that decides how to classify inputs\nbased on a single feature. It contains one leaf for each possible feature value, specifying\nthe class label that should be assigned to inputs whose features have that value. In order\nto build a decision stump, we must first decide which feature should be used. The\nsimplest method is to just build a decision stump for each possible feature, and see\nwhich one achieves the highest accuracy on the training data, although there are other\nalternatives that we will discuss later. Once we\u2019ve picked a feature, we can build the\ndecision stump by assigning a label to each leaf based on the most frequent label for\nthe selected examples in the training set (i.e., the examples where the selected feature\nhas that value).\nGiven the algorithm for choosing decision stumps, the algorithm for growing larger\ndecision trees is straightforward. We begin by selecting the overall best decision stump\nfor the classification task. We then check the accuracy of each of the leaves on the\ntraining set. Leaves that do not achieve sufficient accuracy are then replaced by new\ndecision stumps, trained on the subset of the training corpus that is selected by the path\nto the leaf. For example, we could grow the decision tree in Figure 6-4  by replacing the\nleftmost leaf with a new decision stump, trained on the subset of the training set names\nthat do not start with a k or end with a vowel or an l.\nEntropy and Information Gain\nAs was mentioned before, there are several methods for identifying the most informa-\ntive feature for a decision stump. One popular alternative, called information gain ,\nmeasures how much more organized the input values become when we divide them up\nusing a given feature. To measure how disorganized the original set of input values are,\nwe calculate entropy of their labels, which will be high if the input values have highly\nvaried labels, and low if many input values all have the same label. In particular, entropy\nis defined as the sum of the probability of each label times the log probability of that\nsame label:\n(1)H = \u03a3 l \u2208 labels P(l) \u00d7 log 2P(l).\nFor example, Figure 6-5 shows how the entropy of labels in the name gender prediction\ntask depends on the ratio of male to female names. Note that if most input values have\nthe same label (e.g., if P(male) is near 0 or near 1), then entropy is low. In particular,\nlabels that have low frequency do not contribute much to the entropy (since P(l) is\nsmall), and labels with high frequency also do not contribute much to the entropy (since\nlog2P(l) is small). On the other hand, if the input values have a wide variety of labels,\nthen there are many labels with a \u201cmedium\u201d frequency, where neither P(l) nor\nlog2P(l) is small, so the entropy is high. Example 6-8 demonstrates how to calculate\nthe entropy of a list of labels.\n6.4  Decision Trees | 243\nFigure 6-5. The entropy of labels in the name gender prediction task, as a function of the percentage\nof names in a given set that are male.\nExample 6-8. Calculating the entropy of a list of labels.\nimport math\ndef entropy(labels):\n    freqdist = nltk.FreqDist(labels)\n    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]\n    return -sum([p * math.log(p,2) for p in probs])\n>>> print entropy(['male', 'male', 'male', 'male']) \n0.0\n>>> print entropy(['male', 'female', 'male', 'male'])\n0.811278124459\n \n>>> print entropy(['female', 'male', 'female', 'male'])\n1.0\n>>> print entropy(['female', 'female', 'male', 'female'])\n0.811278124459\n>>> print entropy(['female', 'female', 'female', 'female'])\n0.0\nOnce we \nhave calculated the entropy of the labels of the original set of input values, we\ncan determine how much more organized the labels become once we apply the decision\nstump. To do so, we calculate the entropy for each of the decision stump\u2019s leaves, and\ntake the average of those leaf entropy values (weighted by the number of samples in\neach leaf). The information gain is then equal to the original entropy minus this new,\nreduced entropy. The higher the information gain, the better job the decision stump\ndoes of dividing the input values into coherent groups, so we can build decision trees\nby selecting the decision stumps with the highest information gain.\nAnother consideration for decision trees is efficiency. The simple algorithm for selecting\ndecision stumps described earlier must construct a candidate decision stump for every\npossible feature, and this process must be repeated for every node in the constructed\n244 | Chapter 6: \u2002Learning to Classify Text\ndecision tree. A number of algorithms have been developed to cut down on the training\ntime by storing and reusing information about previously evaluated examples.\nDecision trees \nhave a number of useful qualities. To begin with, they\u2019re simple to un-\nderstand, and easy to interpret. This is especially true near the top of the decision tree,\nwhere it is usually possible for the learning algorithm to find very useful features. De-\ncision trees are especially well suited to cases where many hierarchical categorical dis-\ntinctions can be made. For example, decision trees can be very effective at capturing\nphylogeny trees.\nHowever, decision trees also have a few disadvantages. One problem is that, since each\nbranch in the decision tree splits the training data, the amount of training data available\nto train nodes lower in the tree can become quite small. As a result, these lower decision\nnodes may overfit the training set, learning patterns that reflect idiosyncrasies of the\ntraining set rather than linguistically significant patterns in the underlying problem.\nOne solution to this problem is to stop dividing nodes once the amount of training data\nbecomes too small. Another solution is to grow a full decision tree, but then to\nprune decision nodes that do not improve performance on a dev-test.\nA second problem with decision trees is that they force features to be checked in a\nspecific order, even when features may act relatively independently of one another. For\nexample, when classifying documents into topics (such as sports, automotive, or mur-\nder mystery), features such as hasword(football) are highly indicative of a specific label,\nregardless of what the other feature values are. Since there is limited space near the top\nof the decision tree, most of these features will need to be repeated on many different\nbranches in the tree. And since the number of branches increases exponentially as we\ngo down the tree, the amount of repetition can be very large.\nA related problem is that decision trees are not good at making use of features that are\nweak predictors of the correct label. Since these features make relatively small\nincremental improvements, they tend to occur very low in the decision tree. But by the\ntime the decision tree learner has descended far enough to use these features, there is\nnot enough training data left to reliably determine what effect they should have. If we\ncould instead look at the effect of these features across the entire training set, then we\nmight be able to make some conclusions about how they should affect the choice of\nlabel.\nThe fact that decision trees require that features be checked in a specific order limits\ntheir ability to exploit features that are relatively independent of one another. The naive\nBayes classification method, which we\u2019ll discuss next, overcomes this limitation by\nallowing all features to act \u201cin parallel.\u201d\n6.5  Naive Bayes Classifiers\nIn naive Bayes  classifiers, every feature gets a say in determining which label should\nbe assigned to a given input value. To choose a label for an input value, the naive Bayes\n6.5  Naive Bayes Classifiers | 245\nclassifier begins by calculating the prior probability  of each label, which is determined\nby checking the frequency of each label in the training set. The contribution from each\nfeature is then combined with this prior probability, to arrive at a likelihood estimate\nfor each label. The label whose likelihood estimate is the highest is then assigned to the\ninput value. Figure 6-6 illustrates this process.\nFigure 6-6. An abstract illustration of the procedure used by the naive Bayes classifier to choose the\ntopic for \na document. In the training corpus, most documents are automotive, so the classifier starts\nout at a point closer to the \u201cautomotive\u201d label. But it then considers the effect of each feature. In this\nexample, the input document contains the word dark, which is a weak indicator for murder mysteries,\nbut it also contains the word football, which is a strong indicator for sports documents. After every\nfeature has made its contribution, the classifier checks which label it is closest to, and assigns that\nlabel to the input.\nIndividual features make their contribution to the overall decision by \u201cvoting against\u201d\nlabels that don\u2019t occur with that feature very often. In particular, the likelihood score\nfor each label is reduced by multiplying it by the probability that an input value with\nthat label would have the feature. For example, if the word run occurs in 12% of the\nsports documents, 10% of the murder mystery documents, and 2% of the automotive\ndocuments, then the likelihood score for the sports label will be multiplied by 0.12, the\nlikelihood score for the murder mystery label will be multiplied by 0.1, and the likeli-\nhood score for the automotive label will be multiplied by 0.02. The overall effect will\nbe to reduce the score of the murder mystery label slightly more than the score of the\nsports label, and to significantly reduce the automotive label with respect to the other\ntwo labels. This process is illustrated in Figures 6-7 and 6-8.\n246 | Chapter 6: \u2002Learning to Classify Text\nFigure 6-7. Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior\nprobability of \neach label, based on how frequently each label occurs in the training data. Every feature\nthen contributes to the likelihood estimate for each label, by multiplying it by the probability that\ninput values with that label will have that feature. The resulting likelihood score can be thought of as\nan estimate of the probability that a randomly selected value from the training set would have both\nthe given label and the set of features, assuming that the feature probabilities are all independent.\nFigure 6-8. A Bayesian Network Graph illustrating the generative process that is assumed by the naive\nBayes classifier. \nTo generate a labeled input, the model first chooses a label for the input, and then it\ngenerates each of the input\u2019s features based on that label. Every feature is assumed to be entirely\nindependent of every other feature, given the label.\nUnderlying Probabilistic Model\nAnother way of understanding the naive Bayes classifier is that it chooses the most likely\nlabel for an input, under the assumption that every input value is generated by first\nchoosing a class label for that input value, and then generating each feature, entirely\nindependent of every other feature. Of course, this assumption is unrealistic; features\nare often highly dependent on one another. We\u2019ll return to some of the consequences\nof this assumption at the end of this section. This simplifying assumption, known as\nthe naive Bayes assumption  (or independence assumption ), makes it much easier\n6.5  Naive Bayes Classifiers | 247\nto combine the contributions of the different features, since we don\u2019t need to worry\nabout how they should interact with one another.\nBased on \nthis assumption, we can calculate an expression for P(label|features), the\nprobability that an input will have a particular label given that it has a particular set of\nfeatures. To choose a label for a new input, we can then simply pick the label l that\nmaximizes P(l|features).\nTo begin, we note that P(label|features) is equal to the probability that an input has a\nparticular label and the specified set of features, divided by the probability that it has\nthe specified set of features:\n(2)P(label|features) = P(features, label)/P(features)\nNext, we note that P(features) will be the same for every choice of label, so if we are\nsimply interested in finding the most likely label, it suffices to calculate P(features,\nlabel), which we\u2019ll call the label likelihood.\nIf we want to generate a probability estimate for each label, rather than\njust choosing \nthe most likely label, then the easiest way to compute\nP(features) is to simply calculate the sum over labels of P(features, label):\n(3)P(features) = \u03a3 label \u2208 labels  P(features, label)\nThe label likelihood can be expanded out as the probability of the label times the prob-\nability of the features given the label:\n(4)P(features, label) = P(label) \u00d7 P(features|label)\nFurthermore, since the features are all independent of one another (given the label), we\ncan separate out the probability of each individual feature:\n(5)P(features, label) = P(label) \u00d7 \u2293 f \u2208 features P(f|label)\nThis is exactly the equation we discussed earlier for calculating the label likelihood:\nP(label) is the prior probability for a given label, and each P(f|label) is the contribution\nof a single feature to the label likelihood.\nZero Counts and Smoothing\nThe simplest way to calculate P(f|label), the contribution of a feature f toward the label\nlikelihood for a label label, is to take the percentage of training instances with the given\nlabel that also have the given feature:\n(6)P(f|label) = count(f, label)/count(label)\n248 | Chapter 6: \u2002Learning to Classify Text\nHowever, this simple approach can become problematic when a feature never occurs\nwith a \ngiven label in the training set. In this case, our calculated value for P(f|label) will\nbe zero, which will cause the label likelihood for the given label to be zero. Thus, the\ninput will never be assigned this label, regardless of how well the other features fit the\nlabel.\nThe basic problem here is with our calculation of P(f|label), the probability that an\ninput will have a feature, given a label. In particular, just because we haven\u2019t seen a\nfeature/label combination occur in the training set, doesn\u2019t mean it\u2019s impossible for\nthat combination to occur. For example, we may not have seen any murder mystery\ndocuments that contained the word football, but we wouldn\u2019t want to conclude that\nit\u2019s completely impossible for such documents to exist.\nThus, although count(f,label)/count(label) is a good estimate for P(f|label) when count(f,\nlabel) is relatively high, this estimate becomes less reliable when count(f) becomes\nsmaller. Therefore, when building naive Bayes models, we usually employ more so-\nphisticated techniques, known as smoothing techniques, for calculating P(f|label), the\nprobability of a feature given a label. For example, the Expected Likelihood Estima-\ntion for the probability of a feature given a label basically adds 0.5 to each\ncount(f,label) value, and the Heldout Estimation uses a heldout corpus to calculate\nthe relationship between feature frequencies and feature probabilities. The nltk.prob\nability module provides support for a wide variety of smoothing techniques.\nNon-Binary Features\nWe have assumed here that each feature is binary, i.e., that each input either has a\nfeature or does not. Label-valued features (e.g., a color feature, which could be red,\ngreen, blue, white, or orange) can be converted to binary features by replacing them\nwith binary features, such as \u201ccolor-is-red\u201d. Numeric features can be converted to bi-\nnary features by binning, which replaces them with features such as \u201c4<x<6.\u201d\nAnother alternative is to use regression methods to model the probabilities of numeric\nfeatures. For example, if we assume that the height feature has a bell curve distribution,\nthen we could estimate P(height|label) by finding the mean and variance of the heights\nof the inputs with each label. In this case, P(f=v|label) would not be a fixed value, but\nwould vary depending on the value of v.\nThe Naivete of Independence\nThe reason that naive Bayes classifiers are called \u201cnaive\u201d is that it\u2019s unreasonable to\nassume that all features are independent of one another (given the label). In particular,\nalmost all real-world problems contain features with varying degrees of dependence on\none another. If we had to avoid any features that were dependent on one another, it\nwould be very difficult to construct good feature sets that provide the required infor-\nmation to the machine learning algorithm.\n6.5  Naive Bayes Classifiers | 249\nSo what happens when we ignore the independence assumption, and use the naive\nBayes classifier \nwith features that are not independent? One problem that arises is that\nthe classifier can end up \u201cdouble-counting\u201d the effect of highly correlated features,\npushing the classifier closer to a given label than is justified.\nTo see how this can occur, consider a name gender classifier that contains two identical\nfeatures, f1 and f2. In other words, f2 is an exact copy of f1, and contains no new infor-\nmation. When the classifier is considering an input, it will include the contribution of\nboth f1 and f2 when deciding which label to choose. Thus, the information content of\nthese two features will be given more weight than it deserves.\nOf course, we don\u2019t usually build naive Bayes classifiers that contain two identical\nfeatures. However, we do build classifiers that contain features which are dependent\non one another. For example, the features ends-with(a) and ends-with(vowel) are de-\npendent on one another, because if an input value has the first feature, then it must\nalso have the second feature. For features like these, the duplicated information may\nbe given more weight than is justified by the training set.\nThe Cause of Double-Counting\nThe reason for the double-counting problem is that during training, feature contribu-\ntions are computed separately; but when using the classifier to choose labels for new\ninputs, those feature contributions are combined. One solution, therefore, is to con-\nsider the possible interactions between feature contributions during training. We could\nthen use those interactions to adjust the contributions that individual features make.\nTo make this more precise, we can rewrite the equation used to calculate the likelihood\nof a label, separating out the contribution made by each feature (or label):\n(7)P(features, label) = w[label] \u00d7 \u2293 f \u2208 features  w[f, label]\nHere, w[label] is the \u201cstarting score\u201d for a given label, and w[f, label] is the contribution\nmade by a given feature towards a label\u2019s likelihood. We call these values w[label] and\nw[f, label] the parameters or weights for the model. Using the naive Bayes algorithm,\nwe set each of these parameters independently:\n(8)w[label] = P(label)\n(9)w[f, label] = P(f|label)\nHowever, in the next section, we\u2019ll look at a classifier that considers the possible in-\nteractions between these parameters when choosing their values.\n6.6  Maximum Entropy Classifiers\nThe Maximum Entropy  classifier uses a model that is very similar to the model em-\nployed by the naive Bayes classifier. But rather than using probabilities to set the\n250 | Chapter 6: \u2002Learning to Classify Text\nmodel\u2019s parameters, it uses search techniques to find a set of parameters that will max-\nimize \nthe performance of the classifier. In particular, it looks for the set of parameters\nthat maximizes the total likelihood of the training corpus, which is defined as:\n(10) P(features) = \u03a3 x \u2208 corpus  P(label(x)|features(x))\nWhere P(label|features), the probability that an input whose features are features will\nhave class label label, is defined as:\n(11) P(label|features) = P(label, features)/\u03a3 label P(label, features)\nBecause of the potentially complex interactions between the effects of related features,\nthere is no way to directly calculate the model parameters that maximize the likelihood\nof the training set. Therefore, Maximum Entropy classifiers choose the model param-\neters using iterative optimization  techniques, which initialize the model\u2019s parameters\nto random values, and then repeatedly refine those parameters to bring them closer to\nthe optimal solution. These iterative optimization techniques guarantee that each re-\nfinement of the parameters will bring them closer to the optimal values, but do not\nnecessarily provide a means of determining when those optimal values have been\nreached. Because the parameters for Maximum Entropy classifiers are selected using\niterative optimization techniques, they can take a long time to learn. This is especially\ntrue when the size of the training set, the number of features, and the number of labels\nare all large.\nSome iterative optimization techniques are much faster than others.\nWhen training \nMaximum Entropy models, avoid the use of Generalized\nIterative Scaling (GIS) or Improved Iterative Scaling (IIS), which are both\nconsiderably slower than the Conjugate Gradient (CG) and the BFGS\noptimization methods.\nThe Maximum Entropy Model\nThe Maximum Entropy classifier model is a generalization of the model used by the\nnaive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier\ncalculates the likelihood of each label for a given input value by multiplying together\nthe parameters that are applicable for the input value and label. The naive Bayes clas-\nsifier model defines a parameter for each label, specifying its prior probability, and a\nparameter for each (feature, label) pair, specifying the contribution of individual fea-\ntures toward a label\u2019s likelihood.\nIn contrast, the Maximum Entropy classifier model leaves it up to the user to decide\nwhat combinations of labels and features should receive their own parameters. In par-\nticular, it is possible to use a single parameter to associate a feature with more than one\nlabel; or to associate more than one feature with a given label. This will sometimes\n6.6  Maximum Entropy Classifiers | 251\nallow the model to \u201cgeneralize\u201d over some of the differences between related labels or\nfeatures.\nEach combination \nof labels and features that receives its own parameter is called a\njoint-feature. Note that joint-features are properties of labeled values, whereas (sim-\nple) features are properties of unlabeled values.\nIn literature that describes and discusses Maximum Entropy models,\nthe term \n\u201cfeatures\u201d often refers to joint-features; the term \u201ccontexts\u201d\nrefers to what we have been calling (simple) features.\nTypically, the joint-features that are used to construct Maximum Entropy models ex-\nactly mirror those that are used by the naive Bayes model. In particular, a joint-feature\nis defined for each label, corresponding to w[label], and for each combination of (sim-\nple) feature and label, corresponding to w[f, label]. Given the joint-features for a Max-\nimum Entropy model, the score assigned to a label for a given input is simply the\nproduct of the parameters associated with the joint-features that apply to that input\nand label:\n(12) P(input, label) = \u2293 joint-features(input,label) w[joint-feature]\nMaximizing Entropy\nThe intuition that motivates Maximum Entropy classification is that we should build\na model that captures the frequencies of individual joint-features, without making any\nunwarranted assumptions. An example will help to illustrate this principle.\nSuppose we are assigned the task of picking the correct word sense for a given word,\nfrom a list of 10 possible senses (labeled A\u2013J). At first, we are not told anything more\nabout the word or the senses. There are many probability distributions that we could\nchoose for the 10 senses, such as:\nA B C D E F G H I J\n(i) 10% 10% 10% 10% 10% 10% 10% 10% 10% 10%\n(ii) 5% 15% 0% 30% 0% 8% 12% 0% 6% 24%\n(iii) 0% 100% 0% 0% 0% 0% 0% 0% 0% 0%\nAlthough any of these distributions might be correct, we are likely to choose distribution\n(i), because without any more information, there is no reason to believe that any word\nsense is more likely than any other. On the other hand, distributions (ii) and (iii) reflect\nassumptions that are not supported by what we know.\nOne way to capture this intuition that distribution (i) is more \u201cfair\u201d than the other two\nis to invoke the concept of entropy. In the discussion of decision trees, we described\n252 | Chapter 6: \u2002Learning to Classify Text\nentropy as a measure of how \u201cdisorganized\u201d a set of labels was. In particular, if a single\nlabel dominates \nthen entropy is low, but if the labels are more evenly distributed then\nentropy is high. In our example, we chose distribution (i) because its label probabilities\nare evenly distributed\u2014in other words, because its entropy is high. In general, the\nMaximum Entropy principle  states that, among the distributions that are consistent\nwith what we know, we should choose the distribution whose entropy is highest.\nNext, suppose that we are told that sense A appears 55% of the time. Once again, there\nare many distributions that are consistent with this new piece of information, such as:\nA B C D E F G H I J\n(iv) 55% 45% 0% 0% 0% 0% 0% 0% 0% 0%\n(v) 55% 5% 5% 5% 5% 5% 5% 5% 5% 5%\n(vi) 55% 3% 1% 2% 9% 5% 0% 25% 0% 0%\nBut again, we will likely choose the distribution that makes the fewest unwarranted\nassumptions\u2014in this case, distribution (v).\nFinally, suppose \nthat we are told that the word up appears in the nearby context 10%\nof the time, and that when it does appear in the context there\u2019s an 80% chance that\nsense A or C will be used. In this case, we will have a harder time coming up with an\nappropriate distribution by hand; however, we can verify that the following distribution\nlooks appropriate:\n A B C D E F G H I J\n(vii) +up 5.1% 0.25% 2.9% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25%\n\u2013up 49.9% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46%\nIn particular, the distribution is consistent with what we know: if we add up the prob-\nabilities in \ncolumn A, we get 55%; if we add up the probabilities of row 1, we get 10%;\nand if we add up the boxes for senses A and C in the +up row, we get 8% (or 80% of\nthe +up cases). Furthermore, the remaining probabilities appear to be \u201cevenly\ndistributed.\u201d\nThroughout this example, we have restricted ourselves to distributions that are con-\nsistent with what we know; among these, we chose the distribution with the highest\nentropy. This is exactly what the Maximum Entropy classifier does as well. In\nparticular, for each joint-feature, the Maximum Entropy model calculates the \u201cempir-\nical frequency\u201d of that feature\u2014i.e., the frequency with which it occurs in the training\nset. It then searches for the distribution which maximizes entropy, while still predicting\nthe correct frequency for each joint-feature.\n6.6  Maximum Entropy Classifiers | 253\nGenerative Versus Conditional Classifiers\nAn important \ndifference between the naive Bayes classifier and the Maximum Entropy\nclassifier concerns the types of questions they can be used to answer. The naive Bayes\nclassifier is an example of a generative classifier, which builds a model that predicts\nP(input, label), the joint probability of an ( input, label) pair. As a result, generative\nmodels can be used to answer the following questions:\n1. What is the most likely label for a given input?\n2. How likely is a given label for a given input?\n3. What is the most likely input value?\n4. How likely is a given input value?\n5. How likely is a given input value with a given label?\n6. What is the most likely label for an input that might have one of two values (but\nwe don\u2019t know which)?\nThe Maximum Entropy classifier, on the other hand, is an example of a conditional\nclassifier. Conditional classifiers build models that predict P(label|input)\u2014the proba-\nbility of a label given the input value. Thus, conditional models can still be used to\nanswer questions 1 and 2. However, conditional models cannot be used to answer the\nremaining questions 3\u20136.\nIn general, generative models are strictly more powerful than conditional models, since\nwe can calculate the conditional probability P(label|input) from the joint probability\nP(input, label ), but not vice versa. However, this additional power comes at a price.\nBecause the model is more powerful, it has more \u201cfree parameters\u201d that need to be\nlearned. However, the size of the training set is fixed. Thus, when using a more powerful\nmodel, we end up with less data that can be used to train each parameter\u2019s value, making\nit harder to find the best parameter values. As a result, a generative model may not do\nas good a job at answering questions 1 and 2 as a conditional model, since the condi-\ntional model can focus its efforts on those two questions. However, if we do need\nanswers to questions like 3\u20136, then we have no choice but to use a generative model.\nThe difference between a generative model and a conditional model is analogous to the\ndifference between a topographical map and a picture of a skyline. Although the topo-\ngraphical map can be used to answer a wider variety of questions, it is significantly\nmore difficult to generate an accurate topographical map than it is to generate an ac-\ncurate skyline.\n6.7  Modeling Linguistic Patterns\nClassifiers can help us to understand the linguistic patterns that occur in natural lan-\nguage, by allowing us to create explicit models that capture those patterns. Typically,\nthese models are using supervised classification techniques, but it is also possible to\n254 | Chapter 6: \u2002Learning to Classify Text\nbuild analytically motivated models. Either way, these explicit models serve two im-\nportant purposes: \nthey help us to understand linguistic patterns, and they can be used\nto make predictions about new language data.\nThe extent to which explicit models can give us insights into linguistic patterns depends\nlargely on what kind of model is used. Some models, such as decision trees, are relatively\ntransparent, and give us direct information about which factors are important in mak-\ning decisions and about which factors are related to one another. Other models, such\nas multilevel neural networks, are much more opaque. Although it can be possible to\ngain insight by studying them, it typically takes a lot more work.\nBut all explicit models can make predictions about new unseen language data that was\nnot included in the corpus used to build the model. These predictions can be evaluated\nto assess the accuracy of the model. Once a model is deemed sufficiently accurate, it\ncan then be used to automatically predict information about new language data. These\npredictive models can be combined into systems that perform many useful language\nprocessing tasks, such as document classification, automatic translation, and question\nanswering.\nWhat Do Models Tell Us?\nIt\u2019s important to understand what we can learn about language from an automatically\nconstructed model. One important consideration when dealing with models of lan-\nguage is the distinction between descriptive models and explanatory models. Descrip-\ntive models capture patterns in the data, but they don\u2019t provide any information about\nwhy the data contains those patterns. For example, as we saw in Table 3-1 , the syno-\nnyms absolutely and definitely are not interchangeable: we say absolutely adore not\ndefinitely adore , and definitely prefer , not absolutely prefer . In contrast, explanatory\nmodels attempt to capture properties and relationships that cause the linguistic pat-\nterns. For example, we might introduce the abstract concept of \u201cpolar adjective\u201d as an\nadjective that has an extreme meaning, and categorize some adjectives, such as adore\nand detest as polar. Our explanatory model would contain the constraint that abso-\nlutely can combine only with polar adjectives, and definitely can only combine with\nnon-polar adjectives. In summary, descriptive models provide information about cor-\nrelations in the data, while explanatory models go further to postulate causal\nrelationships.\nMost models that are automatically constructed from a corpus are descriptive models;\nin other words, they can tell us what features are relevant to a given pattern or con-\nstruction, but they can\u2019t necessarily tell us how those features and patterns relate to\none another. If our goal is to understand the linguistic patterns, then we can use this\ninformation about which features are related as a starting point for further experiments\ndesigned to tease apart the relationships between features and patterns. On the other\nhand, if we\u2019re just interested in using the model to make predictions (e.g., as part of a\nlanguage processing system), then we can use the model to make predictions about\nnew data without worrying about the details of underlying causal relationships.\n6.7  Modeling Linguistic Patterns | 255\n6.8  Summary\n\u2022 Modeling the \nlinguistic data found in corpora can help us to understand linguistic\npatterns, and can be used to make predictions about new language data.\n\u2022 Supervised classifiers use labeled training corpora to build models that predict the\nlabel of an input based on specific features of that input.\n\u2022 Supervised classifiers can perform a wide variety of NLP tasks, including document\nclassification, part-of-speech tagging, sentence segmentation, dialogue act type\nidentification, and determining entailment relations, and many other tasks.\n\u2022 When training a supervised classifier, you should split your corpus into three da-\ntasets: a training set for building the classifier model, a dev-test set for helping select\nand tune the model\u2019s features, and a test set for evaluating the final model\u2019s\nperformance.\n\u2022 When evaluating a supervised classifier, it is important that you use fresh data that\nwas not included in the training or dev-test set. Otherwise, your evaluation results\nmay be unrealistically optimistic.\n\u2022 Decision trees are automatically constructed tree-structured flowcharts that are\nused to assign labels to input values based on their features. Although they\u2019re easy\nto interpret, they are not very good at handling cases where feature values interact\nin determining the proper label.\n\u2022 In naive Bayes classifiers, each feature independently contributes to the decision\nof which label should be used. This allows feature values to interact, but can be\nproblematic when two or more features are highly correlated with one another.\n\u2022 Maximum Entropy classifiers use a basic model that is similar to the model used\nby naive Bayes; however, they employ iterative optimization to find the set of fea-\nture weights that maximizes the probability of the training set.\n\u2022 Most of the models that are automatically constructed from a corpus are descrip-\ntive, that is, they let us know which features are relevant to a given pattern or\nconstruction, but they don\u2019t give any information about causal relationships be-\ntween those features and patterns.\n6.9  Further Reading\nPlease consult http://www.nltk.org/ for further materials on this chapter and on how to\ninstall external machine learning packages, such as Weka, Mallet, TADM, and MegaM.\nFor more examples of classification and machine learning with NLTK, please see the\nclassification HOWTOs at http://www.nltk.org/howto.\nFor a general introduction to machine learning, we recommend (Alpaydin, 2004). For\na more mathematically intense introduction to the theory of machine learning, see\n(Hastie, Tibshirani & Friedman, 2009). Excellent books on using machine learning\n256 | Chapter 6: \u2002Learning to Classify Text\ntechniques for NLP include (Abney, 2008), (Daelemans & Bosch, 2005), (Feldman &\nSanger, 2007), \n(Segaran, 2007), and (Weiss et al., 2004). For more on smoothing tech-\nniques for language problems, see (Manning & Sch\u00fctze, 1999). For more on sequence\nmodeling, and especially hidden Markov models, see (Manning & Sch\u00fctze, 1999) or\n(Jurafsky & Martin, 2008). Chapter 13 of (Manning, Raghavan & Sch\u00fctze, 2008) dis-\ncusses the use of naive Bayes for classifying texts.\nMany of the machine learning algorithms discussed in this chapter are numerically\nintensive, and as a result, they will run slowly when coded naively in Python. For in-\nformation on increasing the efficiency of numerically intensive algorithms in Python,\nsee (Kiusalaas, 2005).\nThe classification techniques described in this chapter can be applied to a very wide\nvariety of problems. For example, (Agirre & Edmonds, 2007) uses classifiers to perform\nword-sense disambiguation; and (Melamed, 2001) uses classifiers to create parallel\ntexts. Recent textbooks that cover text classification include (Manning, Raghavan &\nSch\u00fctze, 2008) and (Croft, Metzler & Strohman, 2009).\nMuch of the current research in the application of machine learning techniques to NLP\nproblems is driven by government-sponsored \u201cchallenges,\u201d where a set of research\norganizations are all provided with the same development corpus and asked to build a\nsystem, and the resulting systems are compared based on a reserved test set. Examples\nof these challenge competitions include CoNLL Shared Tasks, the Recognizing Textual\nEntailment competitions, the ACE competitions, and the AQUAINT competitions.\nConsult http://www.nltk.org/ for a list of pointers to the web pages for these challenges.\n6.10  Exercises\n1.\u25cb Read up on one of the language technologies mentioned in this section, such as\nword sense disambiguation, semantic role labeling, question answering, machine\ntranslation, or named entity recognition. Find out what type and quantity of an-\nnotated data is required for developing such systems. Why do you think a large\namount of data is required?\n2.\u25cb Using any of the three classifiers described in this chapter, and any features you\ncan think of, build the best name gender classifier you can. Begin by splitting the\nNames Corpus into three subsets: 500 words for the test set, 500 words for the\ndev-test set, and the remaining 6,900 words for the training set. Then, starting with\nthe example name gender classifier, make incremental improvements. Use the dev-\ntest set to check your progress. Once you are satisfied with your classifier, check\nits final performance on the test set. How does the performance on the test set\ncompare to the performance on the dev-test set? Is this what you\u2019d expect?\n3.\u25cb The Senseval 2 Corpus contains data intended to train word-sense disambigua-\ntion classifiers. It contains data for four words: hard, interest, line, and serve.\nChoose one of these four words, and load the corresponding data:\n6.10  Exercises | 257\n>>> from nltk.corpus import senseval\n>>> instances = senseval.instances('hard.pos')\n>>> size = int(len(instances) * 0.1)\n>>> train_set, test_set = instances[size:], instances[:size]\nUsing this \ndataset, build a classifier that predicts the correct sense tag for a given\ninstance. See the corpus HOWTO at http://www.nltk.org/howto for information\non using the instance objects returned by the Senseval 2 Corpus.\n4.\u25cb Using the movie review document classifier discussed in this chapter, generate\na list of the 30 features that the classifier finds to be most informative. Can you\nexplain why these particular features are informative? Do you find any of them\nsurprising?\n5.\u25cb Select one of the classification tasks described in this chapter, such as name\ngender detection, document classification, part-of-speech tagging, or dialogue act\nclassification. Using the same training and test data, and the same feature extractor,\nbuild three classifiers for the task: a decision tree, a naive Bayes classifier, and a\nMaximum Entropy classifier. Compare the performance of the three classifiers on\nyour selected task. How do you think that your results might be different if you\nused a different feature extractor?\n6.\u25cb The synonyms strong and powerful pattern differently (try combining them with\nchip and sales). What features are relevant in this distinction? Build a classifier that\npredicts when each word should be used.\n7.\u25d1 The dialogue act classifier assigns labels to individual posts, without considering\nthe context in which the post is found. However, dialogue acts are highly depend-\nent on context, and some sequences of dialogue act are much more likely than\nothers. For example, a ynQuestion dialogue act is much more likely to be answered\nby a yanswer than by a greeting. Make use of this fact to build a consecutive clas-\nsifier for labeling dialogue acts. Be sure to consider what features might be useful.\nSee the code for the consecutive classifier for part-of-speech tags in Example 6-5\nto get some ideas.\n8.\u25d1 Word features can be very useful for performing document classification, since\nthe words that appear in a document give a strong indication about what its se-\nmantic content is. However, many words occur very infrequently, and some of the\nmost informative words in a document may never have occurred in our training\ndata. One solution is to make use of a lexicon, which describes how different words\nrelate to one another. Using the WordNet lexicon, augment the movie review\ndocument classifier presented in this chapter to use features that generalize the\nwords that appear in a document, making it more likely that they will match words\nfound in the training data.\n9.\u25cf The PP Attachment Corpus is a corpus describing prepositional phrase attach-\nment decisions. Each instance in the corpus is encoded as a PPAttachment object:\n258 | Chapter 6: \u2002Learning to Classify Text\n>>> from nltk.corpus import ppattach\n>>> ppattach.attachments('training') \n[PPAttachment(sent='0', verb='join', noun1='board',\n              prep='as', noun2='director', attachment='V'),\n PPAttachment(sent='1', verb='is', noun1='chairman',\n              prep='of', noun2='N.V.', attachment='N'),\n ...]\n>>> inst = ppattach.attachments('training')[1]\n>>> (inst.noun1, inst.prep, inst.noun2)\n('chairman', 'of', 'N.V.')\nSelect only the instances where inst.attachment is N:\n>>> nattach = [inst for inst in ppattach.attachments('training')\n...            if inst.attachment == 'N']\nUsing this \nsubcorpus, build a classifier that attempts to predict which preposition\nis used to connect a given pair of nouns. For example, given the pair of nouns\nteam and researchers, the classifier should predict the preposition of. See the corpus\nHOWTO at http://www.nltk.org/howto for more information on using the PP At-\ntachment Corpus.\n10.\u25cf Suppose you wanted to automatically generate a prose description of a scene,\nand already had a word to uniquely describe each entity, such as the book, and\nsimply wanted to decide whether to use in or on in relating various items, e.g., the\nbook is in the cupboard versus the book is on the shelf. Explore this issue by looking\nat corpus data and writing programs as needed. Consider the following examples:\n(13) a. in the car versus on the train\nb. in town versus on campus\nc. in the picture versus on the screen\nd. in Macbeth versus on Letterman\n6.10  Exercises | 259\n\nCHAPTER 7\nExtracting Information from Text\nFor any given question, it\u2019s likely that someone has written the answer down some-\nwhere. The \namount of natural language text that is available in electronic form is truly\nstaggering, and is increasing every day. However, the complexity of natural language\ncan make it very difficult to access the information in that text. The state of the art in\nNLP is still a long way from being able to build general-purpose representations of\nmeaning from unrestricted text. If we instead focus our efforts on a limited set of ques-\ntions or \u201centity relations,\u201d such as \u201cwhere are different facilities located\u201d or \u201cwho is\nemployed by what company,\u201d we can make significant progress. The goal of this chap-\nter is to answer the following questions:\n1. How can we build a system that extracts structured data from unstructured text?\n2. What are some robust methods for identifying the entities and relationships de-\nscribed in a text?\n3. Which corpora are appropriate for this work, and how do we use them for training\nand evaluating our models?\nAlong the way, we\u2019ll apply techniques from the last two chapters to the problems of\nchunking and named entity recognition.\n7.1  Information Extraction\nInformation comes in many shapes and sizes. One important form is structured\ndata, where there is a regular and predictable organization of entities and relationships.\nFor example, we might be interested in the relation between companies and locations.\nGiven a particular company, we would like to be able to identify the locations where\nit does business; conversely, given a location, we would like to discover which com-\npanies do business in that location. If our data is in tabular form, such as the example\nin Table 7-1, then answering these queries is straightforward.\n261\nTable 7-1. Locations data\nOrgName LocationName\nOmnicom New York\nDDB Needham New York\nKaplan Thaler Group New York\nBBDO South Atlanta\nGeorgia-Pacific Atlanta\nIf this location data was stored in Python as a list of tuples (entity, relation,\nentity), then \nthe question \u201cWhich organizations operate in Atlanta?\u201d could be trans-\nlated as follows:\n>>> print [org for (e1, rel, e2) if rel=='IN' and e2=='Atlanta'] \n['BBDO South', 'Georgia-Pacific']\nThings are more tricky if we try to get similar information out of text. For example,\nconsider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).\n(1) The fourth Wells account moving to another agency is the packaged paper-\nproducts division of Georgia-Pacific Corp., which arrived at Wells only last fall.\nLike Hertz and the History Channel, it is also leaving for an Omnicom-owned\nagency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta,\nwhich handles corporate advertising for Georgia-Pacific, will assume additional\nduties for brands like Angel Soft toilet tissue and Sparkle paper towels, said\nKen Haldin, a spokesman for Georgia-Pacific in Atlanta.\nIf you read through (1), you will glean the information required to answer the example\nquestion. But how do we get a machine to understand enough about (1) to return the\nlist ['BBDO South', 'Georgia-Pacific'] as an answer? This is obviously a much harder\ntask. Unlike Table 7-1 , (1) contains no structure that links organization names with\nlocation names.\nOne approach to this problem involves building a very general representation of mean-\ning (Chapter 10). In this chapter we take a different approach, deciding in advance that\nwe will only look for very specific kinds of information in text, such as the relation\nbetween organizations and locations. Rather than trying to use text like (1) to answer\nthe question directly, we first convert the unstructured data  of natural language sen-\ntences into the structured data of Table 7-1 . Then we reap the benefits of powerful\nquery tools such as SQL. This method of getting meaning from text is called Infor-\nmation Extraction.\nInformation Extraction has many applications, including business intelligence, resume\nharvesting, media analysis, sentiment detection, patent search, and email scanning. A\nparticularly important area of current research involves the attempt to extract\n262 | Chapter 7: \u2002Extracting Information from Text\nstructured data out of electronically available scientific literature, especially in the do-\nmain of biology and medicine.\nInformation Extraction Architecture\nFigure 7-1  \nshows the architecture for a simple information extraction system. It begins\nby processing a document using several of the procedures discussed in Chapters 3 and\n5: first, the raw text of the document is split into sentences using a sentence segmenter,\nand each sentence is further subdivided into words using a tokenizer. Next, each sen-\ntence is tagged with part-of-speech tags, which will prove very helpful in the next step,\nnamed entity recognition . In this step, we search for mentions of potentially inter-\nesting entities in each sentence. Finally, we use relation recognition  to search for likely\nrelations between different entities in the text.\nFigure 7-1. Simple pipeline architecture for an information extraction system. This system takes the\nraw text \nof a document as its input, and generates a list of ( entity , relation , entity ) tuples as its\noutput. For example, given a document that indicates that the company Georgia-Pacific is located in\nAtlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']) .\nTo perform the first three tasks, we can define a function that simply connects together\nNLTK\u2019s default sentence segmenter \n , word tokenizer \n , and part-of-speech\ntagger \n :\n>>> def ie_preprocess(document):\n...    sentences = nltk.sent_tokenize(document) \n...    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n...    sentences = [nltk.pos_tag(sent) for sent in sentences] \n7.1  Information Extraction | 263\nRemember that our program samples assume you begin your interactive\nsession or your program with import nltk, re, pprint.\nNext, in \nnamed entity recognition, we segment and label the entities that might par-\nticipate in interesting relations with one another. Typically, these will be definite noun\nphrases such as the knights who say \u201cni\u201d , or proper names such as Monty Python . In\nsome tasks it is useful to also consider indefinite nouns or noun chunks, such as every\nstudent or cats, and these do not necessarily refer to entities in the same way as definite\nNPs and proper names.\nFinally, in relation extraction, we search for specific patterns between pairs of entities\nthat occur near one another in the text, and use those patterns to build tuples recording\nthe relationships between the entities.\n7.2  Chunking\nThe basic technique we will use for entity recognition is chunking, which segments\nand labels multitoken sequences as illustrated in Figure 7-2 . The smaller boxes show\nthe word-level tokenization and part-of-speech tagging, while the large boxes show\nhigher-level chunking. Each of these larger boxes is called a chunk. Like tokenization,\nwhich omits whitespace, chunking usually selects a subset of the tokens. Also like\ntokenization, the pieces produced by a chunker do not overlap in the source text.\nIn this section, we will explore chunking in some depth, beginning with the definition\nand representation of chunks. We will see regular expression and n-gram approaches\nto chunking, and will develop and evaluate chunkers using the CoNLL-2000 Chunking\nCorpus. We will then return in Sections 7.5 and 7.6 to the tasks of named entity rec-\nognition and relation extraction.\nNoun Phrase Chunking\nWe will begin by considering the task of noun phrase chunking, or NP-chunking,\nwhere we search for chunks corresponding to individual noun phrases. For example,\nhere is some Wall Street Journal text with NP-chunks marked using brackets:\nFigure 7-2. Segmentation and labeling at both the Token and Chunk levels.\n264 | Chapter 7: \u2002Extracting Information from Text\n(2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/\nIN [ \nDigital/NNP ] [ \u2019s/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB\nthat/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ]\nshould/MD do/VB well/RB there/RB ./.\nAs we can see, NP-chunks are often smaller pieces than complete noun phrases. For\nexample, the market for system-management software for Digital\u2019s hardware  is a single\nnoun phrase (containing two nested noun phrases), but it is captured in NP-chunks by\nthe simpler chunk the market . One of the motivations for this difference is that NP-\nchunks are defined so as not to contain other NP-chunks. Consequently, any preposi-\ntional phrases or subordinate clauses that modify a nominal will not be included in the\ncorresponding NP-chunk, since they almost certainly contain further noun phrases.\nOne of the most useful sources of information for NP-chunking is part-of-speech tags.\nThis is one of the motivations for performing part-of-speech tagging in our information\nextraction system. We demonstrate this approach using an example sentence that has\nbeen part-of-speech tagged in Example 7-1 . In order to create an NP-chunker, we will\nfirst define a chunk grammar , consisting of rules that indicate how sentences should\nbe chunked. In this case, we will define a simple grammar with a single regular\nexpression rule \n . This rule says that an NP chunk should be formed whenever the\nchunker finds \nan optional determiner ( DT) followed by any number of adjectives ( JJ)\nand then a noun ( NN). Using this grammar, we create a chunk parser \n , and test it on\nour example \nsentence \n . The result is a tree, which we can either print \n , or display\ngraphically \n .\nExample 7-1. Example of a simple regular expression\u2013based NP chunker.\n>>> sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \n... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n>>> grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n>>> cp = nltk.RegexpParser(grammar) \n>>> result = cp.parse(sentence) \n>>> print result \n(S\n  (NP the/DT little/JJ yellow/JJ dog/NN)\n  barked/VBD\n  at/IN\n  (NP the/DT cat/NN))\n>>> result.draw() \n7.2  Chunking | 265\nTag Patterns\nThe rules \nthat make up a chunk grammar use tag patterns  to describe sequences of\ntagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle\nbrackets, e.g., <DT>?<JJ>*<NN>. Tag patterns are similar to regular expression patterns\n(Section 3.4 ). Now, consider the following noun phrases from the Wall Street Journal :\nanother/DT sharp/JJ dive/NN\ntrade/NN figures/NNS\nany/DT new/JJ policy/NN measures/NNS\nearlier/JJR stages/NNS\nPanamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\nWe can match these noun phrases using a slight refinement of the first tag pattern\nabove, i.e., <DT>?<JJ.*>*<NN.*>+. This will chunk any sequence of tokens beginning\nwith an optional determiner, followed by zero or more adjectives of any type (including\nrelative adjectives like earlier/JJR), followed by one or more nouns of any type. How-\never, it is easy to find many more complicated examples which this rule will not cover:\nhis/PRP$ Mansion/NNP House/NNP speech/NN\nthe/DT price/NN cutting/VBG\n3/CD %/NN to/TO 4/CD %/NN\nmore/JJR than/IN 10/CD %/NN\nthe/DT fastest/JJS developing/VBG trends/NNS\n's/POS skill/NN\nYour Turn: Try to come up with tag patterns to cover these cases. Test\nthem using the graphical interface nltk.app.chunkparser(). Continue\nto refine your tag patterns with the help of the feedback given by this\ntool.\nChunking with Regular Expressions\nTo find the chunk structure for a given sentence, the RegexpParser chunker begins with\na flat structure in which no tokens are chunked. The chunking rules are applied in turn,\nsuccessively updating the chunk structure. Once all of the rules have been invoked, the\nresulting chunk structure is returned.\nExample 7-2  shows a simple chunk grammar consisting of two rules. The first rule\nmatches an optional determiner or possessive pronoun, zero or more adjectives, then\n266 | Chapter 7: \u2002Extracting Information from Text\na noun. The second rule matches one or more proper nouns. We also define an example\nsentence to be chunked \n , and run the chunker on this input \n .\nExample 7-2. Simple noun phrase chunker.\ngrammar = r\"\"\"\n  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and nouns\n      {<NNP>+}                # chunk sequences of proper nouns\n\"\"\"\ncp = nltk.RegexpParser(grammar)\nsentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \n                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n>>> print cp.parse(sentence) \n(S\n  (NP Rapunzel/NNP)\n  let/VBD\n  down/RP\n  (NP her/PP$ long/JJ golden/JJ hair/NN))\nThe $ symbol is a special character in regular expressions, and must be\nbackslash escaped in order to match the tag \nPP$.\nIf a tag pattern matches at overlapping locations, the leftmost match takes precedence.\nFor example, if we apply a rule that matches two consecutive nouns to a text containing\nthree consecutive nouns, then only the first two nouns will be chunked:\n>>> nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n>>> grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\n>>> cp = nltk.RegexpParser(grammar)\n>>> print cp.parse(nouns)\n(S (NP money/NN market/NN) fund/NN)\nOnce we have created the chunk for money market , we have removed the context that\nwould have permitted fund to be included in a chunk. This issue would have been\navoided with a more permissive chunk rule, e.g., NP: {<NN>+}.\nWe have added a comment to each of our chunk rules. These are op-\ntional; when \nthey are present, the chunker prints these comments as\npart of its tracing output.\nExploring Text Corpora\nIn Section 5.2 , we saw how we could interrogate a tagged corpus to extract phrases\nmatching a particular sequence of part-of-speech tags. We can do the same work more\neasily with a chunker, as follows:\n7.2  Chunking | 267\n>>> cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n>>> brown = nltk.corpus.brown\n>>> for sent in brown.tagged_sents():\n...     tree = cp.parse(sent)\n...     for subtree in tree.subtrees():\n...         if subtree.node == 'CHUNK': print subtree\n...\n(CHUNK combined/VBN to/TO achieve/VB)\n(CHUNK continue/VB to/TO place/VB)\n(CHUNK serve/VB to/TO protect/VB)\n(CHUNK wanted/VBD to/TO wait/VB)\n(CHUNK allowed/VBN to/TO place/VB)\n(CHUNK expected/VBN to/TO become/VB)\n...\n(CHUNK seems/VBZ to/TO overtake/VB)\n(CHUNK want/VB to/TO buy/VB)\nYour Turn:  Encapsulate the previous example inside a function\nfind_chunks() that takes a chunk string like \"CHUNK: {<V.*> <TO>\n<V.*>}\" as an argument. Use it to search the corpus for several other\npatterns, such as four or more nouns in a row, e.g., \"NOUNS:\n{<N.*>{4,}}\".\nChinking\nSometimes it is easier to define what we want to exclude from a chunk. We can define\na chink to be a sequence of tokens that is not included in a chunk. In the following\nexample, barked/VBD at/IN is a chink:\n[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\nChinking is the process of removing a sequence of tokens from a chunk. If the matching\nsequence of tokens spans an entire chunk, then the whole chunk is removed; if the\nsequence of tokens appears in the middle of the chunk, these tokens are removed,\nleaving two chunks where there was only one before. If the sequence is at the periphery\nof the chunk, these tokens are removed, and a smaller chunk remains. These three\npossibilities are illustrated in Table 7-2.\nTable 7-2. Three chinking rules applied to the same chunk\nEntire chunk Middle of a chunk End of a chunk\nInput [a/DT little/JJ dog/NN] [a/DT little/JJ dog/NN] [a/DT little/JJ dog/NN]\nOperation Chink \u201cDT JJ NN\u201d Chink \u201cJJ\u201d Chink \u201cNN\u201d\nPattern }DT JJ NN{ }JJ{ }NN{\nOutput a/DT little/JJ dog/NN [a/DT] little/JJ [dog/NN] [a/DT little/JJ] dog/NN\n268 | Chapter 7: \u2002Extracting Information from Text\nIn Example 7-3 , we put the entire sentence into a single chunk, then excise the chinks.\nExample 7-3. Simple chinker.\ngrammar = r\"\"\"\n  NP:\n    {<.*>+}          # Chunk everything\n    }<VBD|IN>+{      # Chink sequences of VBD and IN\n  \"\"\"\nsentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\ncp = nltk.RegexpParser(grammar)\n>>> print cp.parse(sentence)\n(S\n  (NP the/DT little/JJ yellow/JJ dog/NN)\n  barked/VBD\n  at/IN\n  (NP the/DT cat/NN))\nRepresenting Chunks: Tags Versus Trees\nAs befits their intermediate status between tagging and parsing ( Chapter 8 ), chunk\nstructures can be represented using either tags or trees. The most widespread file rep-\nresentation uses IOB tags. In this scheme, each token is tagged with one of three special\nchunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the\nbeginning of a chunk. Subsequent tokens within the chunk are tagged I. All other\ntokens are tagged O. The B and I tags are suffixed with the chunk type, e.g., B-NP, I-\nNP. Of course, it is not necessary to specify a chunk type for tokens that appear outside\na chunk, so these are just labeled O. An example of this scheme is shown in Figure 7-3 .\nFigure 7-3. Tag representation of chunk structures.\nIOB tags \nhave become the standard way to represent chunk structures in files, and we\nwill also be using this format. Here is how the information in Figure 7-3  would appear\nin a file:\nWe PRP B-NP\nsaw VBD O\nthe DT B-NP\nlittle JJ I-NP\nyellow JJ I-NP\ndog NN I-NP\n7.2  Chunking | 269\nIn this representation there is one token per line, each with its part-of-speech tag and\nchunk tag. \nThis format permits us to represent more than one chunk type, so long as\nthe chunks do not overlap. As we saw earlier, chunk structures can also be represented\nusing trees. These have the benefit that each chunk is a constituent that can be manip-\nulated directly. An example is shown in Figure 7-4.\nFigure 7-4. Tree representation of chunk structures.\nNLTK uses trees for its internal representation of chunks, but provides\nmethods for converting between such trees and the IOB format.\n7.3  Developing and Evaluating Chunkers\nNow you \nhave a taste of what chunking does, but we haven\u2019t explained how to evaluate\nchunkers. As usual, this requires a suitably annotated corpus. We begin by looking at\nthe mechanics of converting IOB format into an NLTK tree, then at how this is done\non a larger scale using a chunked corpus. We will see how to score the accuracy of a\nchunker relative to a corpus, then look at some more data-driven ways to search for\nNP chunks. Our focus throughout will be on expanding the coverage of a chunker.\nReading IOB Format and the CoNLL-2000 Chunking Corpus\nUsing the corpora module we can load Wall Street Journal text that has been tagged\nthen chunked using the IOB notation. The chunk categories provided in this corpus\nare NP, VP, and PP. As we have seen, each sentence is represented using multiple lines,\nas shown here:\nhe PRP B-NP\naccepted VBD B-VP\nthe DT B-NP\nposition NN I-NP\n...\n270 | Chapter 7: \u2002Extracting Information from Text\nA conversion function chunk.conllstr2tree() builds a tree representation from one of\nthese multiline strings. Moreover, it permits us to choose any subset of the three chunk\ntypes to use, here just for NP chunks:\n>>> text = '''\n... he PRP B-NP\n... accepted VBD B-VP\n... the DT B-NP\n... position NN I-NP\n... of IN B-PP\n... vice NN B-NP\n... chairman NN I-NP\n... of IN B-PP\n... Carlyle NNP B-NP\n... Group NNP I-NP\n... , , O\n... a DT B-NP\n... merchant NN I-NP\n... banking NN I-NP\n... concern NN I-NP\n... . . O\n... '''\n>>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\nWe can use the NLTK corpus module to access a larger amount of chunked text. The\nCoNLL-2000 Chunking \nCorpus contains 270k words of Wall Street Journal  text, divi-\nded into \u201ctrain\u201d and \u201ctest\u201d portions, annotated with part-of-speech tags and chunk tags\nin the IOB format. We can access the data using nltk.corpus.conll2000. Here is an\nexample that reads the 100th sentence of the \u201ctrain\u201d portion of the corpus:\n>>> from nltk.corpus import conll2000\n>>> print conll2000.chunked_sents('train.txt')[99]\n(S\n  (PP Over/IN)\n  (NP a/DT cup/NN)\n  (PP of/IN)\n  (NP coffee/NN)\n  ,/,\n  (NP Mr./NNP Stone/NNP)\n  (VP told/VBD)\n  (NP his/PRP$ story/NN)\n  ./.)\nAs you can see, the CoNLL-2000 Chunking Corpus contains three chunk types: NP\nchunks, which we have already seen; VP chunks, such as has already delivered ; and PP\n7.3  Developing and Evaluating Chunkers | 271\nchunks, such as because of . Since we are only interested in the NP chunks right now, we\ncan use the chunk_types argument to select them:\n>>> print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]\n(S\n  Over/IN\n  (NP a/DT cup/NN)\n  of/IN\n  (NP coffee/NN)\n  ,/,\n  (NP Mr./NNP Stone/NNP)\n  told/VBD\n  (NP his/PRP$ story/NN)\n  ./.)\nSimple Evaluation and Baselines\nNow that we can access a chunked corpus, we can evaluate chunkers. We start off by\nestablishing a baseline for the trivial chunk parser cp that creates no chunks:\n>>> from nltk.corpus import conll2000\n>>> cp = nltk.RegexpParser(\"\")\n>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n>>> print cp.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  43.4%\n    Precision:      0.0%\n    Recall:         0.0%\n    F-Measure:      0.0%\nThe IOB tag accuracy indicates that more than a third of the words are tagged with O,\ni.e., not in an NP chunk. However, since our tagger did not find any chunks, its precision,\nrecall, and F-measure are all zero. Now let\u2019s try a naive regular expression chunker that\nlooks for tags beginning with letters that are characteristic of noun phrase tags (e.g.,\nCD, DT, and JJ).\n>>> grammar = r\"NP: {<[CDJNP].*>+}\"\n>>> cp = nltk.RegexpParser(grammar)\n>>> print cp.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  87.7%\n    Precision:     70.6%\n    Recall:        67.8%\n    F-Measure:     69.2%\nAs you can see, this approach achieves decent results. However, we can improve on it\nby adopting a more data-driven approach, where we use the training corpus to find the\nchunk tag ( I, O, or B) that is most likely for each part-of-speech tag. In other words, we\ncan build a chunker using a unigram tagger  (Section 5.4 ). But rather than trying to\ndetermine the correct part-of-speech tag for each word, we are trying to determine the\ncorrect chunk tag, given each word\u2019s part-of-speech tag.\n272 | Chapter 7: \u2002Extracting Information from Text\nIn Example 7-4 , we define the UnigramChunker class, which uses a unigram tagger to\nlabel sentences with chunk tags. Most of the code in this class is simply used to convert\nback and forth between the chunk tree representation used by NLTK\u2019s ChunkParserI\ninterface, and the IOB representation used by the embedded tagger. The class defines\ntwo methods: a constructor \n , which is called when we build a new UnigramChunker;\nand the parse method \n , which is used to chunk new sentences.\nExample 7-4. Noun phrase chunking with a unigram tagger.\nclass UnigramChunker(nltk.ChunkParserI):\n    def __init__(self, train_sents): \n        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n                      for sent in train_sents]\n        self.tagger = nltk.UnigramTagger(train_data) \n    def parse(self, sentence): \n        pos_tags = [pos for (word,pos) in sentence]\n        tagged_pos_tags = self.tagger.tag(pos_tags)\n        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n                     in zip(sentence, chunktags)]\n        return nltk.chunk.conlltags2tree(conlltags)\nThe constructor \n  expects a list of training sentences, which will be in the form of\nchunk trees. \nIt first converts training data to a form that\u2019s suitable for training the tagger,\nusing tree2conlltags to map each chunk tree to a list of word,tag,chunk triples. It then\nuses that converted training data to train a unigram tagger, and stores it in self.tag\nger for later use.\nThe parse method \n  takes a tagged sentence as its input, and begins by extracting the\npart-of-speech tags \nfrom that sentence. It then tags the part-of-speech tags with IOB\nchunk tags, using the tagger self.tagger that was trained in the constructor. Next, it\nextracts the chunk tags, and combines them with the original sentence, to yield\nconlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.\nNow that we have UnigramChunker, we can train it using the CoNLL-2000 Chunking\nCorpus, and test its resulting performance:\n>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n>>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n>>> unigram_chunker = UnigramChunker(train_sents)\n>>> print unigram_chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  92.9%\n    Precision:     79.9%\n    Recall:        86.8%\n    F-Measure:     83.2%\nThis chunker does reasonably well, achieving an overall F-measure score of 83%. Let\u2019s\ntake a look at what it\u2019s learned, by using its unigram tagger to assign a tag to each of\nthe part-of-speech tags that appear in the corpus:\n7.3  Developing and Evaluating Chunkers | 273\n>>> postags = sorted(set(pos for sent in train_sents\n...                      for (word,pos) in sent.leaves()))\n>>> print unigram_chunker.tagger.tag(postags)\n[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\nIt has \ndiscovered that most punctuation marks occur outside of NP chunks, with the\nexception of # and $, both of which are used as currency markers. It has also found that\ndeterminers ( DT) and possessives ( PRP$ and WP$) occur at the beginnings of NP chunks,\nwhile noun types (NN, NNP, NNPS, NNS) mostly occur inside of NP chunks.\nHaving built a unigram chunker, it is quite easy to build a bigram chunker: we simply\nchange the class name to BigramChunker, and modify line \n  in Example 7-4 to construct\na BigramTagger rather than a UnigramTagger. The resulting chunker has slightly higher\nperformance than the unigram chunker:\n>>> bigram_chunker = BigramChunker(train_sents)\n>>> print bigram_chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  93.3%\n    Precision:     82.3%\n    Recall:        86.8%\n    F-Measure:     84.5%\nTraining Classifier-Based Chunkers\nBoth the regular expression\u2013based chunkers and the n-gram chunkers decide what\nchunks to create entirely based on part-of-speech tags. However, sometimes part-of-\nspeech tags are insufficient to determine how a sentence should be chunked. For ex-\nample, consider the following two statements:\n(3) a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\nb. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\nThese two sentences have the same part-of-speech tags, yet they are chunked differ-\nently. In the first sentence, the farmer and rice are separate chunks, while the corre-\nsponding material in the second sentence, the computer monitor , is a single chunk.\nClearly, we need to make use of information about the content of the words, in addition\nto just their part-of-speech tags, if we wish to maximize chunking performance.\nOne way that we can incorporate information about the content of words is to use a\nclassifier-based tagger to chunk the sentence. Like the n-gram chunker considered in\nthe previous section, this classifier-based chunker will work by assigning IOB tags to\n274 | Chapter 7: \u2002Extracting Information from Text\nthe words in a sentence, and then converting those tags to chunks. For the classifier-\nbased tagger \nitself, we will use the same approach that we used in Section 6.1  to build\na part-of-speech tagger.\nThe basic code for the classifier-based NP chunker is shown in Example 7-5. It consists\nof two classes. The first class \n  is almost identical to the ConsecutivePosTagger class\nfrom Example \n6-5. The only two differences are that it calls a different feature extractor\n and that it uses a MaxentClassifier rather than a NaiveBayesClassifier \n . The sec-\nond class \n  is basically a wrapper around the tagger class that turns it into a chunker.\nDuring training, \nthis second class maps the chunk trees in the training corpus into tag\nsequences; in the parse() method, it converts the tag sequence provided by the tagger\nback into a chunk tree.\nExample 7-5. Noun phrase chunking with a consecutive classifier.\nclass ConsecutiveNPChunkTagger(nltk.TaggerI): \n    def __init__(self, train_sents):\n        train_set = []\n        for tagged_sent in train_sents:\n            untagged_sent = nltk.tag.untag(tagged_sent)\n            history = []\n            for i, (word, tag) in enumerate(tagged_sent):\n                featureset = npchunk_features(untagged_sent, i, history) \n                train_set.append( (featureset, tag) )\n                history.append(tag)\n        self.classifier = nltk.MaxentClassifier.train( \n            train_set, algorithm='megam', trace=0)\n    def tag(self, sentence):\n        history = []\n        for i, word in enumerate(sentence):\n            featureset = npchunk_features(sentence, i, history)\n            tag = self.classifier.classify(featureset)\n            history.append(tag)\n        return zip(sentence, history)\nclass ConsecutiveNPChunker(nltk.ChunkParserI): \n    def __init__(self, train_sents):\n        tagged_sents = [[((w,t),c) for (w,t,c) in\n                         nltk.chunk.tree2conlltags(sent)]\n                        for sent in train_sents]\n        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n    def parse(self, sentence):\n        tagged_sents = self.tagger.tag(sentence)\n        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n        return nltk.chunk.conlltags2tree(conlltags)\nThe only \npiece left to fill in is the feature extractor. We begin by defining a simple\nfeature extractor, which just provides the part-of-speech tag of the current token. Using\n7.3  Developing and Evaluating Chunkers | 275\nthis feature extractor, our classifier-based chunker is very similar to the unigram chunk-\ner, as is reflected in its performance:\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     return {\"pos\": pos}\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  92.9%\n    Precision:     79.9%\n    Recall:        86.7%\n    F-Measure:     83.2%\nWe can \nalso add a feature for the previous part-of-speech tag. Adding this feature allows\nthe classifier to model interactions between adjacent tags, and results in a chunker that\nis closely related to the bigram chunker.\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     if i == 0:\n...         prevword, prevpos = \"<START>\", \"<START>\"\n...     else:\n...         prevword, prevpos = sentence[i-1]\n...     return {\"pos\": pos, \"prevpos\": prevpos}\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  93.6%\n    Precision:     81.9%\n    Recall:        87.1%\n    F-Measure:     84.4%\nNext, we\u2019ll try adding a feature for the current word, since we hypothesized that word\ncontent should be useful for chunking. We find that this feature does indeed improve\nthe chunker\u2019s performance, by about 1.5 percentage points (which corresponds to\nabout a 10% reduction in the error rate).\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     if i == 0:\n...         prevword, prevpos = \"<START>\", \"<START>\"\n...     else:\n...         prevword, prevpos = sentence[i-1]\n...     return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  94.2%\n    Precision:     83.4%\n    Recall:        88.6%\n    F-Measure:     85.9%\n276 | Chapter 7: \u2002Extracting Information from Text\nFinally, we can try extending the feature extractor with a variety of additional features,\nsuch as \nlookahead features \n , paired features \n , and complex contextual features \n .\nThis last \nfeature, called tags-since-dt, creates a string describing the set of all part-of-\nspeech tags that have been encountered since the most recent determiner.\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     if i == 0:\n...         prevword, prevpos = \"<START>\", \"<START>\"\n...     else:\n...         prevword, prevpos = sentence[i-1]\n...     if i == len(sentence)-1:\n...         nextword, nextpos = \"<END>\", \"<END>\"\n...     else:\n...         nextword, nextpos = sentence[i+1]\n...     return {\"pos\": pos,\n...             \"word\": word,\n...             \"prevpos\": prevpos,\n...             \"nextpos\": nextpos, \n...             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \n...             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n...             \"tags-since-dt\": tags_since_dt(sentence, i)}  \n>>> def tags_since_dt(sentence, i):\n...     tags = set()\n...     for word, pos in sentence[:i]:\n...         if pos == 'DT':\n...             tags = set()\n...         else:\n...             tags.add(pos)\n...     return '+'.join(sorted(tags))\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\n    IOB Accuracy:  95.9%\n    Precision:     88.3%\n    Recall:        90.7%\n    F-Measure:     89.5%\nYour Turn:  Try adding different features to the feature extractor func-\ntion npchunk_features, and see if you can further improve the perform-\nance of the NP chunker.\n7.4  Recursion in Linguistic Structure\nBuilding Nested Structure with Cascaded Chunkers\nSo far, our chunk structures have been relatively flat. Trees consist of tagged tokens,\noptionally grouped under a chunk node such as NP. However, it is possible to build\nchunk structures of arbitrary depth, simply by creating a multistage chunk grammar\n7.4  Recursion in Linguistic Structure | 277\ncontaining recursive rules. Example 7-6  has patterns for noun phrases, prepositional\nphrases, verb phrases, and sentences. This is a four-stage chunk grammar, and can be\nused to create structures having a depth of at most four.\nExample 7-6. A chunker that handles NP, PP, VP, and S.\ngrammar = r\"\"\"\n  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n  \"\"\"\ncp = nltk.RegexpParser(grammar)\nsentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n>>> print cp.parse(sentence)\n(S\n  (NP Mary/NN)\n  saw/VBD\n  (CLAUSE\n    (NP the/DT cat/NN)\n    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\nUnfortunately this result misses the VP headed by saw. It has other shortcomings, too.\nLet\u2019s see what happens when we apply this chunker to a sentence having deeper nesting.\nNotice that it fails to identify the VP chunk starting at \n .\n>>> sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n...     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n...     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n>>> print cp.parse(sentence)\n(S\n  (NP John/NNP)\n  thinks/VBZ\n  (NP Mary/NN)\n  saw/VBD \n  (CLAUSE\n    (NP the/DT cat/NN)\n    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\nThe solution \nto these problems is to get the chunker to loop over its patterns: after\ntrying all of them, it repeats the process. We add an optional second argument loop to\nspecify the number of times the set of patterns should be run:\n>>> cp = nltk.RegexpParser(grammar, loop=2)\n>>> print cp.parse(sentence)\n(S\n  (NP John/NNP)\n  thinks/VBZ\n  (CLAUSE\n    (NP Mary/NN)\n    (VP\n      saw/VBD\n      (CLAUSE\n278 | Chapter 7: \u2002Extracting Information from Text\n        (NP the/DT cat/NN)\n        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\nThis cascading process enables us to create deep structures. However,\ncreating and \ndebugging a cascade is difficult, and there comes a point\nwhere it is more effective to do full parsing (see Chapter 8). Also, the\ncascading process can only produce trees of fixed depth (no deeper than\nthe number of stages in the cascade), and this is insufficient for complete\nsyntactic analysis.\nTrees\nA tree is a set of connected labeled nodes, each reachable by a unique path from a\ndistinguished root node. Here\u2019s an example of a tree (note that they are standardly\ndrawn upside-down):\n(4)\nWe use a \u2018family\u2019 metaphor to talk about the relationships of nodes in a tree: for ex-\nample, S is \nthe parent of VP; conversely VP is a child of S. Also, since NP and VP are both\nchildren of S, they are also siblings. For convenience, there is also a text format for\nspecifying trees:\n(S\n   (NP Alice)\n   (VP\n      (V chased)\n      (NP\n         (Det the)\n         (N rabbit))))\nAlthough we will focus on syntactic trees, trees can be used to encode any homogeneous\nhierarchical structure that spans a sequence of linguistic forms (e.g., morphological\nstructure, discourse structure). In the general case, leaves and node values do not have\nto be strings.\nIn NLTK, we create a tree by giving a node label and a list of children:\n7.4  Recursion in Linguistic Structure | 279\n>>> tree1 = nltk.Tree('NP', ['Alice'])\n>>> print tree1\n(NP Alice)\n>>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n>>> print tree2\n(NP the rabbit)\nWe can incorporate these into successively larger trees as follows:\n>>> tree3 = nltk.Tree('VP', ['chased', tree2])\n>>> tree4 = nltk.Tree('S', [tree1, tree3])\n>>> print tree4\n(S (NP Alice) (VP chased (NP the rabbit)))\nHere are some of the methods available for tree objects:\n>>> print tree4[1]\n(VP chased (NP the rabbit))\n>>> tree4[1].node\n'VP'\n>>> tree4.leaves()\n['Alice', 'chased', 'the', 'rabbit']\n>>> tree4[1][1][1]\n'rabbit'\nThe bracketed \nrepresentation for complex trees can be difficult to read. In these cases,\nthe draw method can be very useful. It opens a new window, containing a graphical\nrepresentation of the tree. The tree display window allows you to zoom in and out, to\ncollapse and expand subtrees, and to print the graphical representation to a postscript\nfile (for inclusion in a document).\n>>> tree3.draw()\nTree Traversal\nIt is \nstandard to use a recursive function to traverse a tree. The listing in Example 7-7\ndemonstrates this.\nExample 7-7. A recursive function to traverse a tree.\ndef traverse(t):\n    try:\n        t.node\n    except AttributeError:\n        print t,\n \n    else:\n280 | Chapter 7: \u2002Extracting Information from Text\n        # Now we know that t.node is defined\n        print '(', t.node,\n        for child in t:\n            traverse(child)\n        print ')',\n>>> t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')\n>>> traverse(t)\n( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )\nWe have used a technique called duck typing  to detect that t is a tree\n(i.e., t.node is defined).\n7.5  Named Entity Recognition\nAt the start of this chapter, we briefly introduced named entities (NEs). Named entities\nare definite noun phrases that refer to specific types of individuals, such as organiza-\ntions, persons, dates, and so on. Table 7-3  lists some of the more commonly used types\nof NEs. These should be self-explanatory, except for \u201cFACILITY\u201d: human-made arti-\nfacts in the domains of architecture and civil engineering; and \u201cGPE\u201d: geo-political\nentities such as city, state/province, and country.\nTable 7-3. Commonly used types of named entity\nNE type Examples\nORGANIZATION Georgia-Pacific Corp., WHO\nPERSON Eddy Bonte, President Obama\nLOCATION Murray River, Mount Everest\nDATE June, 2008-06-29\nTIME two fifty a m, 1:30 p.m.\nMONEY 175 million Canadian Dollars, GBP 10.40\nPERCENT twenty pct, 18.75 %\nFACILITY Washington Monument, Stonehenge\nGPE South East Asia, Midlothian\nThe goal of a named entity recognition (NER) system is to identify all textual men-\ntions of the named entities. This can be broken down into two subtasks: identifying\nthe boundaries of the NE, and identifying its type. While named entity recognition is\nfrequently a prelude to identifying relations in Information Extraction, it can also con-\ntribute to other tasks. For example, in Question Answering (QA), we try to improve\nthe precision of Information Retrieval by recovering not whole pages, but just those\nparts which contain an answer to the user\u2019s question. Most QA systems take the\n7.5  Named Entity Recognition | 281\ndocuments returned by standard Information Retrieval, and then attempt to isolate the\nminimal \ntext snippet in the document containing the answer. Now suppose the\nquestion was Who was the first President of the US? , and one of the documents that was\nretrieved contained the following passage:\n(5) The Washington Monument is the most prominent structure in Washington,\nD.C. and one of the city\u2019s early attractions. It was built in honor of George\nWashington, who led the country to independence and then became its first\nPresident.\nAnalysis of the question leads us to expect that an answer should be of the form X was\nthe first President of the US , where X is not only a noun phrase, but also refers to a\nnamed entity of type PER. This should allow us to ignore the first sentence in the passage.\nAlthough it contains two occurrences of Washington, named entity recognition should\ntell us that neither of them has the correct type.\nHow do we go about identifying named entities? One option would be to look up each\nword in an appropriate list of names. For example, in the case of locations, we could\nuse a gazetteer, or geographical dictionary, such as the Alexandria Gazetteer or the\nGetty Gazetteer. However, doing this blindly runs into problems, as shown in Fig-\nure 7-5.\nFigure 7-5. Location detection by simple lookup for a news story: Looking up every word in a gazetteer\nis error-prone; case distinctions may help, but these are not always present.\nObserve that \nthe gazetteer has good coverage of locations in many countries, and in-\ncorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam.\nOf course we could omit such locations from the gazetteer, but then we won\u2019t be able\nto identify them when they do appear in a document.\nIt gets even harder in the case of names for people or organizations. Any list of such\nnames will probably have poor coverage. New organizations come into existence every\n282 | Chapter 7: \u2002Extracting Information from Text\nday, so if we are trying to deal with contemporary newswire or blog entries, it is unlikely\nthat we will be able to recognize many of the entities using gazetteer lookup.\nAnother major source of difficulty is caused by the fact that many named entity terms\nare ambiguous. \nThus May and North are likely to be parts of named entities for DATE\nand LOCATION, respectively, but could both be part of a PERSON; conversely Chris-\ntian Dior  looks like a PERSON but is more likely to be of type ORGANIZATION. A\nterm like Yankee will be an ordinary modifier in some contexts, but will be marked as\nan entity of type ORGANIZATION in the phrase Yankee infielders.\nFurther challenges are posed by multiword names like Stanford University , and by\nnames that contain other names, such as Cecil H. Green Library  and Escondido Village\nConference Service Center . In named entity recognition, therefore, we need to be able\nto identify the beginning and end of multitoken sequences.\nNamed entity recognition is a task that is well suited to the type of classifier-based\napproach that we saw for noun phrase chunking. In particular, we can build a tagger\nthat labels each word in a sentence using the IOB format, where chunks are labeled by\ntheir appropriate type. Here is part of the CONLL 2002 ( conll2002) Dutch training\ndata:\nEddy N B-PER\nBonte N I-PER\nis V O\nwoordvoerder N O\nvan Prep O\ndiezelfde Pron O\nHogeschool N B-ORG\n. Punc O\nIn this representation, there is one token per line, each with its part-of-speech tag and\nits named entity tag. Based on this training corpus, we can construct a tagger that can\nbe used to label new sentences, and use the nltk.chunk.conlltags2tree() function to\nconvert the tag sequences into a chunk tree.\nNLTK provides a classifier that has already been trained to recognize named entities,\naccessed with the function nltk.ne_chunk(). If we set the parameter binary=True \n ,\nthen named \nentities are just tagged as NE; otherwise, the classifier adds category labels\nsuch as PERSON, ORGANIZATION, and GPE.\n>>> sent = nltk.corpus.treebank.tagged_sents()[22]\n>>> print nltk.ne_chunk(sent, binary=True) \n  \n(S\n  The/DT\n  (NE U.S./NNP)\n  is/VBZ\n  one/CD\n  ...\n  according/VBG\n  to/TO\n  (NE Brooke/NNP T./NNP Mossman/NNP)\n  ...)\n7.5  Named Entity Recognition | 283\n \n>>> print nltk.ne_chunk(sent) \n(S\n  The/DT\n  (GPE U.S./NNP)\n  is/VBZ\n  one/CD\n  ...\n  according/VBG\n  to/TO\n  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n  ...)\n7.6  Relation Extraction\nOnce named \nentities have been identified in a text, we then want to extract the relations\nthat exist between them. As indicated earlier, we will typically be looking for relations\nbetween specified types of named entity. One way of approaching this task is to initially\nlook for all triples of the form ( X, \u03b1, Y), where X and Y are named entities of the required\ntypes, and \u03b1 is the string of words that intervenes between X and Y. We can then use\nregular expressions to pull out just those instances of \u03b1 that express the relation that\nwe are looking for. The following example searches for strings that contain the word\nin. The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that\nallows us to disregard strings such as success in supervising the transition of, where in\nis followed by a gerund.\n>>> IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n>>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n...                                      corpus='ieer', pattern = IN):\n...         print nltk.sem.show_raw_rtuple(rel)\n[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\nSearching for the keyword in works reasonably well, though it will also retrieve false\npositives such as [ORG: House Transportation Committee] , secured the most money\nin the [LOC: New York]; there is unlikely to be a simple string-based method of ex-\ncluding filler strings such as this.\n284 | Chapter 7: \u2002Extracting Information from Text\nAs shown earlier, the Dutch section of the CoNLL 2002 Named Entity Corpus contains\nnot just \nnamed entity annotation, but also part-of-speech tags. This allows us to devise\npatterns that are sensitive to these tags, as shown in the next example. The method\nshow_clause() prints out the relations in a clausal form, where the binary relation sym-\nbol is specified as the value of parameter relsym \n .\n>>> from nltk.corpus import conll2002\n>>> vnv = \"\"\"\n... (\n... is/V|    # 3rd sing present and\n... was/V|   # past forms of the verb zijn ('be')\n... werd/V|  # and also present\n... wordt/V  # past of worden ('become')\n... )\n... .*       # followed by anything\n... van/Prep # followed by van ('of')\n... \"\"\"\n>>> VAN = re.compile(vnv, re.VERBOSE)\n>>> for doc in conll2002.chunked_sents('ned.train'):\n...     for r in nltk.sem.extract_rels('PER', 'ORG', doc,\n...                                    corpus='conll2002', pattern=VAN):\n...         print  nltk.sem.show_clause(r, relsym=\"VAN\") \nVAN(\"cornet_d'elzius\", 'buitenlandse_handel')\nVAN('johan_rottiers', 'kardinaal_van_roey_instituut')\nVAN('annie_lennox', 'eurythmics')\nYour Turn: Replace the last line \n  with print show_raw_rtuple(rel,\nlcon=True, rcon=True) . This \nwill show you the actual words that inter-\nvene between the two NEs and also their left and right context, within\na default 10-word window. With the help of a Dutch dictionary, you\nmight be able to figure out why the result VAN('annie_lennox', 'euryth\nmics') is a false hit.\n7.7  Summary\n\u2022 Information extraction systems search large bodies of unrestricted text for specific\ntypes of entities and relations, and use them to populate well-organized databases.\nThese databases can then be used to find answers for specific questions.\n\u2022 The typical architecture for an information extraction system begins by segment-\ning, tokenizing, and part-of-speech tagging the text. The resulting data is then\nsearched for specific types of entity. Finally, the information extraction system\nlooks at entities that are mentioned near one another in the text, and tries to de-\ntermine whether specific relationships hold between those entities.\n\u2022 Entity recognition is often performed using chunkers, which segment multitoken\nsequences, and label them with the appropriate entity type. Common entity types\ninclude ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and\nGPE (geo-political entity).\n7.7  Summary | 285\n\u2022 Chunkers can be constructed using rule-based systems, such as the RegexpParser\nclass provided by NLTK; or using machine learning techniques, such as the\nConsecutiveNPChunker presented in this chapter. In either case, part-of-speech tags\nare often a very important feature when searching for chunks.\n\u2022 Although chunkers are specialized to create relatively flat data structures, where\nno two chunks are allowed to overlap, they can be cascaded together to build nested\nstructures.\n\u2022 Relation extraction can be performed using either rule-based systems, which typ-\nically look for specific patterns in the text that connect entities and the intervening\nwords; or using machine-learning systems, which typically attempt to learn such\npatterns automatically from a training corpus.\n7.8  Further Reading\nExtra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. For more examples of chunking with NLTK,\nplease see the Chunking HOWTO at http://www.nltk.org/howto.\nThe popularity of chunking is due in great part to pioneering work by Abney, e.g.,\n(Abney, 1996a). Abney\u2019s Cass chunker is described in http://www.vinartus.net/spa/97a\n.pdf.\nThe word chink initially meant a sequence of stopwords, according to a 1975 paper\nby Ross and Tukey (Abney, 1996a).\nThe IOB format (or sometimes BIO Format ) was developed for NP chunking by (Ram-\nshaw & Marcus, 1995), and was used for the shared NP bracketing task run by the\nConference on Natural Language Learning  (CoNLL) in 1999. The same format was\nadopted by CoNLL 2000 for annotating a section of Wall Street Journal  text as part of\na shared task on NP chunking.\nSection 13.5 of (Jurafsky & Martin, 2008) contains a discussion of chunking. Chapter\n22 covers information extraction, including named entity recognition. For information\nabout text mining in biology and medicine, see (Ananiadou & McNaught, 2006).\nFor more information on the Getty and Alexandria gazetteers, see http://en.wikipedia\n.org/wiki/Getty_Thesaurus_of_Geographic_Names and http://www.alexandria.ucsb\n.edu/gazetteer/.\n7.9  Exercises\n1.\u25cb The IOB format categorizes tagged tokens as I, O, and B. Why are three tags\nnecessary? What problem would be caused if we used I and O tags exclusively?\n286 | Chapter 7: \u2002Extracting Information from Text\n2.\u25cb Write a tag pattern to match noun phrases containing plural head nouns, e.g.,\nmany/JJ researchers/NNS\n, two/CD weeks/NNS, both/DT new/JJ positions/NNS. Try\nto do this by generalizing the tag pattern that handled singular noun phrases.\n3.\u25cb Pick one of the three chunk types in the CoNLL-2000 Chunking Corpus. Inspect\nthe data and try to observe any patterns in the POS tag sequences that make up\nthis kind of chunk. Develop a simple chunker using the regular expression chunker\nnltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably.\n4.\u25cb An early definition of chunk was the material that occurs between chinks. De-\nvelop a chunker that starts by putting the whole sentence in a single chunk, and\nthen does the rest of its work solely by chinking. Determine which tags (or tag\nsequences) are most likely to make up chinks with the help of your own utility\nprogram. Compare the performance and simplicity of this approach relative to a\nchunker based entirely on chunk rules.\n5.\u25d1 Write a tag pattern to cover noun phrases that contain gerunds, e.g., the/DT\nreceiving/VBG end/NN , assistant/NN managing/VBG editor/NN . Add these patterns\nto the grammar, one per line. Test your work using some tagged sentences of your\nown devising.\n6.\u25d1 Write one or more tag patterns to handle coordinated noun phrases, e.g., July/\nNNP and/CC August/NNP, all/DT your/PRP$ managers/NNS and/CC supervisors/NNS,\ncompany/NN courts/NNS and/CC adjudicators/NNS.\n7.\u25d1 Carry out the following evaluation tasks for any of the chunkers you have de-\nveloped earlier. (Note that most chunking corpora contain some internal incon-\nsistencies, such that any reasonable rule-based approach will produce errors.)\na. Evaluate your chunker on 100 sentences from a chunked corpus, and report\nthe precision, recall, and F-measure.\nb. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify\nthe errors made by your chunker. Discuss.\nc. Compare the performance of your chunker to the baseline chunker discussed\nin the evaluation section of this chapter.\n8.\u25d1 Develop a chunker for one of the chunk types in the CoNLL Chunking Corpus\nusing a regular expression\u2013based chunk grammar RegexpChunk. Use any combina-\ntion of rules for chunking, chinking, merging, or splitting.\n9.\u25d1 Sometimes a word is incorrectly tagged, e.g., the head noun in 12/CD or/CC so/\nRB cases/VBZ. Instead of requiring manual correction of tagger output, good\nchunkers are able to work with the erroneous output of taggers. Look for other\nexamples of correctly chunked noun phrases with incorrect tags.\n10.\u25d1 The bigram chunker scores about 90% accuracy. Study its errors and try to work\nout why it doesn\u2019t get 100% accuracy. Experiment with trigram chunking. Are you\nable to improve the performance any more?\n7.9  Exercises | 287\n11.\u25cf Apply the n-gram and Brill tagging methods to IOB chunk tagging. Instead of\nassigning POS tags to words, here we will assign IOB tags to the POS tags. E.g., if\nthe tag DT (determiner) often occurs at the start of a chunk, it will be tagged B\n(begin). Evaluate the performance of these chunking methods relative to the regular\nexpression chunking methods covered in this chapter.\n12.\u25cf We saw in Chapter 5  that it is possible to establish an upper limit to tagging\nperformance by looking for ambiguous n-grams, which are n-grams that are tagged\nin more than one possible way in the training data. Apply the same method to\ndetermine an upper bound on the performance of an n-gram chunker.\n13.\u25cf Pick one of the three chunk types in the CoNLL Chunking Corpus. Write func-\ntions to do the following tasks for your chosen type:\na. List all the tag sequences that occur with each instance of this chunk type.\nb. Count the frequency of each tag sequence, and produce a ranked list in order\nof decreasing frequency; each line should consist of an integer (the frequency)\nand the tag sequence.\nc. Inspect the high-frequency tag sequences. Use these as the basis for developing\na better chunker.\n14.\u25cf The baseline chunker presented in the evaluation section tends to create larger\nchunks than it should. For example, the phrase [every/DT time/NN] [she/PRP]\nsees/VBZ [a/DT newspaper/NN] contains two consecutive chunks, and our baseline\nchunker will incorrectly combine the first two: [every/DT time/NN she/PRP]. Write\na program that finds which of these chunk-internal tags typically occur at the start\nof a chunk, then devise one or more rules that will split up these chunks. Combine\nthese with the existing baseline chunker and re-evaluate it, to see if you have dis-\ncovered an improved baseline.\n15.\u25cf Develop an NP chunker that converts POS tagged text into a list of tuples, where\neach tuple consists of a verb followed by a sequence of noun phrases and prepo-\nsitions, e.g., the little cat sat on the mat becomes ('sat', 'on', 'NP')...\n16.\u25cf The Penn Treebank Corpus sample contains a section of tagged Wall Street\nJournal text that has been chunked into noun phrases. The format uses square\nbrackets, and we have encountered it several times in this chapter. The corpus can\nbe accessed using: for sent in nltk.corpus.treebank_chunk.chunked_sents(fil\neid). These are flat trees, just as we got using nltk.cor\npus.conll2000.chunked_sents().\na. The functions nltk.tree.pprint() and nltk.chunk.tree2conllstr() can be\nused to create Treebank and IOB strings from a tree. Write functions\nchunk2brackets() and chunk2iob() that take a single chunk tree as their sole\nargument, and return the required multiline string representation.\nb. Write command-line conversion utilities bracket2iob.py and iob2bracket.py\nthat take a file in Treebank or CoNLL format (respectively) and convert it to\nthe other format. (Obtain some raw Treebank or CoNLL data from the NLTK\n288 | Chapter 7: \u2002Extracting Information from Text\nCorpora, save it to a file, and then use for line in open(filename) to access\nit from Python.)\n17.\u25cf An n-gram \nchunker can use information other than the current part-of-speech\ntag and the n-1 previous chunk tags. Investigate other models of the context, such\nas the n-1 previous part-of-speech tags, or some combination of previous chunk\ntags along with previous and following part-of-speech tags.\n18.\u25cf Consider the way an n-gram tagger uses recent tags to inform its tagging choice.\nNow observe how a chunker may reuse this sequence information. For example,\nboth tasks will make use of the information that nouns tend to follow adjectives\n(in English). It would appear that the same information is being maintained in two\nplaces. Is this likely to become a problem as the size of the rule sets grows? If so,\nspeculate about any ways that this problem might be addressed.\n7.9  Exercises | 289\n\nCHAPTER 8\nAnalyzing Sentence Structure\nEarlier chapters focused on words: how to identify them, analyze their structure, assign\nthem to \nlexical categories, and access their meanings. We have also seen how to identify\npatterns in word sequences or n-grams. However, these methods only scratch the sur-\nface of the complex constraints that govern sentences. We need a way to deal with the\nambiguity that natural language is famous for. We also need to be able to cope with\nthe fact that there are an unlimited number of possible sentences, and we can only write\nfinite programs to analyze their structures and discover their meanings.\nThe goal of this chapter is to answer the following questions:\n1. How can we use a formal grammar to describe the structure of an unlimited set of\nsentences?\n2. How do we represent the structure of sentences using syntax trees?\n3. How do parsers analyze a sentence and automatically build a syntax tree?\nAlong the way, we will cover the fundamentals of English syntax, and see that there\nare systematic aspects of meaning that are much easier to capture once we have iden-\ntified the structure of sentences.\n291\n8.1  Some Grammatical Dilemmas\nLinguistic Data and Unlimited Possibilities\nPrevious chapters \nhave shown you how to process and analyze text corpora, and we\nhave stressed the challenges for NLP in dealing with the vast amount of electronic\nlanguage data that is growing daily. Let\u2019s consider this data more closely, and make the\nthought experiment that we have a gigantic corpus consisting of everything that has\nbeen either uttered or written in English over, say, the last 50 years. Would we be\njustified in calling this corpus \u201cthe language of modern English\u201d? There are a number\nof reasons why we might answer no. Recall that in Chapter 3 , we asked you to search\nthe Web for instances of the pattern the of . Although it is easy to find examples on the\nWeb containing this word sequence, such as New man at the of IMG  (see http://www\n.telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html), speakers of English\nwill say that most such examples are errors, and therefore not part of English after all.\nAccordingly, we can argue that \u201cmodern English\u201d is not equivalent to the very big set\nof word sequences in our imaginary corpus. Speakers of English can make judgments\nabout these sequences, and will reject some of them as being ungrammatical.\nEqually, it is easy to compose a new sentence and have speakers agree that it is perfectly\ngood English. For example, sentences have an interesting property that they can be\nembedded inside larger sentences. Consider the following sentences:\n(1) a. Usain Bolt broke the 100m record.\nb. The Jamaica Observer reported that Usain Bolt broke the 100m record.\nc. Andre said The Jamaica Observer reported that Usain Bolt broke the 100m\nrecord.\nd. I think Andre said the Jamaica Observer reported that Usain Bolt broke\nthe 100m record.\nIf we replaced whole sentences with the symbol S, we would see patterns like Andre\nsaid S and I think S. These are templates for taking a sentence and constructing a bigger\nsentence. There are other templates we can use, such as S but S and S when S. With a\nbit of ingenuity we can construct some really long sentences using these templates.\nHere\u2019s an impressive example from a Winnie the Pooh story by A.A. Milne, In Which\nPiglet Is Entirely Surrounded by Water:\n[You can imagine Piglet\u2019s joy when at last the ship came in sight of him.] In after-years\nhe liked to think that he had been in Very Great Danger during the Terrible Flood, but\nthe only danger he had really been in was the last half-hour of his imprisonment, when\nOwl, who had just flown up, sat on a branch of his tree to comfort him, and told him a\nvery long story about an aunt who had once laid a seagull\u2019s egg by mistake, and the story\nwent on and on, rather like this sentence, until Piglet who was listening out of his window\nwithout much hope, went to sleep quietly and naturally, slipping slowly out of the win-\ndow towards the water until he was only hanging on by his toes, at which moment,\n292 | Chapter 8: \u2002Analyzing Sentence Structure\nluckily, a sudden loud squawk from Owl, which was really part of the story, being what\nhis aunt said, woke the Piglet up and just gave him time to jerk himself back into safety\nand say, \n\u201cHow interesting, and did she?\u201d when\u2014well, you can imagine his joy when at\nlast he saw the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear) coming\nover the sea to rescue him\u2026\nThis long sentence actually has a simple structure that begins S but S when S . We can\nsee from this example that language provides us with constructions which seem to allow\nus to extend sentences indefinitely. It is also striking that we can understand sentences\nof arbitrary length that we\u2019ve never heard before: it\u2019s not hard to concoct an entirely\nnovel sentence, one that has probably never been used before in the history of the\nlanguage, yet all speakers of the language will understand it.\nThe purpose of a grammar is to give an explicit description of a language. But the way\nin which \nwe think of a grammar is closely intertwined with what we consider to be a\nlanguage. Is it a large but finite set of observed utterances and written texts? Is it some-\nthing more abstract like the implicit knowledge that competent speakers have about\ngrammatical sentences? Or is it some combination of the two? We won\u2019t take a stand\non this issue, but instead will introduce the main approaches.\nIn this chapter, we will adopt the formal framework of \u201cgenerative grammar,\u201d in which\na \u201clanguage\u201d is considered to be nothing more than an enormous collection of all\ngrammatical sentences, and a grammar is a formal notation that can be used for \u201cgen-\nerating\u201d the members of this set. Grammars use recursive productions of the form\nS \u2192 S and S, as we will explore in Section 8.3. In Chapter 10 we will extend this, to\nautomatically build up the meaning of a sentence out of the meanings of its parts.\nUbiquitous Ambiguity\nA well-known example of ambiguity is shown in (2), from the Groucho Marx movie,\nAnimal Crackers (1930):\n(2) While hunting in Africa, I shot an elephant in my pajamas. How an elephant\ngot into my pajamas I\u2019ll never know.\nLet\u2019s take a closer look at the ambiguity in the phrase: I shot an elephant in my paja-\nmas. First we need to define a simple grammar:\n>>> groucho_grammar = nltk.parse_cfg(\"\"\"\n... S -> NP VP\n... PP -> P NP\n... NP -> Det N | Det N PP | 'I'\n... VP -> V NP | VP PP\n... Det -> 'an' | 'my'\n... N -> 'elephant' | 'pajamas'\n... V -> 'shot'\n... P -> 'in'\n... \"\"\")\n8.1  Some Grammatical Dilemmas | 293\nThis grammar permits the sentence to be analyzed in two ways, depending on whether\nthe prepositional phrase in my pajamas describes the elephant or the shooting event.\n>>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n>>> parser = nltk.ChartParser(groucho_grammar)\n>>> trees = parser.nbest_parse(sent)\n>>> for tree in trees:\n...     print tree\n(S\n  (NP I)\n  (VP\n    (V shot)\n    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n(S\n  (NP I)\n  (VP\n    (VP (V shot) (NP (Det an) (N elephant)))\n    (PP (P in) (NP (Det my) (N pajamas)))))\nThe program \nproduces two bracketed structures, which we can depict as trees, as\nshown in (3):\n(3) a.\nb.\n294 | Chapter 8: \u2002Analyzing Sentence Structure\nNotice that there\u2019s no ambiguity concerning the meaning of any of the words; e.g., the\nword shot doesn\u2019t \nrefer to the act of using a gun in the first sentence and using a camera\nin the second sentence.\nYour Turn:  Consider the following sentences and see if you can think\nof two quite different interpretations: Fighting animals could be danger-\nous. Visiting relatives can be tiresome. Is ambiguity of the individual\nwords to blame? If not, what is the cause of the ambiguity?\nThis chapter presents grammars and parsing, as the formal and computational methods\nfor investigating and modeling the linguistic phenomena we have been discussing. As\nwe shall see, patterns of well-formedness and ill-formedness in a sequence of words\ncan be understood with respect to the phrase structure and dependencies. We can\ndevelop formal models of these structures using grammars and parsers. As before, a\nkey motivation is natural language understanding. How much more of the meaning of\na text can we access when we can reliably recognize the linguistic structures it contains?\nHaving read in a text, can a program \u201cunderstand\u201d it enough to be able to answer simple\nquestions about \u201cwhat happened\u201d or \u201cwho did what to whom\u201d? Also as before, we will\ndevelop simple programs to process annotated corpora and perform useful tasks.\n8.2  What\u2019s the Use of Syntax?\nBeyond n-grams\nWe gave an example in Chapter 2  of how to use the frequency information in bigrams\nto generate text that seems perfectly acceptable for small sequences of words but rapidly\ndegenerates into nonsense. Here\u2019s another pair of examples that we created by com-\nputing the bigrams over the text of a children\u2019s story, The Adventures of Buster\nBrown (included in the Project Gutenberg Selection Corpus):\n(4) a. He roared with me the pail slip down his back\nb. The worst part and clumsy looking for whoever heard light\nYou intuitively know that these sequences are \u201cword-salad,\u201d but you probably find it\nhard to pin down what\u2019s wrong with them. One benefit of studying grammar is that it\nprovides a conceptual framework and vocabulary for spelling out these intuitions. Let\u2019s\ntake a closer look at the sequence the worst part and clumsy looking . This looks like a\ncoordinate structure, where two phrases are joined by a coordinating conjunction\nsuch as and, but, or or. Here\u2019s an informal (and simplified) statement of how coordi-\nnation works syntactically:\nCoordinate Structure: if v1 and v2 are both phrases of grammatical category X, then v1\nand v 2 is also a phrase of category X.\n8.2  What\u2019s the Use of Syntax? | 295\nHere are a couple of examples. In the first, two NPs (noun phrases) have been conjoined\nto make an NP, while in the second, two APs (adjective phrases) have been conjoined to\nmake an AP.\n(5) a. The book\u2019s ending was (NP the worst part and the best part) for me.\nb. On land they are (AP slow and clumsy looking).\nWhat we can\u2019t do is conjoin an NP and an AP, which is why the worst part and clumsy\nlooking is ungrammatical. Before we can formalize these ideas, we need to understand\nthe concept of constituent structure.\nConstituent structure is based on the observation that words combine with other words\nto form units. The evidence that a sequence of words forms such a unit is given by\nsubstitutability\u2014that is, a sequence of words in a well-formed sentence can be replaced\nby a shorter sequence without rendering the sentence ill-formed. To clarify this idea,\nconsider the following sentence:\n(6) The little bear saw the fine fat trout in the brook.\nThe fact that we can substitute He for The little bear  indicates that the latter sequence\nis a unit. By contrast, we cannot replace little bear saw  in the same way. (We use an\nasterisk at the start of a sentence to indicate that it is ungrammatical.)\n(7) a. He saw the fine fat trout in the brook.\nb. *The he the fine fat trout in the brook.\nIn Figure 8-1 , we systematically substitute longer sequences by shorter ones in a way\nwhich preserves grammaticality. Each sequence that forms a unit can in fact be replaced\nby a single word, and we end up with just two elements.\nFigure 8-1. Substitution of word sequences: Working from the top row, we can replace particular\nsequences of \nwords (e.g., the brook ) with individual words (e.g., it); repeating this process, we arrive\nat a grammatical two-word sentence.\n296 | Chapter 8: \u2002Analyzing Sentence Structure\nIn Figure 8-2 , we have added grammatical category labels to the words we saw in the\nearlier figure. The labels NP, VP, and PP stand for noun phrase , verb phrase , and\nprepositional phrase, respectively.\nIf we now strip out the words apart from the topmost row, add an S node, and flip the\nfigure over, we end up with a standard phrase structure tree, shown in (8). Each node\nin this tree (including the words) is called a constituent. The immediate constitu-\nents of S are NP and VP.\n(8)\nAs we saw in Section 8.1 , sentences can have arbitrary length. Conse-\nquently, phrase structure trees can have arbitrary depth. The cascaded\nchunk parsers we saw in Section 7.4  can only produce structures of\nbounded depth, so chunking methods aren\u2019t applicable here.\nFigure 8-2. Substitution of word sequences plus grammatical categories: This diagram reproduces\nFigure 8-1\n along with grammatical categories corresponding to noun phrases (NP) , verb phrases\n(VP), prepositional phrases (PP) , and nominals (Nom) .\n8.2  What\u2019s the Use of Syntax? | 297\nAs we will see in the next section, a grammar specifies how the sentence can be subdi-\nvided into \nits immediate constituents, and how these can be further subdivided until\nwe reach the level of individual words.\n8.3  Context-Free Grammar\nA Simple Grammar\nLet\u2019s start off by looking at a simple context-free grammar (CFG). By convention,\nthe lefthand side of the first production is the start-symbol of the grammar, typically\nS, and all well-formed trees must have this symbol as their root label. In NLTK, context-\nfree grammars are defined in the nltk.grammar module. In Example 8-1  we define a\ngrammar and show how to parse a simple sentence admitted by the grammar.\nExample 8-1. A simple context-free grammar.\ngrammar1 = nltk.parse_cfg(\"\"\"\n  S -> NP VP\n  VP -> V NP | V NP PP\n  PP -> P NP\n  V -> \"saw\" | \"ate\" | \"walked\"\n  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n  P -> \"in\" | \"on\" | \"by\" | \"with\"\n  \"\"\")\n>>> sent = \"Mary saw Bob\".split()\n>>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n>>> for tree in rd_parser.nbest_parse(sent):\n...      print tree\n(S (NP Mary) (VP (V saw) (NP Bob)))\nThe grammar in Example 8-1 contains productions involving various syntactic cate-\ngories, as laid out in Table 8-1. The recursive descent parser used here can also be\ninspected via a graphical interface, as illustrated in Figure 8-3; we discuss this parser\nin more detail in Section 8.4.\nTable 8-1. Syntactic categories\nSymbol Meaning Example\nS sentence the man walked\nNP noun phrase a dog\nVP verb phrase saw a park\nPP prepositional phrase with a telescope\nDet determiner the\nN noun dog\n298 | Chapter 8: \u2002Analyzing Sentence Structure\nSymbol Meaning Example\nV verb walked\nP preposition in\nA production like VP -> V NP | V NP PP has a disjunction on the righthand side, shown\nby the |, and is an abbreviation for the two productions VP -> V NP and VP -> V NP PP.\nIf we parse the sentence The dog saw a man in the park using the grammar shown in\nExample 8-1, we end up with two trees, similar to those we saw for (3):\n(9) a.\nb.\nSince our grammar licenses two trees for this sentence, the sentence is said to be struc-\nturally ambiguous . The \nambiguity in question is called a prepositional phrase at-\ntachment ambiguity, as we saw earlier in this chapter. As you may recall, it is an\nambiguity about attachment since the PP in the park needs to be attached to one of two\nplaces in the tree: either as a child of VP or else as a child of NP. When the PP is attached\nto VP, the intended interpretation is that the seeing event happened in the park.\n8.3  Context-Free Grammar | 299\nHowever, if the PP is attached to NP, then it was the man who was in the park, and the\nagent of the seeing (the dog) might have been sitting on the balcony of an apartment\noverlooking the park.\nWriting Your Own Grammars\nIf you are interested in experimenting with writing CFGs, you will find it helpful to\ncreate and edit your grammar in a text file, say, mygrammar.cfg. You can then load it\ninto NLTK and parse with it as follows:\n>>> grammar1 = nltk.data.load('file:mygrammar.cfg')\n>>> sent = \"Mary saw Bob\".split()\n>>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n>>> for tree in rd_parser.nbest_parse(sent):\n...      print tree\nMake sure that you put a .cfg suffix on the filename, and that there are no spaces in the\nstring 'file:mygrammar.cfg'. If the command print tree produces no output, this is\nprobably because your sentence sent is not admitted by your grammar. In this case,\ncall the parser with tracing set to be on: rd_parser = nltk.RecursiveDescent\nFigure 8-3. Recursive descent parser demo: This tool allows you to watch the operation of a recursive\ndescent parser as it grows the parse tree and matches it against the input words.\n300 | Chapter 8: \u2002Analyzing Sentence Structure\nParser(grammar1, trace=2) . You can also check what productions are currently in the\ngrammar with the command for p in grammar1.productions(): print p.\nWhen you write CFGs for parsing in NLTK, you cannot combine grammatical cate-\ngories with lexical items on the righthand side of the same production. Thus, a pro-\nduction such as PP -> 'of' NP is disallowed. In addition, you are not permitted to place\nmultiword lexical items on the righthand side of a production. So rather than writing\nNP -> 'New York', you have to resort to something like NP -> 'New_York' instead.\nRecursion in Syntactic Structure\nA grammar is said to be recursive if a category occurring on the lefthand side of a\nproduction also appears on the righthand side of a production, as illustrated in Exam-\nple 8-2 . The production Nom -> Adj Nom (where Nom is the category of nominals) involves\ndirect recursion on the category Nom, whereas indirect recursion on S arises from the\ncombination of two productions, namely S -> NP VP and VP -> V S.\nExample 8-2. A recursive context-free grammar.\ngrammar2 = nltk.parse_cfg(\"\"\"\n  S  -> NP VP\n  NP -> Det Nom | PropN\n  Nom -> Adj Nom | N\n  VP -> V Adj | V NP | V S | V NP PP\n  PP -> P NP\n  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n  Det -> 'the' | 'a'\n  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n  P -> 'on'\n  \"\"\")\nTo see how recursion arises from this grammar, consider the following trees. (10a)\ninvolves nested nominal phrases, while (10b) contains nested sentences.\n8.3  Context-Free Grammar | 301\n(10) a.\nb.\nWe\u2019ve only illustrated two levels of recursion here, but there\u2019s no upper limit on the\ndepth. You \ncan experiment with parsing sentences that involve more deeply nested\nstructures. Beware that the RecursiveDescentParser is unable to handle left-\nrecursive productions of the form X -> X Y; we will return to this in Section 8.4.\n8.4  Parsing with Context-Free Grammar\nA parser processes input sentences according to the productions of a grammar, and\nbuilds one or more constituent structures that conform to the grammar. A grammar is\na declarative specification of well-formedness\u2014it is actually just a string, not a pro-\ngram. A parser is a procedural interpretation of the grammar. It searches through the\nspace of trees licensed by a grammar to find one that has the required sentence along\nits fringe.\n302 | Chapter 8: \u2002Analyzing Sentence Structure\nA parser permits a grammar to be evaluated against a collection of test sentences, help-\ning linguists \nto discover mistakes in their grammatical analysis. A parser can serve as a\nmodel of psycholinguistic processing, helping to explain the difficulties that humans\nhave with processing certain syntactic constructions. Many natural language applica-\ntions involve parsing at some point; for example, we would expect the natural language\nquestions submitted to a question-answering system to undergo parsing as an initial\nstep.\nIn this section, we see two simple parsing algorithms, a top-down method called re-\ncursive descent parsing, and a bottom-up method called shift-reduce parsing. We also\nsee some more sophisticated algorithms, a top-down method with bottom-up filtering\ncalled left-corner parsing, and a dynamic programming technique called chart parsing.\nRecursive Descent Parsing\nThe simplest kind of parser interprets a grammar as a specification of how to break a\nhigh-level goal into several lower-level subgoals. The top-level goal is to find an S. The\nS \u2192 NP VP  production permits the parser to replace this goal with two subgoals: find an\nNP, then find a VP. Each of these subgoals can be replaced in turn by sub-subgoals, using\nproductions that have NP and VP on their lefthand side. Eventually, this expansion\nprocess leads to subgoals such as: find the word telescope. Such subgoals can be directly\ncompared against the input sequence, and succeed if the next word is matched. If there\nis no match, the parser must back up and try a different alternative.\nThe recursive descent parser builds a parse tree during this process. With the initial\ngoal (find an S), the S root node is created. As the process recursively expands its goals\nusing the productions of the grammar, the parse tree is extended downwards (hence\nthe name recursive descent ). We can see this in action using the graphical demonstration\nnltk.app.rdparser(). Six stages of the execution of this parser are shown in Figure 8-4.\nDuring this process, the parser is often forced to choose between several possible pro-\nductions. For example, in going from step 3 to step 4, it tries to find productions with\nN on the lefthand side. The first of these is N \u2192 man. When this does not work it\nbacktracks, and tries other N productions in order, until it gets to N \u2192 dog, which\nmatches the next word in the input sentence. Much later, as shown in step 5, it finds\na complete parse. This is a tree that covers the entire sentence, without any dangling\nedges. Once a parse has been found, we can get the parser to look for additional parses.\nAgain it will backtrack and explore other choices of production in case any of them\nresult in a parse.\nNLTK provides a recursive descent parser:\n>>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n>>> sent = 'Mary saw a dog'.split()\n>>> for t in rd_parser.nbest_parse(sent):\n...     print t\n(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n8.4  Parsing with Context-Free Grammar | 303\nRecursiveDescentParser() takes an optional parameter trace. If trace\nis greater than zero, then the parser will report the steps that it takes as\nit parses a text.\nRecursive descent parsing has three key shortcomings. First, left-recursive productions\nlike NP -> NP PP send it into an infinite loop. Second, the parser wastes a lot of time\nconsidering words and structures that do not correspond to the input sentence. Third,\nthe backtracking process may discard parsed constituents that will need to be rebuilt\nagain later. For example, backtracking over VP -> V NP will discard the subtree created\nfor the NP. If the parser then proceeds with VP -> V NP PP, then the NP subtree must be\ncreated all over again.\nRecursive descent parsing is a kind of top-down parsing. Top-down parsers use a\ngrammar to predict what the input will be, before inspecting the input! However, since\nthe input is available to the parser all along, it would be more sensible to consider the\ninput sentence from the very beginning. This approach is called bottom-up parsing ,\nand we will see an example in the next section.\nShift-Reduce Parsing\nA simple kind of bottom-up parser is the shift-reduce parser . In common with all\nbottom-up parsers, a shift-reduce parser tries to find sequences of words and phrases\nthat correspond to the righthand side of a grammar production, and replace them with\nthe lefthand side, until the whole sentence is reduced to an S.\nFigure 8-4. Six stages of a recursive descent parser: The parser begins with a tree consisting of the\nnode S; \nat each stage it consults the grammar to find a production that can be used to enlarge the tree;\nwhen a lexical production is encountered, its word is compared against the input; after a complete\nparse has been found, the parser backtracks to look for more parses.\n304 | Chapter 8: \u2002Analyzing Sentence Structure\nThe shift-reduce parser repeatedly pushes the next input word onto a stack ( Sec-\ntion 4.1 ); this \nis the shift operation. If the top n items on the stack match the n items\non the righthand side of some production, then they are all popped off the stack, and\nthe item on the lefthand side of the production is pushed onto the stack. This replace-\nment of the top n items with a single item is the reduce operation. The operation may\nbe applied only to the top of the stack; reducing items lower in the stack must be done\nbefore later items are pushed onto the stack. The parser finishes when all the input is\nconsumed and there is only one item remaining on the stack, a parse tree with an S\nnode as its root. The shift-reduce parser builds a parse tree during the above process.\nEach time it pops n items off the stack, it combines them into a partial parse tree, and\npushes this back onto the stack. We can see the shift-reduce parsing algorithm in action\nusing the graphical demonstration nltk.app.srparser(). Six stages of the execution of\nthis parser are shown in Figure 8-5.\nFigure 8-5. Six stages of a shift-reduce parser: The parser begins by shifting the first input word onto\nits stack; \nonce the top items on the stack match the righthand side of a grammar production, they can\nbe replaced with the lefthand side of that production; the parser succeeds once all input is consumed\nand one S item remains on the stack.\nNLTK provides ShiftReduceParser(), a simple implementation of a shift-reduce parser.\nThis parser does not implement any backtracking, so it is not guaranteed to find a parse\nfor a text, even if one exists. Furthermore, it will only find at most one parse, even if\nmore parses exist. We can provide an optional trace parameter that controls how ver-\nbosely the parser reports the steps that it takes as it parses a text:\n8.4  Parsing with Context-Free Grammar | 305\n>>> sr_parse = nltk.ShiftReduceParser(grammar1)\n>>> sent = 'Mary saw a dog'.split()\n>>> print sr_parse.parse(sent)\n  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\nYour Turn:  Run this parser in tracing mode to see the sequence of shift\nand reduce operations, using sr_parse = nltk.ShiftReduceParser(gram\nmar1, trace=2).\nA shift-reduce parser can reach a dead end and fail to find any parse, even if the input\nsentence is well-formed according to the grammar. When this happens, no input re-\nmains, and the stack contains items that cannot be reduced to an S. The problem arises\nbecause there are choices made earlier that cannot be undone by the parser (although\nusers of the graphical demonstration can undo their choices). There are two kinds of\nchoices to be made by the parser: (a) which reduction to do when more than one is\npossible and (b) whether to shift or reduce when either action is possible.\nA shift-reduce parser may be extended to implement policies for resolving such con-\nflicts. For example, it may address shift-reduce conflicts by shifting only when no re-\nductions are possible, and it may address reduce-reduce conflicts by favoring the re-\nduction operation that removes the most items from the stack. (A generalization of the\nshift-reduce parser, a \u201clookahead LR parser,\u201d is commonly used in programming lan-\nguage compilers.)\nThe advantages of shift-reduce parsers over recursive descent parsers is that they only\nbuild structure that corresponds to the words in the input. Furthermore, they only build\neach substructure once; e.g., NP(Det(the), N(man))  is only built and pushed onto the\nstack a single time, regardless of whether it will later be used by the VP -> V NP PP\nreduction or the NP -> NP PP reduction.\nThe Left-Corner Parser\nOne of the problems with the recursive descent parser is that it goes into an infinite\nloop when it encounters a left-recursive production. This is because it applies the\ngrammar productions blindly, without considering the actual input sentence. A left-\ncorner parser is a hybrid between the bottom-up and top-down approaches we have\nseen.\nA left-corner parser  is a top-down parser with bottom-up filtering. Unlike an ordinary\nrecursive descent parser, it does not get trapped in left-recursive productions. Before\nstarting its work, a left-corner parser preprocesses the context-free grammar to build a\ntable where each row contains two cells, the first holding a non-terminal, and the sec-\nond holding the collection of possible left corners of that non-terminal. Table 8-2 il-\nlustrates this for the grammar from grammar2.\n306 | Chapter 8: \u2002Analyzing Sentence Structure\nTable 8-2. Left corners in grammar2\nCategory Left corners (pre-terminals)\nS NP\nNP Det, PropN\nVP V\nPP P\nEach time a production is considered by the parser, it checks that the next input word\nis compatible with at least one of the pre-terminal categories in the left-corner table.\nWell-Formed Substring Tables\nThe simple \nparsers discussed in the previous sections suffer from limitations in both\ncompleteness and efficiency. In order to remedy these, we will apply the algorithm\ndesign technique of dynamic programming to the parsing problem. As we saw in\nSection 4.7 , dynamic programming stores intermediate results and reuses them when\nappropriate, achieving significant efficiency gains. This technique can be applied to\nsyntactic parsing, allowing us to store partial solutions to the parsing task and then\nlook them up as necessary in order to efficiently arrive at a complete solution. This\napproach to parsing is known as chart parsing . We introduce the main idea in this\nsection; see the online materials available for this chapter for more implementation\ndetails.\nDynamic programming allows us to build the PP in my pajamas  just once. The first time\nwe build it we save it in a table, then we look it up when we need to use it as a sub-\nconstituent of either the object NP or the higher VP. This table is known as a well-formed\nsubstring table , or WFST for short. (The term \u201csubstring\u201d refers to a contiguous se-\nquence of words within a sentence.) We will show how to construct the WFST bottom-\nup so as to systematically record what syntactic constituents have been found.\nLet\u2019s set our input to be the sentence in (2). The numerically specified spans of the\nWFST are reminiscent of Python\u2019s slice notation ( Section 3.2 ). Another way to think\nabout the data structure is shown in Figure 8-6, a data structure known as a chart.\nFigure 8-6. The chart data structure: Words are the edge labels of a linear graph structure.\nIn a \nWFST, we record the position of the words by filling in cells in a triangular matrix:\nthe vertical axis will denote the start position of a substring, while the horizontal axis\nwill denote the end position (thus shot will appear in the cell with coordinates (1, 2)).\nTo simplify this presentation, we will assume each word has a unique lexical category,\n8.4  Parsing with Context-Free Grammar | 307\nand we will store this (not the word) in the matrix. So cell (1, 2) will contain the entry\nV. More \ngenerally, if our input string is a1a2 ... an, and our grammar contains a pro-\nduction of the form A \u2192 a i, then we add A to the cell (i-1, i).\nSo, for every word in text, we can look up in our grammar what category it belongs to.\n>>> text = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n[V -> 'shot']\nFor our WFST, we create an ( n-1) \u00d7 (n-1) matrix as a list of lists in Python, and initialize\nit with the lexical categories of each token in the init_wfst() function in Exam-\nple 8-3 . We also define a utility function display() to pretty-print the WFST for us. As\nexpected, there is a V in cell (1, 2).\nExample 8-3. Acceptor using well-formed substring table.\ndef init_wfst(tokens, grammar):\n    numtokens = len(tokens)\n    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\n    for i in range(numtokens):\n        productions = grammar.productions(rhs=tokens[i])\n        wfst[i][i+1] = productions[0].lhs()\n    return wfst\ndef complete_wfst(wfst, tokens, grammar, trace=False):\n    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n    numtokens = len(tokens)\n    for span in range(2, numtokens+1):\n        for start in range(numtokens+1-span):\n            end = start + span\n            for mid in range(start+1, end):\n                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n                if nt1 and nt2 and (nt1,nt2) in index:\n                    wfst[start][end] = index[(nt1,nt2)]\n                    if trace:\n                        print \"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end)\n    return wfst\ndef display(wfst, tokens):\n    print '\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1, len(wfst))])\n    for i in range(len(wfst)-1):\n        print \"%d   \" % i,\n        for j in range(1, len(wfst)):\n            print \"%-4s\" % (wfst[i][j] or '.'),\n        print\n>>> tokens = \"I shot an elephant in my pajamas\".split()\n>>> wfst0 = init_wfst(tokens, groucho_grammar)\n>>> display(wfst0, tokens)\nWFST 1    2    3    4    5    6    7\n0    NP   .    .    .    .    .    .\n1    .    V    .    .    .    .    .\n2    .    .    Det  .    .    .    .\n3    .    .    .    N    .    .    .\n308 | Chapter 8: \u2002Analyzing Sentence Structure\n4    .    .    .    .    P    .    .\n5    .    .    .    .    .    Det  .\n6    .    .    .    .    .    .    N\n>>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\n>>> display(wfst1, tokens)\nWFST 1    2    3    4    5    6    7\n0    NP   .    .    S    .    .    S\n1    .    V    .    VP   .    .    VP\n2    .    .    Det  NP   .    .    .\n3    .    .    .    N    .    .    .\n4    .    .    .    .    P    .    PP\n5    .    .    .    .    .    Det  NP\n6    .    .    .    .    .    .    N\nReturning to \nour tabular representation, given that we have Det in cell (2, 3) for the\nword an, and N in cell (3, 4) for the word elephant, what should we put into cell (2, 4)\nfor an elephant ? We need to find a production of the form A \u2192 Det N . Consulting the\ngrammar, we know that we can enter NP in cell (0, 2).\nMore generally, we can enter A in ( i, j) if there is a production A \u2192 B C, and we find\nnon-terminal B in ( i, k) and C in ( k, j). The program in Example 8-3 uses this rule to\ncomplete the WFST. By setting trace to True when calling the function\ncomplete_wfst(), we see tracing output that shows the WFST being constructed:\n>>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar, trace=True)\n[2] Det [3]   N [4] ==> [2]  NP [4]\n[5] Det [6]   N [7] ==> [5]  NP [7]\n[1]   V [2]  NP [4] ==> [1]  VP [4]\n[4]   P [5]  NP [7] ==> [4]  PP [7]\n[0]  NP [1]  VP [4] ==> [0]   S [4]\n[1]  VP [4]  PP [7] ==> [1]  VP [7]\n[0]  NP [1]  VP [7] ==> [0]   S [7]\nFor example, this says that since we found Det at wfst[0][1] and N at wfst[1][2], we\ncan add NP to wfst[0][2].\nTo help us easily retrieve productions by their righthand sides, we create\nan index \nfor the grammar. This is an example of a space-time trade-off:\nwe do a reverse lookup on the grammar, instead of having to check\nthrough entire list of productions each time we want to look up via the\nrighthand side.\nWe conclude that there is a parse for the whole input string once we have constructed\nan S node in cell (0, 7), showing that we have found a sentence that covers the whole\ninput. The final state of the WFST is depicted in Figure 8-7.\nNotice that we have not used any built-in parsing functions here. We\u2019ve implemented\na complete primitive chart parser from the ground up!\nWFSTs have several shortcomings. First, as you can see, the WFST is not itself a parse\ntree, so the technique is strictly speaking recognizing that a sentence is admitted by a\n8.4  Parsing with Context-Free Grammar | 309\ngrammar, rather than parsing it. Second, it requires every non-lexical grammar pro-\nduction to \nbe binary. Although it is possible to convert an arbitrary CFG into this form,\nwe would prefer to use an approach without such a requirement. Third, as a bottom-\nup approach it is potentially wasteful, being able to propose constituents in locations\nthat would not be licensed by the grammar.\nFinally, the WFST did not represent the structural ambiguity in the sentence (i.e., the\ntwo verb phrase readings). The VP in cell (2,8) was actually entered twice, once for a V\nNP reading, and once for a VP PP reading. These are different hypotheses, and the second\noverwrote the first (as it happens, this didn\u2019t matter since the lefthand side was the\nsame). Chart parsers use a slightly richer data structure and some interesting algorithms\nto solve these problems (see Section 8.8).\nYour Turn: Try out the interactive chart parser application\nnltk.app.chartparser().\n8.5  Dependencies and Dependency Grammar\nPhrase structure grammar is concerned with how words and sequences of words com-\nbine to form constituents. A distinct and complementary approach, dependency\ngrammar, focuses instead on how words relate to other words. Dependency is a binary\nasymmetric relation that holds between a head and its dependents. The head of a\nsentence is usually taken to be the tensed verb, and every other word is either dependent\non the sentence head or connects to it through a path of dependencies.\nA dependency representation is a labeled directed graph, where the nodes are the lexical\nitems and the labeled arcs represent dependency relations from heads to dependents.\nFigure 8-8  illustrates a dependency graph, where arrows point from heads to their\ndependents.\nFigure 8-7. The chart data structure: Non-terminals are represented as extra edges in the chart.\n310 | Chapter 8: \u2002Analyzing Sentence Structure\nThe arcs in Figure 8-8  are labeled with the grammatical function that holds between a\ndependent and its head. For example, I is the SBJ (subject) of shot (which is the head\nof the whole sentence), and in is an NMOD (noun modifier of elephant). In contrast to\nphrase structure grammar, therefore, dependency grammars can be used to directly\nexpress grammatical functions as a type of dependency.\nHere\u2019s one way of encoding a dependency grammar in NLTK\u2014note that it only cap-\ntures bare dependency information without specifying the type of dependency:\n>>> groucho_dep_grammar = nltk.parse_dependency_grammar(\"\"\"\n... 'shot' -> 'I' | 'elephant' | 'in'\n... 'elephant' -> 'an' | 'in'\n... 'in' -> 'pajamas'\n... 'pajamas' -> 'my'\n... \"\"\")\n>>> print groucho_dep_grammar\nDependency grammar with 7 productions\n  'shot' -> 'I'\n  'shot' -> 'elephant'\n  'shot' -> 'in'\n  'elephant' -> 'an'\n  'elephant' -> 'in'\n  'in' -> 'pajamas'\n  'pajamas' -> 'my'\nA dependency graph is projective if, when all the words are written in linear order, the\nedges can be drawn above the words without crossing. This is equivalent to saying that\na word and all its descendants (dependents and dependents of its dependents, etc.)\nform a contiguous sequence of words within the sentence. Figure 8-8  is projective, and\nwe can parse many sentences in English using a projective dependency parser. The next\nexample shows how groucho_dep_grammar provides an alternative approach to captur-\ning the attachment ambiguity that we examined earlier with phrase structure grammar.\n>>> pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n>>> sent = 'I shot an elephant in my pajamas'.split()\n>>> trees = pdp.parse(sent)\n>>> for tree in trees:\n...     print tree\n(shot I (elephant an (in (pajamas my))))\n(shot I (elephant an) (in (pajamas my)))\nFigure 8-8. Dependency structure: Arrows point from heads to their dependents; labels indicate the\ngrammatical function of the dependent as subject, object, or modifier.\n8.5  Dependencies and Dependency Grammar | 311\nThese bracketed dependency structures can also be displayed as trees, where dep-\nendents are shown as children of their heads.\n(11)\nIn languages with more flexible word order than English, non-projective dependencies\nare more frequent.\nVarious criteria \nhave been proposed for deciding what is the head H and what is the\ndependent D in a construction C. Some of the most important are the following:\n1.H determines the distribution class of C; or alternatively, the external syntactic\nproperties of C are due to H.\n2.H determines the semantic type of C.\n3.H is obligatory while D may be optional.\n4.H selects D and determines whether it is obligatory or optional.\n5. The morphological form of D is determined by H (e.g., agreement or case\ngovernment).\nWhen we say in a phrase structure grammar that the immediate constituents of a PP\nare P and NP, we are implicitly appealing to the head/dependent distinction. A prepo-\nsitional phrase is a phrase whose head is a preposition; moreover, the NP is a dependent\nof P. The same distinction carries over to the other types of phrase that we have dis-\ncussed. The key point to note here is that although phrase structure grammars seem\nvery different from dependency grammars, they implicitly embody a recognition of\ndependency relations. Although CFGs are not intended to directly capture dependen-\ncies, more recent linguistic frameworks have increasingly adopted formalisms which\ncombine aspects of both approaches.\nValency and the Lexicon\nLet us take a closer look at verbs and their dependents. The grammar in Example 8-2\ncorrectly generates examples like (12).\n312 | Chapter 8: \u2002Analyzing Sentence Structure\n(12) a. The squirrel was frightened.\nb. Chatterer saw the bear.\nc.\nChatterer thought Buster was angry.\nd. Joe put the fish on the log.\nThese possibilities correspond to the productions in Table 8-3.\nTable 8-3. VP productions and their lexical heads\nProduction Lexical head\nVP -> V Adj was\nVP -> V NP saw\nVP -> V S thought\nVP -> V NP PP put\nThat is, was can occur with a following Adj, saw can occur with a following NP,\nthought can occur with a following S, and put can occur with a following NP and PP. The\ndependents Adj, NP, S, and PP are often called complements of the respective verbs,\nand there are strong constraints on what verbs can occur with what complements. By\ncontrast with (12), the word sequences in (13) are ill-formed:\n(13) a. *The squirrel was Buster was angry.\nb. *Chatterer saw frightened.\nc. *Chatterer thought the bear.\nd. *Joe put on the log.\nWith a little imagination, it is possible to invent contexts in which un-\nusual combinations \nof verbs and complements are interpretable. How-\never, we assume that the examples in (13) are to be interpreted in neutral\ncontexts.\nIn the tradition of dependency grammar, the verbs in Table 8-3 are said to have different\nvalencies. Valency restrictions are not just applicable to verbs, but also to the other\nclasses of heads.\nWithin frameworks based on phrase structure grammar, various techniques have been\nproposed for excluding the ungrammatical examples in (13). In a CFG, we need some\nway of constraining grammar productions which expand VP so that verbs co-occur\nonly with their correct complements. We can do this by dividing the class of verbs into\n\u201csubcategories,\u201d each of which is associated with a different set of complements. For\nexample, transitive verbs  such as chased and saw require a following NP object com-\nplement; that is, they are subcategorized for NP direct objects. If we introduce a new\n8.5  Dependencies and Dependency Grammar | 313\ncategory label for transitive verbs, namely TV (for transitive verb), then we can use it in\nthe following productions:\nVP -> TV NP\nTV -> 'chased' | 'saw'\nNow *Joe thought the bear  is excluded since we haven\u2019t listed thought as a TV, but\nChatterer saw the bear  is still allowed. Table 8-4  provides more examples of labels for\nverb subcategories.\nTable 8-4. Verb subcategories\nSymbol Meaning Example\nIV Intransitive verb barked\nTV Transitive verb saw a man\nDatV Dative verb gave a dog to a man\nSV Sentential verb said that a dog barked\nValency is a property of lexical items, and we will discuss it further in Chapter 9.\nComplements are \noften contrasted with modifiers (or adjuncts), although both are\nkinds of dependents. Prepositional phrases, adjectives, and adverbs typically function\nas modifiers. Unlike complements, modifiers are optional, can often be iterated, and\nare not selected for by heads in the same way as complements. For example, the adverb\nreally can be added as a modifier to all the sentences in (14):\n(14) a. The squirrel really was frightened.\nb. Chatterer really saw the bear.\nc. Chatterer really thought Buster was angry.\nd. Joe really put the fish on the log.\nThe structural ambiguity of PP attachment, which we have illustrated in both phrase\nstructure and dependency grammars, corresponds semantically to an ambiguity in the\nscope of the modifier.\nScaling Up\nSo far, we have only considered \u201ctoy grammars,\u201d small grammars that illustrate the key\naspects of parsing. But there is an obvious question as to whether the approach can be\nscaled up to cover large corpora of natural languages. How hard would it be to construct\nsuch a set of productions by hand? In general, the answer is: very hard . Even if we allow\nourselves to use various formal devices that give much more succinct representations\nof grammar productions, it is still extremely difficult to keep control of the complex\ninteractions between the many productions required to cover the major constructions\nof a language. In other words, it is hard to modularize grammars so that one portion\ncan be developed independently of the other parts. This in turn means that it is difficult\n314 | Chapter 8: \u2002Analyzing Sentence Structure\nto distribute the task of grammar writing across a team of linguists. Another difficulty\nis that \nas the grammar expands to cover a wider and wider range of constructions, there\nis a corresponding increase in the number of analyses that are admitted for any one\nsentence. In other words, ambiguity increases with coverage.\nDespite these problems, some large collaborative projects have achieved interesting and\nimpressive results in developing rule-based grammars for several languages. Examples\nare the Lexical Functional Grammar (LFG) Pargram project, the Head-Driven Phrase\nStructure Grammar (HPSG) LinGO Matrix framework, and the Lexicalized Tree Ad-\njoining Grammar XTAG Project.\n8.6  Grammar Development\nParsing builds trees over sentences, according to a phrase structure grammar. Now, all\nthe examples we gave earlier only involved toy grammars containing a handful of pro-\nductions. What happens if we try to scale up this approach to deal with realistic corpora\nof language? In this section, we will see how to access treebanks, and look at the chal-\nlenge of developing broad-coverage grammars.\nTreebanks and Grammars\nThe corpus module defines the treebank corpus reader, which contains a 10% sample\nof the Penn Treebank Corpus.\n>>> from nltk.corpus import treebank\n>>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n>>> print t\n(S\n  (NP-SBJ\n    (NP (NNP Pierre) (NNP Vinken))\n    (, ,)\n    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n    (, ,))\n  (VP\n    (MD will)\n    (VP\n      (VB join)\n      (NP (DT the) (NN board))\n      (PP-CLR\n        (IN as)\n        (NP (DT a) (JJ nonexecutive) (NN director)))\n      (NP-TMP (NNP Nov.) (CD 29))))\n  (. .))\nWe can use this data to help develop a grammar. For example, the program in Exam-\nple 8-4 uses a simple filter to find verbs that take sentential complements. Assuming\nwe already have a production of the form VP -> SV S, this information enables us to\nidentify particular verbs that would be included in the expansion of SV.\n8.6  Grammar Development | 315\nExample 8-4. Searching a treebank to find sentential complements.\ndef filter(tree):\n    child_nodes = [child.node for child in tree\n                   if isinstance(child, nltk.Tree)]\n    return  (tree.node == 'VP') and ('S' in child_nodes)\n>>> from nltk.corpus import treebank\n>>> [subtree for tree in treebank.parsed_sents()\n...          for subtree in tree.subtrees(filter)]\n [Tree('VP', [Tree('VBN', ['named']), Tree('S', [Tree('NP-SBJ', ...]), ...]), ...]\nThe PP \nAttachment Corpus, nltk.corpus.ppattach, is another source of information\nabout the valency of particular verbs. Here we illustrate a technique for mining this\ncorpus. It finds pairs of prepositional phrases where the preposition and noun are fixed,\nbut where the choice of verb determines whether the prepositional phrase is attached\nto the VP or to the NP.\n>>> entries = nltk.corpus.ppattach.attachments('training')\n>>> table = nltk.defaultdict(lambda: nltk.defaultdict(set))\n>>> for entry in entries:\n...     key = entry.noun1 + '-' + entry.prep + '-' + entry.noun2\n...     table[key][entry.attachment].add(entry.verb)\n...\n>>> for key in sorted(table):\n...     if len(table[key]) > 1:\n...         print key, 'N:', sorted(table[key]['N']), 'V:', sorted(table[key]['V'])\nAmong the output lines of this program we find offer-from-group N: ['rejected'] V:\n['received'], which indicates that received expects a separate PP complement attached\nto the VP, while rejected does not. As before, we can use this information to help con-\nstruct the grammar.\nThe NLTK corpus collection includes data from the PE08 Cross-Framework and Cross\nDomain Parser Evaluation Shared Task. A collection of larger grammars has been pre-\npared for the purpose of comparing different parsers, which can be obtained by down-\nloading the large_grammars package (e.g., python -m nltk.downloader large_grammars).\nThe NLTK corpus collection also includes a sample from the Sinica Treebank Corpus,\nconsisting of 10,000 parsed sentences drawn from the Academia Sinica Balanced Corpus\nof Modern Chinese. Let\u2019s load and display one of the trees in this corpus.\n>>> nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()\n316 | Chapter 8: \u2002Analyzing Sentence Structure\nPernicious Ambiguity\nUnfortunately, as \nthe coverage of the grammar increases and the length of the input\nsentences grows, the number of parse trees grows rapidly. In fact, it grows at an astro-\nnomical rate.\nLet\u2019s explore this issue with the help of a simple example. The word fish is both a noun\nand a verb. We can make up the sentence fish fish fish , meaning fish like to fish for other\nfish. (Try this with police if you prefer something more sensible.) Here is a toy grammar\nfor the \u201cfish\u201d sentences.\n>>> grammar = nltk.parse_cfg(\"\"\"\n... S -> NP V NP\n... NP -> NP Sbar\n... Sbar -> NP V\n... NP -> 'fish'\n... V -> 'fish'\n... \"\"\")\nNow we can try parsing a longer sentence, fish fish fish fish fish , which among other\nthings, means \u201cfish that other fish fish are in the habit of fishing fish themselves.\u201d We\nuse the NLTK chart parser, which is presented earlier in this chapter. This sentence has\ntwo readings.\n>>> tokens = [\"fish\"] * 5\n>>> cp = nltk.ChartParser(grammar)\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\nAs the length of this sentence goes up (3, 5, 7, ...) we get the following numbers of parse\ntrees: 1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; \u2026. (These are\nthe Catalan numbers , which we saw in an exercise in Chapter 4 .) The last of these is\nfor a sentence of length 23, the average length of sentences in the WSJ section of Penn\nTreebank. For a sentence of length 50 there would be over 1012 parses, and this is only\nhalf the length of the Piglet sentence ( Section 8.1 ), which young children process ef-\nfortlessly. No practical NLP system could construct millions of trees for a sentence and\nchoose the appropriate one in the context. It\u2019s clear that humans don\u2019t do this either!\nNote that the problem is not with our choice of example. (Church & Patil, 1982) point\nout that the syntactic ambiguity of PP attachment in sentences like (15) also grows in\nproportion to the Catalan numbers.\n(15) Put the block in the box on the table.\nSo much for structural ambiguity; what about lexical ambiguity? As soon as we try to\nconstruct a broad-coverage grammar, we are forced to make lexical entries highly am-\nbiguous for their part-of-speech. In a toy grammar, a is only a determiner, dog is only\na noun, and runs is only a verb. However, in a broad-coverage grammar, a is also a\n8.6  Grammar Development | 317\nnoun (e.g., part a ), dog is also a verb (meaning to follow closely), and runs is also a noun\n(e.g., ski runs). In fact, all words can be referred to by name: e.g., the verb \u2018ate\u2019 is spelled\nwith three letters ; in speech we do not need to supply quotation marks. Furthermore,\nit is possible to verb most nouns. Thus a parser for a broad-coverage grammar will be\noverwhelmed with ambiguity. Even complete gibberish will often have a reading, e.g.,\nthe a are of I . As (Abney, 1996) has pointed out, this is not word salad but a grammatical\nnoun phrase, in which are is a noun meaning a hundredth of a hectare (or 100 sq m),\nand a and I are nouns designating coordinates, as shown in Figure 8-9.\nFigure 8-9. The a are of I: A schematic drawing of 27 paddocks, each being one are in size, and each\nidentified using coordinates; the top-left cell is the a are of column A (after Abney).\nEven though \nthis phrase is unlikely, it is still grammatical, and a broad-coverage parser\nshould be able to construct a parse tree for it. Similarly, sentences that seem to be\nunambiguous, such as John saw Mary , turn out to have other readings we would not\nhave anticipated (as Abney explains). This ambiguity is unavoidable, and leads to hor-\nrendous inefficiency in parsing seemingly innocuous sentences. The solution to these\nproblems is provided by probabilistic parsing , which allows us to rank the parses of an\nambiguous sentence on the basis of evidence from corpora.\nWeighted Grammar\nAs we have just seen, dealing with ambiguity is a key challenge in developing broad-\ncoverage parsers. Chart parsers improve the efficiency of computing multiple parses of\nthe same sentences, but they are still overwhelmed by the sheer number of possible\nparses. Weighted grammars and probabilistic parsing algorithms have provided an ef-\nfective solution to these problems.\nBefore looking at these, we need to understand why the notion of grammaticality could\nbe gradient. Considering the verb give. This verb requires both a direct object (the thing\nbeing given) and an indirect object (the recipient). These complements can be given in\neither order, as illustrated in (16). In the \u201cprepositional dative\u201d form in (16a), the direct\nobject appears first, followed by a prepositional phrase containing the indirect object.\n318 | Chapter 8: \u2002Analyzing Sentence Structure\n(16) a. Kim gave a bone to the dog.\nb. Kim gave the dog a bone.\nIn the \u201cdouble object\u201d form in (16b)\n, the indirect object appears first, followed by the\ndirect object. In this case, either order is acceptable. However, if the indirect object is\na pronoun, there is a strong preference for the double object construction:\n(17) a. Kim gives the heebie-jeebies to me ( prepositional dative).\nb. Kim gives me the heebie-jeebies (double object).\nUsing the Penn Treebank sample, we can examine all instances of prepositional dative\nand double object constructions involving give, as shown in Example 8-5.\nExample 8-5. Usage of give and gave in the Penn Treebank sample.\ndef give(t):\n    return t.node == 'VP' and len(t) > 2 and t[1].node == 'NP'\\\n           and (t[2].node == 'PP-DTV' or t[2].node == 'NP')\\\n           and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\ndef sent(t):\n    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\ndef print_node(t, width):\n        output = \"%s %s: %s / %s: %s\" %\\\n            (sent(t[0]), t[1].node, sent(t[1]), t[2].node, sent(t[2]))\n        if len(output) > width:\n            output = output[:width] + \"...\"\n        print output\n>>> for tree in nltk.corpus.treebank.parsed_sents():\n...     for t in tree.subtrees(give):\n...         print_node(t, 72)\ngave NP: the chefs / NP: a standing ovation\ngive NP: advertisers / NP: discounts for maintaining or increasing ad sp...\ngive NP: it / PP-DTV: to the politicians\ngave NP: them / NP: similar help\ngive NP: them / NP:\ngive NP: only French history questions / PP-DTV: to students in a Europe...\ngive NP: federal judges / NP: a raise\ngive NP: consumers / NP: the straight scoop on the U.S. waste crisis\ngave NP: Mitsui / NP: access to a high-tech medical product\ngive NP: Mitsubishi / NP: a window on the U.S. glass industry\ngive NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\ngive NP: your Foster Savings Institution / NP: the gift of hope and free...\ngive NP: market operators / NP: the authority to suspend trading in futu...\ngave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\ngive NP: the Transportation Department / NP: up to 50 days to review any...\ngive NP: the president / NP: such power\ngive NP: me / NP: the heebie-jeebies\ngive NP: holders / NP: the right , but not the obligation , to buy a cal...\ngave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...\ngive NP: the president / NP: line-item veto power\n8.6  Grammar Development | 319\nWe can observe a strong tendency for the shortest complement to appear first. How-\never, this \ndoes not account for a form like give NP: federal judges / NP: a raise,\nwhere animacy may play a role. In fact, there turns out to be a large number of\ncontributing factors, as surveyed by (Bresnan & Hay, 2008). Such preferences can be\nrepresented in a weighted grammar.\nA probabilistic context-free grammar  (or PCFG) is a context-free grammar that as-\nsociates a probability with each of its productions. It generates the same set of parses\nfor a text that the corresponding context-free grammar does, and assigns a probability\nto each parse. The probability of a parse generated by a PCFG is simply the product of\nthe probabilities of the productions used to generate it.\nThe simplest way to define a PCFG is to load it from a specially formatted string con-\nsisting of a sequence of weighted productions, where weights appear in brackets, as\nshown in Example 8-6.\nExample 8-6. Defining a probabilistic context-free grammar (PCFG).\ngrammar = nltk.parse_pcfg(\"\"\"\n    S    -> NP VP              [1.0]\n    VP   -> TV NP              [0.4]\n    VP   -> IV                 [0.3]\n    VP   -> DatV NP NP         [0.3]\n    TV   -> 'saw'              [1.0]\n    IV   -> 'ate'              [1.0]\n    DatV -> 'gave'             [1.0]\n    NP   -> 'telescopes'       [0.8]\n    NP   -> 'Jack'             [0.2]\n    \"\"\")\n>>> print grammar\nGrammar with 9 productions (start state = S)\n    S -> NP VP [1.0]\n    VP -> TV NP [0.4]\n    VP -> IV [0.3]\n    VP -> DatV NP NP [0.3]\n    TV -> 'saw' [1.0]\n    IV -> 'ate' [1.0]\n    DatV -> 'gave' [1.0]\n    NP -> 'telescopes' [0.8]\n    NP -> 'Jack' [0.2]\nIt is sometimes convenient to combine multiple productions into a single line, e.g.,\nVP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]. In order to ensure that the trees\ngenerated by the grammar form a probability distribution, PCFG grammars impose the\nconstraint that all productions with a given lefthand side must have probabilities that\nsum to one. The grammar in Example 8-6  obeys this constraint: for S, there is only one\nproduction, with a probability of 1.0; for VP, 0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0.\nThe parse tree returned by parse() includes probabilities:\n320 | Chapter 8: \u2002Analyzing Sentence Structure\n>>> viterbi_parser = nltk.ViterbiParser(grammar)\n>>> print viterbi_parser.parse(['Jack', 'saw', 'telescopes'])\n(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\nNow that \nparse trees are assigned probabilities, it no longer matters that there may be\na huge number of possible parses for a given sentence. A parser will be responsible for\nfinding the most likely parses.\n8.7  Summary\n\u2022 Sentences have internal organization that can be represented using a tree. Notable\nfeatures of constituent structure are: recursion, heads, complements, and\nmodifiers.\n\u2022 A grammar is a compact characterization of a potentially infinite set of sentences;\nwe say that a tree is well-formed according to a grammar, or that a grammar licenses\na tree.\n\u2022 A grammar is a formal model for describing whether a given phrase can be assigned\na particular constituent or dependency structure.\n\u2022 Given a set of syntactic categories, a context-free grammar uses a set of productions\nto say how a phrase of some category A can be analyzed into a sequence of smaller\nparts \u03b1 1 ... \u03b1 n.\n\u2022 A dependency grammar uses productions to specify what the dependents are of a\ngiven lexical head.\n\u2022 Syntactic ambiguity arises when one sentence has more than one syntactic analysis\n(e.g., prepositional phrase attachment ambiguity).\n\u2022 A parser is a procedure for finding one or more trees corresponding to a grammat-\nically well-formed sentence.\n\u2022 A simple top-down parser is the recursive descent parser, which recursively ex-\npands the start symbol (usually S) with the help of the grammar productions, and\ntries to match the input sentence. This parser cannot handle left-recursive pro-\nductions (e.g., productions such as NP -> NP PP). It is inefficient in the way it blindly\nexpands categories without checking whether they are compatible with the input\nstring, and in repeatedly expanding the same non-terminals and discarding the\nresults.\n\u2022 A simple bottom-up parser is the shift-reduce parser, which shifts input onto a\nstack and tries to match the items at the top of the stack with the righthand side\nof grammar productions. This parser is not guaranteed to find a valid parse for the\ninput, even if one exists, and builds substructures without checking whether it is\nglobally consistent with the grammar.\n8.7  Summary | 321\n8.8  Further Reading\nExtra materials \nfor this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web. For more examples of parsing with NLTK, please\nsee the Parsing HOWTO at http://www.nltk.org/howto.\nThere are many introductory books on syntax. (O\u2019Grady et al., 2004) is a general in-\ntroduction to linguistics, while (Radford, 1988) provides a gentle introduction to trans-\nformational grammar, and can be recommended for its coverage of transformational\napproaches to unbounded dependency constructions. The most widely used term in\nlinguistics for formal grammar is generative grammar , though it has nothing to do\nwith generation (Chomsky, 1965).\n(Burton-Roberts, 1997) is a practically oriented textbook on how to analyze constitu-\nency in English, with extensive exemplification and exercises. (Huddleston & Pullum,\n2002) provides an up-to-date and comprehensive analysis of syntactic phenomena in\nEnglish.\nChapter 12 of (Jurafsky & Martin, 2008) covers formal grammars of English; Sections\n13.1\u20133 cover simple parsing algorithms and techniques for dealing with ambiguity;\nChapter 14 covers statistical parsing; and Chapter 16 covers the Chomsky hierarchy\nand the formal complexity of natural language. (Levin, 1993) has categorized English\nverbs into fine-grained classes, according to their syntactic properties.\nThere are several ongoing efforts to build large-scale rule-based grammars, e.g., the\nLFG Pargram project ( http://www2.parc.com/istl/groups/nltt/pargram/), the HPSG Lin-\nGO Matrix framework ( http://www.delph-in.net/matrix/), and the XTAG Project ( http:\n//www.cis.upenn.edu/~xtag/).\n8.9  Exercises\n1.\u25cb Can you come up with grammatical sentences that probably have never been\nuttered before? (Take turns with a partner.) What does this tell you about human\nlanguage?\n2.\u25cb Recall Strunk and White\u2019s prohibition against using a sentence-initial however\nto mean \u201calthough.\u201d Do a web search for however used at the start of the sentence.\nHow widely used is this construction?\n3.\u25cb Consider the sentence Kim arrived or Dana left and everyone cheered . Write down\nthe parenthesized forms to show the relative scope of and and or. Generate tree\nstructures corresponding to both of these interpretations.\n4.\u25cb The Tree class implements a variety of other useful methods. See the Tree help\ndocumentation for more details (i.e., import the Tree class and then type\nhelp(Tree)).\n5.\u25cb In this exercise you will manually construct some parse trees.\n322 | Chapter 8: \u2002Analyzing Sentence Structure\na. Write code to produce two trees, one for each reading of the phrase old men\nand women\n.\nb. Encode any of the trees presented in this chapter as a labeled bracketing, and\nuse nltk.Tree() to check that it is well-formed. Now use draw() to display the\ntree.\nc. As in (a), draw a tree for The woman saw a man last Thursday.\n6.\u25cb Write a recursive function to traverse a tree and return the depth of the tree, such\nthat a tree with a single node would have depth zero. (Hint: the depth of a subtree\nis the maximum depth of its children, plus one.)\n7.\u25cb Analyze the A.A. Milne sentence about Piglet, by underlining all of the sentences\nit contains then replacing these with S (e.g., the first sentence becomes S when S).\nDraw a tree structure for this \u201ccompressed\u201d sentence. What are the main syntactic\nconstructions used for building such a long sentence?\n8.\u25cb In the recursive descent parser demo, experiment with changing the sentence to\nbe parsed by selecting Edit Text in the Edit menu.\n9.\u25cb Can the grammar in grammar1 ( Example 8-1 ) be used to describe sentences that\nare more than 20 words in length?\n10.\u25cb Use the graphical chart-parser interface to experiment with different rule invo-\ncation strategies. Come up with your own strategy that you can execute manually\nusing the graphical interface. Describe the steps, and report any efficiency im-\nprovements it has (e.g., in terms of the size of the resulting chart). Do these im-\nprovements depend on the structure of the grammar? What do you think of the\nprospects for significant performance boosts from cleverer rule invocation\nstrategies?\n11.\u25cb With pen and paper, manually trace the execution of a recursive descent parser\nand a shift-reduce parser, for a CFG you have already seen, or one of your own\ndevising.\n12.\u25cb We have seen that a chart parser adds but never removes edges from a chart.\nWhy?\n13.\u25cb Consider the sequence of words: Buffalo buffalo Buffalo buffalo buffalo buffalo\nBuffalo buffalo . This is a grammatically correct sentence, as explained at http://en\n.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buf\nfalo. Consider the tree diagram presented on this Wikipedia page, and write down\na suitable grammar. Normalize case to lowercase, to simulate the problem that a\nlistener has when hearing this sentence. Can you find other parses for this sentence?\nHow does the number of parse trees grow as the sentence gets longer? (More ex-\namples of these sentences can be found at http://en.wikipedia.org/wiki/List_of_ho\nmophonous_phrases.)\n14.\u25d1 You can modify the grammar in the recursive descent parser demo by selecting\nEdit Grammar in the Edit menu. Change the first expansion production, namely\n8.9  Exercises | 323\nNP -> Det N PP, to NP -> NP PP . Using the Step button, try to build a parse tree.\nWhat happens?\n15.\u25d1 Extend the grammar in grammar2 with productions that expand prepositions as\nintransitive, transitive, and requiring a PP complement. Based on these produc-\ntions, use the method of the preceding exercise to draw a tree for the sentence Lee\nran away home.\n16.\u25d1 Pick some common verbs and complete the following tasks:\na. Write a program to find those verbs in the PP Attachment Corpus nltk.cor\npus.ppattach. Find any cases where the same verb exhibits two different at-\ntachments, but where the first noun, or second noun, or preposition stays\nunchanged (as we saw in our discussion of syntactic ambiguity in Section 8.2).\nb. Devise CFG grammar productions to cover some of these cases.\n17.\u25d1 Write a program to compare the efficiency of a top-down chart parser compared\nwith a recursive descent parser ( Section 8.4 ). Use the same grammar and input\nsentences for both. Compare their performance using the timeit module (see Sec-\ntion 4.7 for an example of how to do this).\n18.\u25d1 Compare the performance of the top-down, bottom-up, and left-corner parsers\nusing the same grammar and three grammatical test sentences. Use timeit to log\nthe amount of time each parser takes on the same sentence. Write a function that\nruns all three parsers on all three sentences, and prints a 3-by-3 grid of times, as\nwell as row and column totals. Discuss your findings.\n19.\u25d1 Read up on \u201cgarden path\u201d sentences. How might the computational work of a\nparser relate to the difficulty humans have with processing these sentences? (See\nhttp://en.wikipedia.org/wiki/Garden_path_sentence.)\n20.\u25d1 To compare multiple trees in a single window, we can use the draw_trees()\nmethod. Define some trees and try it out:\n>>> from nltk.draw.tree import draw_trees\n>>> draw_trees(tree1, tree2, tree3)\n21.\u25d1 Using tree positions, list the subjects of the first 100 sentences in the Penn tree-\nbank; to make the results easier to view, limit the extracted subjects to subtrees\nwhose height is at most 2.\n22.\u25d1 Inspect the PP Attachment Corpus and try to suggest some factors that influence\nPP attachment.\n23.\u25d1 In Section 8.2 , we claimed that there are linguistic regularities that cannot be\ndescribed simply in terms of n-grams. Consider the following sentence, particularly\nthe position of the phrase in his turn. Does this illustrate a problem for an approach\nbased on n-grams?\nWhat was more, the in his turn somewhat youngish Nikolay Parfenovich also turned\nout to be the only person in the entire world to acquire a sincere liking to our \u201cdis-\ncriminated-against\u201d public procurator. (Dostoevsky: The Brothers Karamazov)\n324 | Chapter 8: \u2002Analyzing Sentence Structure\n24.\u25d1 Write a recursive function that produces a nested bracketing for a tree, leaving\nout the leaf nodes and displaying the non-terminal labels after their subtrees. So\nthe example in Section 8.6  about Pierre Vinken would produce: [[[NNP NNP]NP ,\n[ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR\n[NNP CD]NP-TMP]VP .]S. Consecutive categories should be separated by space.\n25.\u25d1 Download several electronic books from Project Gutenberg. Write a program to\nscan these texts for any extremely long sentences. What is the longest sentence you\ncan find? What syntactic construction(s) are responsible for such long sentences?\n26.\u25d1 Modify the functions init_wfst() and complete_wfst() so that the contents of\neach cell in the WFST is a set of non-terminal symbols rather than a single non-\nterminal.\n27.\u25d1 Consider the algorithm in Example 8-3 . Can you explain why parsing context-\nfree grammar is proportional to n3, where n is the length of the input sentence?\n28.\u25d1 Process each tree of the Penn Treebank Corpus sample nltk.corpus.treebank\nand extract the productions with the help of Tree.productions(). Discard the pro-\nductions that occur only once. Productions with the same lefthand side and similar\nrighthand sides can be collapsed, resulting in an equivalent but more compact set\nof rules. Write code to output a compact grammar.\n29.\u25cf One common way of defining the subject of a sentence S in English is as the noun\nphrase that is the child of  S and the sibling of  VP. Write a function that takes the tree\nfor a sentence and returns the subtree corresponding to the subject of the sentence.\nWhat should it do if the root node of the tree passed to this function is not S, or if\nit lacks a subject?\n30.\u25cf Write a function that takes a grammar (such as the one defined in Exam-\nple 8-1 ) and returns a random sentence generated by the grammar. (Use gram\nmar.start() to find the start symbol of the grammar; grammar.productions(lhs) to\nget the list of productions from the grammar that have the specified lefthand side;\nand production.rhs() to get the righthand side of a production.)\n31.\u25cf Implement a version of the shift-reduce parser using backtracking, so that it finds\nall possible parses for a sentence, what might be called a \u201crecursive ascent parser.\u201d\nConsult the Wikipedia entry for backtracking at http://en.wikipedia.org/wiki/Back\ntracking.\n32.\u25cf As we saw in Chapter 7, it is possible to collapse chunks down to their chunk\nlabel. When we do this for sentences involving the word gave, we find patterns\nsuch as the following:\ngave NP\ngave up NP in NP\ngave NP up\ngave NP NP\ngave NP to NP\n8.9  Exercises | 325\na. Use this method to study the complementation patterns of a verb of interest,\nand \nwrite suitable grammar productions. (This task is sometimes called lexical\nacquisition.)\nb. Identify some English verbs that are near-synonyms, such as the dumped/filled/\nloaded example from (64) in Chapter 9. Use the chunking method to study the\ncomplementation patterns of these verbs. Create a grammar to cover these\ncases. Can the verbs be freely substituted for each other, or are there con-\nstraints? Discuss your findings.\n33.\u25cf Develop a left-corner parser based on the recursive descent parser, and inheriting\nfrom ParseI.\n34.\u25cf Extend NLTK\u2019s shift-reduce parser to incorporate backtracking, so that it is\nguaranteed to find all parses that exist (i.e., it is complete).\n35.\u25cf Modify the functions init_wfst() and complete_wfst() so that when a non-\nterminal symbol is added to a cell in the WFST, it includes a record of the cells\nfrom which it was derived. Implement a function that will convert a WFST in this\nform to a parse tree.\n326 | Chapter 8: \u2002Analyzing Sentence Structure\nCHAPTER 9\nBuilding Feature-Based Grammars\nNatural languages have an extensive range of grammatical constructions which are hard\nto handle \nwith the simple methods described in Chapter 8 . In order to gain more flex-\nibility, we change our treatment of grammatical categories like S, NP, and V. In place of\natomic labels, we decompose them into structures like dictionaries, where features can\ntake on a range of values.\nThe goal of this chapter is to answer the following questions:\n1. How can we extend the framework of context-free grammars with features so as\nto gain more fine-grained control over grammatical categories and productions?\n2. What are the main formal properties of feature structures, and how do we use them\ncomputationally?\n3. What kinds of linguistic patterns and grammatical constructions can we now cap-\nture with feature-based grammars?\nAlong the way, we will cover more topics in English syntax, including phenomena such\nas agreement, subcategorization, and unbounded dependency constructions.\n9.1  Grammatical Features\nIn Chapter 6, we described how to build classifiers that rely on detecting features of\ntext. Such features may be quite simple, such as extracting the last letter of a word, or\nmore complex, such as a part-of-speech tag that has itself been predicted by the clas-\nsifier. In this chapter, we will investigate the role of features in building rule-based\ngrammars. In contrast to feature extractors, which record features that have been au-\ntomatically detected, we are now going to declare the features of words and phrases.\nWe start off with a very simple example, using dictionaries to store features and their\nvalues.\n>>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n>>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\n327\nThe objects kim and chase  both have a couple of shared features, CAT (grammatical\ncategory) and ORTH (orthography, i.e., spelling). In addition, each has a more semanti-\ncally oriented feature: kim['REF'] is intended to give the referent of kim, while\nchase['REL'] gives the relation expressed by chase. In the context of rule-based gram-\nmars, such pairings of features and values are known as feature structures , and we\nwill shortly see alternative notations for them.\nFeature structures contain various kinds of information about grammatical entities.\nThe information need not be exhaustive, and we might want to add further properties.\nFor example, in the case of a verb, it is often useful to know what \u201csemantic role\u201d is\nplayed by the arguments of the verb. In the case of chase, the subject plays the role of\n\u201cagent,\u201d whereas the object has the role of \u201cpatient.\u201d Let\u2019s add this information, using\n'sbj' (subject) and 'obj' (object) as placeholders which will get filled once the verb\ncombines with its grammatical arguments:\n>>> chase['AGT'] = 'sbj'\n>>> chase['PAT'] = 'obj'\nIf we now process a sentence Kim chased Lee, we want to \u201cbind\u201d the verb\u2019s agent role\nto the subject and the patient role to the object. We do this by linking to the REF feature\nof the relevant NP. In the following example, we make the simple-minded assumption\nthat the NPs immediately to the left and right of the verb are the subject and object,\nrespectively. We also add a feature structure for Lee to complete the example.\n>>> sent = \"Kim chased Lee\"\n>>> tokens = sent.split()\n>>> lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n>>> def lex2fs(word):\n...     for fs in [kim, lee, chase]:\n...         if fs['ORTH'] == word:\n...             return fs\n>>> subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n >>> verb['AGT'] = subj['REF'] # agent of 'chase' is Kim\n >>> verb['PAT'] = obj['REF']  # patient of 'chase' is Lee\n >>> for k in ['ORTH', 'REL', 'AGT', 'PAT']: # check featstruct of 'chase'\n...     print \"%-5s => %s\" % (k, verb[k])\nORTH  => chased\nREL   => chase\nAGT   => k\nPAT   => l\nThe same approach could be adopted for a different verb\u2014say, surprise\u2014though in\nthis case, the subject would play the role of \u201csource\u201d ( SRC), and the object plays the role\nof \u201cexperiencer\u201d (EXP):\n>>> surprise = {'CAT': 'V', 'ORTH': 'surprised', 'REL': 'surprise',\n...             'SRC': 'sbj', 'EXP': 'obj'}\nFeature structures are pretty powerful, but the way in which we have manipulated them\nis extremely ad hoc . Our next task in this chapter is to show how the framework of\ncontext-free grammar and parsing can be expanded to accommodate feature structures,\nso that we can build analyses like this in a more generic and principled way. We will\n328 | Chapter 9: \u2002Building Feature-Based Grammars\nstart off by looking at the phenomenon of syntactic agreement; we will show how\nagreement constraints \ncan be expressed elegantly using features, and illustrate their use\nin a simple grammar.\nSince feature structures are a general data structure for representing information of any\nkind, we will briefly look at them from a more formal point of view, and illustrate the\nsupport for feature structures offered by NLTK. In the final part of the chapter, we\ndemonstrate that the additional expressiveness of features opens up a wide spectrum\nof possibilities for describing sophisticated aspects of linguistic structure.\nSyntactic Agreement\nThe following examples show pairs of word sequences, the first of which is grammatical\nand the second not. (We use an asterisk at the start of a word sequence to signal that\nit is ungrammatical.)\n(1) a. this dog\nb. *these dog\n(2) a. these dogs\nb. *this dogs\nIn English, nouns are usually marked as being singular or plural. The form of the de-\nmonstrative also varies: this (singular) and these (plural). Examples (1) and (2) show\nthat there are constraints on the use of demonstratives and nouns within a noun phrase:\neither both are singular or both are plural. A similar constraint holds between subjects\nand predicates:\n(3) a. the dog runs\nb. *the dog run\n(4) a. the dogs run\nb. *the dogs runs\nHere we can see that morphological properties of the verb co-vary with syntactic prop-\nerties of the subject noun phrase. This co-variance is called agreement. If we look\nfurther at verb agreement in English, we will see that present tense verbs typically have\ntwo inflected forms: one for third person singular, and another for every other combi-\nnation of person and number, as shown in Table 9-1.\n9.1  Grammatical Features | 329\nTable 9-1. Agreement paradigm for English regular verbs\n Singular Plural\n1st person I run we run\n2nd person you run you run\n3rd person he/she/it runs they run\nWe can make the role of morphological properties a bit more explicit, as illustrated in\n(5) and (6)\n. These representations indicate that the verb agrees with its subject in person\nand number. (We use 3 as an abbreviation for 3rd person, SG for singular, and PL for\nplural.)\n(5) the dog run-s\n dog.3.SG run-3.SG\n(6) the dog-s run\n dog.3.PL run-3.PL\nLet\u2019s see what happens when we encode these agreement constraints in a context-free\ngrammar. We will begin with the simple CFG in (7).\n(7)S   ->   NP VP\nNP  ->   Det N\nVP  ->   V\nDet  ->  'this'\nN    ->  'dog'\nV    ->  'runs'\nGrammar (7) allows \nus to generate the sentence this dog runs ; however, what we really\nwant to do is also generate these dogs run while blocking unwanted sequences like * this\ndogs run  and * these dog runs . The most straightforward approach is to add new non-\nterminals and productions to the grammar:\n(8)S -> NP_SG VP_SG\nS -> NP_PL VP_PL\nNP_SG -> Det_SG N_SG\nNP_PL -> Det_PL N_PL\nVP_SG -> V_SG\nVP_PL -> V_PL\nDet_SG -> 'this'\nDet_PL -> 'these'\nN_SG -> 'dog'\nN_PL -> 'dogs'\nV_SG -> 'runs'\nV_PL -> 'run'\nIn place of a single production expanding S, we now have two productions, one covering\nthe sentences involving singular subject NPs and VPs, the other covering sentences with\n330 | Chapter 9: \u2002Building Feature-Based Grammars\nplural subject NPs and VP s. In fact, every production in (7) has two counterparts in\n(8). With a small grammar, this is not really such a problem, although it is aesthetically\nunappealing. However, with a larger grammar that covers a reasonable subset of Eng-\nlish constructions, the prospect of doubling the grammar size is very unattractive. Let\u2019s\nsuppose now that we used the same approach to deal with first, second, and third\nperson agreement, for both singular and plural. This would lead to the original grammar\nbeing multiplied by a factor of 6, which we definitely want to avoid. Can we do better\nthan this? In the next section, we will show that capturing number and person agree-\nment need not come at the cost of \u201cblowing up\u201d the number of productions.\nUsing Attributes and Constraints\nWe spoke informally of linguistic categories having properties, for example, that a noun\nhas the property of being plural. Let\u2019s make this explicit:\n(9)N[NUM=pl]\nIn (9), we have introduced some new notation which says that the category N has a\n(grammatical) feature called NUM (short for \u201cnumber\u201d) and that the value of this feature\nis pl (short for \u201cplural\u201d). We can add similar annotations to other categories, and use\nthem in lexical entries:\n(10)Det[NUM=sg] -> 'this'\nDet[NUM=pl] -> 'these'\nN[NUM=sg] -> 'dog'\nN[NUM=pl] -> 'dogs'\nV[NUM=sg] -> 'runs'\nV[NUM=pl] -> 'run'\nDoes this help at all? So far, it looks just like a slightly more verbose alternative to what\nwas specified in (8). Things become more interesting when we allow variables over\nfeature values, and use these to state constraints:\n(11)S -> NP[NUM=?n] VP[NUM=?n]\nNP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\nVP[NUM=?n] -> V[NUM=?n]\nWe are using ?n as a variable over values of NUM; it can be instantiated either to sg or\npl, within a given production. We can read the first production as saying that whatever\nvalue NP takes for the feature NUM, VP must take the same value.\nIn order to understand how these feature constraints work, it\u2019s helpful to think about\nhow one would go about building a tree. Lexical productions will admit the following\nlocal trees (trees of depth one):\n9.1  Grammatical Features | 331\n(12) a.\nb.\n(13) a.\nb.\nNow NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n] says that whatever the NUM values of N and\nDet are, they have to be the same. Consequently, this production will permit (12a) and\n(13a) to be combined into an NP, as shown in (14a), and it will also allow (12b) and\n(13b) to be combined, as in (14b). By contrast, (15a) and (15b) are prohibited because\nthe roots of their subtrees differ in their values for the NUM feature; this incompatibility\nof values is indicated informally with a FAIL value at the top node.\n(14) a.\nb.\n332 | Chapter 9: \u2002Building Feature-Based Grammars\n(15) a.\nb.\nProduction VP[NUM=?n] -> V[NUM=?n]  says that the NUM value of the head verb has to be\nthe same as the NUM value of the VP parent. Combined with the production for expanding\nS, we derive the consequence that if the NUM value of the subject head noun is pl, then\nso is the NUM value of the VP\u2019s head verb.\n(16)\nGrammar (10) illustrated lexical productions for determiners like this and these, which\nrequire a singular or plural head noun respectively. However, other determiners in\nEnglish are not choosy about the grammatical number of the noun they combine with.\nOne way of describing this would be to add two lexical entries to the grammar, one\neach for the singular and plural versions of a determiner such as the:\nDet[NUM=sg] -> 'the' | 'some' | 'several'\nDet[NUM=pl] -> 'the' | 'some' | 'several'\nHowever, a more elegant solution is to leave the NUM value underspecified and let it\nagree in number with whatever noun it combines with. Assigning a variable value to\nNUM is one way of achieving this result:\nDet[NUM=?n] -> 'the' | 'some' | 'several'\nBut in fact we can be even more economical, and just omit any specification for NUM in\nsuch productions. We only need to explicitly enter a variable value when this constrains\nanother value elsewhere in the same production.\nThe grammar in Example 9-1  illustrates most of the ideas we have introduced so far in\nthis chapter, plus a couple of new ones.\n9.1  Grammatical Features | 333\nExample 9-1. Example feature-based grammar.\n>>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\n% start S\n# ###################\n# Grammar Productions\n# ###################\n# S expansion productions\nS -> NP[NUM=?n] VP[NUM=?n]\n# NP expansion productions\nNP[NUM=?n] -> N[NUM=?n]\nNP[NUM=?n] -> PropN[NUM=?n]\nNP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\nNP[NUM=pl] -> N[NUM=pl]\n# VP expansion productions\nVP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\nVP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n# ###################\n# Lexical Productions\n# ###################\nDet[NUM=sg] -> 'this' | 'every'\nDet[NUM=pl] -> 'these' | 'all'\nDet -> 'the' | 'some' | 'several'\nPropN[NUM=sg]-> 'Kim' | 'Jody'\nN[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\nN[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\nIV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\nTV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\nIV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\nTV[TENSE=pres, NUM=pl] -> 'see' | 'like'\nIV[TENSE=past] -> 'disappeared' | 'walked'\nTV[TENSE=past] -> 'saw' | 'liked'\nNotice that \na syntactic category can have more than one feature: for example,\nV[TENSE=pres, NUM=pl]. In general, we can add as many features as we like.\nA final detail about Example 9-1  is the statement %start S. This \u201cdirective\u201d tells the\nparser to take S as the start symbol for the grammar.\nIn general, when we are trying to develop even a very small grammar, it is convenient\nto put the productions in a file where they can be edited, tested, and revised. We have\nsaved Example 9-1  as a file named feat0.fcfg in the NLTK data distribution. You can\nmake your own copy of this for further experimentation using nltk.data.load().\nFeature-based grammars are parsed in NLTK using an Earley chart parser (see Sec-\ntion 9.5 for more information about this) and Example 9-2 illustrates how this is carried\nout. After tokenizing the input, we import the load_parser function \n , which takes a\ngrammar filename \nas input and returns a chart parser cp \n . Calling the parser\u2019s\nnbest_parse() method will return a list trees of parse trees; trees will be empty if the\ngrammar fails \nto parse the input and otherwise will contain one or more parse trees,\ndepending on whether the input is syntactically ambiguous.\n334 | Chapter 9: \u2002Building Feature-Based Grammars\nExample 9-2. Trace of feature-based chart parser.\n>>> tokens = 'Kim likes children'.split()\n>>> from nltk import load_parser \n>>> cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)  \n>>> trees = cp.nbest_parse(tokens)\n|.Kim .like.chil.|\n|[----]    .    .| PropN[NUM='sg'] -> 'Kim' *\n|[----]    .    .| NP[NUM='sg'] -> PropN[NUM='sg'] *\n|[---->    .    .| S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n|.    [----]    .| TV[NUM='sg', TENSE='pres'] -> 'likes' *\n|.    [---->    .| VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[]\n                {?n: 'sg', ?t: 'pres'}\n|.    .    [----]| N[NUM='pl'] -> 'children' *\n|.    .    [----]| NP[NUM='pl'] -> N[NUM='pl'] *\n|.    .    [---->| S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n|.    [---------]| VP[NUM='sg', TENSE='pres']\n                -> TV[NUM='sg', TENSE='pres'] NP[] *\n|[==============]| S[] -> NP[NUM='sg'] VP[NUM='sg'] *\nThe details \nof the parsing procedure are not that important for present purposes. How-\never, there is an implementation issue which bears on our earlier discussion of grammar\nsize. One possible approach to parsing productions containing feature constraints is to\ncompile out all admissible values of the features in question so that we end up with a\nlarge, fully specified CFG along the lines of (8). By contrast, the parser process illus-\ntrated in the previous examples works directly with the underspecified productions\ngiven by the grammar. Feature values \u201cflow upwards\u201d from lexical entries, and variable\nvalues are then associated with those values via bindings (i.e., dictionaries) such as\n{?n: 'sg', ?t: 'pres'}. As the parser assembles information about the nodes of the\ntree it is building, these variable bindings are used to instantiate values in these nodes;\nthus the underspecified VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] NP[] becomes\ninstantiated as VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] by\nlooking up the values of ?n and ?t in the bindings.\nFinally, we can inspect the resulting parse trees (in this case, a single one).\n>>> for tree in trees: print tree\n(S[]\n  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n  (VP[NUM='sg', TENSE='pres']\n    (TV[NUM='sg', TENSE='pres'] likes)\n    (NP[NUM='pl'] (N[NUM='pl'] children))))\nTerminology\nSo far, we have only seen feature values like sg and pl. These simple values are usually\ncalled atomic\u2014that is, they can\u2019t be decomposed into subparts. A special case of\natomic values are Boolean values, that is, values that just specify whether a property\nis true or false. For example, we might want to distinguish auxiliary verbs such as can,\n9.1  Grammatical Features | 335\nmay, will, and do  with the Boolean feature AUX. Then the production V[TENSE=pres,\naux=+] -> 'can' means that can receives the value pres for TENSE and + or true for\nAUX. There is a widely adopted convention that abbreviates the representation of Boo-\nlean features f; instead of aux=+ or aux=-, we use +aux and -aux respectively. These are\njust abbreviations, however, and the parser interprets them as though + and - are like\nany other atomic value. (17) shows some representative productions:\n(17)V[TENSE=pres, +aux] -> 'can'\nV[TENSE=pres, +aux] -> 'may'\nV[TENSE=pres, -aux] -> 'walks'\nV[TENSE=pres, -aux] -> 'likes'\nWe have spoken of attaching \u201cfeature annotations\u201d to syntactic categories. A more\nradical approach represents the whole category\u2014that is, the non-terminal symbol plus\nthe annotation\u2014as a bundle of features. For example, N[NUM=sg] contains part-of-\nspeech information which can be represented as POS=N. An alternative notation for this\ncategory, therefore, is [POS=N, NUM=sg].\nIn addition to atomic-valued features, features may take values that are themselves\nfeature structures. For example, we can group together agreement features (e.g., per-\nson, number, and gender) as a distinguished part of a category, serving as the value of\nAGR. In this case, we say that AGR has a complex value. (18) depicts the structure, in a\nformat known as an attribute value matrix (AVM).\n(18)[POS = N           ]\n[                  ]\n[AGR = [PER = 3   ]]\n[      [NUM = pl  ]]\n[      [GND = fem ]]\nIn passing, we should point out that there are alternative approaches for displaying\nAVMs; Figure 9-1 shows an example. Although feature structures rendered in the style\nof (18) are less visually pleasing, we will stick with this format, since it corresponds to\nthe output we will be getting from NLTK.\nFigure 9-1. Rendering a feature structure as an attribute value matrix.\n336 | Chapter 9: \u2002Building Feature-Based Grammars\nOn the topic of representation, we also note that feature structures, like dictionaries,\nassign no particular significance to the order of features. So (18) is equivalent to:\n(19)[AGR = [NUM = pl  ]]\n[      [PER = 3   ]]\n[      [GND = fem ]]\n[                  ]\n[POS = N           ]\nOnce we \nhave the possibility of using features like AGR, we can refactor a grammar like\nExample 9-1  so that agreement features are bundled together. A tiny grammar illus-\ntrating this idea is shown in (20).\n(20)S -> NP[AGR=?n] VP[AGR=?n]\nNP[AGR=?n] -> PropN[AGR=?n]\nVP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\nCop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\nPropN[AGR=[NUM=sg, PER=3]] -> 'Kim'\nAdj -> 'happy'\n9.2  Processing Feature Structures\nIn this section, we will show how feature structures can be constructed and manipulated\nin NLTK. We will also discuss the fundamental operation of unification, which allows\nus to combine the information contained in two different feature structures.\nFeature structures in NLTK are declared with the FeatStruct() constructor. Atomic\nfeature values can be strings or integers.\n>>> fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')\n>>> print fs1\n[ NUM   = 'sg'   ]\n[ TENSE = 'past' ]\nA feature structure is actually just a kind of dictionary, and so we access its values by\nindexing in the usual way. We can use our familiar syntax to assign values to features:\n>>> fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')\n>>> print fs1['GND']\nfem\n>>> fs1['CASE'] = 'acc'\nWe can also define feature structures that have complex values, as discussed earlier.\n>>> fs2 = nltk.FeatStruct(POS='N', AGR=fs1)\n>>> print fs2\n[       [ CASE = 'acc' ] ]\n[ AGR = [ GND  = 'fem' ] ]\n[       [ NUM  = 'pl'  ] ]\n[       [ PER  = 3     ] ]\n[                        ]\n[ POS = 'N'              ]\n9.2  Processing Feature Structures | 337\n>>> print fs2['AGR']\n[ CASE = 'acc' ]\n[ GND  = 'fem' ]\n[ NUM  = 'pl'  ]\n[ PER  = 3     ]\n>>> print fs2['AGR']['PER']\n3\nAn alternative \nmethod of specifying feature structures is to use a bracketed string con-\nsisting of feature-value pairs in the format feature=value, where values may themselves\nbe feature structures:\n>>> print nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\")\n[       [ PER = 3     ] ]\n[ AGR = [ GND = 'fem' ] ]\n[       [ NUM = 'pl'  ] ]\n[                       ]\n[ POS = 'N'             ]\nFeature structures are not inherently tied to linguistic objects; they are general-purpose\nstructures for representing knowledge. For example, we could encode information\nabout a person in a feature structure:\n>>> print nltk.FeatStruct(name='Lee', telno='01 27 86 42 96', age=33)\n[ age   = 33               ]\n[ name  = 'Lee'            ]\n[ telno = '01 27 86 42 96' ]\nIn the next couple of pages, we are going to use examples like this to explore standard\noperations over feature structures. This will briefly divert us from processing natural\nlanguage, but we need to lay the groundwork before we can get back to talking about\ngrammars. Hang on tight!\nIt is often helpful to view feature structures as graphs, more specifically, as directed\nacyclic graphs (DAGs). (21) is equivalent to the preceding AVM.\n(21)\nThe feature names appear as labels on the directed arcs, and feature values appear as\nlabels on the nodes that are pointed to by the arcs.\nJust as before, feature values can be complex:\n338 | Chapter 9: \u2002Building Feature-Based Grammars\n(22)\nWhen we look at such graphs, it is natural to think in terms of paths through the graph.\nA feature path  is \na sequence of arcs that can be followed from the root node. We will\nrepresent paths as tuples of arc labels. Thus, ('ADDRESS', 'STREET') is a feature path\nwhose value in (22) is the node labeled 'rue Pascal'.\nNow let\u2019s consider a situation where Lee has a spouse named Kim, and Kim\u2019s address\nis the same as Lee\u2019s. We might represent this as (23).\n(23)\nHowever, rather than repeating the address information in the feature structure, we\ncan \u201cshare\u201d the same sub-graph between different arcs:\n9.2  Processing Feature Structures | 339\n(24)\nIn other words, the value of the path ('ADDRESS') in (24)  is identical to the value of the\npath ('SPOUSE', 'ADDRESS'). DAGs such as (24) are said to involve structure shar-\ning or reentrancy. When two paths have the same value, they are said to be\nequivalent.\nIn order to indicate reentrancy in our matrix-style representations, we will prefix the\nfirst occurrence of a shared feature structure with an integer in parentheses, such as\n(1). Any later reference to that structure will use the notation ->(1), as shown here.\n>>> print nltk.FeatStruct(\"\"\"[NAME='Lee', ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n...                          SPOUSE=[NAME='Kim', ADDRESS->(1)]]\"\"\")\n[ ADDRESS = (1) [ NUMBER = 74           ] ]\n[               [ STREET = 'rue Pascal' ] ]\n[                                         ]\n[ NAME    = 'Lee'                         ]\n[                                         ]\n[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n[           [ NAME    = 'Kim' ]           ]\nThe bracketed integer is sometimes called a tag or a coindex. The choice of integer is\nnot significant. There can be any number of tags within a single feature structure.\n>>> print nltk.FeatStruct(\"[A='a', B=(1)[C='c'], D->(1), E->(1)]\")\n[ A = 'a'             ]\n[                     ]\n[ B = (1) [ C = 'c' ] ]\n[                     ]\n[ D -> (1)            ]\n[ E -> (1)            ]\n340 | Chapter 9: \u2002Building Feature-Based Grammars\nSubsumption and Unification\nIt is \nstandard to think of feature structures as providing partial information about\nsome object, in the sense that we can order feature structures according to how general\nthey are. For example, (25a) is more general (less specific) than (25b), which in turn is\nmore general than (25c).\n(25) a. [NUMBER = 74]\nb.[NUMBER = 74          ]\n[STREET = 'rue Pascal']\nc.[NUMBER = 74          ]\n[STREET = 'rue Pascal']\n[CITY = 'Paris'       ]\nThis ordering is called subsumption; a more general feature structure subsumes a less\ngeneral one. If FS0 subsumes FS1 (formally, we write FS0 \u2291 FS1), then FS1 must have\nall the paths and path equivalences of FS0, and may have additional paths and equiv-\nalences as well. Thus, (23) subsumes (24) since the latter has additional path equiva-\nlences. It should be obvious that subsumption provides only a partial ordering on fea-\nture structures, since some feature structures are incommensurable. For example,\n(26) neither subsumes nor is subsumed by (25a).\n(26)[TELNO = 01 27 86 42 96]\nSo we have seen that some feature structures are more specific than others. How do we\ngo about specializing a given feature structure? For example, we might decide that\naddresses should consist of not just a street number and a street name, but also a city.\nThat is, we might want to merge graph (27a) with (27b) to yield (27c).\n9.2  Processing Feature Structures | 341\n(27) a.\nb.\nc.\nMerging information from two feature structures is called unification and is supported\nby the unify() method.\n>>> fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n>>> fs2 = nltk.FeatStruct(CITY='Paris')\n>>> print fs1.unify(fs2)\n[ CITY   = 'Paris'      ]\n[ NUMBER = 74           ]\n[ STREET = 'rue Pascal' ]\n342 | Chapter 9: \u2002Building Feature-Based Grammars\nUnification is formally defined as a binary operation: FS0 \u2294 FS1. Unification is sym-\nmetric, so FS 0 \u2294 FS 1 = FS 1 \u2294 FS 0. The same is true in Python:\n>>> print fs2.unify(fs1)\n[ CITY   = 'Paris'      ]\n[ NUMBER = 74           ]\n[ STREET = 'rue Pascal' ]\nIf we unify two feature structures that stand in the subsumption relationship, then the\nresult of unification is the most specific of the two:\n(28) If FS 0 \u2291 FS 1, then FS 0 \u2294 FS 1 = FS 1\nFor example, the result of unifying (25b) with (25c) is (25c).\nUnification between FS0 and FS1 will fail if the two feature structures share a path \u03c0\nwhere the value of \u03c0 in FS0 is a distinct atom from the value of \u03c0 in FS1. This is imple-\nmented by setting the result of unification to be None.\n>>> fs0 = nltk.FeatStruct(A='a')\n>>> fs1 = nltk.FeatStruct(A='b')\n>>> fs2 = fs0.unify(fs1)\n>>> print fs2\nNone\nNow, if we look at how unification interacts with structure-sharing, things become\nreally interesting. First, let\u2019s define (23) in Python:\n>>> fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee,\n...                           ADDRESS=[NUMBER=74,\n...                                    STREET='rue Pascal'],\n...                           SPOUSE= [NAME=Kim,\n...                                    ADDRESS=[NUMBER=74,\n...                                             STREET='rue Pascal']]]\"\"\")\n>>> print fs0\n[ ADDRESS = [ NUMBER = 74           ]               ]\n[           [ STREET = 'rue Pascal' ]               ]\n[                                                   ]\n[ NAME    = 'Lee'                                   ]\n[                                                   ]\n[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n[           [                                     ] ]\n[           [ NAME    = 'Kim'                     ] ]\nWhat happens when we augment Kim\u2019s address with a specification for CITY? Notice\nthat fs1 needs to include the whole path from the root of the feature structure down\nto CITY.\n>>> fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")\n>>> print fs1.unify(fs0)\n[ ADDRESS = [ NUMBER = 74           ]               ]\n[           [ STREET = 'rue Pascal' ]               ]\n[                                                   ]\n \n9.2  Processing Feature Structures | 343\n[ NAME    = 'Lee'                                   ]\n[                                                   ]\n[           [           [ CITY   = 'Paris'      ] ] ]\n[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n[           [                                     ] ]\n[           [ NAME    = 'Kim'                     ] ]\nBy contrast, \nthe result is very different if fs1 is unified with the structure sharing version\nfs2 (also shown earlier as the graph (24)):\n>>> fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n...                           SPOUSE=[NAME=Kim, ADDRESS->(1)]]\"\"\")\n>>> print fs1.unify(fs2)\n[               [ CITY   = 'Paris'      ] ]\n[ ADDRESS = (1) [ NUMBER = 74           ] ]\n[               [ STREET = 'rue Pascal' ] ]\n[                                         ]\n[ NAME    = 'Lee'                         ]\n[                                         ]\n[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n[           [ NAME    = 'Kim' ]           ]\nRather than just updating what was in effect Kim\u2019s \u201ccopy\u201d of Lee\u2019s address, we have\nnow updated both their addresses at the same time. More generally, if a unification\ninvolves specializing the value of some path \u03c0, that unification simultaneously spe-\ncializes the value of any path that is equivalent to \u03c0.\nAs we have already seen, structure sharing can also be stated using variables such as\n?x.\n>>> fs1 = nltk.FeatStruct(\"[ADDRESS1=[NUMBER=74, STREET='rue Pascal']]\")\n>>> fs2 = nltk.FeatStruct(\"[ADDRESS1=?x, ADDRESS2=?x]\")\n>>> print fs2\n[ ADDRESS1 = ?x ]\n[ ADDRESS2 = ?x ]\n>>> print fs2.unify(fs1)\n[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\n[                [ STREET = 'rue Pascal' ] ]\n[                                          ]\n[ ADDRESS2 -> (1)                          ]\n9.3  Extending a Feature-Based Grammar\nIn this section, we return to feature-based grammar and explore a variety of linguistic\nissues, and demonstrate the benefits of incorporating features into the grammar.\nSubcategorization\nIn Chapter 8, we augmented our category labels to represent different kinds of verbs,\nand used the labels IV and TV for intransitive and transitive verbs respectively. This\nallowed us to write productions like the following:\n344 | Chapter 9: \u2002Building Feature-Based Grammars\n(29)VP -> IV\nVP -> TV NP\nAlthough we \nknow that IV and TV are two kinds of V, they are just atomic non-terminal\nsymbols in a CFG and are as distinct from each other as any other pair of symbols. This\nnotation doesn\u2019t let us say anything about verbs in general; e.g., we cannot say \u201cAll\nlexical items of category V can be marked for tense,\u201d since walk, say, is an item of\ncategory IV, not V. So, can we replace category labels such as TV and IV by V along with\na feature that tells us whether the verb combines with a following NP object or whether\nit can occur without any complement?\nA simple approach, originally developed for a grammar framework called Generalized\nPhrase Structure Grammar (GPSG), tries to solve this problem by allowing lexical cat-\negories to bear a SUBCAT feature, which tells us what subcategorization class the item\nbelongs to. In contrast to the integer values for SUBCAT used by GPSG, the example here\nadopts more mnemonic values, namely intrans, trans, and clause:\n(30)VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\nVP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\nVP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\nV[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\nV[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\nV[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\nV[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\nV[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\nV[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\nV[SUBCAT=intrans, TENSE=past] -> 'disappeared' | 'walked'\nV[SUBCAT=trans, TENSE=past] -> 'saw' | 'liked'\nV[SUBCAT=clause, TENSE=past] -> 'said' | 'claimed'\nWhen we see a lexical category like V[SUBCAT=trans], we can interpret the SUBCAT spec-\nification as a pointer to a production in which V[SUBCAT=trans] is introduced as the\nhead child in a VP production. By convention, there is a correspondence between the\nvalues of SUBCAT and the productions that introduce lexical heads. On this approach,\nSUBCAT can appear only on lexical categories; it makes no sense, for example, to specify\na SUBCAT value on VP. As required, walk and like both belong to the category V. Never-\ntheless, walk will occur only in VPs expanded by a production with the feature\nSUBCAT=intrans on the righthand side, as opposed to like, which requires a\nSUBCAT=trans.\nIn our third class of verbs in (30), we have specified a category SBar. This is a label for\nsubordinate clauses, such as the complement of claim in the example You claim that\nyou like children. We require two further productions to analyze such sentences:\n(31)SBar -> Comp S\nComp -> 'that'\n9.3  Extending a Feature-Based Grammar | 345\nThe resulting structure is the following.\n(32)\nAn alternative treatment of subcategorization, due originally to a framework known as\ncategorial grammar, \nis represented in feature-based frameworks such as PATR and\nHead-driven Phrase Structure Grammar. Rather than using SUBCAT values as a way of\nindexing productions, the SUBCAT value directly encodes the valency of a head (the list\nof arguments that it can combine with). For example, a verb like put that takes NP and\nPP complements (put the book on the table) might be represented as (33):\n(33)V[SUBCAT=<NP, NP, PP>]\nThis says that the verb can combine with three arguments. The leftmost element in the\nlist is the subject NP, while everything else\u2014an NP followed by a PP in this case\u2014com-\nprises the subcategorized-for complements. When a verb like put is combined with\nappropriate complements, the requirements which are specified in the SUBCAT are dis-\ncharged, and only a subject NP is needed. This category, which corresponds to what is\ntraditionally thought of as VP, might be represented as follows:\n(34)V[SUBCAT=<NP>]\nFinally, a sentence is a kind of verbal category that has no requirements for further\narguments, and hence has a SUBCAT whose value is the empty list. The tree (35) shows\nhow these category assignments combine in a parse of Kim put the book on the table.\n(35)\n346 | Chapter 9: \u2002Building Feature-Based Grammars\nHeads Revisited\nWe noted \nin the previous section that by factoring subcategorization information out\nof the main category label, we could express more generalizations about properties of\nverbs. Another property of this kind is the following: expressions of category V are heads\nof phrases of category VP. Similarly, Ns are heads of NPs, As (i.e., adjectives) are heads of\nAPs, and Ps (i.e., prepositions) are heads of PPs. Not all phrases have heads\u2014for exam-\nple, it is standard to say that coordinate phrases (e.g., the book and the bell ) lack heads.\nNevertheless, we would like our grammar formalism to express the parent/head-child\nrelation where it holds. At present, V and VP are just atomic symbols, and we need to\nfind a way to relate them using features (as we did earlier to relate IV and TV).\nX-bar syntax addresses this issue by abstracting out the notion of phrasal level . It is\nusual to recognize three such levels. If N represents the lexical level, then N' represents\nthe next level up, corresponding to the more traditional category Nom, and N'' represents\nthe phrasal level, corresponding to the category NP. (36a) illustrates a representative\nstructure, while (36b) is the more conventional counterpart.\n(36) a.\nb.\nThe head of the structure (36a) is N, and N' and N'' are called (phrasal) projections  of\nN. N'' is the maximal projection , and N is sometimes called the zero projection . One\nof the \ncentral claims of X-bar syntax is that all constituents share a structural similarity.\nUsing X as a variable over N, V, A, and P, we say that directly subcategorized comple-\nments of a lexical head X are always placed as siblings of the head, whereas adjuncts are\nplaced as siblings of the intermediate category, X'. Thus, the configuration of the two\nP'' adjuncts in (37) contrasts with that of the complement P'' in (36a).\n9.3  Extending a Feature-Based Grammar | 347\n(37)\nThe productions in (38) illustrate how bar levels can be encoded using feature struc-\ntures. The nested structure in (37) is achieved by two applications of the recursive rule\nexpanding N[BAR=1].\n(38)S -> N[BAR=2] V[BAR=2]\nN[BAR=2] -> Det N[BAR=1]\nN[BAR=1] -> N[BAR=1] P[BAR=2]\nN[BAR=1] -> N[BAR=0] P[BAR=2]\nAuxiliary Verbs and Inversion\nInverted clauses\u2014where the order of subject and verb is switched\u2014occur in English\ninterrogatives and also after \u201cnegative\u201d adverbs:\n(39) a. Do you like children?\nb. Can Jody walk?\n(40) a. Rarely do you see Kim.\nb. Never have I seen this dog.\nHowever, we cannot place just any verb in pre-subject position:\n(41) a. *Like you children?\nb. *Walks Jody?\n(42) a. *Rarely see you Kim.\nb. *Never saw I this dog.\nVerbs that can be positioned initially in inverted clauses belong to the class known as\nauxiliaries, and as well as do, can, and have include be, will, and shall. One way of\ncapturing such structures is with the following production:\n(43)S[+INV] -> V[+AUX] NP VP\n348 | Chapter 9: \u2002Building Feature-Based Grammars\nThat is, a clause marked as [+inv] consists of an auxiliary verb followed by a VP. (In a\nmore detailed grammar, we would need to place some constraints on the form of the\nVP, depending on the choice of auxiliary.) (44) illustrates the structure of an inverted\nclause:\n(44)\nUnbounded Dependency Constructions\nConsider the following contrasts:\n(45) a.\nYou like Jody.\nb. *You like.\n(46) a. You put the card into the slot.\nb. *You put into the slot.\nc. *You put the card.\nd. *You put.\nThe verb like requires an NP complement, while put requires both a following NP and\nPP. (45) and (46) show that these complements are obligatory: omitting them leads to\nungrammaticality. Yet there are contexts in which obligatory complements can be\nomitted, as (47) and (48) illustrate.\n(47) a. Kim knows who you like.\nb. This music, you really like.\n(48) a. Which card do you put into the slot?\nb. Which slot do you put the card into?\nThat is, an obligatory complement can be omitted if there is an appropriate filler in\nthe sentence, such as the question word who in (47a), the preposed topic this music  in\n(47b), or the wh phrases which card/slot  in (48). It is common to say that sentences like\nthose in (47) and (48) contain gaps where the obligatory complements have been\nomitted, and these gaps are sometimes made explicit using an underscore:\n(49) a. Which card do you put __ into the slot?\nb. Which slot do you put the card into __?\n9.3  Extending a Feature-Based Grammar | 349\nSo, a gap can occur if it is licensed by a filler. Conversely, fillers can occur only if there\nis an appropriate gap elsewhere in the sentence, as shown by the following examples:\n(50) a. *Kim knows who you like Jody.\nb. *This music, you really like hip-hop.\n(51) a. *Which card do you put this into the slot?\nb. *Which slot do you put the card into this one?\nThe mutual co-occurrence between filler and gap is sometimes termed a \u201cdependency.\u201d\nOne issue of considerable importance in theoretical linguistics has been the nature of\nthe material that can intervene between a filler and the gap that it licenses; in particular,\ncan we simply list a finite set of sequences that separate the two? The answer is no:\nthere is no upper bound on the distance between filler and gap. This fact can be easily\nillustrated with constructions involving sentential complements, as shown in (52).\n(52) a. Who do you like __?\nb. Who do you claim that you like __?\nc. Who do you claim that Jody says that you like __?\nSince we can have indefinitely deep recursion of sentential complements, the gap can\nbe embedded indefinitely far inside the whole sentence. This constellation of properties\nleads to the notion of an unbounded dependency construction , that is, a filler-gap\ndependency where there is no upper bound on the distance between filler and gap.\nA variety of mechanisms have been suggested for handling unbounded dependencies\nin formal grammars; here we illustrate the approach due to Generalized Phrase Struc-\nture Grammar that involves slash categories . A slash category has the form Y/XP; we\ninterpret this as a phrase of category Y that is missing a subconstituent of category XP.\nFor example, S/NP is an S that is missing an NP. The use of slash categories is illustrated\nin (53).\n(53)\nThe top part of the tree introduces the filler who (treated as an expression of category\nNP[+wh]) together \nwith a corresponding gap-containing constituent S/NP. The gap\n350 | Chapter 9: \u2002Building Feature-Based Grammars\ninformation is then \u201cpercolated\u201d down the tree via the VP/NP category, until it reaches\nthe category NP/NP\n. At this point, the dependency is discharged by realizing the gap\ninformation as the empty string, immediately dominated by NP/NP.\nDo we need to think of slash categories as a completely new kind of object? Fortunately,\nwe can accommodate them within our existing feature-based framework, by treating\nslash as a feature and the category to its right as a value; that is, S/NP is reducible to\nS[SLASH=NP]. In practice, this is also how the parser interprets slash categories.\nThe grammar shown in Example 9-3  illustrates the main principles of slash categories,\nand also includes productions for inverted clauses. To simplify presentation, we have\nomitted any specification of tense on the verbs.\nExample 9-3. Grammar with productions for inverted clauses and long-distance dependencies,\nmaking use of slash categories.\n>>> nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')\n% start S\n# ###################\n# Grammar Productions\n# ###################\nS[-INV] -> NP VP\nS[-INV]/?x -> NP VP/?x\nS[-INV] -> NP S/NP\nS[-INV] -> Adv[+NEG] S[+INV]\nS[+INV] -> V[+AUX] NP VP\nS[+INV]/?x -> V[+AUX] NP VP/?x\nSBar -> Comp S[-INV]\nSBar/?x -> Comp S[-INV]/?x\nVP -> V[SUBCAT=intrans, -AUX]\nVP -> V[SUBCAT=trans, -AUX] NP\nVP/?x -> V[SUBCAT=trans, -AUX] NP/?x\nVP -> V[SUBCAT=clause, -AUX] SBar\nVP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\nVP -> V[+AUX] VP\nVP/?x -> V[+AUX] VP/?x\n# ###################\n# Lexical Productions\n# ###################\nV[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\nV[SUBCAT=trans, -AUX] -> 'see' | 'like'\nV[SUBCAT=clause, -AUX] -> 'say' | 'claim'\nV[+AUX] -> 'do' | 'can'\nNP[-WH] -> 'you' | 'cats'\nNP[+WH] -> 'who'\nAdv[+NEG] -> 'rarely' | 'never'\nNP/NP ->\nComp -> 'that'\nThe grammar in Example 9-3 contains one \u201cgap-introduction\u201d production, namely S[-\nINV] -> NP S/NP. In order to percolate the slash feature correctly, we need to add slashes\nwith variable values to both sides of the arrow in productions that expand S, VP, and\nNP. For example, VP/?x -> V SBar/?x is the slashed version of VP -> V SBar and says\n9.3  Extending a Feature-Based Grammar | 351\nthat a slash value can be specified on the VP parent of a constituent if the same value is\nalso specified on the SBar child. Finally, NP/NP ->  allows the slash information on NP to\nbe discharged as the empty string. Using the grammar in Example 9-3 , we can parse\nthe sequence who do you claim that you like:\n>>> tokens = 'who do you claim that you like'.split()\n>>> from nltk import load_parser\n>>> cp = load_parser('grammars/book_grammars/feat1.fcfg')\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n(S[-INV]\n  (NP[+WH] who)\n  (S[+INV]/NP[]\n    (V[+AUX] do)\n    (NP[-WH] you)\n    (VP[]/NP[]\n      (V[-AUX, SUBCAT='clause'] claim)\n      (SBar[]/NP[]\n        (Comp[] that)\n        (S[-INV]/NP[]\n          (NP[-WH] you)\n          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\nA more readable version of this tree is shown in (54).\n(54)\nThe grammar in Example 9-3 will also allow us to parse sentences without gaps:\n>>> tokens = 'you claim that you like cats'.split()\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n(S[-INV]\n  (NP[-WH] you)\n  (VP[]\n    (V[-AUX, SUBCAT='clause'] claim)\n    (SBar[]\n      (Comp[] that)\n      (S[-INV]\n        (NP[-WH] you)\n        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n352 | Chapter 9: \u2002Building Feature-Based Grammars\nIn addition, it admits inverted sentences that do not involve wh constructions:\n>>> tokens = 'rarely do you sing'.split()\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n(S[-INV]\n  (Adv[+NEG] rarely)\n  (S[+INV]\n    (V[+AUX] do)\n    (NP[-WH] you)\n    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\nCase and Gender in German\nCompared with \nEnglish, German has a relatively rich morphology for agreement. For\nexample, the definite article in German varies with case, gender, and number, as shown\nin Table 9-2.\nTable 9-2. Morphological paradigm for the German definite article\nCase Masculine Feminine Neutral Plural\nNominative der die das die\nGenitive des der des der\nDative dem der dem den\nAccusative den die das die\nSubjects in German take the nominative case, and most verbs govern their objects in\nthe accusative \ncase. However, there are exceptions, such as helfen, that govern the\ndative case:\n(55) a. Die Katze sieht den Hund\nthe.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.ACC.MASC.SG dog.3.MASC.SG\n\u2018the cat sees the dog\u2019\nb. *Die Katze sieht dem Hund\nthe.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.DAT.MASC.SG dog.3.MASC.SG\nc. Die Katze hilft dem Hund\nthe.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.DAT.MASC.SG dog.3.MASC.SG\n\u2018the cat helps the dog\u2019\nd. *Die Katze hilft den Hund\nthe.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.ACC.MASC.SG dog.3.MASC.SG\nThe grammar in Example 9-4  illustrates the interaction of agreement (comprising per-\nson, number, and gender) with case.\n9.3  Extending a Feature-Based Grammar | 353\nExample 9-4. Example feature-based grammar.\n>>> nltk.data.show_cfg('grammars/book_grammars/german.fcfg')\n% start S\n # Grammar Productions\n S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n VP[AGR=?a] -> IV[AGR=?a]\n VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n # Lexical Productions\n # Singular determiners\n # masc\n Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n # fem\n Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n # Plural determiners\n Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n # Nouns\n N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n # Pronouns\n PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n # Verbs\n IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n354 | Chapter 9: \u2002Building Feature-Based Grammars\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\nAs you \ncan see, the feature objcase is used to specify the case that a verb governs on its\nobject. The next example illustrates the parse tree for a sentence containing a verb that\ngoverns the dative case:\n>>> tokens = 'ich folge den Katzen'.split()\n>>> cp = load_parser('grammars/book_grammars/german.fcfg')\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n(S[]\n  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n  (VP[AGR=[NUM='sg', PER=1]]\n    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\nIn developing grammars, excluding ungrammatical word sequences is often as chal-\nlenging as parsing grammatical ones. In order to get an idea where and why a sequence\nfails to parse, setting the trace parameter of the load_parser() method can be crucial.\nConsider the following parse failure:\n>>> tokens = 'ich folge den Katze'.split()\n>>> cp = load_parser('grammars/book_grammars/german.fcfg', trace=2)\n>>> for tree in cp.nbest_parse(tokens):\n...     print tree\n|.ich.fol.den.Kat.|\n|[---]   .   .   .| PRO[AGR=[NUM='sg', PER=1], CASE='nom'] -> 'ich' *\n|[---]   .   .   .| NP[AGR=[NUM='sg', PER=1], CASE='nom']\n                  -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n|[--->   .   .   .| S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a]\n                        {?a: [NUM='sg', PER=1]}\n|.   [---]   .   .| TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] -> 'folge' *\n|.   [--->   .   .| VP[AGR=?a] -> TV[AGR=?a, OBJCASE=?c]\n                        * NP[CASE=?c] {?a: [NUM='sg', PER=1], ?c: 'dat'}\n|.   .   [---]   .| Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] -> 'den' *\n|.   .   [---]   .| Det[AGR=[NUM='pl', PER=3], CASE='dat'] -> 'den' *\n|.   .   [--->   .| NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c]\n                  * N[AGR=?a, CASE=?c] {?a: [NUM='pl', PER=3], ?c: 'dat'}\n|.   .   [--->   .| NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c]\n                 {?a: [GND='masc', NUM='sg', PER=3], ?c: 'acc'}\n|.   .   .   [---]| N[AGR=[GND='fem', NUM='sg', PER=3]] -> 'Katze' *\n9.3  Extending a Feature-Based Grammar | 355\nThe last two Scanner lines in the trace show that den is recognized as admitting two\npossible categories: Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] and\nDet[AGR=[NUM='pl', PER=3], CASE='dat']. We know from the grammar in Exam-\nple 9-4 that Katze has category N[AGR=[GND=fem, NUM=sg, PER=3]]. Thus there is no\nbinding for the variable ?a in production:\nNP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=? a] N[CASE=?c, AGR=?a] \nthat will satisfy these constraints, since the AGR value of Katze will not unify with either\nof the AGR values of den, that is, with either [GND='masc', NUM='sg', PER=3] or\n[NUM='pl', PER=3].\n9.4  Summary\n\u2022 The traditional categories of context-free grammar are atomic symbols. An impor-\ntant motivation for feature structures is to capture fine-grained distinctions that\nwould otherwise require a massive multiplication of atomic categories.\n\u2022 By using variables over feature values, we can express constraints in grammar pro-\nductions that allow the realization of different feature specifications to be inter-\ndependent.\n\u2022 Typically we specify fixed values of features at the lexical level and constrain the\nvalues of features in phrases to unify with the corresponding values in their\nchildren.\n\u2022 Feature values are either atomic or complex. A particular subcase of atomic value\nis the Boolean value, represented by convention as [+/- feat].\n\u2022 Two features can share a value (either atomic or complex). Structures with shared\nvalues are said to be re-entrant. Shared values are represented by numerical indexes\n(or tags) in AVMs.\n\u2022 A path in a feature structure is a tuple of features corresponding to the labels on a\nsequence of arcs from the root of the graph representation.\n\u2022 Two paths are equivalent if they share a value.\n\u2022 Feature structures are partially ordered by subsumption. FS0 subsumes FS1 when\nFS0 is more general (less informative) than FS 1.\n\u2022 The unification of two structures FS0 and FS1, if successful, is the feature structure\nFS2 that contains the combined information of both FS 0 and FS 1.\n\u2022 If unification specializes a path \u03c0 in FS, then it also specializes every path \u03c0' equiv-\nalent to \u03c0.\n\u2022 We can use feature structures to build succinct analyses of a wide variety of lin-\nguistic phenomena, including verb subcategorization, inversion constructions,\nunbounded dependency constructions, and case government.\n356 | Chapter 9: \u2002Building Feature-Based Grammars\n9.5  Further Reading\nPlease consult http://www.nltk.org/\n for further materials on this chapter, including\nHOWTOs feature structures, feature grammars, Earley parsing, and grammar test\nsuites.\nFor an excellent introduction to the phenomenon of agreement, see (Corbett, 2006).\nThe earliest use of features in theoretical linguistics was designed to capture phono-\nlogical properties of phonemes. For example, a sound like /b/ might be decomposed\ninto the structure [+labial, +voice] . An important motivation was to capture gener-\nalizations across classes of segments, for example, that /n/ gets realized as /m/ preceding\nany +labial consonant. Within Chomskyan grammar, it was standard to use atomic\nfeatures for phenomena such as agreement, and also to capture generalizations across\nsyntactic categories, by analogy with phonology. A radical expansion of the use of\nfeatures in theoretical syntax was advocated by Generalized Phrase Structure Grammar\n(GPSG; [Gazdar et al., 1985]), particularly in the use of features with complex values.\nComing more from the perspective of computational linguistics, (Kay, 1985) proposed\nthat functional aspects of language could be captured by unification of attribute-value\nstructures, and a similar approach was elaborated by (Grosz & Stickel, 1983) within\nthe PATR-II formalism. Early work in Lexical-Functional grammar (LFG; [Kaplan &\nBresnan, 1982]) introduced the notion of an f-structure that was primarily intended\nto represent the grammatical relations and predicate-argument structure associated\nwith a constituent structure parse. (Shieber, 1986) provides an excellent introduction\nto this phase of research into feature-based grammars.\nOne conceptual difficulty with algebraic approaches to feature structures arose when\nresearchers attempted to model negation. An alternative perspective, pioneered by\n(Kasper & Rounds, 1986) and (Johnson, 1988), argues that grammars involve descrip-\ntions of feature structures rather than the structures themselves. These descriptions are\ncombined using logical operations such as conjunction, and negation is just the usual\nlogical operation over feature descriptions. This description-oriented perspective was\nintegral to LFG from the outset (Kaplan, 1989), and was also adopted by later versions\nof Head-Driven Phrase Structure Grammar (HPSG; [Sag & Wasow, 1999]). A com-\nprehensive bibliography of HPSG literature can be found at http://www.cl.uni-bremen\n.de/HPSG-Bib/.\nFeature structures, as presented in this chapter, are unable to capture important con-\nstraints on linguistic information. For example, there is no way of saying that the only\npermissible values for NUM are sg and pl, while a specification such as [NUM=masc] is\nanomalous. Similarly, we cannot say that the complex value of AGR must contain spec-\nifications for the features PER, NUM, and GND, but cannot contain a specification such as\n[SUBCAT=trans]. Typed feature structures  were developed to remedy this deficiency. \nA good early review of work on typed feature structures is (Emele & Zajac, 1990). A\nmore comprehensive examination of the formal foundations can be found in\n9.5  Further Reading | 357\n(Carpenter, 1992), while (Copestake, 2002) focuses on implementing an HPSG-orien-\nted approach to typed feature structures.\nThere is a copious literature on the analysis of German within feature-based grammar\nframeworks. (Nerbonne, Netter & Pollard, 1994) is a good starting point for the HPSG\nliterature on this topic, while (M\u00fcller, 2002) gives a very extensive and detailed analysis\nof German syntax in HPSG.\nChapter 15 of (Jurafsky & Martin, 2008) discusses feature structures, the unification\nalgorithm, and the integration of unification into parsing algorithms.\n9.6  Exercises\n1.\u25cb What constraints are required to correctly parse word sequences like I am hap-\npy and she is happy  but not * you is happy  or *they am happy ? Implement two sol-\nutions for the present tense paradigm of the verb be in English, first taking Gram-\nmar (8) as your starting point, and then taking Grammar (20) as the starting point.\n2.\u25cb Develop a variant of grammar in Example 9-1  that uses a feature COUNT to make\nthe distinctions shown here:\n(56) a. The boy sings.\nb. *Boy sings.\n(57) a. The boys sing.\nb. Boys sing.\n(58) a. The water is precious.\nb. Water is precious.\n3.\u25cb Write a function subsumes() that holds of two feature structures fs1 and fs2 just\nin case fs1 subsumes fs2.\n4.\u25cb Modify the grammar illustrated in (30) to incorporate a BAR feature for dealing\nwith phrasal projections.\n5.\u25cb Modify the German grammar in Example 9-4  to incorporate the treatment of\nsubcategorization presented in Section 9.3.\n6.\u25d1 Develop a feature-based grammar that will correctly describe the following\nSpanish noun phrases:\n(59) un cuadro hermos-o\nINDEF.SG.MASC picture beautiful-SG.MASC\n\u2018a beautiful picture\u2019\n(60) un-os cuadro-s hermos-os\nINDEF-PL.MASC picture-PL beautiful-PL.MASC\n\u2018beautiful pictures\u2019\n358 | Chapter 9: \u2002Building Feature-Based Grammars\n(61) un-a cortina hermos-a\nINDEF-SG.FEM curtain beautiful-SG.FEM\n\u2018a beautiful curtain\u2019\n(62) un-as cortina-s hermos-as\nINDEF-PL.FEM curtain beautiful-PL.FEM\n\u2018beautiful curtains\u2019\n7.\u25d1 Develop a wrapper for the earley_parser so that a trace is only printed if the\ninput sequence fails to parse.\n8.\u25d1 Consider the feature structures shown in Example 9-5.\nExample 9-5. Exploring feature structures.\nfs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\nfs2 = nltk.FeatStruct(\"[B = [D = d]]\")\nfs3 = nltk.FeatStruct(\"[B = [C = d]]\")\nfs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\nfs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\nfs6 = nltk.FeatStruct(\"[A = [D = d]]\")\nfs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\nfs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\nfs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\nfs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\nWork out on paper what the result is of the following unifications. (Hint: you might\nfind it useful to draw the graph structures.)\na.fs1 and fs2\nb.fs1 and fs3\nc.fs4 and fs5\nd.fs5 and fs6\ne.fs5 and fs7\nf.fs8 and fs9\ng.fs8 and fs10\nCheck your answers using NLTK.\n9.\u25d1 List two feature structures that subsume [A=?x, B=?x].\n10.\u25d1 Ignoring structure sharing, give an informal algorithm for unifying two feature\nstructures.\n11.\u25d1 Extend the German grammar in Example 9-4  so that it can handle so-called verb-\nsecond structures like the following:\n(63) Heute sieht der Hund die Katze.\n12.\u25d1 Seemingly synonymous verbs have slightly different syntactic properties (Levin,\n1993). Consider the following patterns of grammaticality for the verbs loaded,\nfilled, and dumped. Can you write grammar productions to handle such data?\n9.6  Exercises | 359\n(64) a. The farmer loaded the cart with sand\nb. The farmer \nloaded sand into the cart\nc. The farmer filled the cart with sand\nd. *The farmer filled sand into the cart\ne. *The farmer dumped the cart with sand\nf. The farmer dumped sand into the cart\n13.\u25cf Morphological paradigms are rarely completely regular, in the sense of every cell\nin the matrix having a different realization. For example, the present tense conju-\ngation of the lexeme walk has only two distinct forms: walks for the third-person\nsingular, and walk for all other combinations of person and number. A successful\nanalysis should not require redundantly specifying that five out of the six possible\nmorphological combinations have the same realization. Propose and implement a\nmethod for dealing with this.\n14.\u25cf So-called head features are shared between the parent node and head child. For\nexample, TENSE is a head feature that is shared between a VP and its head V child.\nSee (Gazdar et al., 1985) for more details. Most of the features we have looked at\nare head features\u2014exceptions are SUBCAT and SLASH. Since the sharing of head fea-\ntures is predictable, it should not need to be stated explicitly in the grammar\nproductions. Develop an approach that automatically accounts for this regular\nbehavior of head features.\n15.\u25cf Extend NLTK\u2019s treatment of feature structures to allow unification into list-\nvalued features, and use this to implement an HPSG-style analysis of subcategori-\nzation, whereby the SUBCAT of a head category is the concatenation of its\ncomplements\u2019 categories with the SUBCAT value of its immediate parent.\n16.\u25cf Extend NLTK\u2019s treatment of feature structures to allow productions with un-\nderspecified categories, such as S[-INV] -> ?x S/?x.\n17.\u25cf Extend NLTK\u2019s treatment of feature structures to allow typed feature structures.\n18.\u25cf Pick some grammatical constructions described in (Huddleston & Pullum,\n2002), and develop a feature-based grammar to account for them.\n360 | Chapter 9: \u2002Building Feature-Based Grammars\nCHAPTER 10\nAnalyzing the Meaning of Sentences\nWe have seen how useful it is to harness the power of a computer to process text on a\nlarge scale. \nHowever, now that we have the machinery of parsers and feature-based\ngrammars, can we do anything similarly useful by analyzing the meaning of sentences?\nThe goal of this chapter is to answer the following questions:\n1. How can we represent natural language meaning so that a computer can process\nthese representations?\n2. How can we associate meaning representations with an unlimited set of sentences?\n3. How can we use programs that connect the meaning representations of sentences\nto stores of knowledge?\nAlong the way we will learn some formal techniques in the field of logical semantics,\nand see how these can be used for interrogating databases that store facts about the\nworld.\n10.1  Natural Language Understanding\nQuerying a Database\nSuppose we have a program that lets us type in a natural language question and gives\nus back the right answer:\n(1) a. Which country is Athens in?\nb. Greece.\nHow hard is it to write such a program? And can we just use the same techniques that\nwe\u2019ve encountered so far in this book, or does it involve something new? In this section,\nwe will show that solving the task in a restricted domain is pretty straightforward. But\nwe will also see that to address the problem in a more general way, we have to open up\na whole new box of ideas and techniques, involving the representation of meaning.\n361\nSo let\u2019s start off by assuming that we have data about cities and countries in a structured\nform. To \nbe concrete, we will use a database table whose first few rows are shown in\nTable 10-1.\nThe data illustrated in Table 10-1  is drawn from the Chat-80 system\n(Warren & Pereira, 1982). Population figures are given in thousands,\nbut note that the data used in these examples dates back at least to the\n1980s, and was already somewhat out of date at the point when (Warren\n& Pereira, 1982) was published.\nTable 10-1. city_table: A table of cities, countries, and populations\nCity Country Population\nathens greece 1368\nbangkok thailand 1178\nbarcelona spain 1280\nberlin east_germany 3481\nbirmingham united_kingdom 1112\nThe obvious way to retrieve answers from this tabular data involves writing queries in\na database query language such as SQL.\nSQL (Structured Query Language) is a language designed for retrieving\nand managing \ndata in relational databases. If you want to find out more\nabout SQL, http://www.w3schools.com/sql/ is a convenient online\nreference.\nFor example, executing the query (2) will pull out the value 'greece':\n(2)SELECT Country FROM city_table WHERE City = 'athens'\nThis specifies a result set consisting of all values for the column Country in data rows\nwhere the value of the City column is 'athens'.\nHow can we get the same effect using English as our input to the query system? The\nfeature-based grammar formalism described in Chapter 9  makes it easy to translate\nfrom English to SQL. The grammar sql0.fcfg illustrates how to assemble a meaning\nrepresentation for a sentence in tandem with parsing the sentence. Each phrase struc-\nture rule is supplemented with a recipe for constructing a value for the feature SEM. You\ncan see that these recipes are extremely simple; in each case, we use the string concat-\nenation operation + to splice the values for the child constituents to make a value for\nthe parent constituent.\n362 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n>>> nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')\n% start S\nS[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]\nVP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]\nVP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]\nNP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]\nPP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]\nAP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]\nNP[SEM='Country=\"greece\"'] -> 'Greece'\nNP[SEM='Country=\"china\"'] -> 'China'\nDet[SEM='SELECT'] -> 'Which' | 'What'\nN[SEM='City FROM city_table'] -> 'cities'\nIV[SEM=''] -> 'are'\nA[SEM=''] -> 'located'\nP[SEM=''] -> 'in'\nThis allows us to parse a query into SQL:\n>>> from nltk import load_parser\n>>> cp = load_parser('grammars/book_grammars/sql0.fcfg')\n>>> query = 'What cities are located in China'\n>>> trees = cp.nbest_parse(query.split())\n>>> answer = trees[0].node['sem']\n>>> q = ' '.join(answer)\n>>> print q\nSELECT City FROM city_table WHERE Country=\"china\"\nYour Turn:  Run the parser with maximum tracing on, i.e., cp =\nload_parser('grammars/book_grammars/sql0.fcfg', trace=3), and ex-\namine how the values of SEM are built up as complete edges are added\nto the chart.\nFinally, we execute the query over the database city.db and retrieve some results:\n>>> from nltk.sem import chat80\n>>> rows = chat80.sql_query('corpora/city_database/city.db', q)\n>>> for r in rows: print r[0], \ncanton chungking dairen harbin kowloon mukden peking shanghai sian tientsin\nSince each \nrow r is a one-element tuple, we print out the member of the tuple rather\nthan the tuple itself \n .\nTo summarize, \nwe have defined a task where the computer returns useful data in re-\nsponse to a natural language query, and we implemented this by translating a small\nsubset of English into SQL. We can say that our NLTK code already \u201cunderstands\u201d\nSQL, given that Python is able to execute SQL queries against a database, and by ex-\ntension it also \u201cunderstands\u201d queries such as What cities are located in China . This\nparallels being able to translate from Dutch into English as an example of natural lan-\nguage understanding. Suppose that you are a native speaker of English, and have started\nto learn Dutch. Your teacher asks if you understand what (3) means:\n(3) Margrietje houdt van Brunoke.\n10.1  Natural Language Understanding | 363\nIf you know the meanings of the individual words in (3), and know how these meanings\nare combined to make up the meaning of the whole sentence, you might say that (3)\nmeans the same as Margrietje loves Brunoke.\nAn observer\u2014let\u2019s call her Olga\u2014might well take this as evidence that you do grasp\nthe meaning of (3). But this would depend on Olga herself understanding English. If\nshe doesn\u2019t, then your translation from Dutch to English is not going to convince her\nof your ability to understand Dutch. We will return to this issue shortly.\nThe grammar sql0.fcfg, together with the NLTK Earley parser, is instrumental in car-\nrying out the translation from English to SQL. How adequate is this grammar? You saw\nthat the SQL translation for the whole sentence was built up from the translations of\nthe components. However, there does not seem to be a lot of justification for these\ncomponent meaning representations. For example, if we look at the analysis of the\nnoun phrase Which cities, the determiner and noun correspond respectively to the SQL\nfragments SELECT and City FROM city_table. But neither of these has a well-defined\nmeaning in isolation from the other.\nThere is another criticism we can level at the grammar: we have \u201chard-wired\u201d an em-\nbarrassing amount of detail about the database into it. We need to know the name of\nthe relevant table (e.g., city_table) and the names of the fields. But our database could\nhave contained exactly the same rows of data yet used a different table name and dif-\nferent field names, in which case the SQL queries would not be executable. Equally,\nwe could have stored our data in a different format, such as XML, in which case re-\ntrieving the same results would require us to translate our English queries into an XML\nquery language rather than SQL. These considerations suggest that we should be trans-\nlating English into something that is more abstract and generic than SQL.\nIn order to sharpen the point, let\u2019s consider another English query and its translation:\n(4) a. What cities are in China and have populations above 1,000,000?\nb.SELECT City FROM city_table WHERE Country = 'china' AND Population >\n1000\nYour Turn:  Extend the grammar sql0.fcfg so that it will translate (4a)\ninto (4b), and check the values returned by the query. Remember that\nfigures in the Chat-80 database are given in thousands, hence 1000 in\n(4b) represents one million inhabitants.\nYou will probably find it easiest to first extend the grammar to handle\nqueries like What cities have populations above 1,000,000  before tack-\nling conjunction. After you have had a go at this task, you can compare\nyour solution to grammars/book_grammars/sql1.fcfg in the NLTK data\ndistribution.\n364 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nObserve that the and conjunction in (4a) is translated into an AND in the SQL counter-\npart, (4b). The \nlatter tells us to select results from rows where two conditions are true\ntogether: the value of the Country column is 'china' and the value of the Population\ncolumn is greater than 1000. This interpretation for and involves a new idea: it talks\nabout what is true in some particular situation, and tells us that Cond1 AND Cond2 is true\nin situation s if and only if condition Cond1 is true in s and condition Cond2 is true in s.\nAlthough this doesn\u2019t account for the full range of meanings of and in English, it has\nthe nice property that it is independent of any query language. In fact, we have given\nit the standard interpretation from classical logic. In the following sections, we will\nexplore an approach in which sentences of natural language are translated into logic\ninstead of an executable query language such as SQL. One advantage of logical for-\nmalisms is that they are more abstract and therefore more generic. If we wanted to,\nonce we had our translation into logic, we could then translate it into various other\nspecial-purpose languages. In fact, most serious attempts to query databases via natural\nlanguage have used this methodology.\nNatural Language, Semantics, and Logic\nWe started out trying to capture the meaning of (1a) by translating it into a query in\nanother language, SQL, which the computer could interpret and execute. But this still\nbegged the question whether the translation was correct. Stepping back from database\nquery, we noted that the meaning of and seems to depend on being able to specify when\nstatements are true or not in a particular situation. Instead of translating a sentence S\nfrom one language to another, we try to say what S is about by relating it to a situation\nin the world. Let\u2019s pursue this further. Imagine there is a situation s where there are\ntwo entities, Margrietje and her favorite doll, Brunoke. In addition, there is a relation\nholding between the two entities, which we will call the love relation. If you understand\nthe meaning of (3), then you know that it is true in situation s. In part, you know this\nbecause you know that Margrietje refers to Margrietje, Brunoke refers to Brunoke, and\nhoudt van refers to the love relation.\nWe have introduced two fundamental notions in semantics. The first is that declarative\nsentences are true or false in certain situations . The second is that definite noun phrases\nand proper nouns refer to things in the world . So (3) is true in a situation where Mar-\ngrietje loves the doll Brunoke, here illustrated in Figure 10-1.\nOnce we have adopted the notion of truth in a situation, we have a powerful tool for\nreasoning. In particular, we can look at sets of sentences, and ask whether they could\nbe true together in some situation. For example, the sentences in (5) can be both true,\nwhereas those in (6) and (7) cannot be. In other words, the sentences in (5) are con-\nsistent, whereas those in (6) and (7) are inconsistent.\n(5) a. Sylvania is to the north of Freedonia.\nb. Freedonia is a republic.\n10.1  Natural Language Understanding | 365\n(6) a. The capital of Freedonia has a population of 9,000.\nb.No city in Freedonia has a population of 9,000.\n(7)\na. Sylvania is to the north of Freedonia.\nb. Freedonia is to the north of Sylvania.\nWe have chosen sentences about fictional countries (featured in the Marx Brothers\u2019\n1933 movie Duck Soup ) to emphasize that your ability to reason about these examples\ndoes not depend on what is true or false in the actual world. If you know the meaning\nof the word no, and also know that the capital of a country is a city in that country,\nthen you should be able to conclude that the two sentences in (6) are inconsistent,\nregardless of where Freedonia is or what the population of its capital is. That is, there\u2019s\nno possible situation in which both sentences could be true. Similarly, if you know that\nthe relation expressed by to the north of  is asymmetric, then you should be able to\nconclude that the two sentences in (7) are inconsistent.\nBroadly speaking, logic-based approaches to natural language semantics focus on those\naspects of natural language that guide our judgments of consistency and inconsistency.\nThe syntax of a logical language is designed to make these features formally explicit.\nAs a result, determining properties like consistency can often be reduced to symbolic\nmanipulation, that is, to a task that can be carried out by a computer. In order to pursue\nthis approach, we first want to develop a technique for representing a possible situation.\nWe do this in terms of something that logicians call a \u201cmodel.\u201d\nFigure 10-1. Depiction of a situation in which Margrietje loves Brunoke.\n366 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nA model for a set W of sentences is a formal representation of a situation in which all\nthe sentences in W are true. The usual way of representing models involves set theory.\nThe domain D of discourse (all the entities we currently care about) is a set of individ-\nuals, while relations are treated as sets built up from D. Let\u2019s look at a concrete example.\nOur domain D will consist of three children, Stefan, Klaus, and Evi, represented re-\nspectively as s, k, and e. We write this as D = {s, k, e}. The expression boy denotes\nthe set consisting of Stefan and Klaus, the expression girl denotes the set consisting of\nEvi, and the expression is running  denotes the set consisting of Stefan and Evi. Fig-\nure 10-2 is a graphical rendering of the model.\nFigure 10-2. Diagram of a model containing a domain D and subsets of D corresponding to the\npredicates boy, girl, and is running.\nLater in this chapter we will use models to help evaluate the truth or falsity of English\nsentences, and \nin this way to illustrate some methods for representing meaning. How-\never, before going into more detail, let\u2019s put the discussion into a broader perspective,\nand link back to a topic that we briefly raised in Section 1.5. Can a computer understand\nthe meaning of a sentence? And how could we tell if it did? This is similar to asking\n\u201cCan a computer think?\u201d Alan Turing famously proposed to answer this by examining\nthe ability of a computer to hold sensible conversations with a human (Turing, 1950).\nSuppose you are having a chat session with a person and a computer, but you are not\ntold at the outset which is which. If you cannot identify which of your partners is the\ncomputer after chatting with each of them, then the computer has successfully imitated\na human. If a computer succeeds in passing itself off as human in this \u201cimitation game\u201d\n(or \u201cTuring Test\u201d as it is popularly known), then according to Turing, we should be\nprepared to say that the computer can think and can be said to be intelligent. So Turing\nside-stepped the question of somehow examining the internal states of a computer by\ninstead using its behavior as evidence of intelligence. By the same reasoning, we have\nassumed that in order to say that a computer understands English, it just needs to\n10.1  Natural Language Understanding | 367\nbehave as though it did. What is important here is not so much the specifics of Turing\u2019s\nimitation game, \nbut rather the proposal to judge a capacity for natural language un-\nderstanding in terms of observable behavior.\n10.2  Propositional Logic\nA logical language is designed to make reasoning formally explicit. As a result, it can\ncapture aspects of natural language which determine whether a set of sentences is con-\nsistent. As part of this approach, we need to develop logical representations of a sen-\ntence \u03c6 that formally capture the truth-conditions of \u03c6. We\u2019ll start off with a simple\nexample:\n(8) [Klaus chased Evi] and [Evi ran away].\nLet\u2019s replace the two sub-sentences in (8) by \u03c6 and \u03c8 respectively, and put & for the\nlogical operator corresponding to the English word and: \u03c6 & \u03c8. This structure is the\nlogical form of (8).\nPropositional logic  allows us to represent just those parts of linguistic structure that\ncorrespond to certain sentential connectives. We have just looked at and. Other such\nconnectives are not, or, and if..., then... . In the formalization of propositional logic, the\ncounterparts of such connectives are sometimes called Boolean operators . The basic\nexpressions of propositional logic are propositional symbols , often written as P, Q,\nR, etc. There are varying conventions for representing Boolean operators. Since we will\nbe focusing on ways of exploring logic within NLTK, we will stick to the following\nASCII versions of the operators:\n>>> nltk.boolean_ops()\nnegation            -\nconjunction         &\ndisjunction         |\nimplication         ->\nequivalence         <->\nFrom the propositional symbols and the Boolean operators we can build an infinite set\nof well-formed formulas (or just formulas, for short) of propositional logic. First,\nevery propositional letter is a formula. Then if \u03c6 is a formula, so is -\u03c6. And if \u03c6 and\n\u03c8 are formulas, then so are (\u03c6 & \u03c8), (\u03c6 | \u03c8), (\u03c6 -> \u03c8), and(\u03c6 <-> \u03c8).\nTable 10-2  specifies the truth-conditions for formulas containing these operators. As\nbefore we use \u03c6 and \u03c8 as variables over sentences, and abbreviate if and only if as iff.\nTable 10-2. Truth conditions for the Boolean operators in propositional logic\nBoolean operator Truth conditions\nnegation (it is not the case that ...) -\u03c6 is true in s iff \u03c6 is false in s\nconjunction (and) (\u03c6 & \u03c8) is true in s iff \u03c6 is true in s and \u03c8 is true in s\n368 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nBoolean operator Truth conditions\ndisjunction (or) (\u03c6 | \u03c8) is true in s iff \u03c6 is true in s or \u03c8 is true in s\nimplication (if ..., then ...) (\u03c6 -> \u03c8) is true in s iff \u03c6 is false in s or \u03c8 is true in s\nequivalence (if and only if) (\u03c6 <-> \u03c8) is true in s iff \u03c6 and \u03c8 are both true in s or both false in s\nThese rules are generally straightforward, though the truth conditions for implication\ndepart in \nmany cases from our usual intuitions about the conditional in English. A\nformula of the form (P -> Q)  is false only when P is true and Q is false. If P is false (say,\nP corresponds to The moon is made of green cheese ) and Q is true (say, Q corresponds to\nTwo plus two equals four), then P -> Q will come out true.\nNLTK\u2019s LogicParser() parses logical expressions into various subclasses of Expression:\n>>> lp = nltk.LogicParser()\n>>> lp.parse('-(P & Q)')\n<NegatedExpression -(P & Q)>\n>>> lp.parse('P & Q')\n<AndExpression (P & Q)>\n>>> lp.parse('P | (R -> Q)')\n<OrExpression (P | (R -> Q))>\n>>> lp.parse('P <-> -- P')\n<IffExpression (P <-> --P)>\nFrom a \ncomputational perspective, logics give us an important tool for performing\ninference. Suppose you state that Freedonia is not to the north of Sylvania, and you\ngive as your reasons that Sylvania is to the north of Freedonia. In this case, you have\nproduced an argument. The sentence Sylvania is to the north of Freedonia  is the\nassumption of the argument, while Freedonia is not to the north of Sylvania  is the\nconclusion. The step of moving from one or more assumptions to a conclusion is called\ninference. Informally, it is common to write arguments in a format where the conclu-\nsion is preceded by therefore.\n(9) Sylvania is to the north of Freedonia.\nTherefore, Freedonia is not to the north of Sylvania.\nAn argument is valid if there is no possible situation in which its premises are all true\nand its conclusion is not true.\nNow, the validity of (9) crucially depends on the meaning of the phrase to the north\nof, in particular, the fact that it is an asymmetric relation:\n(10) if x is to the north of y then y is not to the north of x.\nUnfortunately, we can\u2019t express such rules in propositional logic: the smallest elements\nwe have to play with are atomic propositions, and we cannot \u201clook inside\u201d these to\ntalk about relations between individuals x and y. The best we can do in this case is\ncapture a particular case of the asymmetry. Let\u2019s use the propositional symbol SnF to\n10.2  Propositional Logic | 369\nstand for Sylvania is to the north of Freedonia  and FnS for Freedonia is to the north of\nSylvania. To say that Freedonia is not to the north of Sylvania , we write -FnS. That is,\nwe treat not as equivalent to the phrase it is not the case that ... , and translate this as the\none-place Boolean operator -. Replacing x and y in (10) by Sylvania and Freedonia\nrespectively gives us an implication that can be written as:\n(11)SnF -> -FnS\nHow about giving a version of the complete argument? We will replace the first sentence\nof (9) by two formulas of propositional logic: SnF, and also the implication in (11),\nwhich expresses (rather poorly) our background knowledge of the meaning of to the\nnorth of . We\u2019ll write [A1, ..., An] / C to represent the argument that conclusion C\nfollows from assumptions [A1, ..., An]. This leads to the following as a representation\nof argument (9):\n(12)[SnF, SnF -> -FnS] / -FnS\nThis is a valid argument: if SnF and SnF -> -FnS are both true in a situation s, then\n-FnS must also be true in s. By contrast, if FnS were true, this would conflict with our\nunderstanding that two objects cannot both be to the north of each other in any possible\nsituation. Equivalently, the list [SnF, SnF -> -FnS, FnS] is inconsistent\u2014these sen-\ntences cannot all be true together.\nArguments can be tested for \u201csyntactic validity\u201d by using a proof system. We will say\na little bit more about this later on in Section 10.3 . Logical proofs can be carried out\nwith NLTK\u2019s inference module, for example, via an interface to the third-party theo-\nrem prover Prover9. The inputs to the inference mechanism first have to be parsed into\nlogical expressions by LogicParser().\n>>> lp = nltk.LogicParser()\n>>> SnF = lp.parse('SnF')\n>>> NotFnS = lp.parse('-FnS')\n>>> R = lp.parse('SnF -> -FnS')\n>>> prover = nltk.Prover9()\n>>> prover.prove(NotFnS, [SnF, R])\nTrue\nHere\u2019s another way of seeing why the conclusion follows. SnF -> -FnS  is semantically\nequivalent to -SnF | -FnS, where | is the two-place operator corresponding to or. In\ngeneral, \u03c6 | \u03c8 is true in a situation s if either \u03c6 is true in s or \u03c6 is true in s. Now, suppose\nboth SnF and -SnF | -FnS are true in situation s. If SnF is true, then -SnF cannot also be\ntrue; a fundamental assumption of classical logic is that a sentence cannot be both true\nand false in a situation. Consequently, -FnS must be true.\nRecall that we interpret sentences of a logical language relative to a model, which is a\nvery simplified version of the world. A model for propositional logic needs to assign\nthe values True or False to every possible formula. We do this inductively: first, every\npropositional symbol is assigned a value, and then we compute the value of complex\n370 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nformulas by consulting the meanings of the Boolean operators (i.e., Table 10-2 ) and\napplying them to the values of the formula\u2019s components. A Valuation is a mapping\nfrom basic symbols of the logic to their values. Here\u2019s an example:\n>>> val = nltk.Valuation([('P', True), ('Q', True), ('R', False)])\nWe initialize a Valuation with a list of pairs, each of which consists of a semantic symbol\nand a semantic value. The resulting object is essentially just a dictionary that maps\nlogical symbols (treated as strings) to appropriate values.\n>>> val['P']\nTrue\nAs we will see later, our models need to be somewhat more complicated in order to\nhandle the more complex logical forms discussed in the next section; for the time being,\njust ignore the dom and g parameters in the following declarations.\n>>> dom = set([])\n>>> g = nltk.Assignment(dom)\nNow let\u2019s initialize a model m that uses val:\n>>> m = nltk.Model(dom, val)\nEvery model comes with an evaluate() method, which will determine the semantic\nvalue of logical expressions, such as formulas of propositional logic; of course, these\nvalues depend on the initial truth values we assigned to propositional symbols such as P,\nQ, and R.\n>>> print m.evaluate('(P & Q)', g)\nTrue\n>>> print m.evaluate('-(P & Q)', g)\nFalse\n>>> print m.evaluate('(P & R)', g)\nFalse\n>>> print m.evaluate('(P | R)', g)\nTrue\nYour Turn:  Experiment with evaluating different formulas of proposi-\ntional logic. Does the model give the values that you expected?\nUp until now, we have been translating our English sentences into propositional logic.\nBecause we are confined to representing atomic sentences with letters such as P and\nQ, we cannot dig into their internal structure. In effect, we are saying that there is no\nsemantic benefit in dividing atomic sentences into subjects, objects, and predicates.\nHowever, this seems wrong: if we want to formalize arguments such as (9), we have to\nbe able to \u201clook inside\u201d basic sentences. As a result, we will move beyond propositional\nlogic to something more expressive, namely first-order logic. This is what we turn to\nin the next section.\n10.2  Propositional Logic | 371\n10.3  First-Order Logic\nIn the \nremainder of this chapter, we will represent the meaning of natural language\nexpressions by translating them into first-order logic. Not all of natural language se-\nmantics can be expressed in first-order logic. But it is a good choice for computational\nsemantics because it is expressive enough to represent many aspects of semantics, and\non the other hand, there are excellent systems available off the shelf for carrying out\nautomated inference in first-order logic.\nOur next step will be to describe how formulas of first-order logic are constructed, and\nthen how such formulas can be evaluated in a model.\nSyntax\nFirst-order logic keeps all the Boolean operators of propositional logic, but it adds some\nimportant new mechanisms. To start with, propositions are analyzed into predicates\nand arguments, which takes us a step closer to the structure of natural languages. The\nstandard construction rules for first-order logic recognize terms such as individual\nvariables and individual constants, and predicates that take differing numbers of ar-\nguments. For example, Angus walks  might be formalized as walk(angus) and Angus\nsees Bertie  as see (angus, bertie). We will call walk a unary predicate , and see a binary\npredicate. The symbols used as predicates do not have intrinsic meaning, although it\nis hard to remember this. Returning to one of our earlier examples, there is no logical\ndifference between (13a) and (13b).\n(13) a. love(margrietje, brunoke)\nb.houden_van(margrietje, brunoke)\nBy itself, first-order logic has nothing substantive to say about lexical semantics\u2014the\nmeaning of individual words\u2014although some theories of lexical semantics can be en-\ncoded in first-order logic. Whether an atomic predication like see(angus, bertie) is true\nor false in a situation is not a matter of logic, but depends on the particular valuation\nthat we have chosen for the constants see, angus, and bertie. For this reason, such\nexpressions are called non-logical constants . By contrast, logical constants  (such\nas the Boolean operators) always receive the same interpretation in every model for\nfirst-order logic.\nWe should mention here that one binary predicate has special status, namely equality,\nas in formulas such as angus = aj . Equality is regarded as a logical constant, since for\nindividual terms t1 and t2, the formula t1 = t2 is true if and only if t1 and t2 refer to one\nand the same entity.\nIt is often helpful to inspect the syntactic structure of expressions of first-order logic,\nand the usual way of doing this is to assign types to expressions. Following the tradition\nof Montague grammar, we will use two basic types : e is the type of entities, while t is\nthe type of formulas, i.e., expressions that have truth values. Given these two basic\n372 | Chapter 10: \u2002Analyzing the Meaning of Sentences\ntypes, we can form complex types  for function expressions. That is, given any types\n\u03c3 and \u03c4, \u2329\u03c3, \u03c4\u232a is a complex type corresponding to functions from ' \u03c3 things\u2019 to ' \u03c4 things\u2019.\nFor example, \u2329e, t\u232a is the type of expressions from entities to truth values, namely unary\npredicates. The LogicParser can be invoked so that it carries out type checking.\n>>> tlp = nltk.LogicParser(type_check=True)\n>>> parsed = tlp.parse('walk(angus)')\n>>> parsed.argument\n<ConstantExpression angus>\n>>> parsed.argument.type\ne\n>>> parsed.function\n<ConstantExpression walk>\n>>> parsed.function.type\n<e,?>\nWhy do we see <e,?> at the end of this example? Although the type-checker will try to\ninfer as many types as possible, in this case it has not managed to fully specify the type\nof walk, since its result type is unknown. Although we are intending walk to receive type\n<e, t> , as far as the type-checker knows, in this context it could be of some other type,\nsuch as <e, e> or <e, <e, t>. To help the type-checker, we need to specify a signa-\nture, implemented as a dictionary that explicitly associates types with non-logical con-\nstants:\n>>> sig = {'walk': '<e, t>'}\n>>> parsed = tlp.parse('walk(angus)', sig)\n>>> parsed.function.type\n<e,t>\nA binary predicate has type \u2329e, \u2329e, t\u232a\u232a. Although this is the type of something which\ncombines first with an argument of type e to make a unary predicate, we represent\nbinary predicates as combining directly with their two arguments. For example, the\npredicate see in the translation of Angus sees Cyril  will combine with its arguments to\ngive the result see(angus, cyril).\nIn first-order logic, arguments of predicates can also be individual variables such as x,\ny, and z. In NLTK, we adopt the convention that variables of type e are all lowercase.\nIndividual variables are similar to personal pronouns like he, she, and it, in that we need\nto know about the context of use in order to figure out their denotation. One way of\ninterpreting the pronoun in (14) is by pointing to a relevant individual in the local\ncontext.\n(14) He disappeared.\nAnother way is to supply a textual antecedent for the pronoun he, for example, by\nuttering (15a) prior to (14). Here, we say that he is coreferential with the noun phrase\nCyril. In such a context, (14) is semantically equivalent to (15b).\n(15) a. Cyril is Angus\u2019s dog.\nb. Cyril disappeared.\n10.3  First-Order Logic | 373\nConsider by contrast the occurrence of he in (16a) . In this case, it is bound by the\nindefinite NP a dog, and this is a different relationship than coreference. If we replace\nthe pronoun he by a dog, the result (16b) is not semantically equivalent to (16a).\n(16) a. Angus had a dog but he disappeared.\nb. Angus had a dog but a dog disappeared.\nCorresponding to (17a), we can construct an open formula (17b) with two occurrences\nof the variable x. (We ignore tense to simplify exposition.)\n(17) a. He is a dog and he disappeared.\nb.dog(x) & disappear(x)\nBy placing an existential quantifier \u2203x (\u201cfor some x\u201d) in front of (17b), we can\nbind these variables, as in (18a), which means (18b) or, more idiomatically, (18c).\n(18) a. \u2203x.(dog(x) & disappear(x))\nb. At least one entity is a dog and disappeared.\nc. A dog disappeared.\nHere is the NLTK counterpart of (18a):\n(19)exists x.(dog(x) & disappear(x))\nIn addition to the existential quantifier, first-order logic offers us the universal quan-\ntifier \u2200x (\u201cfor all x\u201d), illustrated in (20).\n(20) a. \u2200x.(dog(x) \u2192 disappear(x))\nb. Everything has the property that if it is a dog, it disappears.\nc. Every dog disappeared.\nHere is the NLTK counterpart of (20a):\n(21)all x.(dog(x) -> disappear(x))\nAlthough (20a) is the standard first-order logic translation of (20c), the truth conditions\naren\u2019t necessarily what you expect. The formula says that if some x is a dog, then x\ndisappears\u2014but it doesn\u2019t say that there are any dogs. So in a situation where there are\nno dogs, (20a) will still come out true. (Remember that (P -> Q) is true when P is false.)\nNow you might argue that every dog disappeared  does presuppose the existence of dogs,\nand that the logic formalization is simply wrong. But it is possible to find other examples\nthat lack such a presupposition. For instance, we might explain that the value of the\nPython expression astring.replace('ate', '8')  is the result of replacing every occur-\nrence of 'ate' in astring by '8', even though there may in fact be no such occurrences\n(Table 3-2).\n374 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nWe have seen a number of examples where variables are bound by quantifiers. What\nhappens in formulas such as the following?\n((exists x. dog(x)) -> bark(x))\nThe scope \nof the exists x quantifier is dog(x), so the occurrence of x in bark(x) is\nunbound. Consequently it can become bound by some other quantifier, for example,\nall x in the next formula:\nall x.((exists x. dog(x)) -> bark(x))\nIn general, an occurrence of a variable x in a formula \u03c6 is free in \u03c6 if that occurrence\ndoesn\u2019t fall within the scope of all x  or some x  in \u03c6. Conversely, if x is free in formula\n\u03c6, then it is bound in all x. \u03c6 and exists x. \u03c6. If all variable occurrences in a formula\nare bound, the formula is said to be closed.\nWe mentioned before that the parse() method of NLTK\u2019s LogicParser returns objects\nof class Expression. Each instance expr of this class comes with a method free(), which\nreturns the set of variables that are free in expr.\n>>> lp = nltk.LogicParser()\n>>> lp.parse('dog(cyril)').free()\nset([])\n>>> lp.parse('dog(x)').free()\nset([Variable('x')])\n>>> lp.parse('own(angus, cyril)').free()\nset([])\n>>> lp.parse('exists x.dog(x)').free()\nset([])\n>>> lp.parse('((some x. walk(x)) -> sing(x))').free()\nset([Variable('x')])\n>>> lp.parse('exists x.own(y, x)').free()\nset([Variable('y')])\nFirst-Order Theorem Proving\nRecall the constraint on to the north of, which we proposed earlier as (10):\n(22) if x is to the north of y then y is not to the north of x.\nWe observed that propositional logic is not expressive enough to represent generali-\nzations about binary predicates, and as a result we did not properly capture the argu-\nment Sylvania is to the north of Freedonia. Therefore, Freedonia is not to the north of\nSylvania.\nYou have no doubt realized that first-order logic, by contrast, is ideal for formalizing\nsuch rules:\nall x. all y.(north_of(x, y) -> -north_of(y, x))\nEven better, we can perform automated inference to show the validity of the argument.\n10.3  First-Order Logic | 375\nThe general case in theorem proving is to determine whether a formula that we want\nto prove \n(a proof goal ) can be derived by a finite sequence of inference steps from a\nlist of assumed formulas. We write this as A \u22a2 g, where A is a (possibly empty) list of\nassumptions, and g is a proof goal. We will illustrate this with NLTK\u2019s interface to the\ntheorem prover Prover9. First, we parse the required proof goal \n  and the two as-\nsumptions \n  \n. Then we create a Prover9 instance \n , and call its prove() method on\nthe goal, given the list of assumptions \n .\n>>> NotFnS = lp.parse('-north_of(f, s)')  \n>>> SnF = lp.parse('north_of(s, f)')    \n>>> R = lp.parse('all x. all y. (north_of(x, y) -> -north_of(y, x))')  \n>>> prover = nltk.Prover9()   \n>>> prover.prove(NotFnS, [SnF, R])  \nTrue\nHappily, the \ntheorem prover agrees with us that the argument is valid. By contrast, it\nconcludes that it is not possible to infer north_of(f, s) from our assumptions:\n>>> FnS = lp.parse('north_of(f, s)')\n>>> prover.prove(FnS, [SnF, R])\nFalse\nSummarizing the Language of First-Order Logic\nWe\u2019ll take this opportunity to restate our earlier syntactic rules for propositional logic\nand add the formation rules for quantifiers; together, these give us the syntax of first-\norder logic. In addition, we make explicit the types of the expressions involved. We\u2019ll\nadopt the convention that \u2329en, t\u232a is the type of a predicate that combines with n argu-\nments of type e to yield an expression of type t. In this case, we say that n is the arity\nof the predicate.\n1. If P is a predicate of type \u2329en, t\u232a, and \u03b11, ... \u03b1n are terms of type e, then\nP(\u03b1 1, ... \u03b1 n) is of type t.\n2. If \u03b1 and \u03b2 are both of type e, then (\u03b1 = \u03b2) and (\u03b1 != \u03b2) are of type t.\n3. If \u03c6 is of type t, then so is -\u03c6.\n4. If \u03c6 and \u03c8 are of type t, then so are (\u03c6 & \u03c8), (\u03c6 | \u03c8), (\u03c6 -> \u03c8), and (\u03c6 <-> \u03c8).\n5. If \u03c6 is of type t, and x is a variable of type e, then exists x. \u03c6 and all x. \u03c6 are of\ntype t.\nTable 10-3  summarizes the new logical constants of the logic module, and two of the\nmethods of Expressions.\n376 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nTable 10-3. Summary of new logical relations and operators required for first-order logic\nExample Description\n= Equality\n!= Inequality\nexists Existential quantifier\nall Universal quantifier\nTruth in Model\nWe have \nlooked at the syntax of first-order logic, and in Section 10.4  we will examine\nthe task of translating English into first-order logic. Yet as we argued in Section 10.1 ,\nthis gets us further forward only if we can give a meaning to sentences of first-order\nlogic. In other words, we need to give a truth-conditional semantics  to first-order logic.\nFrom the point of view of computational semantics, there are obvious limits to how far\none can push this approach. Although we want to talk about sentences being true or\nfalse in situations, we only have the means of representing situations in the computer\nin a symbolic manner. Despite this limitation, it is still possible to gain a clearer picture\nof truth-conditional semantics by encoding models in NLTK.\nGiven a first-order logic language L, a model M for L is a pair \u2329D, Val\u232a, where D is an\nnon-empty set called the domain of the model, and Val is a function called the valu-\nation function, which assigns values from D to expressions of L as follows:\n1. For every individual constant c in L, Val(c) is an element of D.\n2. For every predicate symbol P of arity n \u2265 0, Val(P) is a function from Dn to\n{True, False}. (If the arity of P is 0, then Val(P) is simply a truth value, and P is\nregarded as a propositional symbol.)\nAccording to 2, if P is of arity 2, then Val(P) will be a function f from pairs of elements\nof D to { True, False}. In the models we shall build in NLTK, we\u2019ll adopt a more con-\nvenient alternative, in which Val(P) is a set S of pairs, defined as follows:\n(23) S = {s | f(s) = True}\nSuch an f is called the characteristic function of S (as discussed in the further\nreadings).\nRelations are represented semantically in NLTK in the standard set-theoretic way: as\nsets of tuples. For example, let\u2019s suppose we have a domain of discourse consisting of\nthe individuals Bertie, Olive, and Cyril, where Bertie is a boy, Olive is a girl, and Cyril\nis a dog. For mnemonic reasons, we use b, o, and c as the corresponding labels in the\nmodel. We can declare the domain as follows:\n>>> dom = set(['b', 'o', 'c'])\n10.3  First-Order Logic | 377\nWe will use the utility function parse_valuation() to convert a sequence of strings of\nthe form symbol => value into a Valuation object.\n>>> v = \"\"\"\n... bertie => b\n... olive => o\n... cyril => c\n... boy => {b}\n... girl => {o}\n... dog => {c}\n... walk => {o, c}\n... see => {(b, o), (c, b), (o, c)}\n... \"\"\"\n>>> val = nltk.parse_valuation(v)\n>>> print val\n{'bertie': 'b',\n 'boy': set([('b',)]),\n 'cyril': 'c',\n 'dog': set([('c',)]),\n 'girl': set([('o',)]),\n 'olive': 'o',\n 'see': set([('o', 'c'), ('c', 'b'), ('b', 'o')]),\n 'walk': set([('c',), ('o',)])}\nSo according to this valuation, the value of see is a set of tuples such that Bertie sees\nOlive, Cyril sees Bertie, and Olive sees Cyril.\nYour Turn:  Draw a picture of the domain dom and the sets correspond-\ning to each of the unary predicates, by analogy with the diagram shown\nin Figure 10-2.\nYou may have noticed that our unary predicates (i.e, boy, girl, dog) also come out as\nsets of singleton tuples, rather than just sets of individuals. This is a convenience which\nallows us to have a uniform treatment of relations of any arity. A predication of the\nform P(\u03c41, ... \u03c4n), where P is of arity n, comes out true just in case the tuple of values\ncorresponding to (\u03c4 1, ... \u03c4 n) belongs to the set of tuples in the value of P.\n>>> ('o', 'c') in val['see']\nTrue\n>>> ('b',) in val['boy']\nTrue\nIndividual Variables and Assignments\nIn our models, the counterpart of a context of use is a variable assignment. This is a\nmapping from individual variables to entities in the domain. Assignments are created\nusing the Assignment constructor, which also takes the model\u2019s domain of discourse as\na parameter. We are not required to actually enter any bindings, but if we do, they are\nin a (variable, value) format similar to what we saw earlier for valuations.\n378 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n>>> g = nltk.Assignment(dom, [('x', 'o'), ('y', 'c')])\n>>> g\n{'y': 'c', 'x': 'o'}\nIn addition, \nthere is a print() format for assignments which uses a notation closer to\nthat often found in logic textbooks:\n>>> print g\ng[c/y][o/x]\nLet\u2019s now look at how we can evaluate an atomic formula of first-order logic. First, we\ncreate a model, and then we call the evaluate() method to compute the truth value:\n>>> m = nltk.Model(dom, val)\n>>> m.evaluate('see(olive, y)', g)\nTrue\nWhat\u2019s happening here? We are evaluating a formula which is similar to our earlier\nexample, see(olive, cyril). However, when the interpretation function encounters\nthe variable y, rather than checking for a value in val, it asks the variable assignment\ng to come up with a value:\n>>> g['y']\n'c'\nSince we already know that individuals o and c stand in the see relation, the value\nTrue is what we expected. In this case, we can say that assignment g satisfies the for-\nmula see(olive, y). By contrast, the following formula evaluates to False relative to\ng (check that you see why this is).\n>>> m.evaluate('see(y, x)', g)\nFalse\nIn our approach (though not in standard first-order logic), variable assignments are\npartial. For example, g says nothing about any variables apart from x and y. The method\npurge() clears all bindings from an assignment.\n>>> g.purge()\n>>> g\n{}\nIf we now try to evaluate a formula such as see(olive, y)  relative to g, it is like trying\nto interpret a sentence containing a him when we don\u2019t know what him refers to. In\nthis case, the evaluation function fails to deliver a truth value.\n>>> m.evaluate('see(olive, y)', g)\n'Undefined'\nSince our models already contain rules for interpreting Boolean operators, arbitrarily\ncomplex formulas can be composed and evaluated.\n>>> m.evaluate('see(bertie, olive) & boy(bertie) & -walk(bertie)', g)\nTrue\nThe general process of determining truth or falsity of a formula in a model is called\nmodel checking.\n10.3  First-Order Logic | 379\nQuantification\nOne of \nthe crucial insights of modern logic is that the notion of variable satisfaction\ncan be used to provide an interpretation for quantified formulas. Let\u2019s use (24) as an\nexample.\n(24)exists x.(girl(x) & walk(x))\nWhen is it true? Let\u2019s think about all the individuals in our domain, i.e., in dom. We\nwant to check whether any of these individuals has the property of being a girl and\nwalking. In other words, we want to know if there is some u in dom such that g[u/x]\nsatisfies the open formula (25).\n(25)girl(x) & walk(x)\nConsider the following:\n>>> m.evaluate('exists x.(girl(x) & walk(x))', g)\nTrue\nevaluate() returns True here because there is some u in dom such that (25) is satisfied\nby an assignment which binds x to u. In fact, o is such a u:\n>>> m.evaluate('girl(x) & walk(x)', g.add('x', 'o'))\nTrue\nOne useful tool offered by NLTK is the satisfiers() method. This returns a set of all\nthe individuals that satisfy an open formula. The method parameters are a parsed for-\nmula, a variable, and an assignment. Here are a few examples:\n>>> fmla1 = lp.parse('girl(x) | boy(x)')\n>>> m.satisfiers(fmla1, 'x', g)\nset(['b', 'o'])\n>>> fmla2 = lp.parse('girl(x) -> walk(x)')\n>>> m.satisfiers(fmla2, 'x', g)\nset(['c', 'b', 'o'])\n>>> fmla3 = lp.parse('walk(x) -> girl(x)')\n>>> m.satisfiers(fmla3, 'x', g)\nset(['b', 'o'])\nIt\u2019s useful to think about why fmla2 and fmla3 receive the values they do. The truth\nconditions for -> mean that fmla2 is equivalent to -girl(x) | walk(x), which is satisfied\nby something that either isn\u2019t a girl or walks. Since neither b (Bertie) nor c (Cyril) are\ngirls, according to model m, they both satisfy the whole formula. And of course o satisfies\nthe formula because o satisfies both disjuncts. Now, since every member of the domain\nof discourse satisfies fmla2, the corresponding universally quantified formula is also\ntrue.\n>>> m.evaluate('all x.(girl(x) -> walk(x))', g)\nTrue\n380 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nIn other words, a universally quantified formula \u2200x.\u03c6 is true with respect to g just in\ncase for every u, \u03c6 is true with respect to g[u/x].\nYour Turn:  Try to figure out, first with pencil and paper, and then using\nm.evaluate(), what the truth values are for all x.(girl(x) &\nwalk(x)) and exists x.(boy(x) -> walk(x)). Make sure you understand\nwhy they receive these values.\nQuantifier Scope Ambiguity\nWhat happens when we want to give a formal representation of a sentence with two\nquantifiers, such as the following?\n(26) Everybody admires someone.\nThere are (at least) two ways of expressing (26) in first-order logic:\n(27) a. all x.(person(x) -> exists y.(person(y) & admire(x,y)))\nb.exists y.(person(y) & all x.(person(x) -> admire(x,y)))\nCan we use both of these? The answer is yes, but they have different meanings. (27b)\nis logically stronger than (27a): it claims that there is a unique person, say, Bruce, who\nis admired by everyone. (27a), on the other hand, just requires that for every person\nu, we can find some person u' whom u admires; but this could be a different person\nu' in each case. We distinguish between (27a) and (27b) in terms of the scope of the\nquantifiers. In the first, \u2200 has wider scope than \u2203, whereas in (27b), the scope ordering\nis reversed. So now we have two ways of representing the meaning of (26), and they\nare both quite legitimate. In other words, we are claiming that (26) is ambiguous with\nrespect to quantifier scope, and the formulas in (27) give us a way to make the two\nreadings explicit. However, we are not just interested in associating two distinct rep-\nresentations with (26); we also want to show in detail how the two representations lead\nto different conditions for truth in a model.\nIn order to examine the ambiguity more closely, let\u2019s fix our valuation as follows:\n>>> v2 = \"\"\"\n... bruce => b\n... cyril => c\n... elspeth => e\n... julia => j\n... matthew => m\n... person => {b, e, j, m}\n... admire => {(j, b), (b, b), (m, e), (e, m), (c, a)}\n... \"\"\"\n>>> val2 = nltk.parse_valuation(v2)\nThe admire relation can be visualized using the mapping diagram shown in (28).\n10.3  First-Order Logic | 381\n(28)\nIn (28), an arrow between two individuals x  and y indicates that x admires y. So j and\nb both admire b (Bruce is very vain), while e admires m and m admires e. In this model,\nformula (27a) is true but (27b) is false. One way of exploring these results is by using\nthe satisfiers() method of Model objects.\n>>> dom2 = val2.domain\n>>> m2 = nltk.Model(dom2, val2)\n>>> g2 = nltk.Assignment(dom2)\n>>> fmla4 = lp.parse('(person(x) -> exists y.(person(y) & admire(x, y)))')\n>>> m2.satisfiers(fmla4, 'x', g2)\nset(['a', 'c', 'b', 'e', 'j', 'm'])\nThis shows that fmla4 holds of every individual in the domain. By contrast, consider\nthe formula fmla5; this has no satisfiers for the variable y.\n>>> fmla5 = lp.parse('(person(y) & all x.(person(x) -> admire(x, y)))')\n>>> m2.satisfiers(fmla5, 'y', g2)\nset([])\nThat is, there is no person that is admired by everybody. Taking a different open for-\nmula, fmla6, we can verify that there is a person, namely Bruce, who is admired by both\nJulia and Bruce.\n>>> fmla6 = lp.parse('(person(y) & all x.((x = bruce | x = julia) -> admire(x, y)))')\n>>> m2.satisfiers(fmla6, 'y', g2)\nset(['b'])\nYour Turn: Devise a new model based on m2 such that (27a) comes out\nfalse in your model; similarly, devise a new model such that (27b) comes\nout true.\n382 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nModel Building\nWe have \nbeen assuming that we already had a model, and wanted to check the truth\nof a sentence in the model. By contrast, model building tries to create a new model,\ngiven some set of sentences. If it succeeds, then we know that the set is consistent, since\nwe have an existence proof of the model.\nWe invoke the Mace4 model builder by creating an instance of Mace() and calling its\nbuild_model() method, in an analogous way to calling the Prover9 theorem prover. One\noption is to treat our candidate set of sentences as assumptions, while leaving the goal\nunspecified. The following interaction shows how both [a, c1]  and [a, c2]  are con-\nsistent lists, since Mace succeeds in building a model for each of them, whereas [c1,\nc2] is inconsistent.\n>>> a3 = lp.parse('exists x.(man(x) & walks(x))')\n>>> c1 = lp.parse('mortal(socrates)')\n>>> c2 = lp.parse('-mortal(socrates)')\n>>> mb = nltk.Mace(5)\n>>> print mb.build_model(None, [a3, c1])\nTrue\n>>> print mb.build_model(None, [a3, c2])\nTrue\n>>> print mb.build_model(None, [c1, c2])\nFalse\nWe can also use the model builder as an adjunct to the theorem prover. Let\u2019s suppose\nwe are trying to prove A \u22a2 g, i.e., that g is logically derivable from assumptions A = [a1,\na2, ..., an]. We can feed this same input to Mace4, and the model builder will try to\nfind a counterexample, that is, to show that g does not follow from A. So, given this\ninput, Mace4 will try to find a model for the assumptions A together with the negation\nof g, namely the list A' = [a1, a2, ..., an, -g]. If g fails to follow from S, then Mace4\nmay well return with a counterexample faster than Prover9 concludes that it cannot\nfind the required proof. Conversely, if g is provable from S, Mace4 may take a long time\nunsuccessfully trying to find a countermodel, and will eventually give up.\nLet\u2019s consider a concrete scenario. Our assumptions are the list [ There is a woman that\nevery man loves , Adam is a man , Eve is a woman ]. Our conclusion is Adam loves Eve .\nCan Mace4 find a model in which the premises are true but the conclusion is false? In\nthe following code, we use MaceCommand(), which will let us inspect the model that has\nbeen built.\n>>> a4 = lp.parse('exists y. (woman(y) & all x. (man(x) -> love(x,y)))')\n>>> a5 = lp.parse('man(adam)')\n>>> a6 = lp.parse('woman(eve)')\n>>> g = lp.parse('love(adam,eve)')\n>>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6])\n>>> mc.build_model()\nTrue\n10.3  First-Order Logic | 383\nSo the answer is yes: Mace4 found a countermodel in which there is some woman other\nthan Eve \nthat Adam loves. But let\u2019s have a closer look at Mace4\u2019s model, converted to\nthe format we use for valuations:\n>>> print mc.valuation\n{'C1': 'b',\n 'adam': 'a',\n 'eve': 'a',\n 'love': set([('a', 'b')]),\n 'man': set([('a',)]),\n 'woman': set([('a',), ('b',)])}\nThe general form of this valuation should be familiar to you: it contains some individual\nconstants and predicates, each with an appropriate kind of value. What might be puz-\nzling is the C1. This is a \u201cSkolem constant\u201d that the model builder introduces as a\nrepresentative of the existential quantifier. That is, when the model builder encoun-\ntered the exists y part of a4, it knew that there is some individual b in the domain\nwhich satisfies the open formula in the body of a4. However, it doesn\u2019t know whether\nb is also the denotation of an individual constant anywhere else in its input, so it makes\nup a new name for b on the fly, namely C1. Now, since our premises said nothing about\nthe individual constants adam and eve, the model builder has decided there is no reason\nto treat them as denoting different entities, and they both get mapped to a. Moreover,\nwe didn\u2019t specify that man and woman denote disjoint sets, so the model builder lets their\ndenotations overlap. This illustrates quite dramatically the implicit knowledge that we\nbring to bear in interpreting our scenario, but which the model builder knows nothing\nabout. So let's add a new assumption which makes the sets of men and women disjoint.\nThe model builder still produces a countermodel, but this time it is more in accord with\nour intuitions about the situation:\n>>> a7 = lp.parse('all x. (man(x) -> -woman(x))')\n>>> g = lp.parse('love(adam,eve)')\n>>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6, a7])\n>>> mc.build_model()\nTrue\n>>> print mc.valuation\n{'C1': 'c',\n 'adam': 'a',\n 'eve': 'b',\n 'love': set([('a', 'c')]),\n 'man': set([('a',)]),\n 'woman': set([('b',), ('c',)])}\nOn reflection, we can see that there is nothing in our premises which says that Eve is\nthe only woman in the domain of discourse, so the countermodel in fact is acceptable.\nIf we wanted to rule it out, we would have to add a further assumption such as exists\ny. all x. (woman(x) -> (x = y)) to ensure that there is only one woman in the model.\n384 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n10.4  The Semantics of English Sentences\nCompositional Semantics in Feature-Based Grammar\nAt the \nbeginning of the chapter we briefly illustrated a method of building semantic\nrepresentations on the basis of a syntactic parse, using the grammar framework devel-\noped in Chapter 9 . This time, rather than constructing an SQL query, we will build a\nlogical form. One of our guiding ideas for designing such grammars is the Principle of\nCompositionality. (Also known as Frege\u2019s Principle; see [Partee, 1995] for the for-\nmulation given.)\nPrinciple of Compositionality:  the meaning of a whole is a function of the meanings\nof the parts and of the way they are syntactically combined.\nWe will assume that the semantically relevant parts of a complex expression are given\nby a theory of syntactic analysis. Within this chapter, we will take it for granted that\nexpressions are parsed against a context-free grammar. However, this is not entailed\nby the Principle of Compositionality.\nOur goal now is to integrate the construction of a semantic representation in a manner\nthat can be smoothly with the process of parsing. (29) illustrates a first approximation\nto the kind of analyses we would like to build.\n(29)\nIn (29), the SEM  value at the root node shows a semantic representation for the whole\nsentence, while the SEM values at lower nodes show semantic representations for con-\nstituents of the sentence. Since the values of SEM have to be treated in a special manner,\nthey are distinguished from other feature values by being enclosed in angle brackets.\nSo far, so good, but how do we write grammar rules that will give us this kind of result?\nOur approach will be similar to that adopted for the grammar sql0.fcfg at the start of\nthis chapter, in that we will assign semantic representations to lexical nodes, and then\ncompose the semantic representations for each phrase from those of its child nodes.\nHowever, in the present case we will use function application rather than string con-\ncatenation as the mode of composition. To be more specific, suppose we have NP and\nVP constituents with appropriate values for their SEM nodes. Then the SEM value of an\nS is handled by a rule like (30). (Observe that in the case where the value of SEM is a\nvariable, we omit the angle brackets.)\n10.4  The Semantics of English Sentences | 385\n(30)S[SEM=<?vp(?np)>] -> NP[SEM=?subj] VP[SEM=?vp]\n(30) tells us that given some SEM value ?subj for the subject NP and some SEM value ?vp\nfor the VP\n, the SEM value of the S parent is constructed by applying ?vp as a function\nexpression to ?np. From this, we can conclude that ?vp has to denote a function which\nhas the denotation of ?np in its domain. (30) is a nice example of building semantics\nusing the principle of compositionality.\nTo complete the grammar is very straightforward; all we require are the rules shown\nhere:\nVP[SEM=?v] -> IV[SEM=?v]\nNP[SEM=<cyril>] -> 'Cyril'\nIV[SEM=<\\x.bark(x)>] -> 'barks'\nThe VP rule says that the parent\u2019s semantics is the same as the head child\u2019s semantics.\nThe two lexical rules provide non-logical constants to serve as the semantic values of\nCyril and barks respectively. There is an additional piece of notation in the entry for\nbarks which we will explain shortly.\nBefore launching into compositional semantic rules in more detail, we need to add a\nnew tool to our kit, namely the \u03bb-calculus. This provides us with an invaluable tool for\ncombining expressions of first-order logic as we assemble a meaning representation for\nan English sentence.\nThe \u03bb-Calculus\nIn Section 1.3 , we pointed out that mathematical set notation was a helpful method of\nspecifying properties P of words that we wanted to select from a document. We illus-\ntrated this with (31), which we glossed as \u201cthe set of all w such that w is an element of\nV (the vocabulary) and w has property P\u201d.\n(31) {w | w \u2208 V & P(w)}\nIt turns out to be extremely useful to add something to first-order logic that will achieve\nthe same effect. We do this with the \u03bb-operator (pronounced \u201clambda\u201d). The \u03bb coun-\nterpart to (31) is (32). (Since we are not trying to do set theory here, we just treat V as\na unary predicate.)\n(32) \u03bbw. (V(w) & P(w))\n\u03bb expressions were originally designed by Alonzo Church to represent\ncomputable \nfunctions and to provide a foundation for mathematics and\nlogic. The theory in which \u03bb expressions are studied is known as the\n\u03bb-calculus. Note that the \u03bb-calculus is not part of first-order logic\u2014both\ncan be used independently of the other.\n386 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n\u03bb is a binding operator, just as the first-order logic quantifiers are. If we have an open\nformula, such as (33a)\n, then we can bind the variable x with the \u03bb operator, as shown\nin (33b). The corresponding NLTK representation is given in (33c).\n(33) a. ( walk(x) & chew_gum(x))\nb.\u03bbx.(walk(x) & chew_gum(x))\nc.\\x.(walk(x) & chew_gum(x))\nRemember that \\ is a special character in Python strings. We must either escape it (with\nanother \\), or else use \u201craw strings\u201d (Section 3.4) as shown here:\n>>> lp = nltk.LogicParser()\n>>> e = lp.parse(r'\\x.(walk(x) & chew_gum(x))')\n>>> e\n<LambdaExpression \\x.(walk(x) & chew_gum(x))>\n>>> e.free()\nset([])\n>>> print lp.parse(r'\\x.(walk(x) & chew_gum(y))')\n\\x.(walk(x) & chew_gum(y))\nWe have a special name for the result of binding the variables in an expression:\n\u03bb-abstraction. When you first encounter \u03bb-abstracts, it can be hard to get an intuitive\nsense of their meaning. A couple of English glosses for (33b) are: \u201cbe an x such that x\nwalks and x chews gum\u201d or \u201chave the property of walking and chewing gum.\u201d It has\noften been suggested that \u03bb-abstracts are good representations for verb phrases (or\nsubjectless clauses), particularly when these occur as arguments in their own right. This\nis illustrated in (34a) and its translation, (34b).\n(34) a. To walk and chew gum is hard\nb.hard(\\x.(walk(x) & chew_gum(x))\nSo the general picture is this: given an open formula \u03c6 with free variable x, abstracting\nover x yields a property expression \u03bbx.\u03c6\u2014the property of being an x such that \u03c6. Here\u2019s\na more official version of how abstracts are built:\n(35) If \u03b1 is of type \u03c4, and x is a variable of type e, then \\x.\u03b1 is of type \u2329e, \u03c4\u232a.\n(34b) illustrated a case where we say something about a property, namely that it is hard.\nBut what we usually do with properties is attribute them to individuals. And in fact, if\n\u03c6 is an open formula, then the abstract \u03bbx.\u03c6 can be used as a unary predicate. In (36),\n(33b) is predicated of the term gerald.\n(36)\\x.(walk(x) & chew_gum(x)) (gerald)\nNow (36) says that Gerald has the property of walking and chewing gum, which has\nthe same meaning as (37).\n(37)(walk(gerald) & chew_gum(gerald))\n10.4  The Semantics of English Sentences | 387\nWhat we have done here is remove the \\x from the beginning of \\x.(walk(x) &\nchew_gum(x)) and replaced all occurrences of x in (walk(x) & chew_gum(x))  by gerald.\nWe\u2019ll use \u03b1[\u03b2/x] as notation for the operation of replacing all free occurrences of x in\n\u03b1 by the expression \u03b2. So\n(walk(x) & chew_gum(x))[gerald/x]\nrepresents the same expression as (37). The \u201creduction\u201d of (36) to (37) is an extremely\nuseful operation in simplifying semantic representations, and we shall use it a lot in the\nrest of this chapter. The operation is often called \u03b2-reduction. In order for it to be\nsemantically justified, we want it to hold that \u03bbx. \u03b1(\u03b2) has the same semantic value as\n\u03b1[\u03b2/x]. This is indeed true, subject to a slight complication that we will come to shortly.\nIn order to carry out \u03b2-reduction of expressions in NLTK, we can call the simplify()\nmethod \n .\n>>> e = lp.parse(r'\\x.(walk(x) & chew_gum(x))(gerald)')\n>>> print e\n\\x.(walk(x) & chew_gum(x))(gerald)\n>>> print e.simplify() \n(walk(gerald) & chew_gum(gerald))\nAlthough we \nhave so far only considered cases where the body of the \u03bb-abstract is an\nopen formula, i.e., of type t, this is not a necessary restriction; the body can be any well-\nformed expression. Here\u2019s an example with two \u03bbs:\n(38)\\x.\\y.(dog(x) & own(y, x))\nJust as (33b) plays the role of a unary predicate, (38) works like a binary predicate: it\ncan be applied directly to two arguments \n . The LogicParser allows nested \u03bbs such as\n\\x.\\y. to be written in the abbreviated form \\x y. \n .\n>>> print lp.parse(r'\\x.\\y.(dog(x) & own(y, x))(cyril)').simplify()\n\\y.(dog(cyril) & own(y,cyril))\n>>> print lp.parse(r'\\x y.(dog(x) & own(y, x))(cyril, angus)').simplify() \n(dog(cyril) & own(angus,cyril))\nAll our \u03bb\n-abstracts so far have involved the familiar first-order variables: x, y, and so on\n\u2014variables of type e. But suppose we want to treat one abstract, say, \\x.walk(x), as\nthe argument of another \u03bb-abstract? We might try this:\n\\y.y(angus)(\\x.walk(x))\nBut since the variable y is stipulated to be of type e, \\y.y(angus) only applies to argu-\nments of type e while \\x.walk(x) is of type \u2329e, t\u232a! Instead, we need to allow abstraction\nover variables of higher type. Let\u2019s use P and Q as variables of type \u2329e, t\u232a, and then we\ncan have an abstract such as \\P.P(angus). Since P is of type \u2329e, t\u232a, the whole abstract is\nof type \u2329\u2329e, t\u232a, t\u232a. Then \\P.P(angus)(\\x.walk(x)) is legal, and can be simplified via \u03b2-\nreduction to \\x.walk(x)(angus) and then again to walk(angus).\n388 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nWhen carrying out \u03b2-reduction, some care has to be taken with variables. Consider,\nfor example, the \u03bb-terms (39a) and (39b), which differ only in the identity of a free\nvariable.\n(39) a. \\y.see(y, x)\nb.\\y.see(y, z)\nSuppose now that we apply the \u03bb-term \\P.exists x.P(x) to each of these terms:\n(40) a. \\P.exists x.P(x)(\\y.see(y, x))\nb.\\P.exists x.P(x)(\\y.see(y, z))\nWe pointed out earlier that the results of the application should be semantically equiv-\nalent. But if we let the free variable x in (39a) fall inside the scope of the existential\nquantifier in (40a), then after reduction, the results will be different:\n(41) a. exists x.see(x, x)\nb.exists x.see(x, z)\n(41a) means there is some x that sees him/herself, whereas (41b) means that there is\nsome x that sees an unspecified individual z. What has gone wrong here? Clearly, we\nwant to forbid the kind of variable \u201ccapture\u201d shown in (41a).\nIn order to deal with this problem, let\u2019s step back a moment. Does it matter what\nparticular name we use for the variable bound by the existential quantifier in the func-\ntion expression of (40a)? The answer is no. In fact, given any variable-binding expres-\nsion (involving \u2200, \u2203, or \u03bb), the name chosen for the bound variable is completely arbi-\ntrary. For example, exists x.P(x) and exists y.P(y) are equivalent; they are called\n\u03b1-equivalents, or alphabetic variants . The process of relabeling bound variables is\nknown as \u03b1-conversion. When we test for equality of VariableBinderExpressions in\nthe logic module (i.e., using ==), we are in fact testing for \u03b1-equivalence:\n>>> e1 = lp.parse('exists x.P(x)')\n>>> print e1\nexists x.P(x)\n>>> e2 = e1.alpha_convert(nltk.Variable('z'))\n>>> print e2\nexists z.P(z)\n>>> e1 == e2\nTrue\nWhen \u03b2-reduction is carried out on an application f(a), we check whether there are\nfree variables in a that also occur as bound variables in any subterms of f. Suppose, as\nin the example just discussed, that x is free in a, and that f contains the subterm exists\nx.P(x). In this case, we produce an alphabetic variant of exists x.P(x), say, exists\nz1.P(z1), and then carry on with the reduction. This relabeling is carried out automat-\nically by the \u03b2-reduction code in logic, and the results can be seen in the following\nexample:\n10.4  The Semantics of English Sentences | 389\n>>> e3 = lp.parse('\\P.exists x.P(x)(\\y.see(y, x))')\n>>> print e3\n(\\P.exists x.P(x))(\\y.see(y,x))\n>>> print e3.simplify()\nexists z1.see(z1,x)\nAs you work through examples like these in the following sections, you\nmay find that the logical expressions which are returned have different\nvariable names; \nfor example, you might see z14 in place of z1 in the\npreceding formula. This change in labeling is innocuous\u2014in fact, it is\njust an illustration of alphabetic variants.\nAfter this excursus, let\u2019s return to the task of building logical forms for English\nsentences.\nQuantified NPs\nAt the start of this section, we briefly described how to build a semantic representation\nfor Cyril barks . You would be forgiven for thinking this was all too easy\u2014surely there\nis a bit more to building compositional semantics. What about quantifiers, for instance?\nRight, this is a crucial issue. For example, we want (42a) to be given the logical form\nin (42b). How can this be accomplished?\n(42) a. A dog barks.\nb.exists x.(dog(x) & bark(x))\nLet\u2019s make the assumption that our only operation for building complex semantic rep-\nresentations is function application. Then our problem is this: how do we give a se-\nmantic representation to the quantified NPs a dog  so that it can be combined with\nbark to give the result in (42b)? As a first step, let\u2019s make the subject\u2019s SEM value act as\nthe function expression rather than the argument. (This is sometimes called type-\nraising.) Now we are looking for a way of instantiating ?np so that\n[SEM=<?np(\\x.bark(x))>] is equivalent to [SEM=<exists x.(dog(x) & bark(x))>].\nDoesn\u2019t this look a bit reminiscent of carrying out \u03b2-reduction in the \u03bb-calculus? In\nother words, we want a \u03bb-term M to replace ?np so that applying M to \\x.bark(x) yields\n(42b). To do this, we replace the occurrence of \\x.bark(x) in (42b) by a predicate\nvariable P, and bind the variable with \u03bb, as shown in (43).\n(43)\\P.exists x.(dog(x) & P(x))\nWe have used a different style of variable in (43)\u2014that is, 'P' rather than 'x' or 'y'\u2014\nto signal that we are abstracting over a different kind of object\u2014not an individual, but\na function expression of type \u2329e, t\u232a. So the type of (43) as a whole is \u2329\u2329e, t\u232a, t\u232a. We will\ntake this to be the type of NPs in general. To illustrate further, a universally quantified\nNP will look like (44).\n390 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n(44)\\P.all x.(dog(x) -> P(x))\nWe are \npretty much done now, except that we also want to carry out a further abstrac-\ntion plus application for the process of combining the semantics of the determiner a,\nnamely (45), with the semantics of dog.\n(45)\\Q P.exists x.(Q(x) & P(x))\nApplying (45) as a function expression to \\x.dog(x)yields (43), and applying that to\n\\x.bark(x) gives us \\P.exists x.(dog(x) & P(x))(\\x.bark(x)). Finally, carrying out \u03b2-\nreduction yields just what we wanted, namely (42b).\nTransitive Verbs\nOur next challenge is to deal with sentences containing transitive verbs, such as (46).\n(46) Angus chases a dog.\nThe output semantics that we want to build is exists x.(dog(x) & chase(angus, x)) .\nLet\u2019s look at how we can use \u03bb-abstraction to get this result. A significant constraint\non possible solutions is to require that the semantic representation of a dog be inde-\npendent of whether the NP acts as subject or object of the sentence. In other words, we\nwant to get the formula just shown as our output while sticking to (43) as the NP se-\nmantics. A second constraint is that VPs should have a uniform type of interpretation,\nregardless of whether they consist of just an intransitive verb or a transitive verb plus\nobject. More specifically, we stipulate that VPs are always of type \u2329e, t\u232a. Given these\nconstraints, here\u2019s a semantic representation for chases a dog that does the trick.\n(47)\\y.exists x.(dog(x) & chase(y, x))\nThink of (47) as the property of being a y such that for some dog x, y chases x; or more\ncolloquially, being a y who chases a dog. Our task now resolves to designing a semantic\nrepresentation for chases which can combine with (43) so as to allow (47) to be derived.\nLet\u2019s carry out the inverse of \u03b2-reduction on (47), giving rise to (48).\n(48)\\P.exists x.(dog(x) & P(x))(\\z.chase(y, z))\n(48) may be slightly hard to read at first; you need to see that it involves applying the\nquantified NP representation from (43) to \\z.chase(y,z). (48) is equivalent via \u03b2-\nreduction to exists x.(dog(x) & chase(y, x)).\nNow let\u2019s replace the function expression in (48) by a variable X of the same type as an\nNP, that is, of type \u2329\u2329e, t\u232a, t\u232a.\n(49)X(\\z.chase(y, z))\n10.4  The Semantics of English Sentences | 391\nThe representation of a transitive verb will have to apply to an argument of the type of\nX to \nyield a function expression of the type of VPs, that is, of type \u2329e, t\u232a. We can ensure\nthis by abstracting over both the X variable in (49) and also the subject variable y. So\nthe full solution is reached by giving chases the semantic representation shown in (50).\n(50)\\X y.X(\\x.chase(y, x))\nIf (50) is applied to (43), the result after \u03b2-reduction is equivalent to (47), which is what\nwe wanted all along:\n>>> lp = nltk.LogicParser()\n>>> tvp = lp.parse(r'\\X x.X(\\y.chase(x,y))')\n>>> np = lp.parse(r'(\\P.exists x.(dog(x) & P(x)))')\n>>> vp = nltk.ApplicationExpression(tvp, np)\n>>> print vp\n(\\X x.X(\\y.chase(x,y)))(\\P.exists x.(dog(x) & P(x)))\n>>> print vp.simplify()\n\\x.exists z2.(dog(z2) & chase(x,z2))\nIn order to build a semantic representation for a sentence, we also need to combine in\nthe semantics of the subject NP. If the latter is a quantified expression, such as every\ngirl, everything proceeds in the same way as we showed for a dog barks  earlier on; the\nsubject is translated as a function expression which is applied to the semantic repre-\nsentation of the VP. However, we now seem to have created another problem for our-\nselves with proper names. So far, these have been treated semantically as individual\nconstants, and these cannot be applied as functions to expressions like (47). Conse-\nquently, we need to come up with a different semantic representation for them. What\nwe do in this case is reinterpret proper names so that they too are function expressions,\nlike quantified NPs. Here is the required \u03bb-expression for Angus:\n(51)\\P.P(angus)\n(51) denotes the characteristic function corresponding to the set of all properties which\nare true of Angus. Converting from an individual constant angus to \\P.P(angus) is an-\nother example of type-raising, briefly mentioned earlier, and allows us to replace a\nBoolean-valued application such as \\x.walk(x)(angus) with an equivalent function ap-\nplication \\P.P(angus)(\\x.walk(x)). By \u03b2-reduction, both expressions reduce to\nwalk(angus).\nThe grammar simple-sem.fcfg contains a small set of rules for parsing and translating\nsimple examples of the kind that we have been looking at. Here\u2019s a slightly more com-\nplicated example:\n>>> from nltk import load_parser\n>>> parser = load_parser('grammars/book_grammars/simple-sem.fcfg', trace=0)\n>>> sentence = 'Angus gives a bone to every dog'\n>>> tokens = sentence.split()\n>>> trees = parser.nbest_parse(tokens)\n \n392 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n>>> for tree in trees:\n...     print tree.node['SEM']\nall z2.(dog(z2) -> exists z1.(bone(z1) & give(angus,z1,z2)))\nNLTK provides \nsome utilities to make it easier to derive and inspect semantic inter-\npretations. The function batch_interpret() is intended for batch interpretation of a list\nof input sentences. It builds a dictionary d where for each sentence sent in the input,\nd[sent] is a list of pairs ( synrep, semrep) consisting of trees and semantic representations\nfor sent. The value is a list since sent may be syntactically ambiguous; in the following\nexample, however, there is only one parse tree per sentence in the list.\n(S[SEM=<walk(irene)>]\n  (NP[-LOC, NUM='sg', SEM=<\\P.P(irene)>]\n    (PropN[-LOC, NUM='sg', SEM=<\\P.P(irene)>] Irene))\n  (VP[NUM='sg', SEM=<\\x.walk(x)>]\n    (IV[NUM='sg', SEM=<\\x.walk(x)>, TNS='pres'] walks)))\n(S[SEM=<exists z1.(ankle(z1) & bite(cyril,z1))>]\n  (NP[-LOC, NUM='sg', SEM=<\\P.P(cyril)>]\n    (PropN[-LOC, NUM='sg', SEM=<\\P.P(cyril)>] Cyril))\n  (VP[NUM='sg', SEM=<\\x.exists z1.(ankle(z1) & bite(x,z1))>]\n    (TV[NUM='sg', SEM=<\\X x.X(\\y.bite(x,y))>, TNS='pres'] bites)\n    (NP[NUM='sg', SEM=<\\Q.exists x.(ankle(x) & Q(x))>]\n      (Det[NUM='sg', SEM=<\\P Q.exists x.(P(x) & Q(x))>] an)\n      (Nom[NUM='sg', SEM=<\\x.ankle(x)>]\n        (N[NUM='sg', SEM=<\\x.ankle(x)>] ankle)))))\nWe have seen now how to convert English sentences into logical forms, and earlier we\nsaw how logical forms could be checked as true or false in a model. Putting these two\nmappings together, we can check the truth value of English sentences in a given model.\nLet\u2019s take model m as defined earlier. The utility batch_evaluate() resembles\nbatch_interpret(), except that we need to pass a model and a variable assignment as\nparameters. The output is a triple ( synrep, semrep, value), where synrep, semrep are as\nbefore, and value is a truth value. For simplicity, the following example only processes\na single sentence.\n>>> v = \"\"\"\n... bertie => b\n... olive => o\n... cyril => c\n... boy => {b}\n... girl => {o}\n... dog => {c}\n... walk => {o, c}\n... see => {(b, o), (c, b), (o, c)}\n... \"\"\"\n>>> val = nltk.parse_valuation(v)\n>>> g = nltk.Assignment(val.domain)\n>>> m = nltk.Model(val.domain, val)\n>>> sent = 'Cyril sees every boy'\n>>> grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n>>> results = nltk.batch_evaluate([sent], grammar_file, m, g)[0]\n>>> for (syntree, semrel, value) in results:\n...     print semrep\n10.4  The Semantics of English Sentences | 393\n...     print value\nexists z3.(ankle(z3) & bite(cyril,z3))\nTrue\nQuantifier Ambiguity Revisited\nOne important \nlimitation of the methods described earlier is that they do not deal with\nscope ambiguity. Our translation method is syntax-driven, in the sense that the se-\nmantic representation is closely coupled with the syntactic analysis, and the scope of\nthe quantifiers in the semantics therefore reflects the relative scope of the corresponding\nNPs in the syntactic parse tree. Consequently, a sentence like (26), repeated here, will\nalways be translated as (53a), not (53b).\n(52) Every girl chases a dog.\n(53) a. all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))\nb.exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))\nThere are numerous approaches to dealing with scope ambiguity, and we will look very\nbriefly at one of the simplest. To start with, let\u2019s briefly consider the structure of scoped\nformulas. Figure 10-3 depicts the way in which the two readings of (52) differ.\nFigure 10-3. Quantifier scopings.\nLet\u2019s consider \nthe lefthand structure first. At the top, we have the quantifier corre-\nsponding to every girl . The \u03c6 can be thought of as a placeholder for whatever is inside\nthe scope of the quantifier. Moving downward, we see that we can plug in the quantifier\ncorresponding to a dog  as an instantiation of \u03c6. This gives a new placeholder \u03c8, rep-\nresenting the scope of a dog, and into this we can plug the \u201ccore\u201d of the semantics,\nnamely the open sentence corresponding to x chases y. The structure on the righthand\nside is identical, except we have swapped round the order of the two quantifiers.\nIn the method known as Cooper storage, a semantic representation is no longer an\nexpression of first-order logic, but instead a pair consisting of a \u201ccore\u201d semantic rep-\nresentation plus a list of binding operators . For the moment, think of a binding op-\nerator as being identical to the semantic representation of a quantified NP such as (44) or\n394 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n(45). Following along the lines indicated in Figure 10-3 , let\u2019s assume that we have\nconstructed a Cooper-storage-style semantic representation of sentence (52), and let\u2019s\ntake our core to be the open formula chase(x,y). Given a list of binding operators\ncorresponding to the two NPs in (52), we pick a binding operator off the list, and com-\nbine it with the core.\n\\P.exists y.(dog(y) & P(y))(\\z2.chase(z1,z2))\nThen we take the result, and apply the next binding operator from the list to it.\n\\P.all x.(girl(x) -> P(x))(\\z1.exists x.(dog(x) & chase(z1,x)))\nOnce the list is empty, we have a conventional logical form for the sentence. Combining\nbinding operators with the core in this way is called S-Retrieval. If we are careful to\nallow every possible order of binding operators (for example, by taking all permutations\nof the list; see Section 4.5 ), then we will be able to generate every possible scope ordering\nof quantifiers.\nThe next question to address is how we build up a core+store representation compo-\nsitionally. As before, each phrasal and lexical rule in the grammar will have a SEM feature,\nbut now there will be embedded features CORE and STORE. To illustrate the machinery,\nlet\u2019s consider a simpler example, namely Cyril smiles . Here\u2019s a lexical rule for the verb\nsmiles (taken from the grammar storage.fcfg), which looks pretty innocuous:\nIV[SEM=[CORE=<\\x.smile(x)>, STORE=(/)]] -> 'smiles'\nThe rule for the proper name Cyril is more complex.\nNP[SEM=[CORE=<@x>, STORE=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril'\nThe bo predicate has two subparts: the standard (type-raised) representation of a proper\nname, and the expression @x, which is called the address of the binding operator. (We\u2019ll\nexplain the need for the address variable shortly.) @x is a metavariable, that is, a variable\nthat ranges over individual variables of the logic and, as you will see, also provides the\nvalue of core. The rule for VP just percolates up the semantics of the IV, and the inter-\nesting work is done by the S rule.\nVP[SEM=?s] -> IV[SEM=?s]\nS[SEM=[CORE=<?vp(?subj)>, STORE=(?b1+?b2)]] ->\n   NP[SEM=[CORE=?subj, STORE=?b1]] VP[SEM=[core=?vp, store=?b2]]\nThe core value at the S node is the result of applying the VP\u2019s core value, namely\n\\x.smile(x), to the subject NP\u2019s value. The latter will not be @x, but rather an instan-\ntiation of @x, say, z3. After \u03b2-reduction, <?vp(?subj)> will be unified with\n<smile(z3)>. Now, when @x is instantiated as part of the parsing process, it will be\ninstantiated uniformly. In particular, the occurrence of @x in the subject NP\u2019s STORE will\nalso be mapped to z3, yielding the element bo(\\P.P(cyril),z3). These steps can be seen\nin the following parse tree.\n10.4  The Semantics of English Sentences | 395\n(S[SEM=[CORE=<smile(z3)>, STORE=(bo(\\P.P(cyril),z3))]]\n  (NP[SEM=[CORE=<z3>, STORE=(bo(\\P.P(cyril),z3))]] Cyril)\n  (VP[SEM=[CORE=<\\x.smile(x)>, STORE=()]]\n    (IV[SEM=[CORE=<\\x.smile(x)>, STORE=()]] smiles)))\nLet\u2019s return \nto our more complex example, (52), and see what the storage style SEM\nvalue is, after parsing with grammar storage.fcfg.\nCORE  = <chase(z1,z2)>\nSTORE = (bo(\\P.all x.(girl(x) -> P(x)),z1), bo(\\P.exists x.(dog(x) & P(x)),z2))\nIt should be clearer now why the address variables are an important part of the binding\noperator. Recall that during S-retrieval, we will be taking binding operators off the\nSTORE list and applying them successively to the CORE. Suppose we start with bo(\\P.all\nx.(girl(x) -> P(x)),z1), which we want to combine with chase(z1,z2). The quantifier\npart of the binding operator is \\P.all x.(girl(x) -> P(x)) , and to combine this with\nchase(z1,z2), the latter needs to first be turned into a \u03bb-abstract. How do we know\nwhich variable to abstract over? This is what the address z1 tells us, i.e., that every\ngirl has the role of chaser rather than chasee.\nThe module nltk.sem.cooper_storage deals with the task of turning storage-style se-\nmantic representations into standard logical forms. First, we construct a CooperStore\ninstance, and inspect its STORE and CORE.\n>>> from nltk.sem import cooper_storage as cs\n>>> sentence = 'every girl chases a dog'\n>>> trees = cs.parse_with_bindops(sentence, grammar='grammars/book_grammars/storage.fcfg')\n>>> semrep = trees[0].node['sem']\n>>> cs_semrep = cs.CooperStore(semrep)\n>>> print cs_semrep.core\nchase(z1,z2)\n>>> for bo in cs_semrep.store:\n...     print bo\nbo(\\P.all x.(girl(x) -> P(x)),z1)\nbo(\\P.exists x.(dog(x) & P(x)),z2)\nFinally, we call s_retrieve() and check the readings.\n>>> cs_semrep.s_retrieve(trace=True)\nPermutation 1\n   (\\P.all x.(girl(x) -> P(x)))(\\z1.chase(z1,z2))\n   (\\P.exists x.(dog(x) & P(x)))(\\z2.all x.(girl(x) -> chase(x,z2)))\nPermutation 2\n   (\\P.exists x.(dog(x) & P(x)))(\\z2.chase(z1,z2))\n   (\\P.all x.(girl(x) -> P(x)))(\\z1.exists x.(dog(x) & chase(z1,x)))\n>>> for reading in cs_semrep.readings:\n...     print reading\nexists x.(dog(x) & all z3.(girl(z3) -> chase(z3,x)))\nall x.(girl(x) -> exists z4.(dog(z4) & chase(x,z4)))\n396 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n10.5  Discourse Semantics\nA discourse is \na sequence of sentences. Very often, the interpretation of a sentence in\na discourse depends on what preceded it. A clear example of this comes from anaphoric\npronouns, such as he, she, and it. Given a discourse such as Angus used to have a dog.\nBut he recently disappeared. , you will probably interpret he as referring to Angus\u2019s dog.\nHowever, in Angus used to have a dog. He took him for walks in New Town. , you are\nmore likely to interpret he as referring to Angus himself.\nDiscourse Representation Theory\nThe standard approach to quantification in first-order logic is limited to single senten-\nces. Yet there seem to be examples where the scope of a quantifier can extend over two\nor more sentences. We saw one earlier, and here\u2019s a second example, together with a\ntranslation.\n(54) a. Angus owns a dog. It bit Irene.\nb.\u2203x.(dog(x) & own(Angus, x) & bite(x, Irene))\nThat is, the NP a dog acts like a quantifier which binds the it in the second sentence.\nDiscourse Representation Theory (DRT) was developed with the specific goal of pro-\nviding a means for handling this and other semantic phenomena which seem to be\ncharacteristic of discourse. A discourse representation structure  (DRS) presents the\nmeaning of discourse in terms of a list of discourse referents and a list of conditions.\nThe discourse referents are the things under discussion in the discourse, and they\ncorrespond to the individual variables of first-order logic. The DRS conditions  apply\nto those discourse referents, and correspond to atomic open formulas of first-order\nlogic. Figure 10-4  illustrates how a DRS for the first sentence in (54a) is augmented to\nbecome a DRS for both sentences.\nWhen the second sentence of (54a) is processed, it is interpreted in the context of what\nis already present in the lefthand side of Figure 10-4 . The pronoun it triggers the addi-\ntion of a new discourse referent, say, u, and we need to find an anaphoric\nantecedent for it\u2014that is, we want to work out what it refers to. In DRT, the task of\nfinding the antecedent for an anaphoric pronoun involves linking it to a discourse ref-\nerent already within the current DRS, and y is the obvious choice. (We will say more\nabout anaphora resolution shortly.) This processing step gives rise to a new condition\nu = y. The remaining content contributed by the second sentence is also merged with\nthe content of the first, and this is shown on the righthand side of Figure 10-4.\nFigure 10-4 illustrates how a DRS can represent more than just a single sentence. In\nthis case, it is a two-sentence discourse, but in principle a single DRS could correspond\nto the interpretation of a whole text. We can inquire into the truth conditions of the\nrighthand DRS in Figure 10-4 . Informally, it is true in some situation s if there are\nentities a, c, and i in s corresponding to the discourse referents in the DRS such that\n10.5  Discourse Semantics | 397\nall the conditions are true in s; that is, a is named Angus, c is a dog, a owns c, i is named\nIrene, and c bit i.\nIn order to process DRSs computationally, we need to convert them into a linear format.\nHere\u2019s an example, where the DRS is a pair consisting of a list of discourse referents\nand a list of DRS conditions:\n([x, y], [angus(x), dog(y), own(x,y)])\nThe easiest way to build a DRS object in NLTK is by parsing a string representation \n .\n>>> dp = nltk.DrtParser()\n>>> drs1 = dp.parse('([x, y], [angus(x), dog(y), own(x, y)])') \n>>> print drs1\n([x,y],[angus(x), dog(y), own(x,y)])\nWe can use the draw() method \n  to visualize the result, as shown in Figure 10-5.\n>>> drs1.draw() \nFigure 10-5. DRS screenshot.\nWhen we discussed the truth conditions of the DRSs in Figure 10-4 , we assumed that\nthe topmost \ndiscourse referents were interpreted as existential quantifiers, while the\nFigure 10-4. Building a DRS: The DRS on the lefthand side represents the result of processing the first\nsentence in \nthe discourse, while the DRS on the righthand side shows the effect of processing the second\nsentence and integrating its content.\n398 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nconditions were interpreted as though they are conjoined. In fact, every DRS can be\ntranslated into \na formula of first-order logic, and the fol() method implements this\ntranslation.\n>>> print drs1.fol()\nexists x y.((angus(x) & dog(y)) & own(x,y))\nIn addition to the functionality available for first-order logic expressions, DRT\nExpressions have a DRS-concatenation operator, represented as the + symbol. The\nconcatenation of two DRSs is a single DRS containing the merged discourse referents\nand the conditions from both arguments. DRS-concatenation automatically \u03b1-converts\nbound variables to avoid name-clashes.\n>>> drs2 = dp.parse('([x], [walk(x)]) + ([y], [run(y)])')\n>>> print drs2\n(([x],[walk(x)]) + ([y],[run(y)]))\n>>> print drs2.simplify()\n([x,y],[walk(x), run(y)])\nWhile all the conditions seen so far have been atomic, it is possible to embed one DRS\nwithin another, and this is how universal quantification is handled. In drs3, there are\nno top-level discourse referents, and the sole condition is made up of two sub-DRSs,\nconnected by an implication. Again, we can use fol() to get a handle on the truth\nconditions.\n>>> drs3 = dp.parse('([], [(([x], [dog(x)]) -> ([y],[ankle(y), bite(x, y)]))])')\n>>> print drs3.fol()\nall x.(dog(x) -> exists y.(ankle(y) & bite(x,y)))\nWe pointed out earlier that DRT is designed to allow anaphoric pronouns to be inter-\npreted by linking to existing discourse referents. DRT sets constraints on which dis-\ncourse referents are \u201caccessible\u201d as possible antecedents, but is not intended to explain\nhow a particular antecedent is chosen from the set of candidates. The module\nnltk.sem.drt_resolve_anaphora adopts a similarly conservative strategy: if the DRS\ncontains a condition of the form PRO(x), the method resolve_anaphora() replaces this\nwith a condition of the form x = [...], where [...] is a list of possible antecedents.\n>>> drs4 = dp.parse('([x, y], [angus(x), dog(y), own(x, y)])')\n>>> drs5 = dp.parse('([u, z], [PRO(u), irene(z), bite(u, z)])')\n>>> drs6 = drs4 + drs5\n>>> print drs6.simplify()\n([x,y,u,z],[angus(x), dog(y), own(x,y), PRO(u), irene(z), bite(u,z)])\n>>> print drs6.simplify().resolve_anaphora()\n([x,y,u,z],[angus(x), dog(y), own(x,y), (u = [x,y,z]), irene(z), bite(u,z)])\nSince the algorithm for anaphora resolution has been separated into its own module,\nthis facilitates swapping in alternative procedures that try to make more intelligent\nguesses about the correct antecedent.\nOur treatment of DRSs is fully compatible with the existing machinery for handling \u03bb-\nabstraction, and consequently it is straightforward to build compositional semantic\nrepresentations that are based on DRT rather than first-order logic. This technique is\n10.5  Discourse Semantics | 399\nillustrated in the following rule for indefinites (which is part of the grammar drt.fcfg).\nFor ease \nof comparison, we have added the parallel rule for indefinites from simple-\nsem.fcfg.\nDet[NUM=sg,SEM=<\\P Q.([x],[]) + P(x) + Q(x)>] -> 'a'\nDet[NUM=sg,SEM=<\\P Q. exists x.(P(x) & Q(x))>] -> 'a'\nTo get a better idea of how the DRT rule works, look at this subtree for the NP a dog:\n(NP[NUM='sg', SEM=<\\Q.(([x],[dog(x)]) + Q(x))>]\n  (Det[NUM'sg', SEM=<\\P Q.((([x],[]) + P(x)) + Q(x))>] a)\n  (Nom[NUM='sg', SEM=<\\x.([],[dog(x)])>]\n    (N[NUM='sg', SEM=<\\x.([],[dog(x)])>] dog)))))\nThe \u03bb-abstract for the indefinite is applied as a function expression to \\x.([],\n[dog(x)]) which leads to \\Q.(([x],[]) + ([],[dog(x)]) + Q(x)) ; after simplification,\nwe get \\Q.(([x],[dog(x)]) + Q(x)) as the representation for the NP as a whole.\nIn order to parse with grammar drt.fcfg, we specify in the call to load_earley() that\nSEM values in feature structures are to be parsed using DrtParser in place of the default\nLogicParser.\n>>> from nltk import load_parser\n>>> parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.DrtParser())\n>>> trees = parser.nbest_parse('Angus owns a dog'.split())\n>>> print trees[0].node['sem'].simplify()\n([x,z2],[Angus(x), dog(z2), own(x,z2)])\nDiscourse Processing\nWhen we interpret a sentence, we use a rich context for interpretation, determined in\npart by the preceding context and in part by our background assumptions. DRT pro-\nvides a theory of how the meaning of a sentence is integrated into a representation of\nthe prior discourse, but two things have been glaringly absent from the processing\napproach just discussed. First, there has been no attempt to incorporate any kind of\ninference; and second, we have only processed individual sentences. These omissions\nare redressed by the module nltk.inference.discourse.\nWhereas a discourse is a sequence s1, ... sn of sentences, a discourse thread  is a sequence\ns1-ri, ... sn-rj of readings, one for each sentence in the discourse. The module processes\nsentences incrementally, keeping track of all possible threads when there is ambiguity.\nFor simplicity, the following example ignores scope ambiguity:\n>>> dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n>>> dt.readings()\ns0 readings: s0-r0: exists x.(student(x) & dance(x))\ns1 readings: s1-r0: all x.(student(x) -> person(x))\nWhen a new sentence is added to the current discourse, setting the parameter\nconsistchk=True causes consistency to be checked by invoking the model checker for\neach thread, i.e., each sequence of admissible readings. In this case, the user has the\noption of retracting the sentence in question.\n400 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n>>> dt.add_sentence('No person dances', consistchk=True)\nInconsistent discourse d0 ['s0-r0', 's1-r0', 's2-r0']:\ns0-r0: exists x.(student(x) & dance(x))\ns1-r0: all x.(student(x) -> person(x))\ns2-r0: -exists x.(person(x) & dance(x))\n>>> dt.retract_sentence('No person dances', verbose=True)\nCurrent sentences are\ns0: A student dances\ns1: Every student is a person\nIn a \nsimilar manner, we use informchk=True to check whether a new sentence \u03c6 is\ninformative relative to the current discourse. The theorem prover treats existing sen-\ntences in the thread as assumptions and attempts to prove \u03c6; it is informative if no such\nproof can be found.\n>>> dt.add_sentence('A person dances', informchk=True)\nSentence 'A person dances' under reading 'exists x.(person(x) & dance(x))':\nNot informative relative to thread 'd0'\nIt is also possible to pass in an additional set of assumptions as background knowledge\nand use these to filter out inconsistent readings; see the Discourse HOWTO at http://\nwww.nltk.org/howto for more details.\nThe discourse module can accommodate semantic ambiguity and filter out readings\nthat are not admissible. The following example invokes both Glue Semantics as well\nas DRT. Since the Glue Semantics module is configured to use the wide-coverage Malt\ndependency parser, the input ( Every dog chases a boy. He runs .) needs to be tagged as\nwell as tokenized.\n>>> from nltk.tag import RegexpTagger\n>>> tagger = RegexpTagger(\n...     [('^(chases|runs)$', 'VB'),\n...      ('^(a)$', 'ex_quant'),\n...      ('^(every)$', 'univ_quant'),\n...      ('^(dog|boy)$', 'NN'),\n...      ('^(He)$', 'PRP')\n... ])\n>>> rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n>>> dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n>>> dt.readings()\ns0 readings:\ns0-r0: ([],[(([x],[dog(x)]) -> ([z3],[boy(z3), chases(x,z3)]))]) \ns0-r1: ([z4],[boy(z4), (([x],[dog(x)]) -> ([],[chases(x,z4)]))])\ns1 readings:\ns1-r0: ([x],[PRO(x), runs(x)])\nThe first sentence of the discourse has two possible readings, depending on the quan-\ntifier scoping. The unique reading of the second sentence represents the pronoun He\nvia the condition PRO(x). Now let\u2019s look at the discourse threads that result:\n>>> dt.readings(show_thread_readings=True)\nd0: ['s0-r0', 's1-r0'] : INVALID: AnaphoraResolutionException\n10.5  Discourse Semantics | 401\nd1: ['s0-r1', 's1-r0'] : ([z6,z10],[boy(z6), (([x],[dog(x)]) ->\n([],[chases(x,z6)])), (z10 = z6), runs(z10)])\nWhen we \nexamine threads d0 and d1, we see that reading s0-r0, where every dog  out-\nscopes a boy, is deemed inadmissible because the pronoun in the second sentence\ncannot be resolved. By contrast, in thread d1 the pronoun (relettered to z10) has been\nbound via the equation (z10 = z6).\nInadmissible readings can be filtered out by passing the parameter filter=True.\n>>> dt.readings(show_thread_readings=True, filter=True)\nd1: ['s0-r1', 's1-r0'] : ([z12,z15],[boy(z12), (([x],[dog(x)]) ->\n([],[chases(x,z12)])), (z17 = z15), runs(z15)])\nAlthough this little discourse is extremely limited, it should give you a feel for the kind\nof semantic processing issues that arise when we go beyond single sentences, and also\na feel for the techniques that can be deployed to address them.\n10.6  Summary\n\u2022 First-order logic is a suitable language for representing natural language meaning\nin a computational setting since it is flexible enough to represent many useful as-\npects of natural meaning, and there are efficient theorem provers for reasoning with\nfirst-order logic. (Equally, there are a variety of phenomena in natural language\nsemantics which are believed to require more powerful logical mechanisms.)\n\u2022 As well as translating natural language sentences into first-order logic, we can state\nthe truth conditions of these sentences by examining models of first-order formu-\nlas.\n\u2022 In order to build meaning representations compositionally, we supplement first-\norder logic with the \u03bb-calculus.\n\u2022\u03b2-reduction in the \u03bb-calculus corresponds semantically to application of a function\nto an argument. Syntactically, it involves replacing a variable bound by \u03bb in the\nfunction expression with the expression that provides the argument in the function\napplication.\n\u2022 A key part of constructing a model lies in building a valuation which assigns in-\nterpretations to non-logical constants. These are interpreted as either n-ary predi-\ncates or as individual constants.\n\u2022 An open expression is an expression containing one or more free variables. Open\nexpressions receive an interpretation only when their free variables receive values\nfrom a variable assignment.\n\u2022 Quantifiers are interpreted by constructing, for a formula \u03c6[x] open in variable x,\nthe set of individuals which make \u03c6[x] true when an assignment g assigns them as\nthe value of x. The quantifier then places constraints on that set.\n402 | Chapter 10: \u2002Analyzing the Meaning of Sentences\n\u2022 A closed expression is one that has no free variables; that is, the variables are all\nbound. A closed sentence is true or false with respect to all variable assignments.\n\u2022\nIf two formulas differ only in the label of the variable bound by binding operator\n(i.e., \u03bb or a quantifier) , they are said to be \u03b1-equivalents. The result of relabeling\na bound variable in a formula is called \u03b1-conversion.\n\u2022 Given a formula with two nested quantifiers Q1 and Q2, the outermost quantifier\nQ1 is said to have wide scope (or scope over Q2). English sentences are frequently\nambiguous with respect to the scope of the quantifiers they contain.\n\u2022 English sentences can be associated with a semantic representation by treating\nSEM as a feature in a feature-based grammar. The SEM value of a complex expressions,\ntypically involves functional application of the SEM values of the component\nexpressions.\n10.7  Further Reading\nConsult http://www.nltk.org/ for further materials on this chapter and on how to install\nthe Prover9 theorem prover and Mace4 model builder. General information about these\ntwo inference tools is given by (McCune, 2008).\nFor more examples of semantic analysis with NLTK, please see the semantics and logic\nHOWTOs at http://www.nltk.org/howto. Note that there are implementations of two\nother approaches to scope ambiguity, namely Hole semantics  as described in (Black-\nburn & Bos, 2005), and Glue semantics, as described in (Dalrymple et al., 1999).\nThere are many phenomena in natural language semantics that have not been touched\non in this chapter, most notably:\n1. Events, tense, and aspect\n2. Semantic roles\n3. Generalized quantifiers, such as most\n4. Intensional constructions involving, for example, verbs such as may and believe\nWhile (1) and (2) can be dealt with using first-order logic, (3) and (4) require different\nlogics. These issues are covered by many of the references in the following readings.\nA comprehensive overview of results and techniques in building natural language front-\nends to databases can be found in (Androutsopoulos, Ritchie & Thanisch, 1995).\nAny introductory book to modern logic will present propositional and first-order logic.\n(Hodges, 1977) is highly recommended as an entertaining and insightful text with many\nillustrations from natural language.\nFor a wide-ranging, two-volume textbook on logic that also presents contemporary\nmaterial on the formal semantics of natural language, including Montague Grammar\nand intensional logic, see (Gamut, 1991a, 1991b). (Kamp & Reyle, 1993) provides the\n10.7  Further Reading | 403\ndefinitive account of Discourse Representation Theory, and covers a large and inter-\nesting fragment \nof natural language, including tense, aspect, and modality. Another\ncomprehensive study of the semantics of many natural language constructions is (Car-\npenter, 1997).\nThere are numerous works that introduce logical semantics within the framework of\nlinguistic theory. (Chierchia & McConnell-Ginet, 1990) is relatively agnostic about\nsyntax, while (Heim & Kratzer, 1998) and (Larson & Segal, 1995) are both more ex-\nplicitly oriented toward integrating truth-conditional semantics into a Chomskyan\nframework.\n(Blackburn & Bos, 2005) is the first textbook devoted to computational semantics, and\nprovides an excellent introduction to the area. It expands on many of the topics covered\nin this chapter, including underspecification of quantifier scope ambiguity, first-order\ninference, and discourse processing.\nTo gain an overview of more advanced contemporary approaches to semantics, in-\ncluding treatments of tense and generalized quantifiers, try consulting (Lappin, 1996)\nor (van Benthem & ter Meulen, 1997).\n10.8  Exercises\n1.\u25cb Translate the following sentences into propositional logic and verify that they\nparse with LogicParser. Provide a key that shows how the propositional variables\nin your translation correspond to expressions of English.\na. If Angus sings, it is not the case that Bertie sulks.\nb. Cyril runs and barks.\nc. It will snow if it doesn\u2019t rain.\nd. It\u2019s not the case that Irene will be happy if Olive or Tofu comes.\ne. Pat didn\u2019t cough or sneeze.\nf. If you don\u2019t come if I call, I won\u2019t come if you call.\n2.\u25cb Translate the following sentences into predicate-argument formulas of first-order\nlogic.\na. Angus likes Cyril and Irene hates Cyril.\nb. Tofu is taller than Bertie.\nc. Bruce loves himself and Pat does too.\nd. Cyril saw Bertie, but Angus didn\u2019t.\ne. Cyril is a four-legged friend.\nf. Tofu and Olive are near each other.\n3.\u25cb Translate the following sentences into quantified formulas of first-order logic.\na. Angus likes someone and someone likes Julia.\n404 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nb. Angus loves a dog who loves him.\nc. Nobody smiles at Pat.\nd.\nSomebody coughs and sneezes.\ne. Nobody coughed or sneezed.\nf. Bruce loves somebody other than Bruce.\ng. Nobody other than Matthew loves Pat.\nh. Cyril likes everyone except for Irene.\ni. Exactly one person is asleep.\n4.\u25cb Translate the following verb phrases using \u03bb-abstracts and quantified formulas\nof first-order logic.\na. feed Cyril and give a capuccino to Angus\nb. be given \u2018War and Peace\u2019 by Pat\nc. be loved by everyone\nd. be loved or detested by everyone\ne. be loved by everyone and detested by no-one\n5.\u25cb Consider the following statements:\n>>> lp = nltk.LogicParser()\n>>> e2 = lp.parse('pat')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\nexists y.love(pat, y)\nClearly something is missing here, namely a declaration of the value of e1. In order\nfor ApplicationExpression(e1, e2)  to be \u03b2-convertible to exists y.love(pat, y) ,\ne1 must be a \u03bb-abstract which can take pat as an argument. Your task is to construct\nsuch an abstract, bind it to e1, and satisfy yourself that these statements are all\nsatisfied (up to alphabetic variance). In addition, provide an informal English\ntranslation of e3.simplify().\nNow carry on doing this same task for the further cases of e3.simplify() shown\nhere:\n>>> print e3.simplify()\nexists y.(love(pat,y) | love(y,pat))\n>>> print e3.simplify()\nexists y.(love(pat,y) | love(y,pat))\n>>> print e3.simplify()\nwalk(fido)\n6.\u25cb As in the preceding exercise, find a \u03bb-abstract e1 that yields results equivalent to\nthose shown here:\n>>> e2 = lp.parse('chase')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n10.8  Exercises | 405\n>>> print e3.simplify()\n\\x.all y.(dog(y) -> chase(x,pat))\n>>> e2 = lp.parse('chase')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\n\\x.exists y.(dog(y) & chase(pat,x))\n>>> e2 = lp.parse('give')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\n\\x0 x1.exists y.(present(y) & give(x1,y,x0))\n7.\u25cb As in \nthe preceding exercise, find a \u03bb-abstract e1 that yields results equivalent to\nthose shown here:\n>>> e2 = lp.parse('bark')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\nexists y.(dog(x) & bark(x))\n>>> e2 = lp.parse('bark')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\nbark(fido)\n>>> e2 = lp.parse('\\\\P. all x. (dog(x) -> P(x))')\n>>> e3 = nltk.ApplicationExpression(e1, e2)\n>>> print e3.simplify()\nall x.(dog(x) -> bark(x))\n8.\u25d1 Develop a method for translating English sentences into formulas with binary\ngeneralized quantifiers . In such an approach, given a generalized quantifier Q, a\nquantified formula is of the form Q(A, B), where both A and B are expressions of\ntype \u2329e, t\u232a. Then, for example, all(A, B) is true iff A denotes a subset of what B\ndenotes.\n9.\u25d1 Extend the approach in the preceding exercise so that the truth conditions for\nquantifiers such as most and exactly three can be computed in a model.\n10.\u25d1 Modify the sem.evaluate code so that it will give a helpful error message if an\nexpression is not in the domain of a model\u2019s valuation function.\n11.\u25cf Select three or four contiguous sentences from a book for children. A possible\nsource of examples are the collections of stories in nltk.corpus.gutenberg: bryant-\nstories.txt, burgess-busterbrown.txt, and edgeworth-parents.txt. Develop a\ngrammar that will allow your sentences to be translated into first-order logic, and\nbuild a model that will allow those translations to be checked for truth or falsity.\n12.\u25cf Carry out the preceding exercise, but use DRT as the meaning representation.\n13.\u25cf Taking (Warren & Pereira, 1982) as a starting point, develop a technique for\nconverting a natural language query into a form that can be evaluated more effi-\nciently in a model. For example, given a query of the form (P(x) & Q(x)) , convert\nit to (Q(x) & P(x)) if the extension of Q is smaller than the extension of P.\n406 | Chapter 10: \u2002Analyzing the Meaning of Sentences\nCHAPTER 11\nManaging Linguistic Data\nStructured collections of annotated linguistic data are essential in most areas of NLP;\nhowever, we \nstill face many obstacles in using them. The goal of this chapter is to answer\nthe following questions:\n1. How do we design a new language resource and ensure that its coverage, balance,\nand documentation support a wide range of uses?\n2. When existing data is in the wrong format for some analysis tool, how can we\nconvert it to a suitable format?\n3. What is a good way to document the existence of a resource we have created so\nthat others can easily find it?\nAlong the way, we will study the design of existing corpora, the typical workflow for\ncreating a corpus, and the life cycle of a corpus. As in other chapters, there will be many\nexamples drawn from practical experience managing linguistic data, including data\nthat has been collected in the course of linguistic fieldwork, laboratory work, and web\ncrawling.\n11.1  Corpus Structure: A Case Study\nThe TIMIT Corpus was the first annotated speech database to be widely distributed,\nand it has an especially clear organization. TIMIT was developed by a consortium in-\ncluding Texas Instruments and MIT, from which it derives its name. It was designed\nto provide data for the acquisition of acoustic-phonetic knowledge and to support the\ndevelopment and evaluation of automatic speech recognition systems.\nThe Structure of TIMIT\nLike the Brown Corpus, which displays a balanced selection of text genres and sources,\nTIMIT includes a balanced selection of dialects, speakers, and materials. For each of\neight dialect regions, 50 male and female speakers having a range of ages and educa-\ntional backgrounds each read 10 carefully chosen sentences. Two sentences, read by\nall speakers, were designed to bring out dialect variation:\n407\n(1) a. she had your dark suit in greasy wash water all year\nb. don\u2019t ask me to carry an oily rag like that\nThe \nremaining sentences were chosen to be phonetically rich, involving all phones\n(sounds) and a comprehensive range of diphones (phone bigrams). Additionally, the\ndesign strikes a balance between multiple speakers saying the same sentence in order\nto permit comparison across speakers, and having a large range of sentences covered\nby the corpus to get maximal coverage of diphones. Five of the sentences read by each\nspeaker are also read by six other speakers (for comparability). The remaining three\nsentences read by each speaker were unique to that speaker (for coverage).\nNLTK includes a sample from the TIMIT Corpus. You can access its documentation\nin the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids()\nto see a list of the 160 recorded utterances in the corpus sample. Each filename has\ninternal structure, as shown in Figure 11-1.\nFigure 11-1. Structure of a TIMIT identifier: Each recording is labeled using a string made up of the\nspeaker\u2019s dialect region, gender, speaker identifier, sentence type, and sentence identifier.\nEach item \nhas a phonetic transcription which can be accessed using the phones() meth-\nod. We can access the corresponding word tokens in the customary way. Both access\nmethods permit an optional argument offset=True, which includes the start and end\noffsets of the corresponding span in the audio file.\n>>> phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n>>> phonetic\n['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl',\n408 | Chapter 11: \u2002Managing Linguistic Data\n's', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa',\n'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n>>> nltk.corpus.timit.word_times('dr1-fvmh0/sa1')\n[('she', 7812, 10610), ('had', 10610, 14496), ('your', 14496, 15791),\n('dark', 15791, 20720), ('suit', 20720, 25647), ('in', 25647, 26906),\n('greasy', 26906, 32668), ('wash', 32668, 37890), ('water', 38531, 42417),\n('all', 43091, 46052), ('year', 46052, 50522)]\nIn addition \nto this text data, TIMIT includes a lexicon that provides the canonical\npronunciation of every word, which can be compared with a particular utterance:\n>>> timitdict = nltk.corpus.timit.transcription_dict()\n>>> timitdict['greasy'] + timitdict['wash'] + timitdict['water']\n['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']\n>>> phonetic[17:30]\n['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']\nThis gives us a sense of what a speech processing system would have to do in producing\nor recognizing speech in this particular dialect (New England). Finally, TIMIT includes\ndemographic data about the speakers, permitting fine-grained study of vocal, social,\nand gender characteristics.\n>>> nltk.corpus.timit.spkrinfo('dr1-fvmh0')\nSpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86',\nbirthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS',\ncomments='BEST NEW ENGLAND ACCENT SO FAR')\nNotable Design Features\nTIMIT illustrates several key features of corpus design. First, the corpus contains two\nlayers of annotation, at the phonetic and orthographic levels. In general, a text or speech\ncorpus may be annotated at many different linguistic levels, including morphological,\nsyntactic, and discourse levels. Moreover, even at a given level there may be different\nlabeling schemes or even disagreement among annotators, such that we want to rep-\nresent multiple versions. A second property of TIMIT is its balance across multiple\ndimensions of variation, for coverage of dialect regions and diphones. The inclusion of\nspeaker demographics brings in many more independent variables that may help to\naccount for variation in the data, and which facilitate later uses of the corpus for pur-\nposes that were not envisaged when the corpus was created, such as sociolinguistics.\nA third property is that there is a sharp division between the original linguistic event\ncaptured as an audio recording and the annotations of that event. The same holds true\nof text corpora, in the sense that the original text usually has an external source, and\nis considered to be an immutable artifact. Any transformations of that artifact which\ninvolve human judgment\u2014even something as simple as tokenization\u2014are subject to\nlater revision; thus it is important to retain the source material in a form that is as close\nto the original as possible.\nA fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per\nsentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These are\norganized into a tree structure, shown schematically in Figure 11-2 . At the top level\n11.1  Corpus Structure: A Case Study | 409\nthere is a split between training and testing sets, which gives away its intended use for\ndeveloping and evaluating statistical models.\nFinally, notice \nthat even though TIMIT is a speech corpus, its transcriptions and asso-\nciated data are just text, and can be processed using programs just like any other text\ncorpus. Therefore, many of the computational methods described in this book are ap-\nplicable. Moreover, notice that all of the data types included in the TIMIT Corpus fall\ninto the two basic categories of lexicon and text, which we will discuss later. Even the\nspeaker demographics data is just another instance of the lexicon data type.\nThis last observation is less surprising when we consider that text and record structures\nare the primary domains for the two subfields of computer science that focus on data\nmanagement, namely text retrieval and databases. A notable feature of linguistic data\nmanagement is that it usually brings both data types together, and that it can draw on\nresults and techniques from both fields.\nFigure 11-2. Structure of the published TIMIT Corpus: The CD-ROM contains doc, train, and test\ndirectories \nat the top level; the train and test directories both have eight sub-directories, one per dialect\nregion; each of these contains further subdirectories, one per speaker; the contents of the directory for\nfemale speaker aks0 are listed, showing 10 wav files accompanied by a text transcription, a word-\naligned transcription, and a phonetic transcription.\n410 | Chapter 11: \u2002Managing Linguistic Data\nFundamental Data Types\nDespite its \ncomplexity, the TIMIT Corpus contains only two fundamental data types,\nnamely lexicons and texts. As we saw in Chapter 2, most lexical resources can be rep-\nresented using a record structure, i.e., a key plus one or more fields, as shown in\nFigure 11-3. A lexical resource could be a conventional dictionary or comparative\nwordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a phrase\nrather than a single word. A thesaurus also consists of record-structured data, where\nwe look up entries via non-key fields that correspond to topics. We can also construct\nspecial tabulations (known as paradigms) to illustrate contrasts and systematic varia-\ntion, as shown in Figure 11-3  for three verbs. TIMIT\u2019s speaker table is also a kind of\nlexicon.\nFigure 11-3. Basic linguistic data types\u2014lexicons and texts: Amid their diversity, lexicons have a\nrecord structure, whereas annotated texts have a temporal organization.\nAt the most abstract level, a text is a representation of a real or fictional speech event,\nand the \ntime-course of that event carries over into the text itself. A text could be a small\nunit, such as a word or sentence, or a complete narrative or dialogue. It may come with\nannotations such as part-of-speech tags, morphological analysis, discourse structure,\nand so forth. As we saw in the IOB tagging technique (Chapter 7), it is possible to\nrepresent higher-level constituents using tags on individual words. Thus the abstraction\nof text shown in Figure 11-3 is sufficient.\n11.1  Corpus Structure: A Case Study | 411\nDespite the complexities and idiosyncrasies of individual corpora, at base they are col-\nlections of \ntexts together with record-structured data. The contents of a corpus are\noften biased toward one or the other of these types. For example, the Brown Corpus\ncontains 500 text files, but we still use a table to relate the files to 15 different genres.\nAt the other end of the spectrum, WordNet contains 117,659 synset records, yet it\nincorporates many example sentences (mini-texts) to illustrate word usages. TIMIT is\nan interesting midpoint on this spectrum, containing substantial free-standing material\nof both the text and lexicon types.\n11.2  The Life Cycle of a Corpus\nCorpora are not born fully formed, but involve careful preparation and input from\nmany people over an extended period. Raw data needs to be collected, cleaned up,\ndocumented, and stored in a systematic structure. Various layers of annotation might\nbe applied, some requiring specialized knowledge of the morphology or syntax of the\nlanguage. Success at this stage depends on creating an efficient workflow involving\nappropriate tools and format converters. Quality control procedures can be put in place\nto find inconsistencies in the annotations, and to ensure the highest possible level of\ninter-annotator agreement. Because of the scale and complexity of the task, large cor-\npora may take years to prepare, and involve tens or hundreds of person-years of effort.\nIn this section, we briefly review the various stages in the life cycle of a corpus.\nThree Corpus Creation Scenarios\nIn one type of corpus, the design unfolds over in the course of the creator\u2019s explorations.\nThis is the pattern typical of traditional \u201cfield linguistics,\u201d in which material from elic-\nitation sessions is analyzed as it is gathered, with tomorrow\u2019s elicitation often based on\nquestions that arise in analyzing today\u2019s. The resulting corpus is then used during sub-\nsequent years of research, and may serve as an archival resource indefinitely. Comput-\nerization is an obvious boon to work of this type, as exemplified by the popular program\nShoebox, now over two decades old and re-released as Toolbox (see Section 2.4). Other\nsoftware tools, even simple word processors and spreadsheets, are routinely used to\nacquire the data. In the next section, we will look at how to extract data from these\nsources.\nAnother corpus creation scenario is typical of experimental research where a body of\ncarefully designed material is collected from a range of human subjects, then analyzed\nto evaluate a hypothesis or develop a technology. It has become common for such\ndatabases to be shared and reused within a laboratory or company, and often to be\npublished more widely. Corpora of this type are the basis of the \u201ccommon task\u201d method\nof research management, which over the past two decades has become the norm in\ngovernment-funded research programs in language technology. We have already en-\ncountered many such corpora in the earlier chapters; we will see how to write Python\n412 | Chapter 11: \u2002Managing Linguistic Data\nprograms to implement the kinds of curation tasks that are necessary before such cor-\npora are published.\nFinally, there are efforts to gather a \u201creference corpus\u201d for a particular language, such\nas the American National Corpus  (ANC) \nand the British National Corpus  (BNC). Here\nthe goal has been to produce a comprehensive record of the many forms, styles, and\nuses of a language. Apart from the sheer challenge of scale, there is a heavy reliance on\nautomatic annotation tools together with post-editing to fix any errors. However, we\ncan write programs to locate and repair the errors, and also to analyze the corpus for\nbalance.\nQuality Control\nGood tools for automatic and manual preparation of data are essential. However, the\ncreation of a high-quality corpus depends just as much on such mundane things as\ndocumentation, training, and workflow. Annotation guidelines define the task and\ndocument the markup conventions. They may be regularly updated to cover difficult\ncases, along with new rules that are devised to achieve more consistent annotations.\nAnnotators need to be trained in the procedures, including methods for resolving cases\nnot covered in the guidelines. A workflow needs to be established, possibly with sup-\nporting software, to keep track of which files have been initialized, annotated, validated,\nmanually checked, and so on. There may be multiple layers of annotation, provided by\ndifferent specialists. Cases of uncertainty or disagreement may require adjudication.\nLarge annotation tasks require multiple annotators, which raises the problem of\nachieving consistency. How consistently can a group of annotators perform? We can\neasily measure consistency by having a portion of the source material independently\nannotated by two people. This may reveal shortcomings in the guidelines or differing\nabilities with the annotation task. In cases where quality is paramount, the entire corpus\ncan be annotated twice, and any inconsistencies adjudicated by an expert.\nIt is considered best practice to report the inter-annotator agreement that was achieved\nfor a corpus (e.g., by double-annotating 10% of the corpus). This score serves as a\nhelpful upper bound on the expected performance of any automatic system that is\ntrained on this corpus.\nCaution!\nCare should \nbe exercised when interpreting an inter-annotator agree-\nment score, since annotation tasks vary greatly in their difficulty. For\nexample, 90% agreement would be a terrible score for part-of-speech\ntagging, but an exceptional score for semantic role labeling.\nThe Kappa coefficient \u03ba measures agreement between two people making category\njudgments, correcting for expected chance agreement. For example, suppose an item\nis to be annotated, and four coding options are equally likely. In this case, two people\ncoding randomly would be expected to agree 25% of the time. Thus, an agreement of\n11.2  The Life Cycle of a Corpus | 413\n25% will be assigned \u03ba = 0, and better levels of agreement will be scaled accordingly.\nFor an agreement of 50%, we would get \u03ba = 0.333, as 50 is a third of the way from 25\nto 100. Many other agreement measures exist; see help(nltk.metrics.agreement) for\ndetails.\nWe can also measure the agreement between two independent segmentations of lan-\nguage input, e.g., for tokenization, sentence segmentation, and named entity recogni-\ntion. In Figure 11-4  we see three possible segmentations of a sequence of items which\nmight have been produced by annotators (or programs). Although none of them agree\nexactly, S 1 and S 2 are in close agreement, and we would like a suitable measure. Win-\ndowdiff is a simple algorithm for evaluating the agreement of two segmentations by\nrunning a sliding window over the data and awarding partial credit for near misses. If\nwe preprocess our tokens into a sequence of zeros and ones, to record when a token is\nfollowed by a boundary, we can represent the segmentations as strings and apply the\nwindowdiff scorer.\n>>> s1 = \"00000010000000001000000\"\n>>> s2 = \"00000001000000010000000\"\n>>> s3 = \"00010000000000000001000\"\n>>> nltk.windowdiff(s1, s1, 3)\n0\n>>> nltk.windowdiff(s1, s2, 3)\n4\n>>> nltk.windowdiff(s2, s3, 3)\n16\nIn this example, the window had a size of 3. The windowdiff computation slides this\nwindow across a pair of strings. At each position it totals up the number of boundaries\nfound inside this window, for both strings, then computes the difference. These dif-\nferences are then summed. We can increase or shrink the window size to control the\nsensitivity of the measure.\nCuration Versus Evolution\nAs large corpora are published, researchers are increasingly likely to base their inves-\ntigations on balanced, focused subsets that were derived from corpora produced for\nFigure 11-4. Three segmentations of a sequence: The small rectangles represent characters, words,\nsentences, in \nshort, any sequence which might be divided into linguistic units; S 1 and S 2 are in close\nagreement, but both differ significantly from S 3.\n414 | Chapter 11: \u2002Managing Linguistic Data\nentirely different reasons. For instance, the Switchboard database, originally collected\nfor speaker \nidentification research, has since been used as the basis for published studies\nin speech recognition, word pronunciation, disfluency, syntax, intonation, and dis-\ncourse structure. The motivations for recycling linguistic corpora include the desire to\nsave time and effort, the desire to work on material available to others for replication,\nand sometimes a desire to study more naturalistic forms of linguistic behavior than\nwould be possible otherwise. The process of choosing a subset for such a study may\ncount as a non-trivial contribution in itself.\nIn addition to selecting an appropriate subset of a corpus, this new work could involve\nreformatting a text file (e.g., converting to XML), renaming files, retokenizing the text,\nselecting a subset of the data to enrich, and so forth. Multiple research groups might\ndo this work independently, as illustrated in Figure 11-5 . At a later date, should some-\none want to combine sources of information from different versions, the task will\nprobably be extremely onerous.\nFigure 11-5. Evolution of a corpus over time: After a corpus is published, research groups will use it\nindependently, selecting \nand enriching different pieces; later research that seeks to integrate separate\nannotations confronts the difficult challenge of aligning the annotations.\nThe task of using derived corpora is made even more difficult by the lack of any record\nabout how the derived version was created, and which version is the most up-to-date.\nAn alternative to this chaotic situation is for a corpus to be centrally curated, and for\ncommittees of experts to revise and extend it at periodic intervals, considering sub-\nmissions from third parties and publishing new releases from time to time. Print dic-\ntionaries and national corpora may be centrally curated in this way. However, for most\ncorpora this model is simply impractical.\nA middle course is for the original corpus publication to have a scheme for identifying\nany sub-part. Each sentence, tree, or lexical entry could have a globally unique identi-\nfier, and each token, node, or field (respectively) could have a relative offset. Annota-\ntions, including segmentations, could reference the source using this identifier scheme\n(a method which is known as standoff annotation). This way, new annotations could\nbe distributed independently of the source, and multiple independent annotations of\nthe same source could be compared and updated without touching the source.\nIf the corpus publication is provided in multiple versions, the version number or date\ncould be part of the identification scheme. A table of correspondences between\n11.2  The Life Cycle of a Corpus | 415\nidentifiers across editions of the corpus would permit any standoff annotations to be\nupdated easily.\nCaution!\nSometimes an \nupdated corpus contains revisions of base material that\nhas been externally annotated. Tokens might be split or merged, and\nconstituents may have been rearranged. There may not be a one-to-one\ncorrespondence between old and new identifiers. It is better to cause\nstandoff annotations to break on such components of the new version\nthan to silently allow their identifiers to refer to incorrect locations.\n11.3  Acquiring Data\nObtaining Data from the Web\nThe Web is a rich source of data for language analysis purposes. We have already\ndiscussed methods for accessing individual files, RSS feeds, and search engine results\n(see Section 3.1 ). However, in some cases we want to obtain large quantities of web text.\nThe simplest approach is to obtain a published corpus of web text. The ACL Special\nInterest Group on Web as Corpus (SIGWAC) maintains a list of resources at http://\nwww.sigwac.org.uk/. The advantage of using a well-defined web corpus is that they are\ndocumented, stable, and permit reproducible experimentation.\nIf the desired content is localized to a particular website, there are many utilities for\ncapturing all the accessible contents of a site, such as GNU Wget ( http://www.gnu.org/\nsoftware/wget/). For maximal flexibility and control, a web crawler can be used, such\nas Heritrix ( http://crawler.archive.org/). Crawlers permit fine-grained control over\nwhere to look, which links to follow, and how to organize the results. For example, if\nwe want to compile a bilingual text collection having corresponding pairs of documents\nin each language, the crawler needs to detect the structure of the site in order to extract\nthe correspondence between the documents, and it needs to organize the downloaded\npages in such a way that the correspondence is captured. It might be tempting to write\nyour own web crawler, but there are dozens of pitfalls having to do with detecting\nMIME types, converting relative to absolute URLs, avoiding getting trapped in cyclic\nlink structures, dealing with network latencies, avoiding overloading the site or being\nbanned from accessing the site, and so on.\nObtaining Data from Word Processor Files\nWord processing software is often used in the manual preparation of texts and lexicons\nin projects that have limited computational infrastructure. Such projects often provide\ntemplates for data entry, though the word processing software does not ensure that the\ndata is correctly structured. For example, each text may be required to have a title and\ndate. Similarly, each lexical entry may have certain obligatory fields. As the data grows\n416 | Chapter 11: \u2002Managing Linguistic Data\nin size and complexity, a larger proportion of time may be spent maintaining its con-\nsistency.\nHow can \nwe extract the content of such files so that we can manipulate it in external\nprograms? Moreover, how can we validate the content of these files to help authors\ncreate well-structured data, so that the quality of the data can be maximized in the\ncontext of the original authoring process?\nConsider a dictionary in which each entry has a part-of-speech field, drawn from a set\nof 20 possibilities, displayed after the pronunciation field, and rendered in 11-point\nbold type. No conventional word processor has search or macro functions capable of\nverifying that all part-of-speech fields have been correctly entered and displayed. This\ntask requires exhaustive manual checking. If the word processor permits the document\nto be saved in a non-proprietary format, such as text, HTML, or XML, we can some-\ntimes write programs to do this checking automatically.\nConsider the following fragment of a lexical entry: \u201csleep [sli:p] v.i. condition of body\nand mind... \u201d. We can key in such text using MSWord, then \u201cSave as Web Page,\u201d then\ninspect the resulting HTML file:\n<p class=MsoNormal>sleep\n  <span style='mso-spacerun:yes'> </span>\n  [<span class=SpellE>sli:p</span>]\n  <span style='mso-spacerun:yes'> </span>\n  <b><span style='font-size:11.0pt'>v.i.</span></b>\n  <span style='mso-spacerun:yes'> </span>\n  <i>a condition of body and mind ...<o:p></o:p></i>\n</p>\nObserve that the entry is represented as an HTML paragraph, using the <p> element,\nand that the part of speech appears inside a <span style='font-size:11.0pt'> element.\nThe following program defines the set of legal parts-of-speech, legal_pos. Then it ex-\ntracts all 11-point content from the dict.htm file and stores it in the set used_pos. Observe\nthat the search pattern contains a parenthesized sub-expression; only the material that\nmatches this subexpression is returned by re.findall. Finally, the program constructs\nthe set of illegal parts-of-speech as the set difference between used_pos and legal_pos:\n>>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n>>> pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n>>> document = open(\"dict.htm\").read()\n>>> used_pos = set(re.findall(pattern, document))\n>>> illegal_pos = used_pos.difference(legal_pos)\n>>> print list(illegal_pos)\n['v.i', 'intrans']\nThis simple program represents the tip of the iceberg. We can develop sophisticated\ntools to check the consistency of word processor files, and report errors so that the\nmaintainer of the dictionary can correct the original file using the original word\nprocessor.\n11.3  Acquiring Data | 417\nOnce we know the data is correctly formatted, we can write other programs to convert\nthe data \ninto a different format. The program in Example 11-1 strips out the HTML\nmarkup using nltk.clean_html(), extracts the words and their pronunciations, and\ngenerates output in \u201ccomma-separated value\u201d (CSV) format.\nExample 11-1. Converting HTML created by Microsoft Word into comma-separated values.\ndef lexical_data(html_file):\n    SEP = '_ENTRY'\n    html = open(html_file).read()\n    html = re.sub(r'<p', SEP + '<p', html)\n    text = nltk.clean_html(html)\n    text = ' '.join(text.split())\n    for entry in text.split(SEP):\n        if entry.count(' ') > 2:\n            yield entry.split(' ', 3)\n>>> import csv\n>>> writer = csv.writer(open(\"dict1.csv\", \"wb\"))\n>>> writer.writerows(lexical_data(\"dict.htm\"))\nObtaining Data from Spreadsheets and Databases\nSpreadsheets are often used for acquiring wordlists or paradigms. For example, a com-\nparative wordlist may be created using a spreadsheet, with a row for each cognate set\nand a column for each language (see nltk.corpus.swadesh and www.rosettapro\nject.org). Most spreadsheet software can export their data in CSV format. As we will\nsee later, it is easy for Python programs to access these using the csv module.\nSometimes lexicons are stored in a full-fledged relational database. When properly\nnormalized, these databases can ensure the validity of the data. For example, we can\nrequire that all parts-of-speech come from a specified vocabulary by declaring that the\npart-of-speech field is an enumerated type  or a foreign key that references a separate\npart-of-speech table. However, the relational model requires the structure of the data\n(the schema) be declared in advance, and this runs counter to the dominant approach\nto structuring linguistic data, which is highly exploratory. Fields which were assumed\nto be obligatory and unique often turn out to be optional and repeatable. A relational\ndatabase can accommodate this when it is fully known in advance; however, if it is not,\nor if just about every property turns out to be optional or repeatable, the relational\napproach is unworkable.\nNevertheless, when our goal is simply to extract the contents from a database, it is\nenough to dump out the tables (or SQL query results) in CSV format and load them\ninto our program. Our program might perform a linguistically motivated query that\ncannot easily be expressed in SQL, e.g., select all words that appear in example sentences\nfor which no dictionary entry is provided . For this task, we would need to extract enough\ninformation from a record for it to be uniquely identified, along with the headwords\nand example sentences. Let\u2019s suppose this information was now available in a CSV file\ndict.csv:\n418 | Chapter 11: \u2002Managing Linguistic Data\n\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\nNow we can express this query as shown here:\n>>> import csv\n>>> lexicon = csv.reader(open('dict.csv'))\n>>> pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n>>> lexemes, defns = zip(*pairs)\n>>> defn_words = set(w for defn in defns for w in defn.split())\n>>> sorted(defn_words.difference(lexemes))\n['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',\n'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']\nThis information \nwould then guide the ongoing work to enrich the lexicon, work that\nupdates the content of the relational database.\nConverting Data Formats\nAnnotated linguistic data rarely arrives in the most convenient format, and it is often\nnecessary to perform various kinds of format conversion. Converting between character\nencodings has already been discussed (see Section 3.3 ). Here we focus on the structure\nof the data.\nIn the simplest case, the input and output formats are isomorphic. For instance, we\nmight be converting lexical data from Toolbox format to XML, and it is straightforward\nto transliterate the entries one at a time ( Section 11.4). The structure of the data is\nreflected in the structure of the required program: a for loop whose body takes care of\na single entry.\nIn another common case, the output is a digested form of the input, such as an inverted\nfile index. Here it is necessary to build an index structure in memory (see Example 4.8),\nthen write it to a file in the desired format. The following example constructs an index\nthat maps the words of a dictionary definition to the corresponding lexeme \n  for each\nlexical entry \n , having tokenized the definition text \n , and discarded short words \n .\nOnce the \nindex has been constructed, we open a file and then iterate over the index\nentries, to write out the lines in the required format \n .\n>>> idx = nltk.Index((defn_word, lexeme) \n...                  for (lexeme, defn) in pairs \n...                  for defn_word in nltk.word_tokenize(defn) \n...                  if len(defn_word) > 3) \n>>> idx_file = open(\"dict.idx\", \"w\")\n>>> for word in sorted(idx):\n...     idx_words = ', '.join(idx[word])\n...     idx_line = \"%s: %s\\n\" % (word, idx_words) \n...     idx_file.write(idx_line)\n>>> idx_file.close()\nThe resulting \nfile dict.idx contains the following lines. (With a larger dictionary, we\nwould expect to find multiple lexemes listed for each index entry.)\n11.3  Acquiring Data | 419\nbody: sleep\ncease: wake\ncondition: sleep\ndown: walk\neach: walk\nfoot: walk\nlifting: walk\nmind: sleep\nprogress: walk\nsetting: walk\nsleep: wake\nIn some \ncases, the input and output data both consist of two or more dimensions. For\ninstance, the input might be a set of files, each containing a single column of word\nfrequency data. The required output might be a two-dimensional table in which the\noriginal columns appear as rows. In such cases we populate an internal data structure\nby filling up one column at a time, then read off the data one row at a time as we write\ndata to the output file.\nIn the most vexing cases, the source and target formats have slightly different coverage\nof the domain, and information is unavoidably lost when translating between them.\nFor example, we could combine multiple Toolbox files to create a single CSV file con-\ntaining a comparative wordlist, losing all but the \\lx field of the input files. If the CSV\nfile was later modified, it would be a labor-intensive process to inject the changes into\nthe original Toolbox files. A partial solution to this \u201cround-tripping\u201d problem is to\nassociate explicit identifiers with each linguistic object, and to propagate the identifiers\nwith the objects.\nDeciding Which Layers of Annotation to Include\nPublished corpora vary greatly in the richness of the information they contain. At a\nminimum, a corpus will typically contain at least a sequence of sound or orthographic\nsymbols. At the other end of the spectrum, a corpus could contain a large amount of\ninformation about the syntactic structure, morphology, prosody, and semantic content\nof every sentence, plus annotation of discourse relations or dialogue acts. These extra\nlayers of annotation may be just what someone needs for performing a particular data\nanalysis task. For example, it may be much easier to find a given linguistic pattern if\nwe can search for specific syntactic structures; and it may be easier to categorize a\nlinguistic pattern if every word has been tagged with its sense. Here are some commonly\nprovided annotation layers:\nWord tokenization\nThe orthographic form of text does not unambiguously identify its tokens. A to-\nkenized and normalized version, in addition to the conventional orthographic ver-\nsion, may be a very convenient resource.\nSentence segmentation\nAs we saw in Chapter 3 , sentence segmentation can be more difficult than it seems.\nSome corpora therefore use explicit annotations to mark sentence segmentation.\n420 | Chapter 11: \u2002Managing Linguistic Data\nParagraph segmentation\nParagraphs and \nother structural elements (headings, chapters, etc.) may be explic-\nitly annotated.\nPart-of-speech\nThe syntactic category of each word in a document.\nSyntactic structure\nA tree structure showing the constituent structure of a sentence.\nShallow semantics\nNamed entity and coreference annotations, and semantic role labels.\nDialogue and discourse\nDialogue act tags and rhetorical structure.\nUnfortunately, there is not much consistency between existing corpora in how they\nrepresent their annotations. However, two general classes of annotation representation\nshould be distinguished. Inline annotation  modifies the original document by insert-\ning special symbols or control sequences that carry the annotated information. For\nexample, when part-of-speech tagging a document, the string \"fly\" might be replaced\nwith the string \"fly/NN\", to indicate that the word fly is a noun in this context. In\ncontrast, standoff annotation does not modify the original document, but instead\ncreates a new file that adds annotation information using pointers that reference the\noriginal document. For example, this new document might contain the string \"<token\nid=8 pos='NN'/>\", to indicate that token 8 is a noun.\nStandards and Tools\nFor a corpus to be widely useful, it needs to be available in a widely supported format.\nHowever, the cutting edge of NLP research depends on new kinds of annotations,\nwhich by definition are not widely supported. In general, adequate tools for creation,\npublication, and use of linguistic data are not widely available. Most projects must\ndevelop their own set of tools for internal use, which is no help to others who lack the\nnecessary resources. Furthermore, we do not have adequate, generally accepted stand-\nards for expressing the structure and content of corpora. Without such standards, gen-\neral-purpose tools are impossible\u2014though at the same time, without available tools,\nadequate standards are unlikely to be developed, used, and accepted.\nOne response to this situation has been to forge ahead with developing a generic format\nthat is sufficiently expressive to capture a wide variety of annotation types (see Sec-\ntion 11.8  for examples). The challenge for NLP is to write programs that cope with the\ngenerality of such formats. For example, if the programming task involves tree data,\nand the file format permits arbitrary directed graphs, then input data must be validated\nto check for tree properties such as rootedness, connectedness, and acyclicity. If the\ninput files contain other layers of annotation, the program would need to know how\nto ignore them when the data was loaded, but not invalidate or obliterate those layers\nwhen the tree data was saved back to the file.\n11.3  Acquiring Data | 421\nAnother response has been to write one-off scripts to manipulate corpus formats; such\nscripts litter \nthe filespaces of many NLP researchers. NLTK\u2019s corpus readers are a more\nsystematic approach, founded on the premise that the work of parsing a corpus format\nshould be done only once (per programming language).\nInstead of focusing on a common format, we believe it is more promising to develop a\ncommon interface (see nltk.corpus). Consider the case of treebanks, an important\ncorpus type for work in NLP. There are many ways to store a phrase structure tree in\na file. We can use nested parentheses, or nested XML elements, or a dependency no-\ntation with a ( child-id, parent-id) pair on each line, or an XML version of the dependency\nnotation, etc. However, in each case the logical structure is almost the same. It is much\neasier to devise a common interface that allows application programmers to write code\nto access tree data using methods such as children(), leaves(), depth(), and so forth.\nNote that this approach follows accepted practice within computer science, viz. ab-\nstract data types, object-oriented design, and the three-layer architecture ( Fig-\nure 11-6 ). The last of these\u2014from the world of relational databases\u2014allows end-user\napplications to use a common model (the \u201crelational model\u201d) and a common language\n(SQL) to abstract away from the idiosyncrasies of file storage. It also allows innovations\nin filesystem technologies to occur without disturbing end-user applications. In the\nsame way, a common corpus interface insulates application programs from data\nformats.\nFigure 11-6. A common format versus a common interface.\nIn this context, when creating a new corpus for dissemination, it is expedient to use a\nwidely used \nformat wherever possible. When this is not possible, the corpus could be\naccompanied with software\u2014such as an nltk.corpus module\u2014that supports existing\ninterface methods.\nSpecial Considerations When Working with Endangered Languages\nThe importance of language to science and the arts is matched in significance by the\ncultural treasure embodied in language. Each of the world\u2019s ~7,000 human languages\n422 | Chapter 11: \u2002Managing Linguistic Data\nis rich in unique respects, in its oral histories and creation legends, down to its gram-\nmatical constructions \nand its very words and their nuances of meaning. Threatened\nremnant cultures have words to distinguish plant subspecies according to therapeutic\nuses that are unknown to science. Languages evolve over time as they come into contact\nwith each other, and each one provides a unique window onto human pre-history. In\nmany parts of the world, small linguistic variations from one town to the next add up\nto a completely different language in the space of a half-hour drive. For its breathtaking\ncomplexity and diversity, human language is as a colorful tapestry stretching through\ntime and space.\nHowever, most of the world\u2019s languages face extinction. In response to this, many\nlinguists are hard at work documenting the languages, constructing rich records of this\nimportant facet of the world\u2019s linguistic heritage. What can the field of NLP offer to\nhelp with this effort? Developing taggers, parsers, named entity recognizers, etc., is not\nan early priority, and there is usually insufficient data for developing such tools in any\ncase. Instead, the most frequently voiced need is to have better tools for collecting and\ncurating data, with a focus on texts and lexicons.\nOn the face of things, it should be a straightforward matter to start collecting texts in\nan endangered language. Even if we ignore vexed issues such as who owns the texts,\nand sensitivities surrounding cultural knowledge contained in the texts, there is the\nobvious practical issue of transcription. Most languages lack a standard orthography.\nWhen a language has no literary tradition, the conventions of spelling and punctuation\nare not well established. Therefore it is common practice to create a lexicon in tandem\nwith a text collection, continually updating the lexicon as new words appear in the\ntexts. This work could be done using a text processor (for the texts) and a spreadsheet\n(for the lexicon). Better still, SIL\u2019s free linguistic software Toolbox and Fieldworks\nprovide sophisticated support for integrated creation of texts and lexicons.\nWhen speakers of the language in question are trained to enter texts themselves, a\ncommon obstacle is an overriding concern for correct spelling. Having a lexicon greatly\nhelps this process, but we need to have lookup methods that do not assume someone\ncan determine the citation form of an arbitrary word. The problem may be acute for\nlanguages having a complex morphology that includes prefixes. In such cases it helps\nto tag lexical items with semantic domains, and to permit lookup by semantic domain\nor by gloss.\nPermitting lookup by pronunciation similarity is also a big help. Here\u2019s a simple dem-\nonstration of how to do this. The first step is to identify confusible letter sequences,\nand map complex versions to simpler versions. We might also notice that the relative\norder of letters within a cluster of consonants is a source of spelling errors, and so we\nnormalize the order of consonants.\n11.3  Acquiring Data | 423\n>>> mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n...             ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]\n>>> def signature(word):\n...     for patt, repl in mappings:\n...         word = re.sub(patt, repl, word)\n...     pieces = re.findall('[^aeiou]+', word)\n...     return ''.join(char for piece in pieces for char in sorted(piece))[:8]\n>>> signature('illefent')\n'lfnt'\n>>> signature('ebsekwieous')\n'bskws'\n>>> signature('nuculerr')\n'nclr'\nNext, we \ncreate a mapping from signatures to words, for all the words in our lexicon.\nWe can use this to get candidate corrections for a given input word (but we must first\ncompute that word\u2019s signature).\n>>> signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())\n>>> signatures[signature('nuculerr')]\n['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']\nFinally, we should rank the results in terms of similarity with the original word. This\nis done by the function rank(). The only remaining function provides a simple interface\nto the user:\n>>> def rank(word, wordlist):\n...     ranked = sorted((nltk.edit_dist(word, w), w) for w in wordlist)\n...     return [word for (_, word) in ranked]\n>>> def fuzzy_spell(word):\n...     sig = signature(word)\n...     if sig in signatures:\n...         return rank(word, signatures[sig])\n...     else:\n...         return []\n>>> fuzzy_spell('illefent')\n['olefiant', 'elephant', 'oliphant', 'elephanta']\n>>> fuzzy_spell('ebsekwieous')\n['obsequious']\n>>> fuzzy_spell('nucular')\n['nuclear', 'nucellar', 'anicular', 'inocular', 'unocular', 'unicolor', 'uniocular']\nThis is just one illustration where a simple program can facilitate access to lexical data\nin a context where the writing system of a language may not be standardized, or where\nusers of the language may not have a good command of spellings. Other simple appli-\ncations of NLP in this area include building indexes to facilitate access to data, gleaning\nwordlists from texts, locating examples of word usage in constructing a lexicon, de-\ntecting prevalent or exceptional patterns in poorly understood data, and performing\nspecialized validation on data created using various linguistic software tools. We will\nreturn to the last of these in Section 11.5.\n424 | Chapter 11: \u2002Managing Linguistic Data\n11.4  Working with XML\nThe Extensible \nMarkup Language (XML) provides a framework for designing domain-\nspecific markup languages. It is sometimes used for representing annotated text and\nfor lexical resources. Unlike HTML with its predefined tags, XML permits us to make\nup our own tags. Unlike a database, XML permits us to create data without first spec-\nifying its structure, and it permits us to have optional and repeatable elements. In this\nsection, we briefly review some features of XML that are relevant for representing lin-\nguistic data, and show how to access data stored in XML files using Python programs.\nUsing XML for Linguistic Structures\nThanks to its flexibility and extensibility, XML is a natural choice for representing\nlinguistic structures. Here\u2019s an example of a simple lexical entry.\n(2)<entry>\n  <headword>whale</headword>\n  <pos>noun</pos>\n  <gloss>any of the larger cetacean mammals having a streamlined\n    body and breathing through a blowhole on the head</gloss>\n</entry>\nIt consists \nof a series of XML tags enclosed in angle brackets. Each opening tag, such\nas <gloss>, is matched with a closing tag, </gloss>; together they constitute an XML\nelement. The preceding example has been laid out nicely using whitespace, but it could\nequally have been put on a single long line. Our approach to processing XML will\nusually not be sensitive to whitespace. In order for XML to be well formed, all opening\ntags must have corresponding closing tags, at the same level of nesting (i.e., the XML\ndocument must be a well-formed tree).\nXML permits us to repeat elements, e.g., to add another gloss field, as we see next. We\nwill use different whitespace to underscore the point that layout does not matter.\n(3)<entry><headword>whale</headword><pos>noun</pos><gloss>any of the\nlarger cetacean mammals having a streamlined body and breathing\nthrough a blowhole on the head</gloss><gloss>a very large person;\nimpressive in size or qualities</gloss></entry>\nA further step might be to link our lexicon to some external resource, such as WordNet,\nusing external identifiers. In (4) we group the gloss and a synset identifier inside a new\nelement, which we have called \u201csense.\u201d\n(4)<entry>\n  <headword>whale</headword>\n  <pos>noun</pos>\n  <sense>\n    <gloss>any of the larger cetacean mammals having a streamlined\n      body and breathing through a blowhole on the head</gloss>\n    <synset>whale.n.02</synset>\n11.4  Working with XML | 425\n  </sense>\n    <gloss>a very large person; impressive in size or qualities</gloss>\n    <synset>giant.n.04</synset>\n  </sense>\n</entry>\nAlternatively, we \ncould have represented the synset identifier using an XML\nattribute, without the need for any nested structure, as in (5).\n(5)<entry>\n  <headword>whale</headword>\n  <pos>noun</pos>\n  <gloss synset=\"whale.n.02\">any of the larger cetacean mammals having\n      a streamlined body and breathing through a blowhole on the head</gloss>\n  <gloss synset=\"giant.n.04\">a very large person; impressive in size or\n      qualities</gloss>\n</entry>\nThis illustrates some of the flexibility of XML. If it seems somewhat arbitrary, that\u2019s\nbecause it is! Following the rules of XML, we can invent new attribute names, and nest\nthem as deeply as we like. We can repeat elements, leave them out, and put them in a\ndifferent order each time. We can have fields whose presence depends on the value of\nsome other field; e.g., if the part of speech is verb, then the entry can have a\npast_tense element to hold the past tense of the verb, but if the part of speech is noun,\nno past_tense element is permitted. To impose some order over all this freedom, we\ncan constrain the structure of an XML file using a \u201cschema,\u201d which is a declaration\nakin to a context-free grammar. Tools exist for testing the validity of an XML file with\nrespect to a schema.\nThe Role of XML\nWe can use XML to represent many kinds of linguistic information. However, the\nflexibility comes at a price. Each time we introduce a complication, such as by permit-\nting an element to be optional or repeated, we make more work for any program that\naccesses the data. We also make it more difficult to check the validity of the data, or to\ninterrogate the data using one of the XML query languages.\nThus, using XML to represent linguistic structures does not magically solve the data\nmodeling problem. We still have to work out how to structure the data, then define\nthat structure with a schema, and then write programs to read and write the format\nand convert it to other formats. Similarly, we still need to follow some standard prin-\nciples concerning data normalization. It is wise to avoid making duplicate copies of the\nsame information, so that we don\u2019t end up with inconsistent data when only one copy\nis changed. For example, a cross-reference that was represented as <xref>headword</\nxref> would duplicate the storage of the headword of some other lexical entry, and the\nlink would break if the copy of the string at the other location was modified. Existential\ndependencies between information types need to be modeled, so that we can\u2019t create\nelements without a home. For example, if sense definitions cannot exist independently\n426 | Chapter 11: \u2002Managing Linguistic Data\nof a lexical entry, the sense element can be nested inside the entry element. Many-to-\nmany relations need to be abstracted out of hierarchical structures. For example, if a\nword can have many corresponding senses, and a sense can have several corresponding\nwords, then both words and senses must be enumerated separately, as must the list of\n(word, sense) pairings. This complex structure might even be split across three separate\nXML files.\nAs we can see, although XML provides us with a convenient format accompanied by\nan extensive collection of tools, it offers no panacea.\nThe ElementTree Interface\nPython\u2019s ElementTree module provides a convenient way to access data stored in XML\nfiles. ElementTree is part of Python\u2019s standard library (since Python 2.5), and is also\nprovided as part of NLTK in case you are using Python 2.4.\nWe will illustrate the use of ElementTree using a collection of Shakespeare plays that\nhave been formatted using XML. Let\u2019s load the XML file and inspect the raw data, first\nat the top of the file \n , where we see some XML headers and the name of a schema\ncalled play.dtd, followed \nby the root element  PLAY. We pick it up again at the start of\nAct 1 \n . (Some blank lines have been omitted from the output.)\n>>> merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n>>> raw = open(merchant_file).read()\n>>> print raw[0:168] \n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n<PLAY>\n<TITLE>The Merchant of Venice</TITLE>\n>>> print raw[1850:2075] \n<TITLE>ACT I</TITLE>\n<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n<SPEECH>\n<SPEAKER>ANTONIO</SPEAKER>\n<LINE>In sooth, I know not why I am so sad:</LINE>\nWe have \njust accessed the XML data as a string. As we can see, the string at the start\nof Act 1 contains XML tags for title, scene, stage directions, and so forth.\nThe next step is to process the file contents as structured XML data, using Element\nTree. We are processing a file (a multiline string) and building a tree, so it\u2019s not sur-\nprising that the method name is parse \n . The variable merchant contains an XML ele-\nment PLAY \n . This element has internal structure; we can use an index to get its first\nchild, a TITLE element \n . We can also see the text content of this element, the title of\nthe play \n . To get a list of all the child elements, we use the getchildren() method \n .\n>>> from nltk.etree.ElementTree import ElementTree\n>>> merchant = ElementTree().parse(merchant_file) \n>>> merchant\n11.4  Working with XML | 427\n<Element PLAY at 22fa800> \n>>> merchant[0]\n<Element TITLE at 22fa828> \n>>> merchant[0].text\n'The Merchant of Venice' \n>>> merchant.getchildren() \n[<Element TITLE at 22fa828>, <Element PERSONAE at 22fa7b0>, <Element SCNDESCR at 2300170>,\n<Element PLAYSUBT at 2300198>, <Element ACT at 23001e8>, <Element ACT at 234ec88>,\n<Element ACT at 23c87d8>, <Element ACT at 2439198>, <Element ACT at 24923c8>]\nThe play \nconsists of a title, the personae, a scene description, a subtitle, and five acts.\nEach act has a title and some scenes, and each scene consists of speeches which are\nmade up of lines, a structure with four levels of nesting. Let\u2019s dig down into Act IV:\n>>> merchant[-2][0].text\n'ACT IV'\n>>> merchant[-2][1]\n<Element SCENE at 224cf80>\n>>> merchant[-2][1][0].text\n'SCENE I.  Venice. A court of justice.'\n>>> merchant[-2][1][54]\n<Element SPEECH at 226ee40>\n>>> merchant[-2][1][54][0]\n<Element SPEAKER at 226ee90>\n>>> merchant[-2][1][54][0].text\n'PORTIA'\n>>> merchant[-2][1][54][1]\n<Element LINE at 226eee0>\n>>> merchant[-2][1][54][1].text\n\"The quality of mercy is not strain'd,\"\nYour Turn:  Repeat some of the methods just shown, for one of the\nother Shakespeare plays included in the corpus, such as Romeo and Ju-\nliet or Macbeth. For a list, see nltk.corpus.shakespeare.fileids().\nAlthough we can access the entire tree this way, it is more convenient to search for sub-\nelements with particular names. Recall that the elements at the top level have several\ntypes. We can iterate over just the types we are interested in (such as the acts), using\nmerchant.findall('ACT'). Here\u2019s an example of doing such tag-specific searches at ev-\nery level of nesting:\n>>> for i, act in enumerate(merchant.findall('ACT')):\n...     for j, scene in enumerate(act.findall('SCENE')):\n...         for k, speech in enumerate(scene.findall('SPEECH')):\n...             for line in speech.findall('LINE'):\n...                 if 'music' in str(line.text):\n...                     print \"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text)\nAct 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\nAct 3 Scene 2 Speech 9: Fading in music: that the comparison\nAct 3 Scene 2 Speech 9: And what is music then? Then music is\nAct 5 Scene 1 Speech 23: And bring your music forth into the air.\nAct 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n428 | Chapter 11: \u2002Managing Linguistic Data\nAct 5 Scene 1 Speech 23: And draw her home with music.\nAct 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\nAct 5 Scene 1 Speech 25: Or any air of music touch their ears,\nAct 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\nAct 5 Scene 1 Speech 25: But music for the time doth change his nature.\nAct 5 Scene 1 Speech 25: The man that hath no music in himself,\nAct 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\nAct 5 Scene 1 Speech 29: It is your music, madam, of the house.\nAct 5 Scene 1 Speech 32: No better a musician than the wren.\nInstead of \nnavigating each step of the way down the hierarchy, we can search for par-\nticular embedded elements. For example, let\u2019s examine the sequence of speakers. We\ncan use a frequency distribution to see who has the most to say:\n>>> speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n>>> speaker_freq = nltk.FreqDist(speaker_seq)\n>>> top5 = speaker_freq.keys()[:5]\n>>> top5\n['PORTIA', 'SHYLOCK', 'BASSANIO', 'GRATIANO', 'ANTONIO']\nWe can also look for patterns in who follows whom in the dialogues. Since there are\n23 speakers, we need to reduce the \u201cvocabulary\u201d to a manageable size first, using the\nmethod described in Section 5.3.\n>>> mapping = nltk.defaultdict(lambda: 'OTH')\n>>> for s in top5:\n...     mapping[s] = s[:4]\n...\n>>> speaker_seq2 = [mapping[s] for s in speaker_seq]\n>>> cfd = nltk.ConditionalFreqDist(nltk.ibigrams(speaker_seq2))\n>>> cfd.tabulate()\n     ANTO BASS GRAT  OTH PORT SHYL\nANTO    0   11    4   11    9   12\nBASS   10    0   11   10   26   16\nGRAT    6    8    0   19    9    5\n OTH    8   16   18  153   52   25\nPORT    7   23   13   53    0   21\nSHYL   15   15    2   26   21    0\nIgnoring the entry of 153 for exchanges between people other than the top five, the\nlargest values suggest that Othello and Portia have the most significant interactions.\nUsing ElementTree for Accessing Toolbox Data\nIn Section 2.4, we saw a simple interface for accessing Toolbox data, a popular and\nwell-established format used by linguists for managing data. In this section, we discuss\na variety of techniques for manipulating Toolbox data in ways that are not supported\nby the Toolbox software. The methods we discuss could be applied to other record-\nstructured data, regardless of the actual file format.\nWe can use the toolbox.xml() method to access a Toolbox file and load it into an\nElementTree object. This file contains a lexicon for the Rotokas language of Papua New\nGuinea.\n11.4  Working with XML | 429\n>>> from nltk.corpus import toolbox\n>>> lexicon = toolbox.xml('rotokas.dic')\nThere are \ntwo ways to access the contents of the lexicon object: by indexes and by\npaths. Indexes use the familiar syntax; thus lexicon[3] returns entry number 3 (which\nis actually the fourth entry counting from zero) and lexicon[3][0] returns its first field:\n>>> lexicon[3][0]\n<Element lx at 77bd28>\n>>> lexicon[3][0].tag\n'lx'\n>>> lexicon[3][0].text\n'kaa'\nThe second way to access the contents of the lexicon object uses paths. The lexicon is\na series of record objects, each containing a series of field objects, such as lx and ps.\nWe can conveniently address all of the lexemes using the path record/lx. Here we use\nthe findall() function to search for any matches to the path record/lx, and we access\nthe text content of the element, normalizing it to lowercase:\n>>> [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]\n['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',\n'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']\nLet\u2019s view the Toolbox data in XML format. The write() method of ElementTree ex-\npects a file object. We usually create one of these using Python\u2019s built-in open() func-\ntion. In order to see the output displayed on the screen, we can use a special predefined\nfile object called stdout \n  (standard output), defined in Python\u2019s sys module.\n>>> import sys\n>>> from nltk.etree.ElementTree import ElementTree\n>>> tree = ElementTree(lexicon[3])\n>>> tree.write(sys.stdout) \n<record>\n  <lx>kaa</lx>\n  <ps>N</ps>\n  <pt>MASC</pt>\n  <cl>isi</cl>\n  <ge>cooking banana</ge>\n  <tkp>banana bilong kukim</tkp>\n  <pt>itoo</pt>\n  <sf>FLORA</sf>\n  <dt>12/Aug/2005</dt>\n  <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n  <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n  <xe>Taeavi planted banana in order to cook it.</xe>\n</record>\nFormatting Entries\nWe can \nuse the same idea we saw in the previous section to generate HTML tables\ninstead of plain text. This would be useful for publishing a Toolbox lexicon on the\nWeb. It produces HTML elements <table>, <tr> (table row), and <td> (table data).\n430 | Chapter 11: \u2002Managing Linguistic Data\n>>> html = \"<table>\\n\"\n>>> for entry in lexicon[70:80]:\n...     lx = entry.findtext('lx')\n...     ps = entry.findtext('ps')\n...     ge = entry.findtext('ge')\n...     html += \"  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (lx, ps, ge)\n>>> html += \"</table>\"\n>>> print html\n<table>\n  <tr><td>kakae</td><td>???</td><td>small</td></tr>\n  <tr><td>kakae</td><td>CLASS</td><td>child</td></tr>\n  <tr><td>kakaevira</td><td>ADV</td><td>small-like</td></tr>\n  <tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\n  <tr><td>kakapikoto</td><td>N</td><td>newborn baby</td></tr>\n  <tr><td>kakapu</td><td>V</td><td>place in sling for purpose of carrying</td></tr>\n  <tr><td>kakapua</td><td>N</td><td>sling for lifting</td></tr>\n  <tr><td>kakara</td><td>N</td><td>arm band</td></tr>\n  <tr><td>Kakarapaia</td><td>N</td><td>village name</td></tr>\n  <tr><td>kakarau</td><td>N</td><td>frog</td></tr>\n</table>\n11.5  Working with Toolbox Data\nGiven the \npopularity of Toolbox among linguists, we will discuss some further methods\nfor working with Toolbox data. Many of the methods discussed in previous chapters,\nsuch as counting, building frequency distributions, and tabulating co-occurrences, can\nbe applied to the content of Toolbox entries. For example, we can trivially compute\nthe average number of fields for each entry:\n>>> from nltk.corpus import toolbox\n>>> lexicon = toolbox.xml('rotokas.dic')\n>>> sum(len(entry) for entry in lexicon) / len(lexicon)\n13.635955056179775\nIn this section, we will discuss two tasks that arise in the context of documentary lin-\nguistics, neither of which is supported by the Toolbox software.\nAdding a Field to Each Entry\nIt is often convenient to add new fields that are derived automatically from existing\nones. Such fields often facilitate search and analysis. For instance, in Example 11-2  we\ndefine a function cv(), which maps a string of consonants and vowels to the corre-\nsponding CV sequence, e.g., kakapua would map to CVCVCVV. This mapping has four\nsteps. First, the string is converted to lowercase, then we replace any non-alphabetic\ncharacters [^a-z] with an underscore. Next, we replace all vowels with V. Finally, any-\nthing that is not a V or an underscore must be a consonant, so we replace it with a C.\nNow, we can scan the lexicon and add a new cv field after every lx field. Exam-\nple 11-2  shows what this does to a particular entry; note the last line of output, which\nshows the new cv field.\n11.5  Working with Toolbox Data | 431\nExample 11-2. Adding a new cv field to a lexical entry.\nfrom nltk.etree.ElementTree import SubElement\ndef cv(s):\n    s = s.lower()\n    s = re.sub(r'[^a-z]',     r'_', s)\n    s = re.sub(r'[aeiou]',    r'V', s)\n    s = re.sub(r'[^V_]',      r'C', s)\n    return (s)\ndef add_cv_field(entry):\n    for field in entry:\n        if field.tag == 'lx':\n            cv_field = SubElement(entry, 'cv')\n            cv_field.text = cv(field.text)\n>>> lexicon = toolbox.xml('rotokas.dic')\n>>> add_cv_field(lexicon[53])\n>>> print nltk.to_sfm_string(lexicon[53])\n\\lx kaeviro\n\\ps V\n\\pt A\n\\ge lift off\n\\ge take off\n\\tkp go antap\n\\sc MOTION\n\\vx 1\n\\nt used to describe action of plane\n\\dt 03/Jun/2005\n\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n\\xp Pita i go antap na lukim haus win i bagarapim.\n\\xe Peter went to look at the house that the wind destroyed.\n\\cv CVVCVCV\nIf a Toolbox file is being continually updated, the program in Exam-\nple 11-2\n will need to be run more than once. It would be possible to\nmodify add_cv_field() to modify the contents of an existing entry.\nHowever, it is a safer practice to use such programs to create enriched\nfiles for the purpose of data analysis, without replacing the manually\ncurated source files.\nValidating a Toolbox Lexicon\nMany lexicons in Toolbox format do not conform to any particular schema. Some\nentries may include extra fields, or may order existing fields in a new way. Manually\ninspecting thousands of lexical entries is not practicable. However, we can easily iden-\ntify frequent versus exceptional field sequences, with the help of a FreqDist:\n>>> fd = nltk.FreqDist(':'.join(field.tag for field in entry) for entry in lexicon)\n>>> fd.items()\n[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),\n432 | Chapter 11: \u2002Managing Linguistic Data\n('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27), ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20),\n..., ('lx:alt:rt:ps:pt:ge:eng:eng:eng:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1)]\nAfter inspecting \nthe high-frequency field sequences, we could devise a context-free\ngrammar for lexical entries. The grammar in Example 11-3  uses the CFG format we\nsaw in Chapter 8. Such a grammar models the implicit nested structure of Toolbox\nentries, building a tree structure, where the leaves of the tree are individual field names.\nWe iterate over the entries and report their conformance with the grammar, as shown\nin Example 11-3 . Those that are accepted by the grammar are prefixed with a '+' \n ,\nand those \nthat are rejected are prefixed with a '-' \n . During the process of developing\nsuch a grammar, it helps to filter out some of the tags \n .\nExample 11-3. Validating Toolbox entries using a context-free grammar.\ngrammar = nltk.parse_cfg('''\n  S -> Head PS Glosses Comment Date Sem_Field Examples\n  Head -> Lexeme Root\n  Lexeme -> \"lx\"\n  Root -> \"rt\" |\n  PS -> \"ps\"\n  Glosses -> Gloss Glosses |\n  Gloss -> \"ge\" | \"tkp\" | \"eng\"\n  Date -> \"dt\"\n  Sem_Field -> \"sf\"\n  Examples -> Example Ex_Pidgin Ex_English Examples |\n  Example -> \"ex\"\n  Ex_Pidgin -> \"xp\"\n  Ex_English -> \"xe\"\n  Comment -> \"cmt\" | \"nt\" |\n  ''')\ndef validate_lexicon(grammar, lexicon, ignored_tags):\n    rd_parser = nltk.RecursiveDescentParser(grammar)\n    for entry in lexicon:\n        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n        if rd_parser.nbest_parse(marker_list):\n            print \"+\", ':'.join(marker_list) \n        else:\n            print \"-\", ':'.join(marker_list) \n>>> lexicon = toolbox.xml('rotokas.dic')[10:20]\n>>> ignored_tags = ['arg', 'dcsv', 'pt', 'vx'] \n>>> validate_lexicon(grammar, lexicon, ignored_tags)\n- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\n- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n- lx:ps:ge:tkp:nt:sf:dt\n- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\n- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\n- lx:rt:ps:ge:ge:tkp:dt\n- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\n- lx:rt:ps:ge:tkp:dt:ex:xp:xe\n- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\n11.5  Working with Toolbox Data | 433\nAnother approach would be to use a chunk parser ( Chapter 7 ), since these are much\nmore effective at identifying partial structures and can report the partial structures that\nhave been identified. In Example 11-4  we set up a chunk grammar for the entries of a\nlexicon, then parse each entry. A sample of the output from this program is shown in\nFigure 11-7.\nFigure 11-7. XML representation of a lexical entry, resulting from chunk parsing a Toolbox record.\nExample 11-4. Chunking a Toolbox lexicon: A chunk grammar describing the structure of entries for\na lexicon for Iu Mien, a language of China.\nfrom nltk_contrib import toolbox\ngrammar = r\"\"\"\n      lexfunc: {<lf>(<lv><ln|le>*)*}\n      example: {<rf|xv><xn|xe>*}\n      sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\n      record:   {<lx><hm><sense>+<dt>}\n    \"\"\"\n>>> from nltk.etree.ElementTree import ElementTree\n>>> db = toolbox.ToolboxData()\n>>> db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))\n>>> lexicon = db.parse(grammar, encoding='utf8')\n>>> toolbox.data.indent(lexicon)\n>>> tree = ElementTree(lexicon)\n>>> output = open(\"iu_mien_samp.xml\", \"w\")\n>>> tree.write(output, encoding='utf8')\n>>> output.close()\n434 | Chapter 11: \u2002Managing Linguistic Data\n11.6  Describing Language Resources Using OLAC Metadata\nMembers of \nthe NLP community have a common need for discovering language re-\nsources with high precision and recall. The solution which has been developed by the\nDigital Libraries community involves metadata aggregation.\nWhat Is Metadata?\nThe simplest definition of metadata is \u201cstructured data about data.\u201d Metadata is de-\nscriptive information about an object or resource, whether it be physical or electronic.\nAlthough the term \u201cmetadata\u201d itself is relatively new, the underlying concepts behind\nmetadata have been in use for as long as collections of information have been organized.\nLibrary catalogs represent a well-established type of metadata; they have served as col-\nlection management and resource discovery tools for decades. Metadata can be gen-\nerated either \u201cby hand\u201d or automatically using software.\nThe Dublin Core Metadata Initiative began in 1995 to develop conventions for finding,\nsharing, and managing information. The Dublin Core metadata elements represent a\nbroad, interdisciplinary consensus about the core set of elements that are likely to be\nwidely useful to support resource discovery. The Dublin Core consists of 15 metadata\nelements, where each element is optional and repeatable: Title, Creator, Subject, De-\nscription, Publisher, Contributor, Date, Type, Format, Identifier, Source, Language,\nRelation, Coverage, and Rights. This metadata set can be used to describe resources\nthat exist in digital or traditional formats.\nThe Open Archives Initiative (OAI) provides a common framework across digital re-\npositories of scholarly materials, regardless of their type, including documents, data,\nsoftware, recordings, physical artifacts, digital surrogates, and so forth. Each repository\nconsists of a network-accessible server offering public access to archived items. Each\nitem has a unique identifier, and is associated with a Dublin Core metadata record (and\npossibly additional records in other formats). The OAI defines a protocol for metadata\nsearch services to \u201charvest\u201d the contents of repositories.\nOLAC: Open Language Archives Community\nThe Open Language Archives Community, or OLAC, is an international partnership\nof institutions and individuals who are creating a worldwide virtual library of language\nresources by: (i) developing consensus on best current practices for the digital archiving\nof language resources, and (ii) developing a network of interoperating repositories and\nservices for housing and accessing such resources. OLAC\u2019s home on the Web is at http:\n//www.language-archives.org/.\nOLAC Metadata is a standard for describing language resources. Uniform description\nacross repositories is ensured by limiting the values of certain metadata elements to the\nuse of terms from controlled vocabularies. OLAC metadata can be used to describe\ndata and tools, in both physical and digital formats. OLAC metadata extends the\n11.6  Describing Language Resources Using OLAC Metadata | 435\nDublin Core Metadata Set, a widely accepted standard for describing resources of all\ntypes. \nTo this core set, OLAC adds descriptors to cover fundamental properties of\nlanguage resources, such as subject language and linguistic type. Here\u2019s an example of\na complete OLAC record:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<olac:olac xmlns:olac=\"http://www.language-archives.org/OLAC/1.1/\"\n           xmlns=\"http://purl.org/dc/elements/1.1/\"\n           xmlns:dcterms=\"http://purl.org/dc/terms/\"\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n           xsi:schemaLocation=\"http://www.language-archives.org/OLAC/1.1/\n                http://www.language-archives.org/OLAC/1.1/olac.xsd\">\n  <title>A grammar of Kayardild. With comparative notes on Tangkic.</title>\n  <creator>Evans, Nicholas D.</creator>\n  <subject>Kayardild grammar</subject>\n  <subject xsi:type=\"olac:language\" olac:code=\"gyd\">Kayardild</subject>\n  <language xsi:type=\"olac:language\" olac:code=\"en\">English</language>\n  <description>Kayardild Grammar (ISBN 3110127954)</description>\n  <publisher>Berlin - Mouton de Gruyter</publisher>\n  <contributor xsi:type=\"olac:role\" olac:code=\"author\">Nicholas Evans</contributor>\n  <format>hardcover, 837 pages</format>\n  <relation>related to ISBN 0646119966</relation>\n  <coverage>Australia</coverage>\n  <type xsi:type=\"olac:linguistic-type\" olac:code=\"language_description\"/>\n  <type xsi:type=\"dcterms:DCMIType\">Text</type>\n</olac:olac>\nParticipating language archives publish their catalogs in an XML format, and these\nrecords are regularly \u201charvested\u201d by OLAC services using the OAI protocol. In addition\nto this software infrastructure, OLAC has documented a series of best practices for\ndescribing language resources, through a process that involved extended consultation\nwith the language resources community (e.g., see http://www.language-archives.org/\nREC/bpr.html).\nOLAC repositories can be searched using a query engine on the OLAC website. Search-\ning for \u201cGerman lexicon\u201d finds the following resources, among others:\n\u2022 CALLHOME German Lexicon, at http://www.language-archives.org/item/oai:\nwww.ldc.upenn.edu:LDC97L18\n\u2022 MULTILEX multilingual lexicon, at http://www.language-archives.org/item/oai:el\nra.icp.inpg.fr:M0001\n\u2022 Slelex Siemens Phonetic lexicon, at http://www.language-archives.org/item/oai:elra\n.icp.inpg.fr:S0048\nSearching for \u201cKorean\u201d finds a newswire corpus, and a treebank, a lexicon, a child-\nlanguage corpus, and interlinear glossed texts. It also finds software, including a syn-\ntactic analyzer and a morphological analyzer.\nObserve that the previous URLs include a substring of the form:\noai:www.ldc.upenn.edu:LDC97L18. This is an OAI identifier, using a URI scheme regis-\ntered with ICANN (the Internet Corporation for Assigned Names and Numbers). These\n436 | Chapter 11: \u2002Managing Linguistic Data\nidentifiers have the format oai:archive:local_id, where oai  is the name of the URI\nscheme, archive is an archive identifier, such as www.ldc.upenn.edu, and local_id is the\nresource identifier assigned by the archive, e.g., LDC97L18.\nGiven an OAI identifier for an OLAC resource, it is possible to retrieve the complete\nXML record for the resource using a URL of the following form: http://www.language-\narchives.org/static-records/oai:archive:local_id.\n11.7  Summary\n\u2022 Fundamental data types, present in most corpora, are annotated texts and lexicons.\nTexts have a temporal structure, whereas lexicons have a record structure.\n\u2022 The life cycle of a corpus includes data collection, annotation, quality control, and\npublication. The life cycle continues after publication as the corpus is modified\nand enriched during the course of research.\n\u2022 Corpus development involves a balance between capturing a representative sample\nof language usage, and capturing enough material from any one source or genre to\nbe useful; multiplying out the dimensions of variability is usually not feasible be-\ncause of resource limitations.\n\u2022 XML provides a useful format for the storage and interchange of linguistic data,\nbut provides no shortcuts for solving pervasive data modeling problems.\n\u2022 Toolbox format is widely used in language documentation projects; we can write\nprograms to support the curation of Toolbox files, and to convert them to XML.\n\u2022 The Open Language Archives Community (OLAC) provides an infrastructure for\ndocumenting and discovering language resources.\n11.8  Further Reading\nExtra materials for this chapter are posted at http://www.nltk.org/, including links to\nfreely available resources on the Web.\nThe primary sources of linguistic corpora are the Linguistic Data Consortium  and the\nEuropean Language Resources Agency , both with extensive online catalogs. More de-\ntails concerning the major corpora mentioned in the chapter are available: American\nNational Corpus (Reppen, Ide & Suderman, 2005), British National Corpus (BNC,\n1999), Thesaurus Linguae Graecae (TLG, 1999), Child Language Data Exchange Sys-\ntem (CHILDES) (MacWhinney, 1995), and TIMIT (Garofolo et al., 1986).\nTwo special interest groups of the Association for Computational Linguistics that or-\nganize regular workshops with published proceedings are SIGWAC, which promotes\nthe use of the Web as a corpus and has sponsored the CLEANEVAL task for removing\nHTML markup, and SIGANN, which is encouraging efforts toward interoperability of\n11.8  Further Reading | 437\nlinguistic annotations. An extended discussion of web crawling is provided by (Croft,\nMetzler & Strohman, 2009).\nFull details \nof the Toolbox data format are provided with the distribution (Buseman,\nBuseman & Early, 1996), and with the latest distribution freely available from http://\nwww.sil.org/computing/toolbox/. For guidelines on the process of constructing a Tool-\nbox lexicon, see http://www.sil.org/computing/ddp/. More examples of our efforts with\nthe Toolbox are documented in (Bird, 1999) and (Robinson, Aumann & Bird, 2007).\nDozens of other tools for linguistic data management are available, some surveyed by\n(Bird & Simons, 2003). See also the proceedings of the LaTeCH workshops on language\ntechnology for cultural heritage data.\nThere are many excellent resources for XML (e.g., http://zvon.org/) and for writing\nPython programs to work with XML http://www.python.org/doc/lib/markup.html.\nMany editors have XML modes. XML formats for lexical information include OLIF\n(http://www.olif.net/) and LIFT (http://code.google.com/p/lift-standard/).\nFor a survey of linguistic annotation software, see the Linguistic Annotation Page at\nhttp://www.ldc.upenn.edu/annotation/. The initial proposal for standoff annotation was\n(Thompson & McKelvie, 1997). An abstract data model for linguistic annotations,\ncalled \u201cannotation graphs,\u201d was proposed in (Bird & Liberman, 2001). A general-\npurpose ontology for linguistic description (GOLD) is documented at http://www.lin\nguistics-ontology.org/.\nFor guidance on planning and constructing a corpus, see (Meyer, 2002) and (Farghaly,\n2003). More details of methods for scoring inter-annotator agreement are available in\n(Artstein & Poesio, 2008) and (Pevzner & Hearst, 2002).\nRotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg\nAumann.\nFor more information about the Open Language Archives Community, visit http://www\n.language-archives.org/, or see (Simons & Bird, 2003).\n11.9  Exercises\n1.\u25d1 In Example 11-2  the new field appeared at the bottom of the entry. Modify this\nprogram so that it inserts the new subelement right after the lx field. (Hint: create\nthe new cv field using Element('cv'), assign a text value to it, then use the\ninsert() method of the parent element.)\n2.\u25d1 Write a function that deletes a specified field from a lexical entry. (We could use\nthis to sanitize our lexical data before giving it to others, e.g., by removing fields\ncontaining irrelevant or uncertain content.)\n3.\u25d1 Write a program that scans an HTML dictionary file to find entries having an\nillegal part-of-speech field, and then reports the headword for each entry.\n438 | Chapter 11: \u2002Managing Linguistic Data\n4.\u25d1 Write a program to find any parts-of-speech ( ps  field) that occurred less than 10\ntimes. Perhaps these are typing mistakes?\n5.\u25d1 We saw a method for adding a cv field (Section 11.5). There is an interesting\nissue with keeping this up-to-date when someone modifies the content of the lx\nfield on which it is based. Write a version of this program to add a cv field, replacing\nany existing cv field.\n6.\u25d1 Write a function to add a new field syl which gives a count of the number of\nsyllables in the word.\n7.\u25d1 Write a function which displays the complete entry for a lexeme. When the\nlexeme is incorrectly spelled, it should display the entry for the most similarly\nspelled lexeme.\n8.\u25d1 Write a function that takes a lexicon and finds which pairs of consecutive fields\nare most frequent (e.g., ps is often followed by pt). (This might help us to discover\nsome of the structure of a lexical entry.)\n9.\u25d1 Create a spreadsheet using office software, containing one lexical entry per row,\nconsisting of a headword, a part of speech, and a gloss. Save the spreadsheet in\nCSV format. Write Python code to read the CSV file and print it in Toolbox format,\nusing lx for the headword, ps for the part of speech, and gl for the gloss.\n10.\u25d1 Index the words of Shakespeare\u2019s plays, with the help of nltk.Index. The result-\ning data structure should permit lookup on individual words, such as music, re-\nturning a list of references to acts, scenes, and speeches, of the form [(3, 2, 9),\n(5, 1, 23), ...], where (3, 2, 9) indicates Act 3 Scene 2 Speech 9.\n11.\u25d1 Construct a conditional frequency distribution which records the word length\nfor each speech in The Merchant of Venice, conditioned on the name of the char-\nacter; e.g., cfd['PORTIA'][12] would give us the number of speeches by Portia\nconsisting of 12 words.\n12.\u25d1 Write a recursive function to convert an arbitrary NLTK tree into an XML coun-\nterpart, with non-terminals represented as XML elements, and leaves represented\nas text content, e.g.:\n<S>\n  <NP type=\"SBJ\">\n    <NP>\n      <NNP>Pierre</NNP>\n      <NNP>Vinken</NNP>\n    </NP>\n    <COMMA>,</COMMA>\n13.\u25cf Obtain a comparative wordlist in CSV format, and write a program that prints\nthose cognates having an edit-distance of at least three from each other.\n14.\u25cf Build an index of those lexemes which appear in example sentences. Suppose\nthe lexeme for a given entry is w. Then, add a single cross-reference field xrf to this\nentry, referencing the headwords of other entries having example sentences con-\ntaining w. Do this for all entries and save the result as a Toolbox-format file.\n11.9  Exercises | 439\n\nAfterword: The Language Challenge\nNatural language throws up some interesting computational challenges. We\u2019ve ex-\nplored many \nof these in the preceding chapters, including tokenization, tagging, clas-\nsification, information extraction, and building syntactic and semantic representations.\nYou should now be equipped to work with large datasets, to create robust models of\nlinguistic phenomena, and to extend them into components for practical language\ntechnologies. We hope that the Natural Language Toolkit (NLTK) has served to open\nup the exciting endeavor of practical natural language processing to a broader audience\nthan before.\nIn spite of all that has come before, language presents us with far more than a temporary\nchallenge for computation. Consider the following sentences which attest to the riches\nof language:\n1. Overhead the day drives level and grey, hiding the sun by a flight of grey spears.\n(William Faulkner, As I Lay Dying, 1935)\n2. When using the toaster please ensure that the exhaust fan is turned on. (sign in\ndormitory kitchen)\n3. Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activi-\nties with Ki values of 45.1-271.6 \u03bcM (Medline, PMID: 10718780)\n4. Iraqi Head Seeks Arms (spoof news headline)\n5. The earnest prayer of a righteous man has great power and wonderful results.\n(James 5:16b)\n6. Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll,\nJabberwocky, 1872)\n7. There are two ways to do this, AFAIK :smile: (Internet discussion archive)\nOther evidence for the riches of language is the vast array of disciplines whose work\ncenters on language. Some obvious disciplines include translation, literary criticism,\nphilosophy, anthropology, and psychology. Many less obvious disciplines investigate\nlanguage use, including law, hermeneutics, forensics, telephony, pedagogy, archaeol-\nogy, cryptanalysis, and speech pathology. Each applies distinct methodologies to gather\n441\nobservations, develop theories, and test hypotheses. All serve to deepen our under-\nstanding of language and of the intellect that is manifested in language.\nIn view \nof the complexity of language and the broad range of interest in studying it\nfrom different angles, it\u2019s clear that we have barely scratched the surface here. Addi-\ntionally, within NLP itself, there are many important methods and applications that\nwe haven\u2019t mentioned.\nIn our closing remarks we will take a broader view of NLP, including its foundations\nand the further directions you might want to explore. Some of the topics are not well\nsupported by NLTK, and you might like to rectify that problem by contributing new\nsoftware and data to the toolkit.\nLanguage Processing Versus Symbol Processing\nThe very notion that natural language could be treated in a computational manner grew\nout of a research program, dating back to the early 1900s, to reconstruct mathematical\nreasoning using logic, most clearly manifested in work by Frege, Russell, Wittgenstein,\nTarski, Lambek, and Carnap. This work led to the notion of language as a formal system\namenable to automatic processing. Three later developments laid the foundation for\nnatural language processing. The first was formal language theory . This defined a\nlanguage as a set of strings accepted by a class of automata, such as context-free lan-\nguages and pushdown automata, and provided the underpinnings for computational\nsyntax.\nThe second development was symbolic logic . This provided a formal method for cap-\nturing selected aspects of natural language that are relevant for expressing logical\nproofs. A formal calculus in symbolic logic provides the syntax of a language, together\nwith rules of inference and, possibly, rules of interpretation in a set-theoretic model;\nexamples are propositional logic and first-order logic. Given such a calculus, with a\nwell-defined syntax and semantics, it becomes possible to associate meanings with\nexpressions of natural language by translating them into expressions of the formal cal-\nculus. For example, if we translate John saw Mary  into a formula saw(j, m) , we (im-\nplicitly or explicitly) interpret the English verb saw as a binary relation, and John and\nMary as denoting individuals. More general statements like All birds fly  require quan-\ntifiers, in this case \u2200, meaning for all : \u2200x (bird(x) \u2192 fly(x)). This use of logic provided\nthe technical machinery to perform inferences that are an important part of language\nunderstanding.\nA closely related development was the principle of compositionality , namely that\nthe meaning of a complex expression is composed from the meaning of its parts and\ntheir mode of combination ( Chapter 10). This principle provided a useful corre-\nspondence between syntax and semantics, namely that the meaning of a complex ex-\npression could be computed recursively. Consider the sentence It is not true that  p,\nwhere p is a proposition. We can represent the meaning of this sentence as not(p).\n442 | Afterword: The Language Challenge\nSimilarly, we can represent the meaning of John saw Mary  as saw(j, m). Now we can\ncompute the interpretation of It is not true that John saw Mary  recursively, using the\nforegoing information, to get not(saw(j,m)).\nThe approaches just outlined share the premise that computing with natural language\ncrucially relies on rules for manipulating symbolic representations. For a certain period\nin the development of NLP, particularly during the 1980s, this premise provided a\ncommon starting point for both linguists and practitioners of NLP, leading to a family\nof grammar formalisms known as unification-based (or feature-based) grammar (see\nChapter 9 ), and to NLP applications implemented in the Prolog programming lan-\nguage. Although grammar-based NLP is still a significant area of research, it has become\nsomewhat eclipsed in the last 15\u201320 years due to a variety of factors. One significant\ninfluence came from automatic speech recognition. Although early work in speech\nprocessing adopted a model that emulated the kind of rule-based phonological pho-\nnology processing typified by the Sound Pattern of English  (Chomsky & Halle, 1968),\nthis turned out to be hopelessly inadequate in dealing with the hard problem of rec-\nognizing actual speech in anything like real time. By contrast, systems which involved\nlearning patterns from large bodies of speech data were significantly more accurate,\nefficient, and robust. In addition, the speech community found that progress in building\nbetter systems was hugely assisted by the construction of shared resources for quanti-\ntatively measuring performance against common test data. Eventually, much of the\nNLP community embraced a data-intensive orientation to language processing, cou-\npled with a growing use of machine-learning techniques and evaluation-led\nmethodology.\nContemporary Philosophical Divides\nThe contrasting approaches to NLP described in the preceding section relate back to\nearly metaphysical debates about rationalism versus empiricism and realism versus\nidealism that occurred in the Enlightenment period of Western philosophy. These\ndebates took place against a backdrop of orthodox thinking in which the source of all\nknowledge was believed to be divine revelation. During this period of the 17th and 18th\ncenturies, philosophers argued that human reason or sensory experience has priority\nover revelation. Descartes and Leibniz, among others, took the rationalist position,\nasserting that all truth has its origins in human thought, and in the existence of \u201cinnate\nideas\u201d implanted in our minds from birth. For example, they argued that the principles\nof Euclidean geometry were developed using human reason, and were not the result of\nsupernatural revelation or sensory experience. In contrast, Locke and others took the\nempiricist view, that our primary source of knowledge is the experience of our faculties,\nand that human reason plays a secondary role in reflecting on that experience. Often-\ncited evidence for this position was Galileo\u2019s discovery\u2014based on careful observation\nof the motion of the planets\u2014that the solar system is heliocentric and not geocentric.\nIn the context of linguistics, this debate leads to the following question: to what extent\ndoes human linguistic experience, versus our innate \u201clanguage faculty,\u201d provide the\nAfterword: The Language Challenge | 443\nbasis for our knowledge of language? In NLP this issue surfaces in debates about the\npriority of corpus data versus linguistic introspection in the construction of computa-\ntional models.\nA further \nconcern, enshrined in the debate between realism and idealism, was the\nmetaphysical status of the constructs of a theory. Kant argued for a distinction between\nphenomena, the manifestations we can experience, and \u201cthings in themselves\u201d which\ncan never been known directly. A linguistic realist would take a theoretical construct\nlike noun phrase  to be a real-world entity that exists independently of human percep-\ntion and reason, and which actually causes the observed linguistic phenomena. A lin-\nguistic idealist, on the other hand, would argue that noun phrases, along with more\nabstract constructs, like semantic representations, are intrinsically unobservable, and\nsimply play the role of useful fictions. The way linguists write about theories often\nbetrays a realist position, whereas NLP practitioners occupy neutral territory or else\nlean toward the idealist position. Thus, in NLP, it is often enough if a theoretical ab-\nstraction leads to a useful result; it does not matter whether this result sheds any light\non human linguistic processing.\nThese issues are still alive today, and show up in the distinctions between symbolic\nversus statistical methods, deep versus shallow processing, binary versus gradient clas-\nsifications, and scientific versus engineering goals. However, such contrasts are now\nhighly nuanced, and the debate is no longer as polarized as it once was. In fact, most\nof the discussions\u2014and most of the advances, even\u2014involve a \u201cbalancing act.\u201d For\nexample, one intermediate position is to assume that humans are innately endowed\nwith analogical and memory-based learning methods (weak rationalism), and use these\nmethods to identify meaningful patterns in their sensory language experience (empiri-\ncism).\nWe have seen many examples of this methodology throughout this book. Statistical\nmethods inform symbolic models anytime corpus statistics guide the selection of pro-\nductions in a context-free grammar, i.e., \u201cgrammar engineering.\u201d Symbolic methods\ninform statistical models anytime a corpus that was created using rule-based methods\nis used as a source of features for training a statistical language model, i.e., \u201cgrammatical\ninference.\u201d The circle is closed.\nNLTK Roadmap\nThe Natural Language Toolkit is a work in progress, and is being continually expanded\nas people contribute code. Some areas of NLP and linguistics are not (yet) well sup-\nported in NLTK, and contributions in these areas are especially welcome. Check http:\n//www.nltk.org/ for news about developments after the publication date of this book.\nContributions in the following areas are particularly encouraged:\n444 | Afterword: The Language Challenge\nPhonology and morphology\nComputational approaches \nto the study of sound patterns and word structures\ntypically use a finite-state toolkit. Phenomena such as suppletion and non-concat-\nenative morphology are difficult to address using the string-processing methods\nwe have been studying. The technical challenge is not only to link NLTK to a high-\nperformance finite-state toolkit, but to avoid duplication of lexical data and to link\nthe morphosyntactic features needed by morph analyzers and syntactic parsers.\nHigh-performance components\nSome NLP tasks are too computationally intensive for pure Python implementa-\ntions to be feasible. However, in some cases the expense arises only when training\nmodels, not when using them to label inputs. NLTK\u2019s package system provides a\nconvenient way to distribute trained models, even models trained using corpora\nthat cannot be freely distributed. Alternatives are to develop Python interfaces to\nhigh-performance machine learning tools, or to expand the reach of Python by\nusing parallel programming techniques such as MapReduce.\nLexical semantics\nThis is a vibrant area of current research, encompassing inheritance models of the\nlexicon, ontologies, multiword expressions, etc., mostly outside the scope of NLTK\nas it stands. A conservative goal would be to access lexical information from rich\nexternal stores in support of tasks in word sense disambiguation, parsing, and\nsemantic interpretation.\nNatural language generation\nProducing coherent text from underlying representations of meaning is an impor-\ntant part of NLP; a unification-based approach to NLG has been developed in\nNLTK, and there is scope for more contributions in this area.\nLinguistic fieldwork\nA major challenge faced by linguists is to document thousands of endangered lan-\nguages, work which generates heterogeneous and rapidly evolving data in large\nquantities. More fieldwork data formats, including interlinear text formats and\nlexicon interchange formats, could be supported in NLTK, helping linguists to\ncurate and analyze this data, while liberating them to spend as much time as pos-\nsible on data elicitation.\nOther languages\nImproved support for NLP in languages other than English could involve work in\ntwo areas: obtaining permission to distribute more corpora with NLTK\u2019s data col-\nlection; and writing language-specific HOWTOs for posting at http://www.nltk\n.org/howto, illustrating the use of NLTK and discussing language-specific problems\nfor NLP, including character encodings, word segmentation, and morphology.\nNLP researchers with expertise in a particular language could arrange to translate\nthis book and host a copy on the NLTK website; this would go beyond translating\nthe discussions to providing equivalent worked examples using data in the target\nlanguage, a non-trivial undertaking.\nAfterword: The Language Challenge | 445\nNLTK-Contrib\nMany of \nNLTK\u2019s core components were contributed by members of the NLP com-\nmunity, and were initially housed in NLTK\u2019s \u201cContrib\u201d package, nltk_contrib.\nThe only requirement for software to be added to this package is that it must be\nwritten in Python, relevant to NLP, and given the same open source license as the\nrest of NLTK. Imperfect software is welcome, and will probably be improved over\ntime by other members of the NLP community.\nTeaching materials\nSince the earliest days of NLTK development, teaching materials have accompa-\nnied the software, materials that have gradually expanded to fill this book, plus a\nsubstantial quantity of online materials as well. We hope that instructors who\nsupplement these materials with presentation slides, problem sets, solution sets,\nand more detailed treatments of the topics we have covered will make them avail-\nable, and will notify the authors so we can link them from http://www.nltk.org/. Of\nparticular value are materials that help NLP become a mainstream course in the\nundergraduate programs of computer science and linguistics departments, or that\nmake NLP accessible at the secondary level, where there is significant scope for\nincluding computational content in the language, literature, computer science, and\ninformation technology curricula.\nOnly a toolkit\nAs stated in the preface, NLTK is a toolkit, not a system. Many problems will be\ntackled with a combination of NLTK, Python, other Python libraries, and interfaces\nto external NLP tools and formats.\n446 | Afterword: The Language Challenge\nEnvoi...\nLinguists are \nsometimes asked how many languages they speak, and have to explain\nthat this field actually concerns the study of abstract structures that are shared by lan-\nguages, a study which is more profound and elusive than learning to speak as many\nlanguages as possible. Similarly, computer scientists are sometimes asked how many\nprogramming languages they know, and have to explain that computer science actually\nconcerns the study of data structures and algorithms that can be implemented in any\nprogramming language, a study which is more profound and elusive than striving for\nfluency in as many programming languages as possible.\nThis book has covered many topics in the field of Natural Language Processing. Most\nof the examples have used Python and English. However, it would be unfortunate if\nreaders concluded that NLP is about how to write Python programs to manipulate\nEnglish text, or more broadly, about how to write programs (in any programming lan-\nguage) to manipulate text (in any natural language). Our selection of Python and Eng-\nlish was expedient, nothing more. Even our focus on programming itself was only a\nmeans to an end: as a way to understand data structures and algorithms for representing\nand manipulating collections of linguistically annotated text, as a way to build new\nlanguage technologies to better serve the needs of the information society, and ulti-\nmately as a pathway into deeper understanding of the vast riches of human language.\nBut for the present: happy hacking!\nAfterword: The Language Challenge | 447\n\nBibliography\n[Abney, 1989] Steven P. Abney. A computational model of human parsing. Journal of\nPsycholinguistic Research, 18:129\u2013144, 1989.\n[Abney, 1991] \nSteven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P.\nAbney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycho-\nlinguistics, volume 44 of Studies in Linguistics and Philosophy. Kluwer Academic Pub-\nlishers, Dordrecht, 1991.\n[Abney, 1996a] Steven Abney. Part-of-speech tagging and partial parsing. In Ken\nChurch, Steve Young, and Gerrit Bloothooft, editors, Corpus-Based Methods in Lan-\nguage and Speech. Kluwer Academic Publishers, Dordrecht, 1996.\n[Abney, 1996b] Steven Abney. Statistical methods and linguistics . In Judith Klavans\nand Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Ap-\nproaches to Language. MIT Press, 1996.\n[Abney, 2008] Steven Abney. Semisupervised Learning for Computational Linguistics .\nChapman and Hall, 2008.\n[Agirre and Edmonds, 2007] Eneko Agirre and Philip Edmonds. Word Sense Disam-\nbiguation: Algorithms and Applications. Springer, 2007.\n[Alpaydin, 2004] Ethem Alpaydin. Introduction to Machine Learning . MIT Press, 2004.\n[Ananiadou and McNaught, 2006] Sophia Ananiadou and John McNaught, editors.\nText Mining for Biology and Biomedicine. Artech House, 2006.\n[Androutsopoulos et al., 1995] Ion Androutsopoulos, Graeme Ritchie, and Peter Tha-\nnisch. Natural language interfaces to databases\u2014an introduction. Journal of Natural\nLanguage Engineering, 1:29\u201381, 1995.\n[Artstein and Poesio, 2008] Ron Artstein and Massimo Poesio. Inter-coder agreement\nfor computational linguistics. Computational Linguistics, pages 555\u2013596, 2008.\n[Baayen, 2008] Harald Baayen. Analyzing Linguistic Data: A Practical Introduction to\nStatistics Using R. Cambridge University Press, 2008.\n449\n[Bachenko and Fitzpatrick, 1990] J. Bachenko and E. Fitzpatrick. A computational\ngrammar \nof discourse-neutral prosodic phrasing in English. Computational Linguis-\ntics, 16:155\u2013170, 1990.\n[Baldwin & Kim, 2010] Timothy Baldwin and Su Nam Kim. Multiword Expressions.\nIn Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Pro-\ncessing, second edition. Morgan and Claypool, 2010.\n[Beazley, 2006] David M. Beazley. Python Essential Reference . Developer\u2019s Library.\nSams Publishing, third edition, 2006.\n[Biber et al., 1998] Douglas Biber, Susan Conrad, and Randi Reppen. Corpus Linguis-\ntics: Investigating Language Structure and Use. Cambridge University Press, 1998.\n[Bird, 1999] Steven Bird. Multidimensional exploration of online linguistic field data.\nIn Pius Tamanji, Masako Hirotani, and Nancy Hall, editors, Proceedings of the 29th\nAnnual Meeting of the Northeast Linguistics Society , pages 33\u201347. GLSA, University of\nMassachussetts at Amherst, 1999.\n[Bird and Liberman, 2001] Steven Bird and Mark Liberman. A formal framework for\nlinguistic annotation. Speech Communication, 33:23\u201360, 2001.\n[Bird and Simons, 2003] Steven Bird and Gary Simons. Seven dimensions of portability\nfor language documentation and description. Language, 79:557\u2013582, 2003.\n[Blackburn and Bos, 2005] Patrick Blackburn and Johan Bos. Representation and In-\nference for Natural Language: A First Course in Computational Semantics . CSLI Publi-\ncations, Stanford, CA, 2005.\n[BNC, 1999] BNC. British National Corpus, 1999. [http://info.ox.ac.uk/bnc/].\n[Brent and Cartwright, 1995] Michael Brent and Timothy Cartwright. Distributional\nregularity and phonotactic constraints are useful for segmentation. In Michael Brent,\neditor, Computational Approaches to Language Acquisition. MIT Press, 1995.\n[Bresnan and Hay, 2006] Joan Bresnan and Jennifer Hay. Gradient grammar: An effect\nof animacy on the syntax of give in New Zealand and American English. Lingua 118:\n254\u201359, 2008.\n[Budanitsky and Hirst, 2006] Alexander Budanitsky and Graeme Hirst. Evaluating\nwordnet-based measures of lexical semantic relatedness. Computational Linguistics,\n32:13\u201348, 2006.\n[Burton-Roberts, 1997] Noel Burton-Roberts. Analysing Sentences. Longman, 1997.\n[Buseman et al., 1996] Alan Buseman, Karen Buseman, and Rod Early. The Linguist\u2019s\nShoebox: Integrated Data Management and Analysis for the Field Linguist . Waxhaw NC:\nSIL, 1996.\n[Carpenter, 1992] Bob Carpenter. The Logic of Typed Feature Structures . Cambridge\nUniversity Press, 1992.\n450 | Bibliography\n[Carpenter, 1997] Bob Carpenter. Type-Logical Semantics. MIT Press, 1997.\n[Chierchia and McConnell-Ginet, 1990] Gennaro Chierchia and Sally McConnell-Gi-\nnet. Meaning and \nGrammar: An Introduction to Meaning . MIT Press, Cambridge, MA,\n1990.\n[Chomsky, 1965] Noam Chomsky. Aspects of the Theory of Syntax . MIT Press, Cam-\nbridge, MA, 1965.\n[Chomsky, 1970] Noam Chomsky. Remarks on nominalization. In R. Jacobs and P.\nRosenbaum, editors, Readings in English Transformational Grammar. Blaisdell, Wal-\ntham, MA, 1970.\n[Chomsky and Halle, 1968] Noam Chomsky and Morris Halle. The Sound Pattern of\nEnglish. New York: Harper and Row, 1968.\n[Church and Patil, 1982] Kenneth Church and Ramesh Patil. Coping with syntactic\nambiguity or how to put the block in the box on the table. American Journal of Com-\nputational Linguistics, 8:139\u2013149, 1982.\n[Cohen and Hunter, 2004] K. Bretonnel Cohen and Lawrence Hunter. Natural lan-\nguage processing and systems biology. In Werner Dubitzky and Francisco Azuaje, ed-\nitors, Artificial Intelligence Methods and Tools for Systems Biology , page 147\u2013174\nSpringer Verlag, 2004.\n[Cole, 1997] Ronald Cole, editor. Survey of the State of the Art in Human Language\nTechnology. Studies in Natural Language Processing. Cambridge University Press,\n1997.\n[Copestake, 2002] Ann Copestake. Implementing Typed Feature Structure Grammars .\nCSLI Publications, Stanford, CA, 2002.\n[Corbett, 2006] Greville G. Corbett. Agreement. Cambridge University Press, 2006.\n[Croft et al., 2009] Bruce Croft, Donald Metzler, and Trevor Strohman. Search Engines:\nInformation Retrieval in Practice. Addison Wesley, 2009.\n[Daelemans and van den Bosch, 2005] Walter Daelemans and Antal van den Bosch.\nMemory-Based Language Processing. Cambridge University Press, 2005.\n[Dagan et al., 2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL\nrecognising textual entailment challenge. In J. Quinonero-Candela, I. Dagan, B. Mag-\nnini, and F. d\u2019Alch\u00e9 Buc, editors, Machine Learning Challenges , volume 3944 of Lecture\nNotes in Computer Science, pages 177\u2013190. Springer, 2006.\n[Dale et al., 2000] Robert Dale, Hermann Moisl, and Harold Somers, editors. Handbook\nof Natural Language Processing. Marcel Dekker, 2000.\n[Dalrymple, 2001] Mary Dalrymple. Lexical Functional Grammar , volume 34 of Syntax\nand Semantics. Academic Press, New York, 2001.\nBibliography | 451\n[Dalrymple et al., 1999] Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat.\nRelating \nresource-based semantics to categorial semantics. In Mary Dalrymple, editor,\nSemantics and Syntax in Lexical Functional Grammar: The Resource Logic Approach ,\npages 261\u2013280. MIT Press, Cambridge, MA, 1999.\n[Dowty et al., 1981] David R. Dowty, Robert E. Wall, and Stanley Peters. Introduction\nto Montague Semantics. Kluwer Academic Publishers, 1981.\n[Earley, 1970] Jay Earley. An efficient context-free parsing algorithm. Communications\nof the Association for Computing Machinery, 13:94\u2013102, 1970.\n[Emele and Zajac, 1990] Martin C. Emele and R\u00e9mi Zajac. Typed unification gram-\nmars. In Proceedings of the 13th Conference on Computational Linguistics , pages 293\u2013\n298. Association for Computational Linguistics, Morristown, NJ, 1990.\n[Farghaly, 2003] Ali Farghaly, editor. Handbook for Language Engineers. CSLI Publi-\ncations, Stanford, CA, 2003.\n[Feldman and Sanger, 2007] Ronen Feldman and James Sanger. The Text Mining\nHandbook: Advanced Approaches in Analyzing Unstructured Data . Cambridge Univer-\nsity Press, 2007.\n[Fellbaum, 1998] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Data-\nbase. MIT Press, 1998. http://wordnet.princeton.edu/.\n[Finegan, 2007] Edward Finegan. Language: Its Structure and Use . Wadsworth, Fifth\nedition, 2007.\n[Forsyth and Martell, 2007] Eric N. Forsyth and Craig H. Martell. Lexical and discourse\nanalysis of online chat dialog. In Proceedings of the First IEEE International Conference\non Semantic Computing, pages 19\u201326, 2007.\n[Friedl, 2002] Jeffrey E. F. Friedl. Mastering Regular Expressions . O\u2019Reilly, second ed-\nition, 2002.\n[Gamut, 1991a] L. T. F. Gamut. Intensional Logic and Logical Grammar , volume 2 of\nLogic, Language and Meaning. University of Chicago Press, Chicago, 1991.\n[Gamut, 1991b] L. T. F. Gamut. Introduction to Logic , volume 1 of Logic, Language\nand Meaning. University of Chicago Press, 1991.\n[Garofolo et al., 1986] John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon\nG. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-\nPhonetic Continuous Speech Corpus CDROM. NIST, 1986.\n[Gazdar et al., 1985] Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag (1985).\nGeneralized Phrase Structure Grammar. Basil Blackwell, 1985.\n[Gomes et al., 2006] Bruce Gomes, William Hayes, and Raf Podowski. Text mining.\nIn Darryl Leon and Scott Markel, editors, In Silico Technologies in Drug Target Identi-\nfication and Validation, Taylor & Francis, 2006.\n452 | Bibliography\n[Gries, 2009] Stefan Gries. Quantitative Corpus Linguistics with R: A Practical Intro-\nduction. Routledge, 2009.\n[Guzdial, 2005] Mark Guzdial. Introduction to Computing and Programming in Python:\nA Multimedia Approach. Prentice Hall, 2005.\n[Harel, 2004] David Harel. Algorithmics: The Spirit of Computing. Addison Wesley,\n2004.\n[Hastie et al., 2009] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The El-\nements of Statistical Learning: Data Mining, Inference, and Prediction . Springer, second\nedition, 2009.\n[Hearst, 1992] Marti Hearst. Automatic acquisition of hyponyms from large text cor-\npora. In Proceedings of the 14th Conference on Computational Linguistics (COLING) ,\npages 539\u2013545, 1992.\n[Heim and Kratzer, 1998] Irene Heim and Angelika Kratzer. Semantics in Generative\nGrammar. Blackwell, 1998.\n[Hirschman et al., 2005] Lynette Hirschman, Alexander Yeh, Christian Blaschke, and\nAlfonso Valencia. Overview of BioCreAtIvE: critical assessment of information extrac\ntion for biology. BMC Bioinformatics, 6, May 2005. Supplement 1.\n[Hodges, 1977] Wilfred Hodges. Logic. Penguin Books, Harmondsworth, 1977.\n[Huddleston and Pullum, 2002] Rodney D. Huddleston and Geoffrey K. Pullum. The\nCambridge Grammar of the English Language. Cambridge University Press, 2002.\n[Hunt and Thomas, 2000] Andrew Hunt and David Thomas. The Pragmatic Program-\nmer: From Journeyman to Master. Addison Wesley, 2000.\n[Indurkhya and Damerau, 2010] Nitin Indurkhya and Fred Damerau, editors. Hand-\nbook of Natural Language Processing . CRC Press, Taylor and Francis Group, second\nedition, 2010.\n[Jackendoff, 1977] Ray Jackendoff. X-Syntax: a Study of Phrase Strucure . Number 2 in\nLinguistic Inquiry Monograph. MIT Press, Cambridge, MA, 1977.\n[Johnson, 1988] Mark Johnson. Attribute Value Logic and Theory of Grammar. CSLI\nLecture Notes Series. University of Chicago Press, 1988.\n[Jurafsky and Martin, 2008] Daniel Jurafsky and James H. Martin. Speech and\nLanguage Processing. Prentice Hall, second edition, 2008.\n[Kamp and Reyle, 1993] Hans Kamp and Uwe Reyle. From Discourse to the Lexicon:\nIntroduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Dis-\ncourse Representation Theory. Kluwer Academic Publishers, 1993.\n[Kaplan, 1989] Ronald Kaplan. The formal architecture of lexical-functional grammar.\nIn Chu-Ren Huang and Keh-Jiann Chen, editors, Proceedings of ROCLING II , pages\n1\u201318. CSLI, 1989. Reprinted in Dalrymple, Kaplan, Maxwell, and Zaenen (eds), Formal\nBibliography | 453\nIssues in Lexical-Functional Grammar , pages 7\u201327. CSLI Publications, Stanford, CA,\n1995.\n[Kaplan and Bresnan, 1982] Ronald Kaplan and Joan Bresnan. Lexical-functional\ngrammar: A formal system for grammatical representation. In Joan Bresnan, editor,\nThe Mental Representation of Grammatical Relations , pages 173\u2013281. MIT Press, Cam-\nbridge, MA, 1982.\n[Kasper and Rounds, 1986] Robert T. Kasper and William C. Rounds. A logical se-\nmantics for feature structures. In Proceedings of the 24th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 257\u2013266. Association for Computational\nLinguistics, 1986.\n[Kathol, 1999] Andreas Kathol. Agreement and the syntax-morphology interface in\nHPSG. In Robert D. Levine and Georgia M. Green, editors, Studies in Contemporary\nPhrase Structure Grammar, pages 223\u2013274. Cambridge University Press, 1999.\n[Kay, 1985] Martin Kay. Unification in grammar. In Ver\u00f3nica Dahl and Patrick Saint-\nDizier, editors, Natural Language Understanding and Logic Programming , pages 233\u2013\n240. North-Holland, 1985. Proceedings of the First International Workshop on Natural\nLanguage Understanding and Logic Programming.\n[Kiss and Strunk, 2006] Tibor Kiss and Jan Strunk. Unsupervised multilingual sentence\nboundary detection. Computational Linguistics, 32: 485\u2013525, 2006.\n[Kiusalaas, 2005] Jaan Kiusalaas. Numerical Methods in Engineering with Python . Cam-\nbridge University Press, 2005.\n[Klein and Manning, 2003] Dan Klein and Christopher D. Manning. A* parsing: Fast\nexact viterbi parse selection. In Proceedings of HLT-NAACL 03, 2003.\n[Knuth, 2006] Donald E. Knuth. The Art of Computer Programming, Volume 4: Gen-\nerating All Trees. Addison Wesley, 2006.\n[Lappin, 1996] Shalom Lappin, editor. The Handbook of Contemporary Semantic\nTheory. Blackwell Publishers, Oxford, 1996.\n[Larson and Segal, 1995] Richard Larson and Gabriel Segal. Knowledge of Meaning: An\nIntroduction to Semantic Theory. MIT Press, Cambridge, MA, 1995.\n[Levin, 1993] Beth Levin. English Verb Classes and Alternations . University of Chicago\nPress, 1993.\n[Levitin, 2004] Anany Levitin. The Design and Analysis of Algorithms . Addison Wesley,\n2004.\n[Lutz and Ascher, 2003] Mark Lutz and David Ascher. Learning Python . O\u2019Reilly, sec-\nond edition, 2003.\n[MacWhinney, 1995] Brian MacWhinney. The CHILDES Project: Tools for Analyzing\nTalk. Mahwah, NJ: Lawrence Erlbaum, second edition, 1995. [ http://childes.psy.cmu\n.edu/].\n454 | Bibliography\n[Madnani, 2007] Nitin Madnani. Getting started on natural language processing with\nPython. \nACM Crossroads, 13(4), 2007.\n[Manning, 2003] Christopher Manning. Probabilistic syntax. In Probabilistic Linguis-\ntics, pages 289\u2013341. MIT Press, Cambridge, MA, 2003.\n[Manning and Sch\u00fctze, 1999] Christopher Manning and Hinrich Sch\u00fctze. Foundations\nof Statistical Natural Language Processing. MIT Press, Cambridge, MA, 1999.\n[Manning et al., 2008] Christopher Manning, Prabhakar Raghavan, and Hinrich Sch\u00fc-\ntze. Introduction to Information Retrieval. Cambridge University Press, 2008.\n[McCawley, 1998] James McCawley. The Syntactic Phenomena of English . University\nof Chicago Press, 1998.\n[McConnell, 2004] Steve McConnell. Code Complete: A Practical Handbook of Software\nConstruction. Microsoft Press, 2004.\n[McCune, 2008] William McCune. Prover9: Automated theorem prover for first-order\nand equational logic, 2008.\n[McEnery, 2006] Anthony McEnery. Corpus-Based Language Studies: An Advanced\nResource Book. Routledge, 2006.\n[Melamed, 2001] Dan Melamed. Empirical Methods for Exploiting Parallel Texts . MIT\nPress, 2001.\n[Mertz, 2003] David Mertz. Text Processing in Python . Addison-Wesley, Boston, MA,\n2003.\n[Meyer, 2002] Charles Meyer. English Corpus Linguistics: An Introduction . Cambridge\nUniversity Press, 2002.\n[Miller and Charles, 1998] George Miller and Walter Charles. Contextual correlates of\nsemantic similarity. Language and Cognitive Processes, 6:1\u201328, 1998.\n[Mitkov, 2002a] Ruslan Mitkov. Anaphora Resolution. Longman, 2002.\n[Mitkov, 2002b] Ruslan Mitkov, editor. Oxford Handbook of Computational Linguis-\ntics. Oxford University Press, 2002.\n[M\u00fcller, 2002] Stefan M\u00fcller. Complex Predicates: Verbal Complexes, Resultative Con-\nstructions, and Particle Verbs in German . Number 13 in Studies in Constraint-Based\nLexicalism. Center for the Study of Language and Information, Stanford, 2002. http://\nwww.dfki.de/~stefan/Pub/complex.html.\n[Nerbonne et al., 1994] John Nerbonne, Klaus Netter, and Carl Pollard. German in\nHead-Driven Phrase Structure Grammar. CSLI Publications, Stanford, CA, 1994.\n[Nespor and Vogel, 1986] Marina Nespor and Irene Vogel. Prosodic Phonology . Num-\nber 28 in Studies in Generative Grammar. Foris Publications, Dordrecht, 1986.\nBibliography | 455\n[Nivre et al., 2006] J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-\ngenerator for dependency parsing\n. In Proceedings of LREC, pages 2216\u20132219, 2006.\n[Niyogi, 2006] Partha Niyogi. The Computational Nature of Language Learning and\nEvolution. MIT Press, 2006.\n[O\u2019Grady et al., 2004] William O\u2019Grady, John Archibald, Mark Aronoff, and Janie\nRees-Miller. Contemporary Linguistics: An Introduction. St. Martin\u2019s Press, fifth edition,\n2004.\n[OSU, 2007] OSU, editor. Language Files: Materials for an Introduction to Language\nand Linguistics. Ohio State University Press, tenth edition, 2007.\n[Partee, 1995] Barbara Partee. Lexical semantics and compositionality. In L. R. Gleit-\nman and M. Liberman, editors, An Invitation to Cognitive Science: Language , volume\n1, pages 311\u2013360. MIT Press, 1995.\n[Pasca, 2003] Marius Pasca. Open-Domain Question Answering from Large Text Col-\nlections. CSLI Publications, Stanford, CA, 2003.\n[Pevzner and Hearst, 2002] L. Pevzner and M. Hearst. A critique and improvement of\nan evaluation metric for text segmentation. Computational Linguistics , 28:19\u201336, 2002.\n[Pullum, 2005] Geoffrey K. Pullum. Fossilized prejudices about \u201chowever\u201d, 2005.\n[Radford, 1988] Andrew Radford. Transformational Grammar: An Introduction . Cam-\nbridge University Press, 1988.\n[Ramshaw and Marcus, 1995] Lance A. Ramshaw and Mitchell P. Marcus. Text chunk-\ning using transformation-based learning. In Proceedings of the Third ACL Workshop on\nVery Large Corpora, pages 82\u201394, 1995.\n[Reppen et al., 2005] Randi Reppen, Nancy Ide, and Keith Suderman. American Na\ntional Corpus. Linguistic Data Consortium, 2005.\n[Robinson et al., 2007] Stuart Robinson, Greg Aumann, and Steven Bird. Managing\nfieldwork data with toolbox and the natural language toolkit . Language Documentation\nand Conservation, 1:44\u201357, 2007.\n[Sag and Wasow, 1999] Ivan A. Sag and Thomas Wasow. Syntactic Theory: A Formal\nIntroduction. CSLI Publications, Stanford, CA, 1999.\n[Sampson and McCarthy, 2005] Geoffrey Sampson and Diana McCarthy. Corpus Lin-\nguistics: Readings in a Widening Discipline. Continuum, 2005.\n[Scott and Tribble, 2006] Mike Scott and Christopher Tribble. Textual Patterns: Key\nWords and Corpus Analysis in Language Education. John Benjamins, 2006.\n[Segaran, 2007] Toby Segaran. Collective Intelligence. O\u2019Reilly Media, 2007.\n[Shatkay and Feldman, 2004] Hagit Shatkay and R. Feldman. Mining the biomedical\nliterature in the genomic era: An overview. Journal of Computational Biology , 10:821\u2013\n855, 2004.\n456 | Bibliography\n[Shieber, 1986] Stuart M. Shieber. An Introduction to Unification-Based Approaches to\nGrammar, volume 4 of CSLI Lecture Notes Series .CSLI Publications, Stanford, CA,\n1986.\n[Shieber et al., 1983] Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson,\nand Mabry Tyson. The formalism and implementation of PATR-II. In Barbara J. Grosz\nand Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge ,\ntechreport 4, pages 39\u201379. SRI International, Menlo Park, CA, November 1983. ( http:\n//www.eecs.harvard.edu/ shieber/Biblio/Papers/Shieber-83-FIP.pdf)\n[Simons and Bird, 2003] Gary Simons and Steven Bird. The Open Language Archives\nCommunity: An infrastructure for distributed archiving of language resources. Literary\nand Linguistic Computing, 18:117\u2013128, 2003.\n[Sproat et al., 2001] Richard Sproat, Alan Black, Stanley Chen, Shankar Kumar, Mari\nOstendorf, and Christopher Richards. Normalization of non-standard words. Com-\nputer Speech and Language, 15:287\u2013333, 2001.\n[Strunk and White, 1999] William Strunk and E. B. White. The Elements of Style. Bos-\nton, Allyn and Bacon, 1999.\n[Thompson and McKelvie, 1997] Henry S. Thompson and David McKelvie. Hyperlink\nsemantics for standoff markup of read-only documents. In SGML Europe \u201997 , 1997.\nhttp://www.ltg.ed.ac.uk/~ht/sgmleu97.html.\n[TLG, 1999] TLG. Thesaurus Linguae Graecae, 1999.\n[Turing, 1950] Alan M. Turing. Computing machinery and intelligence . Mind, 59(236):\n433\u2013460, 1950.\n[van Benthem and ter Meulen, 1997] Johan van Benthem and Alice ter Meulen, editors.\nHandbook of Logic and Language. MIT Press, Cambridge, MA, 1997.\n[van Rossum and Drake, 2006a] Guido van Rossum and Fred L. Drake. An Introduction\nto Python\u2014The Python Tutorial. Network Theory Ltd, Bristol, 2006.\n[van Rossum and Drake, 2006b] Guido van Rossum and Fred L. Drake. The Python\nLanguage Reference Manual. Network Theory Ltd, Bristol, 2006.\n[Warren and Pereira, 1982] David H. D. Warren and Fernando C. N. Pereira. An effi-\ncient easily adaptable system for interpreting natural language queries. American Jour-\nnal of Computational Linguistics, 8(3-4):110\u2013122, 1982.\n[Wechsler and Zlatic, 2003] Stephen Mark Wechsler and Larisa Zlatic. The Many Faces\nof Agreement . Stanford Monographs in Linguistics. CSLI Publications, Stanford, CA,\n2003.\n[Weiss et al., 2004] Sholom Weiss, Nitin Indurkhya, Tong Zhang, and Fred Damerau.\nText Mining: Predictive Methods for Analyzing Unstructured Information . Springer,\n2004.\nBibliography | 457\n[Woods et al., 1986] Anthony Woods, Paul Fletcher, and Arthur Hughes. Statistics in\nLanguage Studies\n. Cambridge University Press, 1986.\n[Zhao and Zobel, 2007] Y. Zhao and J. Zobel. Search with style: Authorship attribution\nin classic literature. In Proceedings of the Thirtieth Australasian Computer Science Con-\nference. Association for Computing Machinery, 2007.\n458 | Bibliography\nNLTK Index\nSymbols\nA\nabspath, 50\naccuracy, 119, 149, 217\nAnaphoraResolutionException, 401\nAndExpression, 369\nappend, 11, 86, 127, 197\nApplicationExpression, 405\napply, 10\napply_features, 224\nAssignment, 378\nassumptions, 383\nB\nbabelize_shell, 30\nbackground, 21\nbackoff, 200, 201, 205, 208\nbatch_evaluate, 393\nbatch_interpret, 393\nbigrams, 20, 55, 56, 141\nBigramTagger, 274\nBracketParseCorpusReader, 51\nbuild_model, 383\nC\nchart, 168\nChat, 4\nchat, 105, 163, 215\nchat80, 363\nchat80.sql_query, 363\nchild, 82, 162, 170, 180, 281, 316, 334, 431\nchildren, 187, 334, 335, 422\nchunk, 267, 273, 275, 277\nChunkParserI, 273classifier, 223, 224, 225, 226, 227, 228, 229,\n231, 234, 235, 239\nclassify, 228\ncollocations, 20, 21\ncommon_contexts, 5, 6\nconcordance, 4, 108\nConditionalFreqDist, 52, 53, 56\nconditions, 44, 53, 54, 55, 56\nconlltags2tree, 273\nConstantExpression, 373\ncontext, 5, 108, 180, 210\nCooperStore, 396\ncooper_storage, 396\ncorpora, 51, 85, 94, 270, 363, 427, 434\ncorpus, 40, 51, 241, 284, 285, 315\ncorrect, 210, 226\ncount, 9, 119, 139, 224, 225\nD\ndata, 46, 147, 188\ndefault, 193, 199\ndisplay, 200, 201, 308, 309\ndistance, 155\ndraw, 265, 280, 323, 398, 429\ndraw_trees, 324\nDRS, 398\nDrtParser, 400\nE\nedit_distance, 155\nElement, 427, 428, 430, 438\nElementTree, 427, 430, 434\nellipsis, 111\nem, 67\nencoding, 50, 95, 96, 434, 436\nentries, 63, 64, 66, 316, 433\nentropy, 244\nWe\u2019d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n459\nentry, 63, 316, 418, 419, 425, 426, 427, 431,\n432, 433\nerror, 14, 65\nevaluate, 115, 216, 217, 371, 379, 380\nExpression, 369, 375, 376, 399\nextract_property, 149, 150, 152\nF\nFeatStruct, 337\nfeed, 83\nfileid, 40, 41, 42, 45, 46, 50, 54, 62, 227, 288\nfilename, 125, 289\nfindall, 105, 127, 430\nfol, 399\nformat, 117, 120, 121, 157, 419, 436\nfreq, 17, 21, 213\nFreqDist, 17, \n18, 21, 22, 36, 52, 53, 56, 59, 61,\n135, 147, 153, 177, 185, 432\nfreqdist, 61, 147, 148, 153, 244\nG\ngenerate, 6, 7\nget, 68, 185, 194\ngetchildren, 427, 428\ngrammar, 265, 267, 269, 272, 278, 308, 311, 317,\n320, 321, 396, 433, 434, 436\nGrammar, 320, 334, 351, 354, 436\nH\nhole, 99\nhyp_extra, 236\nI\nic, 176\nieer, 284\nIffExpression, 369\nindex, 13, 14, 16, 24, 90, 127, 134, 308\ninference, 370\nJ\njaccard_distance, 155\nK\nkeys, 17, 192\nL\nLambdaExpression, 387\nlancaster, 107\nleaves, 51, 71, 422\nLemma, 68, 71lemma, 68, 69, 214\nlemmas, 68\nlength, 25, 61, 136, 149\nload, 124, 206\nload_corpus, 147\nload_earley, 335, 352, 355, 363, 392, 400\nload_parser, 334\nlogic, 376, 389\nLogicParser, 369, 370, 373, 375, 388, 400,\n404\nM\nMace, 383\nMaceCommand, 383\nmaxent, 275\nmegam, 275\nmember_holonyms, 70, 74\nmember_meronyms, 74\nmetrics, 154, 155\nmodel, 200, 201\nModel, 201, 382\nN\nnbest_parse, 334\nne, 236, 237, 283\nNegatedExpression, 369\nngrams, 141\nNgramTagger, 204\nnltk.chat.chatbots, 31\nnltk.classify, 224\nnltk.classify.rte_classify, 237\nnltk.cluster, 172\nnltk.corpus, 40, 42, 43, 44, 45, 48, 51, 53, 54,\n60, 65, 67, 90, 105, 106, 119, 162,\n170, 184, 188, 195, 198, 203, 223,\n227, 258, 259, 271, 272, 285, 315,\n316, 422, 430, 431\nnltk.data.find, 85, 94, 427, 434\nnltk.data.load, 112, 300, 334\nnltk.data.show_cfg, 334, 351, 354, 363\nnltk.downloader, 316\nnltk.draw.tree, 324\nnltk.etree.ElementTree, 427, 430, 432, 434\nnltk.grammar, 298\nnltk.help.brown_tagset, 214\nnltk.help.upenn_tagset, 180, 214\nnltk.inference.discourse, 400\nnltk.metrics.agreement, 414\nnltk.metrics.distance, 155\nnltk.parse, 335, 352, 363, 392, 400\nnltk.probability, 219\nnltk.sem, 363, 396\nnltk.sem.cooper_storage, 396\n460 | NLTK Index\nnltk.sem.drt_resolve_anaphora, 399\nnltk.tag, 401\nnltk.tag.brill.demo, 210, 218\nnltk.text.Text, 81\nnode, 170\nnps_chat, 42, 105, 235\nO\nolac, 436\nOrExpression, 369\nP\npackages, 154\nparse, 273, 275, 320, 375, 398, 427\nparsed, 51, 373\nParseI, 326\nparse_valuation, 378\npart_holonyms, 74\npart_meronyms, 70, 74\npath, 85, 94, 95, 96\npath_similarity, 72\nphones, 408\nphonetic, 408, 409\nPlaintextCorpusReader, 51\nporter, 107, 108\nposts, 65, 235\nppattach, 259\nPPAttachment, 258, 259\nproductions, 308, 311, 320, 334\nprove, 376\nProver9, 376\npunkt, 112\nR\nRecursiveDescentParser, 302, 304\nregexp, 102, 103, 105, 122\nRegexpChunk, 287\nRegexpParser, 266, 286\nRegexpTagger, 217, 219, 401\nregexp_tokenize, 111\nresolve_anaphora, 399\nreverse, 195\nrte_features, 236\nS\nsamples, 22, 44, 54, 55, 56\nsatisfiers, 380, 382\nsatisfy, 155\nscore, 115, 272, 273, 274, 276, 277\nsearch, 177\nSEM, 362, 363, 385, 386, 390, 393, 395, 396,\n403sem, 363, 396, 400\nsem.evaluate, 406\nSenseval, 257\nsenseval, 258\nShiftReduceParser, 305\nshow_clause, 285\nshow_most_informative_features, 228\nshow_raw_rtuple, 285\nsimilar, 5, 6, 21, 319\nsimplify, 388\nsort, 12, 136, 192\nSpeakerInfo, 409\nsr, 65\nState, 20, 187\nstem, 104, 105\nstr2tuple, 181\nSubElement, 432\nsubstance_holonyms, 74\nsubstance_meronyms, 70, 74\nSynset, 67, 68, 69, 70, 71, 72\nsynset, 68, 69, 70, 71, 425, 426\ns_retrieve, 396\nT\ntabulate, 54, 55, 119\ntag, 146, 164, 181, 184, 185, 186, 187, 188, 189,\n195, 196, 198, 207, 210, 226, 231,\n232, 233, 241, 273, 275\ntagged_sents, 183, 231, 233, 238, 241, 275\ntagged_words, 182, 187, 229\ntags, 135, 164, 188, 198, 210, 277, 433\nText, 4, 284, 436\ntoken, 26, 105, 139, 319, 421\ntokenize, 263\ntokens, 16, 80, 81, 82, 86, 105, 107, 108, 111,\n139, 140, 153, 198, 206, 234, 308,\n309, 317, 328, 335, 352, 353, 355,\n392\ntoolbox, 66, 67, 430, 431, 434, 438\ntoolbox.ToolboxData, 434\ntrain, 112, 225\ntranslate, 66, 74\ntree, 268, 294, 298, 300, 301, 311, 316, 317,\n319, 335, 352, 353, 355, 393, 430,\n434\nTree, 315, 322\nTree.productions, 325\ntree2conlltags, 273\ntreebank, 51, 315, 316\ntrees, 294, 311, 334, 335, 363, 392, 393, 396,\n400\ntrigrams, 141\nTrigramTagger, 205\ntuples, 192\nNLTK Index | 461\nturns, 12\nType, 2, 4, 169\nU\nUndefined, 379\nunify, 342\nUnigramTagger, 200, 203, 219, 274\nurl, 80, 82, 147, 148\nV\nValuation, 371, 378\nvalues, 149, 192\nVariable, 375\nVariableBinderExpression, 389\nW\nwordlist, 61, 64, 98, 99, 111, 201, 424\nwordnet, 67, 162, 170\nX\nxml, 427, 436\nxml_posts, 235\n462 | NLTK Index\nGeneral Index\nSymbols\n! (exclamation mark)\n!= (not equal to) operator, 22, 376\n\" \" (quotation marks, double), in strings, 87\n$ (dollar sign) in regular expressions, 98, 101\n% (percent sign)\n%% in string formatting, 119\n%*s formatting string, 107, 119\n%s and %d conversion specifiers, 118\n& (ampersand), and operator, 368\n' ' (quotation marks, single) in strings, 88\n' ' (quotation marks, single), in strings, 87\n' (apostrophe) in tokenization, 110\n( ) (parentheses)\nadding extra to break lines of code, 139\nenclosing expressions in Python, 2\nin function names, 9\nin regular expressions, 100, 104\nin tuples, 134\nuse with strings, 88\n* (asterisk)\n*? non-greedy matching in regular\nexpressions, 104\nmultiplication operator, 2\nmultiplying strings, 88\nin regular expressions, 100, 101\n+ (plus sign)\n+= (addition and assignment) operator,\n195\nconcatenating lists, 11\nconcatenating strings, 16, 88\nin regular expressions, 100, 101\n, (comma) operator, 133\n- (hyphen) in tokenization, 110- (minus sign), negation operator, 368\n-> (implication) operator, 368\n. (dot) wildcard character in regular\nexpressions, 98, 101\n/ (slash),\ndivision operator, 2\n: (colon), ending Python statements, 26\n< (less than) operator, 22\n<-> (equivalence) operator, 368\n<= (less than or equal to) operator, 22\n= (equals sign)\n== (equal to) operator, 22\n== (identity) operator, 132\nassignment operator, 14, 130\nequality operator, 376\n> (greater than) operator, 22\n>= (greater than or equal to) operator, 22\n? (question mark) in regular expressions, 99,\n101\n[ ] (brackets)\nenclosing keys in dictionary, 65\nindexing lists, 12\nomitting in list comprehension used as\nfunction parameter, 55\nregular expression character classes, 99\n\\ (backslash)\nending broken line of code, 139\nescaping string literals, 87\nin regular expressions, 100, 101\nuse with multiline strings, 88\n^ (caret)\ncharacter class negation in regular\nexpressions, 100\nend of string matching in regular\nexpressions, 99\nWe\u2019d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n463\nregular expression metacharacter, 101\n{ } (curly braces) in regular expressions, 100\n| (pipe character)\nalternation in regular expressions, 100, 101\nor operator, 368\n\u03b1-conversion, 389\n\u03b1-equivalents, 389\n\u03b2-reduction, 388\n\u03bb (lambda operator), 386\u2013390\nA\naccumulative functions, 150\naccuracy of classification, 239\nACL (Association for Computational\nLinguistics), 34\nSpecial Interest Group on Web as Corpus\n(SIGWAC), 416\nadjectives, categorizing and tagging, 186\nadjuncts of lexical head, 347\nadverbs, categorizing and tagging, 186\nagreement, 329\u2013331\nresources for further reading, 357\nalgorithm design, 160\u2013167\ndynamic programming, 165\nrecursion, 161\nresources for further information, 173\nall operator, 376\nalphabetic variants, 389\nambiguity\nbroad-coverage grammars and, 317\ncapturing structural ambiguity with\ndependency parser, 311\nquantifier scope, 381, 394\u2013397\nscope of modifier, 314\nstructurally ambiguous sentences, 300\nubiquitous ambiguity in sentence structure,\n293\nanagram dictionary, creating, 196\nanaphora resolution, 29\nanaphoric antecedent, 397\nAND (in SQL), 365\nand operator, 24\nannotated text corpora, 46\u201348\nannotation layers\ncreating, 412\ndeciding which to include when acquiring\ndata, 420\nquality control for, 413\nsurvey of annotation software, 438annotation, inline, 421\nantecedent, 28\nantonymy, 71\napostrophes in tokenization, 110\nappending, 11\narguments\nfunctions as, 149\nnamed, 152\npassing to functions (example), 143\narguments in logic, 369, 372\narity, 378\narticles, 186\nassert statements\nusing in defensive programming, 159\nusing to find logical errors, 146\nassignment, 130, 378\ndefined, 14\nto list index values, 13\nAssociation for \nComputational Linguistics (see\nACL)\nassociative arrays, 189\nassumptions, 369\natomic values, 336\nattribute value matrix, 336\nattribute-value pairs (Toolbox lexicon), 67\nattributes, XML, 426\nauxiliaries, 348\nauxiliary verbs, 336\ninversion and, 348\nB\n\\b word boundary in regular expressions, 110\nbackoff, 200\nbacktracking, 303\nbar charts, 168\nbase case, 161\nbasic types, 373\nBayes classifier (see naive Bayes classifier)\nbigram taggers, 204\nbigrams, 20\ngenerating random text with, 55\nbinary formats, text, 85\nbinary predicate, 372\nbinary search, 160\nbinding variables, 374\nbinning, 249\nBIO Format, 286\nbook module (NLTK), downloading, 3\nBoolean operators, 368\n464 | General Index\nin propositional logic, truth conditions for,\n368\nBoolean values, 336\nbottom-up approach to dynamic\nprogramming, 167\nbottom-up parsing, 304\nbound, 374, 375\nbreakpoints, 158\nBrill tagging, 208\ndemonstration of NLTK Brill tagger, 209\nsteps in, 209\nBrown Corpus, 42\u201344\nbugs, 156\nC\ncall structure, 165\ncall-by-value, 144\ncarriage return and linefeed characters, 80\ncase in German, 353\u2013356\nCatalan numbers, 317\ncategorical grammar, 346\ncategorizing and tagging words, 179\u2013219\nadjectives and adverbs, 186\nautomatically adding \nPOS tags to text, 198\u2013\n203\ndetermining word category, 210\u2013213\ndifferences in POS tagsets, 213\nexploring tagged corpora using POS tags,\n187\u2013189\nmapping words to properties using Python\ndictionaries, 189\u2013198\nn-gram tagging, 203\u2013208\nnouns, 184\nresources for further reading, 214\ntagged corpora, 181\u2013189\ntransformation-based tagging, 208\u2013210\nusing POS (part-of-speech) tagger, 179\nusing unsimplified POS tags, 187\nverbs, 185\ncharacter class symbols in regular expressions,\n110\ncharacter encodings, 48, 54, 94\n(see also Unicode)\nusing your local encoding in Python, 97\ncharacteristic function, 377\nchart, 307\nchart parsing, 307\nEarley chart parser, 334\ncharts, displaying information in, 168chat text, 42\nchatbots, 31\nchild nodes, 279\nchink, 268, 286\nchinking, 268\nchunk grammar, 265\nchunking, 214, 264\u2013270\nbuilding nested structure with cascaded\nchunkers, 278\u2013279\nchinking, 268\ndeveloping and evaluating chunkers, 270\u2013\n278\nreading IOB format and CoNLL 2000\ncorpus, 270\u2013272\nsimple evaluation and baselines, 272\u2013\n274\ntraining classifier-based chunkers, 274\u2013\n278\nexploring text corpora with NP chunker,\n267\nnoun phrase (NP), 264\nrepresenting chunks, tags versus trees, 269\nresources for further reading, 286\ntag patterns, 266\nToolbox lexicon, 434\nusing regular expressions, 266\nchunks, 264\nclass labels, 221\nclassification, 221\u2013259\nclassifier trained to recognize named\nentities, 283\ndecision trees, 242\u2013245\ndefined, 221\nevaluating models, 237\u2013241\naccuracy, 239\nconfusion matrices, 240\ncross-validation, 241\nprecision and recall, 239\ntest set, 238\ngenerative versus conditional, 254\nMaximum Entropy classifiers, 251\u2013254\nmodelling linguistic patterns, 255\nnaive Bayes classifiers, 246\u2013250\nsupervised (see supervised classification)\nclassifier-based chunkers, 274\u2013278\nclosed class, 212\nclosed formula, 375\nclosures (+ and *), 100\nclustering package (nltk.cluster), 172\nGeneral Index | 465\nCMU Pronouncing Dictionary for U.S.\nEnglish, 63\ncode blocks, nested, 25\ncode examples, downloading, 57\ncode points, 94\ncodecs module, 95\ncoindex (in feature structure), 340\ncollocations, 20, 81\ncomma operator (,), 133\ncomparative wordlists, 65\ncomparison operators\nnumerical, 22\nfor words, 23\ncomplements of lexical head, 347\ncomplements of verbs, 313\ncomplex types, 373\ncomplex values, 336\ncomponents, language understanding, 31\ncomputational linguistics, \nchallenges of natural\nlanguage, 441\ncomputer understanding of sentence meaning,\n368\nconcatenation, 11, 88\nlists and strings, 87\nstrings, 16\nconclusions in logic, 369\nconcordances\ncreating, 40\ngraphical POS-concordance tool, 184\nconditional classifiers, 254\nconditional expressions, 25\nconditional frequency distributions, 44, 52\u201356\ncombining with regular expressions, 103\ncondition and event pairs, 52\ncounting words by genre, 52\ngenerating random text with bigrams, 55\nmale and female names ending in each\nalphabet letter, 62\nplotting and tabulating distributions, 53\nusing to find minimally contrasting set of\nwords, 64\nConditionalFreqDist, 52\ncommonly used methods, 56\nconditionals, 22, 133\nconfusion matrix, 207, 240\nconsecutive classification, 232\nnon phrase chunking with consecutive\nclassifier, 275\nconsistent, 366constituent structure, 296\nconstituents, 297\ncontext\nexploiting in part-of-speech classifier, 230\nfor taggers, 203\ncontext-free grammar, 298, 300\n(see also grammars)\nprobabilistic context-free grammar, 320\ncontractions in tokenization, 112\ncontrol, 22\ncontrol structures, 26\nconversion specifiers, 118\nconversions of data formats, 419\ncoordinate structures, 295\ncoreferential, 373\ncorpora, 39\u201352\nannotated text corpora, 46\u201348\nBrown Corpus, 42\u201344\ncreating and \naccessing, resources for further\nreading, 438\ndefined, 39\ndifferences in corpus access methods, 50\nexploring text corpora using a chunker,\n267\nGutenberg Corpus, 39\u201342\nInaugural Address Corpus, 45\nfrom languages other than English, 48\nloading your own corpus, 51\nobtaining from Web, 416\nReuters Corpus, 44\nsources of, 73\ntagged, 181\u2013189\ntext corpus structure, 49\u201351\nweb and chat text, 42\nwordlists, 60\u201363\ncorpora, included with NLTK, 46\ncorpus\ncase study, structure of TIMIT, 407\u2013412\ncorpus HOWTOs, 122\nlife cycle of, 412\u2013416\ncreation scenarios, 412\ncuration versus evolution, 415\nquality control, 413\nwidely-used format for, 421\ncounters, legitimate uses of, 141\ncross-validation, 241\nCSV (comma-separated value) format, 418\nCSV (comma-separated-value) format, 170\n466 | General Index\nD\n\\d decimal digits in regular expressions, 110\n\\D nondigit characters in regular expressions,\n111\ndata formats, converting, 419\ndata types\ndictionary, 190\ndocumentation for Python standard types,\n173\nfinding type of Python objects, 86\nfunction parameter, 146\noperations on objects, 86\ndatabase query via natural language, 361\u2013365\ndatabases, obtaining data from, 418\ndebugger (Python), 158\ndebugging techniques, 158\ndecimal integers, formatting, 119\ndecision nodes, 242\ndecision stumps, 243\ndecision trees, 242\u2013245\nentropy and information gain, 243\ndecision-tree classifier, 229\ndeclarative style, 140\ndecoding, 94\ndef keyword, 9\ndefaultdict, 193\ndefensive programming, 159\ndemonstratives, agreement with noun, 329\ndependencies, 310\ncriteria for, 312\nexistential dependencies, modeling in\nXML, 427\nnon-projective, 312\nprojective, 311\nunbounded dependency constructions,\n349\u2013353\ndependency grammars, 310\u2013315\nvalency and the lexicon, 312\ndependents, 310\ndescriptive models, 255\ndeterminers, 186\nagreement with nouns, 333\ndeve-test set, 225\ndevelopment set, 225\nsimilarity to test set, 238\ndialogue act tagging, 214\ndialogue acts, identifying types, 235\ndialogue systems \n(see spoken dialogue systems)\ndictionariesfeature set, 223\nfeature structures as, 337\npronouncing dictionary, 63\u201365\nPython, 189\u2013198\ndefault, 193\ndefining, 193\ndictionary data type, 190\nfinding key given a value, 197\nindexing lists versus, 189\nsummary of dictionary methods, 197\nupdating incrementally, 195\nstoring features and values, 327\ntranslation, 66\ndictionary\nmethods, 197\ndictionary data structure (Python), 65\ndirected acyclic graphs (DAGs), 338\ndiscourse module, 401\ndiscourse semantics, 397\u2013402\ndiscourse processing, 400\u2013402\ndiscourse referents, 397\ndiscourse representation structure (DRS),\n397\nDiscourse Representation Theory (DRT),\n397\u2013400\ndispersion plot, 6\ndivide-and-conquer strategy, 160\ndocstrings, 143\ncontents and structure of, 148\nexample of complete docstring, 148\nmodule-level, 155\ndoctest block, 148\ndoctest module, 160\ndocument classification, 227\ndocumentation\nfunctions, 148\nonline Python documentation, versions\nand, 173\nPython, resources for further information,\n173\ndocutils module, 148\ndomain (of a model), 377\nDRS (discourse representation structure), 397\nDRS conditions, 397\nDRT (Discourse \nRepresentation Theory), 397\u2013\n400\nDublin Core Metadata initiative, 435\nduck typing, 281\ndynamic programming, 165\nGeneral Index | 467\napplication to parsing with context-free\ngrammar, 307\ndifferent approaches to, 167\nE\nEarley chart parser, 334\nelectronic books, 80\nelements, XML, 425\nElementTree interface, 427\u2013429\nusing to access Toolbox data, 429\nelif clause, if . . . elif statement, 133\nelif statements, 26\nelse statements, 26\nencoding, 94\nencoding features, 223\nencoding parameters, codecs module, 95\nendangered languages, special considerations\nwith, 423\u2013424\nentities, 373\nentity detection, using chunking, 264\u2013270\nentries\nadding field to, in Toolbox, 431\ncontents of, 60\nconverting data formats, 419\nformatting in XML, 430\nentropy, 251\n(see also Maximum Entropy classifiers)\ncalculating for gender prediction task, 243\nmaximizing in Maximum Entropy\nclassifier, 252\nepytext markup language, 148\nequality, 132, 372\nequivalence (<->) operator, 368\nequivalent, 340\nerror analysis, 225\nerrors\nruntime, 13\nsources of, 156\nsyntax, 3\nevaluation sets, 238\nevents, pairing with conditions in conditional\nfrequency distribution, 52\nexceptions, 158\nexistential quantifier, 374\nexists operator, 376\nExpected Likelihood Estimation, 249\nexporting data, 117F\nf-structure, 357\nfeature extractors\ndefining for dialogue acts, 235\ndefining for document classification, 228\ndefining for noun phrase (NP) chunker,\n276\u2013278\ndefining for punctuation, 234\ndefining for suffix checking, 229\nRecognizing Textual Entailment (RTE),\n236\nselecting relevant features, 224\u2013227\nfeature paths, 339\nfeature sets, 223\nfeature structures, 328\norder of features, 337\nresources for further reading, 357\nfeature-based grammars, 327\u2013360\nauxiliary verbs and inversion, 348\ncase and gender in German, 353\nexample grammar, 333\nextending, 344\u2013356\nlexical heads, 347\nparsing using Earley chart parser, 334\nprocessing feature structures, 337\u2013344\nsubsumption and unification, 341\u2013344\nresources for further reading, 357\nsubcategorization, 344\u2013347\nsyntactic agreement, 329\u2013331\nterminology, 336\ntranslating from English to SQL, 362\nunbounded dependency constructions,\n349\u2013353\nusing attributes and constraints, 331\u2013336\nfeatures, 223\nnon-binary features in naive Bayes\nclassifier, 249\nfields, 136\nfile formats, libraries for, 172\nfiles\nopening and reading local files, 84\nwriting program output to, 120\nfillers, 349\nfirst-order logic, 372\u2013385\nindividual variables and assignments, 378\nmodel building, 383\nquantifier scope ambiguity, 381\nsummary of language, 376\nsyntax, 372\u2013375\n468 | General Index\ntheorem proving, 375\ntruth in model, 377\nfloating-point numbers, formatting, 119\nfolds, 241\nfor statements, 26\ncombining with if statements, 26\ninside a list comprehension, 63\niterating over characters in strings, 90\nformat strings, 118\nformatting program output, 116\u2013121\nconverting from lists to strings, 116\nstrings and formats, 117\u2013118\ntext wrapping, 120\nwriting results to file, 120\nformulas of propositional logic, 368\nformulas, type (t), 373\nfree, 375\nFrege\u2019s Principle, 385\nfrequency distributions, 17, 22\nconditional (see conditional frequency\ndistributions)\nfunctions defined for, 22\nletters, occurrence in strings, 90\nfunctions, 142\u2013154\nabstraction provided by, 147\naccumulative, 150\nas arguments to another function, 149\ncall-by-value parameter passing, 144\nchecking parameter types, 146\ndefined, 9, 57\ndocumentation for Python built-in\nfunctions, 173\ndocumenting, 148\nerrors from, 157\nfor frequency distributions, 22\nfor iteration over sequences, 134\ngenerating plurals of nouns (example), 58\nhigher-order, 151\ninputs and outputs, 143\nnamed arguments, 152\nnaming, 142\npoorly-designed, 147\nrecursive, call structure, 165\nsaving in modules, 59\nvariable scope, 145\nwell-designed, 147\nG\ngaps, 349gazetteer, 282\ngender identification, 222\nDecision Tree model for, 242\ngender in German, 353\u2013356\nGeneralized Phrase Structure Grammar\n(GPSG), 345\ngenerate_model ( ) function, 55\ngeneration of language output, 29\ngenerative classifiers, 254\ngenerator expressions, 138\nfunctions exemplifying, 151\ngenres, systematic differences between, 42\u201344\nGerman, case and gender in, 353\u2013356\ngerunds, 211\nglyphs, 94\ngold standard, 201\ngovernment-sponsored challenges to machine\nlearning application in NLP, 257\ngradient (grammaticality), 318\ngrammars, 327\n(see also feature-based grammars)\nchunk grammar, 265\ncontext-free, 298\u2013302\nparsing with, 302\u2013310\nvalidating Toolbox entries with, 433\nwriting your own, 300\ndependency, 310\u2013315\ndevelopment, 315\u2013321\nproblems with ambiguity, 317\ntreebanks and grammars, 315\u2013317\nweighted grammar, 318\u2013321\ndilemmas in sentence structure analysis,\n292\u2013295\nresources for further reading, 322\nscaling up, 315\ngrammatical category, 328\ngraphical displays of data\nconditional frequency distributions, 56\nMatplotlib, 168\u2013170\ngraphs\ndefining and manipulating, 170\ndirected acyclic graphs, 338\ngreedy sequence classification, 232\nGutenberg Corpus, 40\u201342, 80\nH\nhapaxes, 19\nhash arrays, 189, 190\n(see also dictionaries)\nGeneral Index | 469\nhead of a sentence, 310\ncriteria for head and dependencies, 312\nheads, lexical, 347\nheadword (lemma), 60\nHeldout Estimation, 249\nhexadecimal notation for Unicode string\nliteral, 95\nHidden Markov Models, 233\nhigher-order functions, 151\nholonyms, 70\nhomonyms, 60\nHTML documents, 82\nHTML markup, stripping out, 418\nhypernyms, 70\nsearching corpora for, 106\nsemantic similarity and, 72\nhyphens in tokenization, 110\nhyponyms, 69\nI\nidentifiers for variables, 15\nidioms, Python, 24\nIDLE (Interactive DeveLopment\nEnvironment), 2\nif . . . elif statements, 133\nif statements, 25\ncombining with for statements, 26\nconditions in, 133\nimmediate constituents, 297\nimmutable, 93\nimplication (->) operator, 368\nin operator, 91\nInaugural Address Corpus, 45\ninconsistent, 366\nindenting code, 138\nindependence assumption, 248\nnaivete of, 249\nindexes\ncounting from zero (0), 12\nlist, 12\u201314\nmapping dictionary definition to lexeme,\n419\nspeeding up program by using, 163\nstring, 15, 89, 91\ntext index created using a stemmer, 107\nwords containing a given consonant-vowel\npair, 103\ninference, 369\ninformation extraction, 261\u2013289architecture of system, 263\nchunking, 264\u2013270\ndefined, 262\ndeveloping and evaluating chunkers, 270\u2013\n278\nnamed entity recognition, 281\u2013284\nrecursion in linguistic structure, 278\u2013281\nrelation extraction, 284\nresources for further reading, 286\ninformation gain, 243\ninside, outside, begin tags (see IOB tags)\ninteger ordinal, finding for character, 95\ninterpreter\n>>> prompt, 2\naccessing, 2\nusing text editor instead of to write\nprograms, 56\ninverted clauses, 348\nIOB tags, 269, 286\nreading, 270\u2013272\nis operator, 145\ntesting for object identity, 132\nISO 639 language codes, 65\niterative optimization techniques, 251\nJ\njoint classifier models, 231\njoint-features (maximum entropy model), 252\nK\nKappa coefficient (k), 414\nkeys, 65, 191\ncomplex, 196\nkeyword arguments, 153\nKleene closures, 100\nL\nlambda expressions, 150, 386\u2013390\nexample, 152\nlambda operator (\u03bb), 386\nLancaster stemmer, 107\nlanguage codes, 65\nlanguage output, generating, 29\nlanguage processing, symbol processing\nversus, 442\nlanguage resources\ndescribing using OLAC metadata, 435\u2013437\nLanguageLog (linguistics blog), 35\n470 | General Index\nlatent semantic analysis, 171\nLatin-2 character encoding, 94\nleaf nodes, 242\nleft-corner parser, 306\nleft-recursive, 302\nlemmas, 60\nlexical relationships between, 71\npairing of synset with a word, 68\nlemmatization, 107\nexample of, 108\nlength of a text, 7\nletter trie, 162\nlexical categories, 179\nlexical entry, 60\nlexical relations, 70\nlexical resources\ncomparative wordlists, 65\npronouncing dictionary, 63\u201365\nShoebox and Toolbox lexicons, 66\nwordlist corpora, 60\u201363\nlexicon, 60\n(see also lexical resources)\nchunking Toolbox lexicon, 434\ndefined, 60\nvalidating in Toolbox, 432\u2013435\nLGB rule of name resolution, 145\nlicensed, 350\nlikelihood ratios, 224\nLinear-Chain Conditional Random Field\nModels, 233\nlinguistic objects, mappings from keys to\nvalues, 190\nlinguistic patterns, modeling, 255\nlinguistics and \nNLP-related concepts, resources\nfor, 34\nlist comprehensions, 24\nfor statement in, 63\nfunction invoked in, 64\nused as function parameters, 55\nlists, 10\nappending item to, 11\nconcatenating, using + operator, 11\nconverting to strings, 116\nindexing, 12\u201314\nindexing, dictionaries versus, 189\nnormalizing and sorting, 86\nPython list type, 86\nsorted, 14\nstrings versus, 92tuples versus, 136\nlocal variables, 58\nlogic\nfirst-order, 372\u2013385\nnatural language, semantics, and, 365\u2013368\npropositional, 368\u2013371\nresources for further reading, 404\nlogical constants, 372\nlogical form, 368\nlogical proofs, 370\nloops, 26\nlooping with conditions, 26\nlowercase, converting text to, 45, 107\nM\nmachine learning\napplication to NLP, web pages for\ngovernment challenges, 257\ndecision trees, 242\u2013245\nMaximum Entropy classifiers, 251\u2013254\nnaive Bayes classifiers, 246\u2013250\npackages, 237\nresources for further reading, 257\nsupervised classification, 221\u2013237\nmachine translation (MT)\nlimitations of, 30\nusing NLTK\u2019s babelizer, 30\nmapping, 189\nMatplotlib package, 168\u2013170\nmaximal projection, 347\nMaximum Entropy classifiers, 251\u2013254\nMaximum Entropy Markov Models, 233\nMaximum Entropy principle, 253\nmemoization, 167\nmeronyms, 70\nmetadata, 435\nOLAC (Open Language Archives\nCommunity), 435\nmodals, 186\nmodel building, 383\nmodel checking, 379\nmodels\ninterpretation of sentences of logical\nlanguage, 371\nof linguistic patterns, 255\nrepresentation using set theory, 367\ntruth-conditional semantics in first-order\nlogic, 377\nGeneral Index | 471\nwhat can be learned from models of\nlanguage, 255\nmodifiers, 314\nmodules\ndefined, 59\nmultimodule programs, 156\nstructure of Python module, 154\nmorphological analysis, 213\nmorphological cues to word category, 211\nmorphological tagging, 214\nmorphosyntactic information in tagsets, 212\nMSWord, text from, 85\nmutable, 93\nN\n\\n newline character in regular expressions,\n111\nn-gram tagging, 203\u2013208\nacross sentence boundaries, 208\ncombining taggers, 205\nn-gram tagger as generalization of unigram\ntagger, 203\nperformance limitations, 206\nseparating training and test data, 203\nstoring taggers, 206\nunigram tagging, 203\nunknown words, 206\nnaive Bayes assumption, 248\nnaive Bayes classifier, 246\u2013250\ndeveloping for gender identification task,\n223\ndouble-counting problem, 250\nas generative classifier, 254\nnaivete of independence assumption, 249\nnon-binary features, 249\nunderlying probabilistic model, 248\nzero counts and smoothing, 248\nname resolution, LGB rule for, 145\nnamed arguments, 152\nnamed entities\ncommonly used types of, 281\nrelations between, 284\nnamed entity recognition (NER), 281\u2013284\nNames Corpus, 61\nnegative lookahead assertion, 284\nNER (see named entity recognition)\nnested code blocks, 25\nNetworkX package, 170\nnew words in languages, 212newlines, 84\nmatching in regular expressions, 109\nprinting with print statement, 90\nresources for further information, 122\nnon-logical constants, 372\nnon-standard words, 108\nnormalizing text, 107\u2013108\nlemmatization, 108\nusing stemmers, 107\nnoun phrase (NP), 297\nnoun phrase (NP) chunking, 264\nregular expression\u2013based NP chunker, 267\nusing unigram tagger, 272\nnoun phrases, quantified, 390\nnouns\ncategorizing and tagging, 184\nprogram to find most frequent noun tags,\n187\nsyntactic agreement, 329\nnumerically intense algorithms in Python,\nincreasing efficiency of, 257\nNumPy package, 171\nO\nobject references, 130\ncopying, 132\nobjective function, 114\nobjects, finding data type for, 86\nOLAC metadata, 74, 435\ndefinition of metadata, 435\nOpen Language Archives Community, 435\nOpen Archives Initiative (OAI), 435\nopen class, 212\nopen formula, 374\nOpen Language Archives Community\n(OLAC), 435\noperators, 369\n(see also names of individual operators)\naddition and multiplication, 88\nBoolean, 368\nnumerical comparison, 22\nscope of, 157\nword comparison, 23\nor operator, 24\northography, 328\nout-of-vocabulary items, 206\noverfitting, 225, 245\n472 | General Index\nP\npackages, 59\nparameters, 57\ncall-by-value parameter passing, 144\nchecking types of, 146\ndefined, 9\ndefining for functions, 143\nparent nodes, 279\nparsing, 318\n(see also grammars)\nwith context-free grammar\nleft-corner parser, 306\nrecursive descent parsing, 303\nshift-reduce parsing, 304\nwell-formed substring tables, 307\u2013310\nEarley chart parser, parsing feature-based\ngrammars, 334\nparsers, 302\nprojective dependency parser, 311\npart-of-speech tagging (see POS tagging)\npartial information, 341\nparts of speech, 179\nPDF text, 85\nPenn Treebank Corpus, 51, 315\npersonal pronouns, 186\nphilosophical divides in contemporary NLP,\n444\nphonetics\ncomputer-readable phonetic alphabet\n(SAMPA), 137\nphones, 63\nresources for further information, 74\nphrasal level, 347\nphrasal projections, 347\npipeline for NLP, 31\npixel images, 169\nplotting functions, Matplotlib, 168\nPorter stemmer, 107\nPOS (part-of-speech) tagging, 179, 208, 229\n(see also tagging)\ndifferences in POS tagsets, 213\nexamining word context, 230\nfinding IOB chunk tag for word's POS tag,\n272\nin information retrieval, 263\nmorphology in POS tagsets, 212\nresources for further reading, 214\nsimplified tagset, 183\nstoring POS tags in tagged corpora, 181tagged data from four Indian languages,\n182\nunsimplifed tags, 187\nuse in noun phrase chunking, 265\nusing consecutive classifier, 231\npre-sorting, 160\nprecision, evaluating search tasks for, 239\nprecision/recall trade-off in information\nretrieval, 205\npredicates (first-order logic), 372\nprepositional phrase (PP), 297\nprepositional phrase attachment ambiguity,\n300\nPrepositional Phrase Attachment Corpus, 316\nprepositions, 186\npresent participles, 211\nPrinciple of Compositionality, 385, 443\nprint statements, 89\nnewline at end, 90\nstring formats and, 117\nprior probability, 246\nprobabilistic context-free grammar (PCFG),\n320\nprobabilistic model, naive Bayes classifier, 248\nprobabilistic parsing, 318\nprocedural style, 139\nprocessing pipeline (NLP), 86\nproductions in grammars, 293\nrules for writing CFGs for parsing in\nNLTK, 301\nprogram development, 154\u2013160\ndebugging techniques, 158\ndefensive programming, 159\nmultimodule programs, 156\nPython module structure, 154\nsources of error, 156\nprogramming style, 139\nprograms, writing, 129\u2013177\nadvanced features of functions, 149\u2013154\nalgorithm design, 160\u2013167\nassignment, 130\nconditionals, 133\nequality, 132\nfunctions, 142\u2013149\nresources for further reading, 173\nsequences, 133\u2013138\nstyle considerations, 138\u2013142\nlegitimate uses for counters, 141\nprocedural versus declarative style, 139\nGeneral Index | 473\nPython coding style, 138\nsummary of important points, 172\nusing Python libraries, 167\u2013172\nProject Gutenberg, 80\nprojections, 347\nprojective, 311\npronouncing dictionary, 63\u201365\npronouns\nanaphoric antecedents, 397\ninterpreting in first-order logic, 373\nresolving in discourse processing, 401\nproof goal, 376\nproperties of linguistic categories, 331\npropositional logic, 368\u2013371\nBoolean operators, 368\npropositional symbols, 368\npruning decision nodes, 245\npunctuation, classifier for, 233\nPython\ncarriage return and linefeed characters, 80\ncodecs module, 95\ndictionary data structure, 65\ndictionary methods, summary of, 197\ndocumentation, 173\ndocumentation and information resources,\n34\nElementTree module, 427\nerrors in understanding semantics of, 157\nfinding type of any object, 86\ngetting started, 2\nincreasing efficiency of numerically intense\nalgorithms, 257\nlibraries, 167\u2013172\nCSV, 170\nMatplotlib, 168\u2013170\nNetworkX, 170\nNumPy, 171\nother, 172\nreference materials, 122\nstyle guide for Python code, 138\ntextwrap module, 120\nPython Package Index, 172\nQ\nquality control in corpus creation, 413\nquantification\nfirst-order logic, 373, 380\nquantified noun phrases, 390\nscope ambiguity, 381, 394\u2013397quantified formulas, interpretation of, 380\nquestions, answering, 29\nquotation marks in strings, 87\nR\nrandom text\ngenerating in various styles, 6\ngenerating using bigrams, 55\nraster (pixel) images, 169\nraw strings, 101\nraw text, processing, 79\u2013128\ncapturing user input, 85\ndetecting word patterns with regular\nexpressions, 97\u2013101\nformatting from lists to strings, 116\u2013121\nHTML documents, 82\nNLP pipeline, 86\nnormalizing text, 107\u2013108\nreading local files, 84\nregular expressions \nfor tokenizing text, 109\u2013\n112\nresources for further reading, 122\nRSS feeds, 83\nsearch engine results, 82\nsegmentation, 112\u2013116\nstrings, lowest level text processing, 87\u201393\nsummary of important points, 121\ntext from web and from disk, 80\ntext in binary formats, 85\nuseful applications of regular expressions,\n102\u2013106\nusing Unicode, 93\u201397\nraw( ) function, 41\nre module, 101, 110\nrecall, evaluating search tasks for, 240\nRecognizing Textual Entailment (RTE), 32,\n235\nexploiting word context, 230\nrecords, 136\nrecursion, 161\nfunction to compute Sanskrit meter\n(example), 165\nin linguistic structure, 278\u2013281\ntree traversal, 280\ntrees, 279\u2013280\nperformance and, 163\nin syntactic structure, 301\nrecursive, 301\nrecursive descent parsing, 303\n474 | General Index\nreentrancy, 340\nreferences (see object references)\nregression testing framework, 160\nregular expressions, 97\u2013106\ncharacter class and other symbols, 110\nchunker based on, evaluating, 272\nextracting word pieces, 102\nfinding word stems, 104\nmatching initial and final vowel sequences\nand all consonants, 102\nmetacharacters, 101\nmetacharacters, summary of, 101\nnoun phrase (NP) chunker based on, 265\nranges and closures, 99\nresources for further information, 122\nsearching tokenized text, 105\nsymbols, 110\ntagger, 199\ntokenizing text, 109\u2013112\nuse in PlaintextCorpusReader, 51\nusing basic metacharacters, 98\nusing for relation extraction, 284\nusing with conditional frequency\ndistributions, 103\nrelation detection, 263\nrelation extraction, 284\nrelational operators, 22\nreserved words, 15\nreturn statements, 144\nreturn value, 57\nreusing code, 56\u201359\ncreating programs using a text editor, 56\nfunctions, 57\nmodules, 59\nReuters Corpus, 44\nroot element (XML), 427\nroot hypernyms, 70\nroot node, 242\nroot synsets, 69\nRotokas language, 66\nextracting all consonant-vowel sequences\nfrom words, 103\nToolbox file containing lexicon, 429\nRSS feeds, 83\nfeedparser library, 172\nRTE (Recognizing Textual Entailment), 32,\n235\nexploiting word context, 230\nruntime errors, 13S\n\\s whitespace characters in regular\nexpressions, 111\n\\S nonwhitespace characters in regular\nexpressions, 111\nSAMPA computer-readable \nphonetic alphabet,\n137\nSanskrit meter, computing, 165\nsatisfies, 379\nscope of quantifiers, 381\nscope of variables, 145\nsearches\nbinary search, 160\nevaluating for precision and recall, 239\nprocessing search engine results, 82\nusing POS tags, 187\nsegmentation, 112\u2013116\nin chunking and tokenization, 264\nsentence, 112\nword, 113\u2013116\nsemantic cues to word category, 211\nsemantic interpretations, NLTK functions for,\n393\nsemantic role labeling, 29\nsemantics\nnatural language, logic and, 365\u2013368\nnatural language, resources for\ninformation, 403\nsemantics of English sentences, 385\u2013397\nquantifier ambiguity, 394\u2013397\ntransitive verbs, 391\u2013394\n\u22cf-calculus, 386\u2013390\nSemCor tagging, 214\nsentence boundaries, tagging across, 208\nsentence segmentation, 112, 233\nin chunking, 264\nin information retrieval process, 263\nsentence structure, analyzing, 291\u2013326\ncontext-free grammar, 298\u2013302\ndependencies and dependency grammar,\n310\u2013315\ngrammar development, 315\u2013321\ngrammatical dilemmas, 292\nparsing with context-free grammar, 302\u2013\n310\nresources for further reading, 322\nsummary of important points, 321\nsyntax, 295\u2013298\nsents( ) function, 41\nGeneral Index | 475\nsequence classification, 231\u2013233\nother methods, 233\nPOS tagging with consecutive classifier,\n232\nsequence iteration, 134\nsequences, 133\u2013138\ncombining different sequence types, 136\nconverting between sequence types, 135\noperations on sequence types, 134\nprocessing using generator expressions,\n137\nstrings and lists as, 92\nshift operation, 305\nshift-reduce parsing, 304\nShoebox, 66, 412\nsibling nodes, 279\nsignature, 373\nsimilarity, semantic, 71\nSinica Treebank Corpus, 316\nslash categories, 350\nslicing\nlists, 12, 13\nstrings, 15, 90\nsmoothing, 249\nspace-time trade-offs in algorihm design, 163\nspaces, matching in regular expressions, 109\nSpeech Synthesis Markup Language (W3C\nSSML), 214\nspellcheckers, Words Corpus used by, 60\nspoken dialogue systems, 31\nspreadsheets, obtaining data from, 418\nSQL (Structured Query Language), 362\ntranslating English sentence to, 362\nstack trace, 158\nstandards for linguistic data creation, 421\nstandoff annotation, 415, 421\nstart symbol for grammars, 298, 334\nstartswith( ) function, 45\nstemming, 107\nNLTK HOWTO, 122\nstemmers, 107\nusing regular expressions, 104\nusing stem( ) fuinction, 105\nstopwords, 60\nstress (in pronunciation), 64\nstring formatting expressions, 117\nstring literals, Unicode string literal in Python,\n95\nstrings, 15, 87\u201393accessing individual characters, 89\naccessing substrings, 90\nbasic operations with, 87\u201389\nconverting lists to, 116\nformats, 117\u2013118\nformatting\nlining things up, 118\ntabulating data, 119\nimmutability of, 93\nlists versus, 92\nmethods, 92\nmore operations on, useful string methods,\n92\nprinting, 89\nPython\u2019s str data type, 86\nregular expressions as, 101\ntokenizing, 86\nstructurally ambiguous sentences, 300\nstructure sharing, 340\ninteraction with unification, 343\nstructured data, 261\nstyle guide for Python code, 138\nstylistics, 43\nsubcategories of verbs, 314\nsubcategorization, 344\u2013347\nsubstrings (WFST), 307\nsubstrings, accessing, 90\nsubsumes, 341\nsubsumption, 341\u2013344\nsuffixes, classifier for, 229\nsupervised classification, 222\u2013237\nchoosing features, 224\u2013227\ndocuments, 227\nexploiting context, 230\ngender identification, 222\nidentifying dialogue act types, 235\npart-of-speech tagging, 229\nRecognizing Textual Entailment (RTE),\n235\nscaling up to large datasets, 237\nsentence segmentation, 233\nsequence classification, 231\u2013233\nSwadesh wordlists, 65\nsymbol processing, language processing\nversus, 442\nsynonyms, 67\nsynsets, 67\nsemantic similarity, 71\nin WordNet concept hierarchy, 69\n476 | General Index\nsyntactic agreement, 329\u2013331\nsyntactic cues to word category, 211\nsyntactic structure, recursion in, 301\nsyntax, 295\u2013298\nsyntax errors, 3\nT\n\\t tab character in regular expressions, 111\nT9 system, entering text on mobile phones, 99\ntabs\navoiding in code indentation, 138\nmatching in regular expressions, 109\ntag patterns, 266\nmatching, precedence in, 267\ntagging, 179\u2013219\nadjectives and adverbs, 186\ncombining taggers, 205\ndefault tagger, 198\nevaluating tagger performance, 201\nexploring tagged corpora, 187\u2013189\nlookup tagger, 200\u2013201\nmapping words to tags using Python\ndictionaries, 189\u2013198\nnouns, 184\npart-of-speech (POS) tagging, 229\nperformance limitations, 206\nreading tagged corpora, 181\nregular expression tagger, 199\nrepresenting tagged tokens, 181\nresources for further reading, 214\nacross sentence boundaries, 208\nseparating training and testing data, 203\nsimplified part-of-speech tagset, 183\nstoring taggers, 206\ntransformation-based, 208\u2013210\nunigram tagging, 202\nunknown words, 206\nunsimplified POS tags, 187\nusing POS (part-of-speech) tagger, 179\nverbs, 185\ntags\nin feature structures, 340\nIOB tags representing chunk structures,\n269\nXML, 425\ntagsets, 179\nmorphosyntactic information in POS\ntagsets, 212\nsimplified POS tagset, 183terms (first-order logic), 372\ntest sets, 44, 223\nchoosing for classification models, 238\ntesting classifier for document classification,\n228\ntext, 1\ncomputing statistics from, 16\u201322\ncounting vocabulary, 7\u201310\nentering on mobile phones (T9 system), 99\nas lists of words, 10\u201316\nsearching, 4\u20137\nexamining common contexts, 5\ntext alignment, 30\ntext editor, creating programs with, 56\ntextonyms, 99\ntextual entailment, 32\ntextwrap module, 120\ntheorem proving in first order logic, 375\ntimeit module, 164\nTIMIT Corpus, 407\u2013412\ntokenization, 80\nchunking and, 264\nin information retrieval, 263\nissues with, 111\nlist produced from tokenizing string, 86\nregular expressions for, 109\u2013112\nrepresenting tagged tokens, 181\nsegmentation and, 112\nwith Unicode strings as input and output,\n97\ntokenized text, searching, 105\ntokens, 8\nToolbox, 66, 412, 431\u2013435\naccessing data from XML, using\nElementTree, 429\nadding field to each entry, 431\nresources for further reading, 438\nvalidating lexicon, 432\u2013435\ntools for creation, publication, and use of\nlinguistic data, 421\ntop-down approach to dynamic programming,\n167\ntop-down parsing, 304\ntotal likelihood, 251\ntraining\nclassifier, 223\nclassifier for document classification, 228\nclassifier-based chunkers, 274\u2013278\ntaggers, 203\nGeneral Index | 477\nunigram chunker using CoNLL 2000\nChunking Corpus, 273\ntraining sets, 223, 225\ntransformation-based tagging, 208\u2013210\ntransitive verbs, 314, 391\u2013394\ntranslations\ncomparative wordlists, 66\nmachine (see machine translation)\ntreebanks, 315\u2013317\ntrees, 279\u2013281\nrepresenting chunks, 270\ntraversal of, 280\ntrie, 162\ntrigram taggers, 204\ntruth conditions, 368\ntruth-conditional semantics \nin first-order logic,\n377\ntuples, 133\nlists versus, 136\nparentheses with, 134\nrepresenting tagged tokens, 181\nTuring Test, 31, 368\ntype-raising, 390\ntype-token distinction, 8\nTypeError, 157\ntypes, 8, 86\n(see also data types)\ntypes (first-order logic), 373\nU\nunary predicate, 372\nunbounded dependency constructions, 349\u2013\n353\ndefined, 350\nunderspecified, 333\nUnicode, 93\u201397\ndecoding and encoding, 94\ndefinition and description of, 94\nextracting gfrom files, 94\nresources for further information, 122\nusing your local encoding in Python, 97\nunicodedata module, 96\nunification, 342\u2013344\nunigram taggers\nconfusion matrix for, 240\nnoun phrase chunking with, 272\nunigram tagging, 202\nlookup tagger (example), 200\nseparating training and test data, 203unique beginners, 69\nUniversal Feed Parser, 83\nuniversal quantifier, 374\nunknown words, tagging, 206\nupdating dictionary incrementally, 195\nUS Presidential Inaugural Addresses Corpus,\n45\nuser input, capturing, 85\nV\nvalencies, 313\nvalidity of arguments, 369\nvalidity of XML documents, 426\nvaluation, 377\nexamining quantifier scope ambiguity, 381\nMace4 model converted to, 384\nvaluation function, 377\nvalues, 191\ncomplex, 196\nvariables\narguments of predicates in first-order logic,\n373\nassignment, 378\nbound by quantifiers in first-order logic,\n373\ndefining, 14\nlocal, 58\nnaming, 15\nrelabeling bound variables, 389\nsatisfaction of, using to interpret quantified\nformulas, 380\nscope of, 145\nverb phrase (VP), 297\nverbs\nagreement paradigm for English regular\nverbs, 329\nauxiliary, 336\nauxiliary verbs and inversion of subject and\nverb, 348\ncategorizing and tagging, 185\nexamining for dependency grammar, 312\nhead of sentence and dependencies, 310\npresent participle, 211\ntransitive, 391\u2013394\nW\n\\W non-word characters in Python, 110, 111\n\\w word characters in Python, 110, 111\n478 | General Index\nweb text, 42\nWeb, obtaining data from, 416\nwebsites, obtaining corpora from, 416\nweighted grammars, 318\u2013321\nprobabilistic context-free \ngrammar (PCFG),\n320\nwell-formed (XML), 425\nwell-formed formulas, 368\nwell-formed substring tables (WFST), 307\u2013\n310\nwhitespace\nregular expression characters for, 109\ntokenizing text on, 109\nwildcard symbol (.), 98\nwindowdiff scorer, 414\nword classes, 179\nword comparison operators, 23\nword occurrence, counting in text, 8\nword offset, 45\nword processor files, obtaining data from, 417\nword segmentation, 113\u2013116\nword sense disambiguation, 28\nword sequences, 7\nwordlist corpora, 60\u201363\nWordNet, 67\u201373\nconcept hierarchy, 69\nlemmatizer, 108\nmore lexical relations, 70\nsemantic similarity, 71\nvisualization of hypernym hierarchy using\nMatplotlib and NetworkX, 170\nWords Corpus, 60\nwords( ) function, 40\nwrapping text, 120\nX\nXML, 425\u2013431\nElementTree interface, 427\u2013429\nformatting entries, 430\nrepresentation of lexical entry from chunk\nparsing Toolbox record, 434\nresources for further reading, 438\nrole of, in using to represent linguistic\nstructures, 426\nusing ElementTree to access Toolbox data,\n429\nusing for linguistic structures, 425\nvalidity of documents, 426Z\nzero counts (naive Bayes classifier), 249\nzero projection, 347\nGeneral Index | 479\n\nAbout the Authors\nSteven Bird  is Associate Professor in the Department of Computer Science and Soft-\nware Engineering at the University of Melbourne, and Senior Research Associate in the\nLinguistic Data Consortium at the University of Pennsylvania. He completed a Ph.D.\non computational phonology at the University of Edinburgh in 1990, supervised by\nEwan Klein. He later moved to Cameroon to conduct linguistic fieldwork on the Grass-\nfields Bantu languages under the auspices of the Summer Institute of Linguistics. More\nrecently, he spent several years as Associate Director of the Linguistic Data Consortium,\nwhere he led an R&D team to create models and tools for large databases of annotated\ntext. At Melbourne University, he established a language technology research group\nand has taught at all levels of the undergraduate computer science curriculum. In 2009,\nSteven is President of the Association for Computational Linguistics.\nEwan Klein  is Professor of Language Technology in the School of Informatics at the\nUniversity of Edinburgh. He completed a Ph.D. on formal semantics at the University\nof Cambridge in 1978. After some years working at the Universities of Sussex and\nNewcastle upon Tyne, Ewan took up a teaching position at Edinburgh. He was involved\nin the establishment of Edinburgh\u2019s Language Technology Group in 1993, and has\nbeen closely associated with it ever since. From 2000 to 2002, he took leave from the\nUniversity to act as Research Manager for the Edinburgh-based Natural Language Re-\nsearch Group of Edify Corporation, Santa Clara, and was responsible for spoken dia-\nlogue processing. Ewan is a past President of the European Chapter of the Association\nfor Computational Linguistics and was a founding member and Coordinator of the\nEuropean Network of Excellence in Human Language Technologies (ELSNET).\nEdward Loper has recently completed a Ph.D. on machine learning for natural lan-\nguage processing at the University of Pennsylvania. Edward was a student in Steven\u2019s\ngraduate course on computational linguistics in the fall of 2000, and went on to be a\nTeacher\u2019s Assistant and share in the development of NLTK. In addition to NLTK, he\nhas helped develop two packages for documenting and testing Python software,\nepydoc and doctest.\nColophon\nThe animal on the cover of Natural Language Processing with Python is a right whale,\nthe rarest \nof all large whales. It is identifiable by its enormous head, which can measure\nup to one-third of its total body length. It lives in temperate and cool seas in both\nhemispheres at the surface of the ocean. It\u2019s believed that the right whale may have\ngotten its name from whalers who thought that it was the \u201cright\u201d whale to kill for oil.\nEven though it has been protected since the 1930s, the right whale is still the most\nendangered of all the great whales.\nThe large and bulky right whale is easily distinguished from other whales by the calluses\non its head. It has a broad back without a dorsal fin and a long arching mouth that\nbegins above the eye. Its body is black, except for a white patch on its belly. Wounds\nand scars \nmay appear bright orange, often becoming infested with whale lice or\ncyamids. The calluses\u2014which are also found near the blowholes, above the eyes, and\non the chin, and upper lip\u2014are black or gray. It has large flippers that are shaped like\npaddles, and a distinctive V-shaped blow, caused by the widely spaced blowholes on\nthe top of its head, which rises to 16 feet above the ocean\u2019s surface.\nThe right whale feeds on planktonic organisms, including shrimp-like krill and cope-\npods. As baleen whales, they have a series of 225\u2013250 fringed overlapping plates hang-\ning from each side of the upper jaw, where teeth would otherwise be located. The plates\nare black and can be as long as 7.2 feet. Right whales are \u201cgrazers of the sea,\u201d often\nswimming slowly with their mouths open. As water flows into the mouth and through\nthe baleen, prey is trapped near the tongue.\nBecause females are not sexually mature until 10 years of age and they give birth to a\nsingle calf after a year-long pregnancy, populations grow slowly. The young right whale\nstays with its mother for one year.\nRight whales are found worldwide but in very small numbers. A right whale is com-\nmonly found alone or in small groups of 1 to 3, but when courting, they may form\ngroups of up to 30. Like most baleen whales, they are seasonally migratory. They inhabit\ncolder waters for feeding and then migrate to warmer waters for breeding and calving.\nAlthough they may move far out to sea during feeding seasons, right whales give birth\nin coastal areas. Interestingly, many of the females do not return to these coastal breed-\ning areas every year, but visit the area only in calving years. Where they go in other\nyears remains a mystery.\nThe right whale\u2019s only predators are orcas and humans. When danger lurks, a group\nof right whales may come together in a circle, with their tails pointing outward, to deter\na predator. This defense is not always successful and calves are occasionally separated\nfrom their mother and killed.\nRight whales are among the slowest swimming whales, although they may reach speeds\nup to 10 mph in short spurts. They can dive to at least 1,000 feet and can stay submerged\nfor up to 40 minutes. The right whale is extremely endangered, even after years of\nprotected status. Only in the past 15 years is there evidence of a population recovery\nin the Southern Hemisphere, and it is still not known if the right whale will survive at\nall in the Northern Hemisphere. Although not presently hunted, current conservation\nproblems include collisions with ships, conflicts with fishing activities, habitat de-\nstruction, oil drilling, and possible competition from other whale species. Right whales\nhave no teeth, so ear bones and, in some cases, eye lenses can be used to estimate the\nage of a right whale at death. It is believed that right whales live at least 50 years, but\nthere is little data on their longevity.\nThe cover image is from the Dover Pictorial Archive. The cover font is Adobe ITC\nGaramond. The text font is Linotype Birka; the heading font is Adobe Myriad Con-\ndensed; and the code font is LucasFont\u2019s TheSansMonoCondensed.\n", "Reinforcement Learning Introduction": "i\nReinforcement Learning:\nAn Introduction\nSecond edition, in progress\nRichard S. Sutton and Andrew G. Barto\nc\r2014, 2015\nA Bradford Book\nThe MIT Press\nCambridge, Massachusetts\nLondon, England\nii\nIn memory of A. Harry Klopf\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\nSeries Forward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nSummary of Notation . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\n1 The Reinforcement Learning Problem 1\n1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Elements of Reinforcement Learning . . . . . . . . . . . . . . 7\n1.4 Limitations and Scope . . . . . . . . . . . . . . . . . . . . . . 9\n1.5 An Extended Example: Tic-Tac-Toe . . . . . . . . . . . . . . 10\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.7 History of Reinforcement Learning . . . . . . . . . . . . . . . 16\n1.8 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . . 25\nI Tabular Solution Methods 27\n2 Multi-arm Bandits 31\n2.1 Ann-Armed Bandit Problem . . . . . . . . . . . . . . . . . . 32\n2.2 Action-Value Methods . . . . . . . . . . . . . . . . . . . . . . 33\n2.3 Incremental Implementation . . . . . . . . . . . . . . . . . . . 36\n2.4 Tracking a Nonstationary Problem . . . . . . . . . . . . . . . 38\n2.5 Optimistic Initial Values . . . . . . . . . . . . . . . . . . . . . 39\n2.6 Upper-Con\fdence-Bound Action Selection . . . . . . . . . . . 41\niii\niv CONTENTS\n2.7 Gradient Bandits . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.8 Associative Search (Contextual Bandits) . . . . . . . . . . . . 46\n2.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3 Finite Markov Decision Processes 53\n3.1 The Agent{Environment Interface . . . . . . . . . . . . . . . . 53\n3.2 Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.3 Returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.4 Uni\fed Notation for Episodic and Continuing Tasks . . . . . . 61\n\u00033.5 The Markov Property . . . . . . . . . . . . . . . . . . . . . . . 62\n3.6 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . 67\n3.7 Value Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n3.8 Optimal Value Functions . . . . . . . . . . . . . . . . . . . . . 75\n3.9 Optimality and Approximation . . . . . . . . . . . . . . . . . 79\n3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4 Dynamic Programming 89\n4.1 Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 90\n4.2 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.4 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n4.5 Asynchronous Dynamic Programming . . . . . . . . . . . . . . 101\n4.6 Generalized Policy Iteration . . . . . . . . . . . . . . . . . . . 104\n4.7 E\u000eciency of Dynamic Programming . . . . . . . . . . . . . . . 106\n4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5 Monte Carlo Methods 113\n5.1 Monte Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . 114\n5.2 Monte Carlo Estimation of Action Values . . . . . . . . . . . . 119\n5.3 Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . 120\n5.4 Monte Carlo Control without Exploring Starts . . . . . . . . . 124\nCONTENTS v\n5.5 O\u000b-policy Prediction via Importance Sampling . . . . . . . . . 127\n5.6 Incremental Implementation . . . . . . . . . . . . . . . . . . . 133\n5.7 O\u000b-Policy Monte Carlo Control . . . . . . . . . . . . . . . . . 135\n\u00035.8 Importance Sampling on Truncated Returns . . . . . . . . . . 136\n5.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n6 Temporal-Di\u000berence Learning 143\n6.1 TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n6.2 Advantages of TD Prediction Methods . . . . . . . . . . . . . 148\n6.3 Optimality of TD(0) . . . . . . . . . . . . . . . . . . . . . . . 151\n6.4 Sarsa: On-Policy TD Control . . . . . . . . . . . . . . . . . . 154\n6.5 Q-Learning: O\u000b-Policy TD Control . . . . . . . . . . . . . . . 157\n6.6 Games, Afterstates, and Other Special Cases . . . . . . . . . . 160\n6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n7 Eligibility Traces 167\n7.1n-Step TD Prediction . . . . . . . . . . . . . . . . . . . . . . . 168\n7.2 The Forward View of TD( \u0015) . . . . . . . . . . . . . . . . . . . 172\n7.3 The Backward View of TD( \u0015) . . . . . . . . . . . . . . . . . . 177\n7.4 Equivalences of Forward and Backward Views . . . . . . . . . 181\n7.5 Sarsa( \u0015) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n7.6 Watkins's Q( \u0015) . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n7.7 O\u000b-policy Eligibility Traces using Importance Sampling . . . . 188\n7.8 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . 189\n\u00037.9 Variable \u0015. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n7.10 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n8 Planning and Learning with Tabular Methods 195\n8.1 Models and Planning . . . . . . . . . . . . . . . . . . . . . . . 195\n8.2 Integrating Planning, Acting, and Learning . . . . . . . . . . . 198\n8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . 203\nvi CONTENTS\n8.4 Prioritized Sweeping . . . . . . . . . . . . . . . . . . . . . . . 206\n8.5 Full vs. Sample Backups . . . . . . . . . . . . . . . . . . . . . 210\n8.6 Trajectory Sampling . . . . . . . . . . . . . . . . . . . . . . . 213\n8.7 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n8.8 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . 220\n8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\nII Approximate Solution Methods 223\n9 On-policy Approximation of Action Values 225\n9.1 Value Prediction with Function Approximation . . . . . . . . 226\n9.2 Gradient-Descent Methods . . . . . . . . . . . . . . . . . . . . 229\n9.3 Linear Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n9.4 Control with Function Approximation . . . . . . . . . . . . . . 241\n9.5 Should We Bootstrap? . . . . . . . . . . . . . . . . . . . . . . 247\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n10 O\u000b-policy Approximation of Action Values 255\n11 Policy Approximation 257\n11.1 Actor{Critic Methods . . . . . . . . . . . . . . . . . . . . . . . 257\n11.2 Eligibility Traces for Actor{Critic Methods . . . . . . . . . . . 259\n11.3 R-Learning and the Average-Reward Setting . . . . . . . . . . 260\nIII Frontiers 265\n12 Psychology 269\n13 Neuroscience 271\n14 Applications and Case Studies 273\n14.1 TD-Gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\nCONTENTS vii\n14.2 Samuel's Checkers Player . . . . . . . . . . . . . . . . . . . . . 279\n14.3 The Acrobot . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n14.4 Elevator Dispatching . . . . . . . . . . . . . . . . . . . . . . . 286\n14.5 Dynamic Channel Allocation . . . . . . . . . . . . . . . . . . . 291\n14.6 Job-Shop Scheduling . . . . . . . . . . . . . . . . . . . . . . . 295\n15 Prospects 303\n15.1 The Uni\fed View . . . . . . . . . . . . . . . . . . . . . . . . . 303\n15.2 State Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 306\n15.3 Temporal Abstraction . . . . . . . . . . . . . . . . . . . . . . 306\n15.4 Predictive Representations . . . . . . . . . . . . . . . . . . . . 306\n15.5 Other Frontier Dimensions . . . . . . . . . . . . . . . . . . . . 306\nReferences 311\nIndex 338\nviii PREFACE\nPreface\nWe \frst came to focus on what is now known as reinforcement learning in late\n1979. We were both at the University of Massachusetts, working on one of\nthe earliest projects to revive the idea that networks of neuronlike adaptive\nelements might prove to be a promising approach to arti\fcial adaptive intel-\nligence. The project explored the \\heterostatic theory of adaptive systems\"\ndeveloped by A. Harry Klopf. Harry's work was a rich source of ideas, and\nwe were permitted to explore them critically and compare them with the long\nhistory of prior work in adaptive systems. Our task became one of teasing\nthe ideas apart and understanding their relationships and relative importance.\nThis continues today, but in 1979 we came to realize that perhaps the simplest\nof the ideas, which had long been taken for granted, had received surprisingly\nlittle attention from a computational perspective. This was simply the idea of\na learning system that wants something, that adapts its behavior in order to\nmaximize a special signal from its environment. This was the idea of a \\he-\ndonistic\" learning system, or, as we would say now, the idea of reinforcement\nlearning.\nLike others, we had a sense that reinforcement learning had been thor-\noughly explored in the early days of cybernetics and arti\fcial intelligence. On\ncloser inspection, though, we found that it had been explored only slightly.\nWhile reinforcement learning had clearly motivated some of the earliest com-\nputational studies of learning, most of these researchers had gone on to other\nthings, such as pattern classi\fcation, supervised learning, and adaptive con-\ntrol, or they had abandoned the study of learning altogether. As a result, the\nspecial issues involved in learning how to get something from the environment\nreceived relatively little attention. In retrospect, focusing on this idea was\nthe critical step that set this branch of research in motion. Little progress\ncould be made in the computational study of reinforcement learning until it\nwas recognized that such a fundamental idea had not yet been thoroughly\nexplored.\nThe \feld has come a long way since then, evolving and maturing in sev-\neral directions. Reinforcement learning has gradually become one of the most\nactive research areas in machine learning, arti\fcial intelligence, and neural net-\nwork research. The \feld has developed strong mathematical foundations and\nimpressive applications. The computational study of reinforcement learning is\nnow a large \feld, with hundreds of active researchers around the world in di-\nverse disciplines such as psychology, control theory, arti\fcial intelligence, and\nneuroscience. Particularly important have been the contributions establishing\nand developing the relationships to the theory of optimal control and dynamic\nprogramming. The overall problem of learning from interaction to achieve\nPREFACE ix\ngoals is still far from being solved, but our understanding of it has improved\nsigni\fcantly. We can now place component ideas, such as temporal-di\u000berence\nlearning, dynamic programming, and function approximation, within a coher-\nent perspective with respect to the overall problem.\nOur goal in writing this book was to provide a clear and simple account of\nthe key ideas and algorithms of reinforcement learning. We wanted our treat-\nment to be accessible to readers in all of the related disciplines, but we could\nnot cover all of these perspectives in detail. For the most part, our treatment\ntakes the point of view of arti\fcial intelligence and engineering. In this second\nedition, we plan to have one chapter summarizing the connections to psychol-\nogy and neuroscience, which are many and rapidly developing. Coverage of\nconnections to other \felds we leave to others or to another time. We also\nchose not to produce a rigorous formal treatment of reinforcement learning.\nWe did not reach for the highest possible level of mathematical abstraction\nand did not rely on a theorem{proof format. We tried to choose a level of\nmathematical detail that points the mathematically inclined in the right di-\nrections without distracting from the simplicity and potential generality of the\nunderlying ideas.\nThe book consists of three parts. Part I is introductory and problem ori-\nented. We focus on the simplest aspects of reinforcement learning and on its\nmain distinguishing features. One full chapter is devoted to introducing the\nreinforcement learning problem whose solution we explore in the rest of the\nbook. Part II presents tabular versions (assuming a small \fnite state space)\nof all the basic solution methods based on estimating action values. We intro-\nduce dynamic programming, Monte Carlo methods, and temporal-di\u000berence\nlearning. There is a chapter on eligibility traces which uni\fes the latter two\nmethods, and a chapter that uni\fes planning methods (such as dynamic pro-\ngramming and state-space search) and learning methods (such as Monte Carlo\nand temporal-di\u000berence learning). Part III is concerned with extending the\ntabular methods to include various forms of approximation including function\napproximation, policy-gradient methods, and methods designed for solving\no\u000b-policy learning problems. Part IV surveys some of the frontiers of rein-\nforcement learning in biology and applications.\nThis book was designed to be used as a text in a one- or two-semester\ncourse, perhaps supplemented by readings from the literature or by a more\nmathematical text such as Bertsekas and Tsitsiklis (1996) or Szepesvari (2010).\nThis book can also be used as part of a broader course on machine learning,\narti\fcial intelligence, or neural networks. In this case, it may be desirable to\ncover only a subset of the material. We recommend covering Chapter 1 for a\nbrief overview, Chapter 2 through Section 2.2, Chapter 3 except Sections 3.4,\n3.5 and 3.9, and then selecting sections from the remaining chapters according\nx PREFACE\nto time and interests. The \fve chapters of Part II build on each other and are\nbest covered in sequence; of these, Chapter 6 is the most important for the\nsubject and for the rest of the book. A course focusing on machine learning\nor neural networks should cover Chapter 9, and a course focusing on arti\fcial\nintelligence or planning should cover Chapter 8. Throughout the book, sections\nthat are more di\u000ecult and not essential to the rest of the book are marked\nwith a\u0003. These can be omitted on \frst reading without creating problems\nlater on. Some exercises are marked with a \u0003to indicate that they are more\nadvanced and not essential to understanding the basic material of the chapter.\nThe book is largely self-contained. The only mathematical background\nassumed is familiarity with elementary concepts of probability, such as expec-\ntations of random variables. Chapter 9 is substantially easier to digest if the\nreader has some knowledge of arti\fcial neural networks or some other kind of\nsupervised learning method, but it can be read without prior background. We\nstrongly recommend working the exercises provided throughout the book. So-\nlution manuals are available to instructors. This and other related and timely\nmaterial is available via the Internet.\nAt the end of most chapters is a section entitled \\Bibliographical and His-\ntorical Remarks,\" wherein we credit the sources of the ideas presented in that\nchapter, provide pointers to further reading and ongoing research, and describe\nrelevant historical background. Despite our attempts to make these sections\nauthoritative and complete, we have undoubtedly left out some important\nprior work. For that we apologize, and welcome corrections and extensions for\nincorporation into a subsequent edition.\nIn some sense we have been working toward this book for thirty years, and\nwe have lots of people to thank. First, we thank those who have personally\nhelped us develop the overall view presented in this book: Harry Klopf, for\nhelping us recognize that reinforcement learning needed to be revived; Chris\nWatkins, Dimitri Bertsekas, John Tsitsiklis, and Paul Werbos, for helping us\nsee the value of the relationships to dynamic programming; John Moore and\nJim Kehoe, for insights and inspirations from animal learning theory; Oliver\nSelfridge, for emphasizing the breadth and importance of adaptation; and,\nmore generally, our colleagues and students who have contributed in countless\nways: Ron Williams, Charles Anderson, Satinder Singh, Sridhar Mahadevan,\nSteve Bradtke, Bob Crites, Peter Dayan, and Leemon Baird. Our view of re-\ninforcement learning has been signi\fcantly enriched by discussions with Paul\nCohen, Paul Utgo\u000b, Martha Steenstrup, Gerry Tesauro, Mike Jordan, Leslie\nKaelbling, Andrew Moore, Chris Atkeson, Tom Mitchell, Nils Nilsson, Stuart\nRussell, Tom Dietterich, Tom Dean, and Bob Narendra. We thank Michael\nLittman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang for pro-\nviding speci\fcs of Sections 4.7, 15.1, 15.4, 15.5, and 15.6 respectively. We\nPREFACE xi\nthank the the Air Force O\u000ece of Scienti\fc Research, the National Science\nFoundation, and GTE Laboratories for their long and farsighted support.\nWe also wish to thank the many people who have read drafts of this book\nand provided valuable comments, including Tom Kalt, John Tsitsiklis, Pawel\nCichosz, Olle G\u007f allmo, Chuck Anderson, Stuart Russell, Ben Van Roy, Paul\nSteenstrup, Paul Cohen, Sridhar Mahadevan, Jette Randlov, Brian Sheppard,\nThomas O'Connell, Richard Coggins, Cristina Versino, John H. Hiett, An-\ndreas Badelt, Jay Ponte, Joe Beck, Justus Piater, Martha Steenstrup, Satin-\nder Singh, Tommi Jaakkola, Dimitri Bertsekas, Torbj\u007f orn Ekman, Christina\nBj\u007f orkman, Jakob Carlstr\u007f om, and Olle Palmgren. Finally, we thank Gwyn\nMitchell for helping in many ways, and Harry Stanton and Bob Prior for be-\ning our champions at MIT Press.\nxii\nSeries Forward\nSUMMARY OF NOTATION xiii\nSummary of Notation\nCapital letters are used for random variables and major algorithm variables.\nLower case letters are used for the values of random variables and for scalar\nfunctions. Quantities that are required to be real-valued vectors are written\nin bold and in lower case (even if random variables).\ns state\na action\nS set of all nonterminal states\nS+set of all states, including the terminal state\nA(s) set of actions possible in state s\nR set of possible rewards\nt discrete time step\nT \fnal time step of an episode\nSt state att\nAt action att\nRt reward att, dependent, like St, onAt\u00001andSt\u00001\nGt return (cumulative discounted reward) following t\nG(n)\ntn-step return (Section 7.1)\nG\u0015\nt\u0015-return (Section 7.2)\n\u0019 policy, decision-making rule\n\u0019(s) action taken in state sunder deterministic policy\u0019\n\u0019(ajs) probability of taking action ain statesunder stochastic policy\u0019\np(s0;rjs;a) probability of transitioning to state s0, with reward r, froms;a\nv\u0019(s) value of state sunder policy \u0019(expected return)\nv\u0003(s) value of state sunder the optimal policy\nq\u0019(s;a) value of taking action ain statesunder policy \u0019\nq\u0003(s;a) value of taking action ain statesunder the optimal policy\nVt(s) estimate (a random variable) of v\u0019(s) orv\u0003(s)\nQt(s;a) estimate (a random variable) of q\u0019(s;a) orq\u0003(s;a)\n^v(s;w) approximate value of state sgiven a vector of weights w\n^q(s;a;w) approximate value of state{action pair s;agiven weights w\nw;wt vector of (possibly learned) weights underlying an approximate value function\nx(s) vector of features visible when in state s\nw>x inner product of vectors, w>x=P\niwixi; e.g., ^v(s;w) =w>x(s)\nxiv SUMMARY OF NOTATION\n\u000et temporal-di\u000berence error at t(a random variable, even though not upper case)\nEt(s) eligibility trace for state satt\nEt(s;a) eligibility trace for a state{action pair\net eligibility trace vector at t\n\r discount-rate parameter\n\" probability of random action in \"-greedy policy\n\u000b;\f step-size parameters\n\u0015 decay-rate parameter for eligibility traces\nChapter 1\nThe Reinforcement Learning\nProblem\nThe idea that we learn by interacting with our environment is probably the\n\frst to occur to us when we think about the nature of learning. When an\ninfant plays, waves its arms, or looks about, it has no explicit teacher, but it\ndoes have a direct sensorimotor connection to its environment. Exercising this\nconnection produces a wealth of information about cause and e\u000bect, about\nthe consequences of actions, and about what to do in order to achieve goals.\nThroughout our lives, such interactions are undoubtedly a major source of\nknowledge about our environment and ourselves. Whether we are learning to\ndrive a car or to hold a conversation, we are acutely aware of how our environ-\nment responds to what we do, and we seek to in\ruence what happens through\nour behavior. Learning from interaction is a foundational idea underlying\nnearly all theories of learning and intelligence.\nIn this book we explore a computational approach to learning from inter-\naction. Rather than directly theorizing about how people or animals learn, we\nexplore idealized learning situations and evaluate the e\u000bectiveness of various\nlearning methods. That is, we adopt the perspective of an arti\fcial intelligence\nresearcher or engineer. We explore designs for machines that are e\u000bective in\nsolving learning problems of scienti\fc or economic interest, evaluating the\ndesigns through mathematical analysis or computational experiments. The\napproach we explore, called reinforcement learning , is much more focused on\ngoal-directed learning from interaction than are other approaches to machine\nlearning.\n1\n2 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\n1.1 Reinforcement Learning\nReinforcement learning is like many topics with names ending in -ing, such\nas machine learning, planning, and mountaineering, in that it is simultane-\nously a problem, a class of solution methods that work well on the class of\nproblems, and the \feld that studies these problems and their solution meth-\nods. Reinforcement learning problems involve learning what to do|how to\nmap situations to actions|so as to maximize a numerical reward signal. In\nan essential way they are closed-loop problems because the learning system's\nactions in\ruence its later inputs. Moreover, the learner is not told which ac-\ntions to take, as in many forms of machine learning, but instead must discover\nwhich actions yield the most reward by trying them out. In the most interest-\ning and challenging cases, actions may a\u000bect not only the immediate reward\nbut also the next situation and, through that, all subsequent rewards. These\nthree characteristics|being closed-loop in an essential way, not having direct\ninstructions as to what actions to take, and where the consequences of actions,\nincluding reward signals, play out over extended time periods|are the three\nmost important distinguishing features of reinforcement learning problems.\nA full speci\fcation of reinforcement learning problems in terms of optimal\ncontrol of Markov decision processes must wait until Chapter 3, but the basic\nidea is simply to capture the most important aspects of the real problem facing\na learning agent interacting with its environment to achieve a goal. Clearly,\nsuch an agent must be able to sense the state of the environment to some extent\nand must be able to take actions that a\u000bect the state. The agent also must\nhave a goal or goals relating to the state of the environment. The formulation\nis intended to include just these three aspects|sensation, action, and goal|in\ntheir simplest possible forms without trivializing any of them.\nAny method that is well suited to solving this kind of problem we consider\nto be a reinforcement learning method. Reinforcement learning is di\u000berent\nfrom supervised learning , the kind of learning studied in most current research\nin \feld of machine learning. Supervised learning is learning from a train-\ning set of labeled examples provided by a knowledgable external supervisor.\nEach example is a description of a situation together with a speci\fcation|the\nlabel|of the correct action the system should take to that situation, which is\noften to identify a category to which the situation belongs. The object of this\nkind of learning is for the system to extrapolate, or generalize, its responses\nso that it acts correctly in situations not present in the training set. This is\nan important kind of learning, but alone it is not adequate for learning from\ninteraction. In interactive problems it is often impractical to obtain examples\nof desired behavior that are both correct and representative of all the situa-\ntions in which the agent has to act. In uncharted territory|where one would\n1.1. REINFORCEMENT LEARNING 3\nexpect learning to be most bene\fcial|an agent must be able to learn from its\nown experience.\nReinforcement learning is also di\u000berent from what machine learning re-\nsearchers call unsupervised learning , which is typically about \fnding struc-\nture hidden in collections of unlabeled data. The terms supervised learning\nand unsupervised learning appear to exhaustively classify machine learning\nparadigms, but they do not. Although one might be tempted to think of rein-\nforcement learning as a kind of unsupervised learning because it does not rely\non examples of correct behavior, reinforcement learning is trying to maximize\na reward signal instead of trying to \fnd hidden structure. Uncovering struc-\nture in an agent's experience can certainly be useful in reinforcement learning,\nbut by itself does not address the reinforcement learning agent's problem of\nmaximizing a reward signal. We therefore consider reinforcement learning to\nbe a third machine learning paradigm, alongside of supervised learning, unsu-\npervised learning, and perhaps other paradigms as well.\nOne of the challenges that arise in reinforcement learning, and not in other\nkinds of learning, is the trade-o\u000b between exploration and exploitation. To\nobtain a lot of reward, a reinforcement learning agent must prefer actions\nthat it has tried in the past and found to be e\u000bective in producing reward.\nBut to discover such actions, it has to try actions that it has not selected\nbefore. The agent has to exploit what it already knows in order to obtain\nreward, but it also has to explore in order to make better action selections in\nthe future. The dilemma is that neither exploration nor exploitation can be\npursued exclusively without failing at the task. The agent must try a variety of\nactions andprogressively favor those that appear to be best. On a stochastic\ntask, each action must be tried many times to gain a reliable estimate its\nexpected reward. The exploration{exploitation dilemma has been intensively\nstudied by mathematicians for many decades (see Chapter 2). For now, we\nsimply note that the entire issue of balancing exploration and exploitation\ndoes not even arise in supervised and unsupervised learning, at least in their\npurist forms.\nAnother key feature of reinforcement learning is that it explicitly considers\nthewhole problem of a goal-directed agent interacting with an uncertain envi-\nronment. This is in contrast with many approaches that consider subproblems\nwithout addressing how they might \ft into a larger picture. For example,\nwe have mentioned that much of machine learning research is concerned with\nsupervised learning without explicitly specifying how such an ability would\n\fnally be useful. Other researchers have developed theories of planning with\ngeneral goals, but without considering planning's role in real-time decision-\nmaking, or the question of where the predictive models necessary for planning\nwould come from. Although these approaches have yielded many useful results,\n4 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\ntheir focus on isolated subproblems is a signi\fcant limitation.\nReinforcement learning takes the opposite tack, starting with a complete,\ninteractive, goal-seeking agent. All reinforcement learning agents have explicit\ngoals, can sense aspects of their environments, and can choose actions to in\ru-\nence their environments. Moreover, it is usually assumed from the beginning\nthat the agent has to operate despite signi\fcant uncertainty about the environ-\nment it faces. When reinforcement learning involves planning, it has to address\nthe interplay between planning and real-time action selection, as well as the\nquestion of how environment models are acquired and improved. When rein-\nforcement learning involves supervised learning, it does so for speci\fc reasons\nthat determine which capabilities are critical and which are not. For learning\nresearch to make progress, important subproblems have to be isolated and\nstudied, but they should be subproblems that play clear roles in complete,\ninteractive, goal-seeking agents, even if all the details of the complete agent\ncannot yet be \flled in.\nOne of the most exciting aspects of modern reinforcement learning is its\nsubstantive and fruitful interactions with other engineering and scienti\fc disci-\nplines. Reinforcement learning is part of a decades-long trend within arti\fcial\nintelligence and machine learning toward greater integration with statistics,\noptimization, and other mathematical subjects. For example, the ability of\nsome reinforcement learning methods to learn with parameterized approxima-\ntors addresses the classical \\curse of dimensionality\" in operations research\nand control theory. More distinctively, reinforcement learning has also in-\nteracted strongly with psychology and neuroscience, with substantial bene\fts\ngoing both ways. Of all the forms of machine learning, reinforcement learn-\ning is the closest to the kind of learning that humans and other animals do,\nand many of the core algorithms of reinforcement learning were originally in-\nspired by biological learning systems. And reinforcement learning has also\ngiven back, both through a psychological model of animal learning that better\nmatches some of the empirical data, and through an in\ruential model of parts\nof the brain's reward system. The body of this book develops the ideas of\nreinforcement learning that pertain to engineering and arti\fcial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters ??\nand??.\nFinally, reinforcement learning is also part of a larger trend in arti\fcial\nintelligence back toward simple general principles. Since the late 1960's, many\narti\fcial intelligence researchers presumed that there are no general principles\nto be discovered, that intelligence is instead due to the possession of vast num-\nbers of special purpose tricks, procedures, and heuristics. It was sometimes\nsaid that if we could just get enough relevant facts into a machine, say one mil-\nlion, or one billion, then it would become intelligent. Methods based on general\n1.2. EXAMPLES 5\nprinciples, such as search or learning, were characterized as \\weak methods,\"\nwhereas those based on speci\fc knowledge were called \\strong methods.\" This\nview is still common today, but much less dominant. From our point of view,\nit was simply premature: too little e\u000bort had been put into the search for\ngeneral principles to conclude that there were none. Modern AI now includes\nmuch research looking for general principles of learning, search, and decision-\nmaking, as well as trying to incorporate vast amounts of domain knowledge. It\nis not clear how far back the pendulum will swing, but reinforcement learning\nresearch is certainly part of the swing back toward simpler and fewer general\nprinciples of arti\fcial intelligence.\n1.2 Examples\nA good way to understand reinforcement learning is to consider some of the\nexamples and possible applications that have guided its development.\n\u000fA master chess player makes a move. The choice is informed both by\nplanning|anticipating possible replies and counterreplies|and by im-\nmediate, intuitive judgments of the desirability of particular positions\nand moves.\n\u000fAn adaptive controller adjusts parameters of a petroleum re\fnery's op-\neration in real time. The controller optimizes the yield/cost/quality\ntrade-o\u000b on the basis of speci\fed marginal costs without sticking strictly\nto the set points originally suggested by engineers.\n\u000fA gazelle calf struggles to its feet minutes after being born. Half an hour\nlater it is running at 20 miles per hour.\n\u000fA mobile robot decides whether it should enter a new room in search of\nmore trash to collect or start trying to \fnd its way back to its battery\nrecharging station. It makes its decision based on the current charge\nlevel of its battery and how quickly and easily it has been able to \fnd\nthe recharger in the past.\n\u000fPhil prepares his breakfast. Closely examined, even this apparently mun-\ndane activity reveals a complex web of conditional behavior and inter-\nlocking goal{subgoal relationships: walking to the cupboard, opening it,\nselecting a cereal box, then reaching for, grasping, and retrieving the\nbox. Other complex, tuned, interactive sequences of behavior are re-\nquired to obtain a bowl, spoon, and milk jug. Each step involves a series\n6 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nof eye movements to obtain information and to guide reaching and loco-\nmotion. Rapid judgments are continually made about how to carry the\nobjects or whether it is better to ferry some of them to the dining table\nbefore obtaining others. Each step is guided by goals, such as grasping\na spoon or getting to the refrigerator, and is in service of other goals,\nsuch as having the spoon to eat with once the cereal is prepared and ul-\ntimately obtaining nourishment. Whether he is aware of it or not, Phil\nis accessing information about the state of his body that determines his\nnutritional needs, level of hunger, and food preferences.\nThese examples share features that are so basic that they are easy to over-\nlook. All involve interaction between an active decision-making agent and\nits environment, within which the agent seeks to achieve a goal despite un-\ncertainty about its environment. The agent's actions are permitted to a\u000bect\nthe future state of the environment (e.g., the next chess position, the level of\nreservoirs of the re\fnery, the robot's next location and the future charge level\nof its battery), thereby a\u000becting the options and opportunities available to\nthe agent at later times. Correct choice requires taking into account indirect,\ndelayed consequences of actions, and thus may require foresight or planning.\nAt the same time, in all these examples the e\u000bects of actions cannot be\nfully predicted; thus the agent must monitor its environment frequently and\nreact appropriately. For example, Phil must watch the milk he pours into\nhis cereal bowl to keep it from over\rowing. All these examples involve goals\nthat are explicit in the sense that the agent can judge progress toward its goal\nbased on what it can sense directly. The chess player knows whether or not\nhe wins, the re\fnery controller knows how much petroleum is being produced,\nthe mobile robot knows when its batteries run down, and Phil knows whether\nor not he is enjoying his breakfast.\nNeither the agent nor its environment may coincide with what we normally\nthink of as an agent and its environment. An agent is not necessarily an entire\nrobot or organism, and its environment is not necessarily only what is outside\nof a robot or organism. The example robot's battery is part of the environment\nof its controlling agent, and Phil's degree of hunger and food preferences are\nfeatures of the environment of his internal decision-making agent. The state\nof an agent's environment often include's information about the state of the\nmachine or organism in which the agent resides, and this can include memories\nand even aspirations. Throughout this book we are being abstract in this way\nwhen we talk about agents and their environments.\nIn all of these examples the agent can use its experience to improve its per-\nformance over time. The chess player re\fnes the intuition he uses to evaluate\npositions, thereby improving his play; the gazelle calf improves the e\u000eciency\n1.3. ELEMENTS OF REINFORCEMENT LEARNING 7\nwith which it can run; Phil learns to streamline making his breakfast. The\nknowledge the agent brings to the task at the start|either from previous ex-\nperience with related tasks or built into it by design or evolution|in\ruences\nwhat is useful or easy to learn, but interaction with the environment is essential\nfor adjusting behavior to exploit speci\fc features of the task.\n1.3 Elements of Reinforcement Learning\nBeyond the agent and the environment, one can identify four main subelements\nof a reinforcement learning system: a policy , areward signal , avalue function ,\nand, optionally, a model of the environment.\nApolicy de\fnes the learning agent's way of behaving at a given time.\nRoughly speaking, a policy is a mapping from perceived states of the envi-\nronment to actions to be taken when in those states. It corresponds to what\nin psychology would be called a set of stimulus{response rules or associations\n(provided that stimuli include those that can come from within the animal).\nIn some cases the policy may be a simple function or lookup table, whereas\nin others it may involve extensive computation such as a search process. The\npolicy is the core of a reinforcement learning agent in the sense that it alone\nis su\u000ecient to determine behavior. In general, policies may be stochastic.\nAreward signal de\fnes the goal in a reinforcement learning problem. On\neach time step, the environment sends to the reinforcement learning agent a\nsingle number, a reward . The agent's sole objective is to maximize the total\nreward it receives over the long run. The reward signal thus de\fnes what are\nthe good and bad events for the agent. In a biological system, we might think\nof rewards as analogous to the experiences of pleasure or pain. They are the\nimmediate and de\fning features of the problem faced by the agent. The\nreward sent to the agent at any time depends on the agent's current action\nand the current state of the agent's environment. The agent cannot alter the\nprocess that does this. The only way the agent can in\ruence the reward signal\nis through its actions, which can have a direct e\u000bect on reward, or an indirect\ne\u000bect through changing the environment's state. In our example above of Phil\neating breakfast, the reinforcement learning agent directing his behavior might\nreceive di\u000berent reward signals when he eats his breakfast depending on how\nhungry he is, what mood he is in, and other features of his of his body, which\nis part of his internal reinforcement learning agent's environment. The reward\nsignal is the primary basis for altering the policy. If an action selected by the\npolicy is followed by low reward, then the policy may be changed to select\nsome other action in that situation in the future. In general, reward signals\nmay be stochastic functions of the state of the environment and the actions\n8 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\ntaken.\nWhereas the reward signal indicates what is good in an immediate sense,\navalue function speci\fes what is good in the long run. Roughly speaking, the\nvalue of a state is the total amount of reward an agent can expect to accumulate\nover the future, starting from that state. Whereas rewards determine the\nimmediate, intrinsic desirability of environmental states, values indicate the\nlong-term desirability of states after taking into account the states that are\nlikely to follow, and the rewards available in those states. For example, a state\nmight always yield a low immediate reward but still have a high value because\nit is regularly followed by other states that yield high rewards. Or the reverse\ncould be true. To make a human analogy, rewards are somewhat like pleasure\n(if high) and pain (if low), whereas values correspond to a more re\fned and\nfarsighted judgment of how pleased or displeased we are that our environment\nis in a particular state. Expressed this way, we hope it is clear that value\nfunctions formalize a basic and familiar idea.\nRewards are in a sense primary, whereas values, as predictions of rewards,\nare secondary. Without rewards there could be no values, and the only purpose\nof estimating values is to achieve more reward. Nevertheless, it is values with\nwhich we are most concerned when making and evaluating decisions. Action\nchoices are made based on value judgments. We seek actions that bring about\nstates of highest value, not highest reward, because these actions obtain the\ngreatest amount of reward for us over the long run. In decision-making and\nplanning, the derived quantity called value is the one with which we are most\nconcerned. Unfortunately, it is much harder to determine values than it is to\ndetermine rewards. Rewards are basically given directly by the environment,\nbut values must be estimated and re-estimated from the sequences of obser-\nvations an agent makes over its entire lifetime. In fact, the most important\ncomponent of almost all reinforcement learning algorithms we consider is a\nmethod for e\u000eciently estimating values. The central role of value estimation\nis arguably the most important thing we have learned about reinforcement\nlearning over the last few decades.\nThe fourth and \fnal element of some reinforcement learning systems is\namodel of the environment. This is something that mimics the behavior of\nthe environment, or more generally, that allows inferences to be made about\nhow the environment will behave. For example, given a state and action,\nthe model might predict the resultant next state and next reward. Models\nare used for planning , by which we mean any way of deciding on a course of\naction by considering possible future situations before they are actually expe-\nrienced. Methods for solving reinforcement learning problems that use models\nand planning are called model-based methods, as opposed to simpler model-\nfreemethods that are explicitly trial-and-error learners|viewed as almost the\n1.4. LIMITATIONS AND SCOPE 9\nopposite of planning. In Chapter 9 we explore reinforcement learning systems\nthat simultaneously learn by trial and error, learn a model of the environ-\nment, and use the model for planning. Modern reinforcement learning spans\nthe spectrum from low-level, trial-and-error learning to high-level, deliberative\nplanning.\n1.4 Limitations and Scope\nMost of the reinforcement learning methods we consider in this book are struc-\ntured around estimating value functions, but it is not strictly necessary to do\nthis to solve reinforcement learning problems. For example, methods such as\ngenetic algorithms, genetic programming, simulated annealing, and other opti-\nmization methods have been used to approach reinforcement learning problems\nwithout ever appealing to value functions. These methods evaluate the \\life-\ntime\" behavior of many non-learning agents, each using a di\u000berent policy for\ninteracting with its environment, and select those that are able to obtain the\nmost reward. We call these evolutionary methods because their operation is\nanalogous to the way biological evolution produces organisms with skilled be-\nhavior even when they do not learn during their individual lifetimes. If the\nspace of policies is su\u000eciently small, or can be structured so that good policies\nare common or easy to \fnd|or if a lot of time is available for the search|then\nevolutionary methods can be e\u000bective. In addition, evolutionary methods have\nadvantages on problems in which the learning agent cannot accurately sense\nthe state of its environment.\nOur focus is on reinforcement learning methods that involve learning while\ninteracting with the environment, which evolutionary methods do not do (un-\nless they evolve learning algorithms, as in some of the approaches that have\nbeen studied). It is our belief that methods able to take advantage of the\ndetails of individual behavioral interactions can be much more e\u000ecient than\nevolutionary methods in many cases. Evolutionary methods ignore much of\nthe useful structure of the reinforcement learning problem: they do not use\nthe fact that the policy they are searching for is a function from states to\nactions; they do not notice which states an individual passes through during\nits lifetime, or which actions it selects. In some cases this information can\nbe misleading (e.g., when states are misperceived), but more often it should\nenable more e\u000ecient search. Although evolution and learning share many fea-\ntures and naturally work together, we do not consider evolutionary methods\nby themselves to be especially well suited to reinforcement learning problems.\nFor simplicity, in this book when we use the term \\reinforcement learning\nmethod\" we do not include evolutionary methods.\n10 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nHowever, we do include some methods that, like evolutionary methods,\ndo not appeal to value functions. These methods search in spaces of policies\nde\fned by a collection of numerical parameters. They estimate the directions\nthe parameters should be adjusted in order to most rapidly improve a policy's\nperformance. Unlike evolutionary methods, however, they produce these es-\ntimates while the agent is interacting with its environment and so can take\nadvantage of the details of individual behavioral interactions. Methods like\nthis, called policy gradient methods , have proven useful in many problems, and\nsome of the simplest reinforcement learning methods fall into this category. In\nfact, some of these methods take advantage of value function estimates to im-\nprove their gradient estimates. Overall, the distinction between policy gradient\nmethods and other methods we include as reinforcement learning methods is\nnot sharply de\fned.\nReinforcement learning's connection to optimization methods deserves some\nadditional comment because it is a source of a common misunderstanding.\nWhen we say that a reinforcement learning agent's goal is to maximize a nu-\nmerical reward signal, we of course are not insisting that the agent has to\nactually achieve the goal of maximum reward. Trying to maximize a quantity\ndoes not mean that that quantity is ever maximized. The point is that a re-\ninforcement learning agent is always trying to increase the amount of reward\nit receives. Many factors can prevent it from achieving the maximum, even if\none exists. In other words, optimization is not the same a optimality.\n1.5 An Extended Example: Tic-Tac-Toe\nTo illustrate the general idea of reinforcement learning and contrast it with\nother approaches, we next consider a single example in more detail.\nConsider the familiar child's game of tic-tac-toe. Two players take turns\nplaying on a three-by-three board. One player plays Xs and the other Os until\none player wins by placing three marks in a row, horizontally, vertically, or\ndiagonally, as the X player has in this game:\nXX\nXOO\nX O\nIf the board \flls up with neither player getting three in a row, the game is\na draw. Because a skilled player can play so as never to lose, let us assume\nthat we are playing against an imperfect player, one whose play is sometimes\n1.5. AN EXTENDED EXAMPLE: TIC-TAC-TOE 11\nincorrect and allows us to win. For the moment, in fact, let us consider draws\nand losses to be equally bad for us. How might we construct a player that will\n\fnd the imperfections in its opponent's play and learn to maximize its chances\nof winning?\nAlthough this is a simple problem, it cannot readily be solved in a satisfac-\ntory way through classical techniques. For example, the classical \\minimax\"\nsolution from game theory is not correct here because it assumes a particular\nway of playing by the opponent. For example, a minimax player would never\nreach a game state from which it could lose, even if in fact it always won from\nthat state because of incorrect play by the opponent. Classical optimization\nmethods for sequential decision problems, such as dynamic programming, can\ncompute an optimal solution for any opponent, but require as input a com-\nplete speci\fcation of that opponent, including the probabilities with which\nthe opponent makes each move in each board state. Let us assume that this\ninformation is not available a priori for this problem, as it is not for the vast\nmajority of problems of practical interest. On the other hand, such informa-\ntion can be estimated from experience, in this case by playing many games\nagainst the opponent. About the best one can do on this problem is \frst to\nlearn a model of the opponent's behavior, up to some level of con\fdence, and\nthen apply dynamic programming to compute an optimal solution given the\napproximate opponent model. In the end, this is not that di\u000berent from some\nof the reinforcement learning methods we examine later in this book.\nAn evolutionary method applied to this problem would directly search the\nspace of possible policies for one with a high probability of winning against\nthe opponent. Here, a policy is a rule that tells the player what move to make\nfor every state of the game|every possible con\fguration of Xs and Os on the\nthree-by-three board. For each policy considered, an estimate of its winning\nprobability would be obtained by playing some number of games against the\nopponent. This evaluation would then direct which policy or policies were\nconsidered next. A typical evolutionary method would hill-climb in policy\nspace, successively generating and evaluating policies in an attempt to obtain\nincremental improvements. Or, perhaps, a genetic-style algorithm could be\nused that would maintain and evaluate a population of policies. Literally\nhundreds of di\u000berent optimization methods could be applied.\nHere is how the tic-tac-toe problem would be approached with a method\nmaking use of a value function. First we set up a table of numbers, one for\neach possible state of the game. Each number will be the latest estimate of\nthe probability of our winning from that state. We treat this estimate as the\nstate's value , and the whole table is the learned value function. State A has\nhigher value than state B, or is considered \\better\" than state B, if the current\nestimate of the probability of our winning from A is higher than it is from B.\n12 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\n..\u2022our move{\nopponent's move {\nour move{starting position\n\u2022\u2022\n\u2022a\nb\nc*\nd\ne e*opponent's move {\nc\u2022f\u2022g*gopponent's move {\nour move{\n.\u2022\nFigure 1.1: A sequence of tic-tac-toe moves. The solid lines represent the\nmoves taken during a game; the dashed lines represent moves that we (our\nreinforcement learning player) considered but did not make. Our second move\nwas an exploratory move, meaning that it was taken even though another\nsibling move, the one leading to e\u0003, was ranked higher. Exploratory moves do\nnot result in any learning, but each of our other moves does, causing backups\nas suggested by the curved arrows and detailed in the text.\nAssuming we always play Xs, then for all states with three Xs in a row the\nprobability of winning is 1, because we have already won. Similarly, for all\nstates with three Os in a row, or that are \\\flled up,\" the correct probability\nis 0, as we cannot win from them. We set the initial values of all the other\nstates to 0.5, representing a guess that we have a 50% chance of winning.\nWe play many games against the opponent. To select our moves we examine\nthe states that would result from each of our possible moves (one for each blank\nspace on the board) and look up their current values in the table. Most of the\ntime we move greedily , selecting the move that leads to the state with greatest\nvalue, that is, with the highest estimated probability of winning. Occasionally,\nhowever, we select randomly from among the other moves instead. These are\ncalled exploratory moves because they cause us to experience states that we\nmight otherwise never see. A sequence of moves made and considered during\na game can be diagrammed as in Figure 1.1.\n1.5. AN EXTENDED EXAMPLE: TIC-TAC-TOE 13\nWhile we are playing, we change the values of the states in which we \fnd\nourselves during the game. We attempt to make them more accurate estimates\nof the probabilities of winning. To do this, we \\back up\" the value of the state\nafter each greedy move to the state before the move, as suggested by the arrows\nin Figure 1.1. More precisely, the current value of the earlier state is adjusted\nto be closer to the value of the later state. This can be done by moving the\nearlier state's value a fraction of the way toward the value of the later state.\nIf we letsdenote the state before the greedy move, and s0the state after\nthe move, then the update to the estimated value of s, denotedV(s), can be\nwritten as\nV(s) V(s) +\u000bh\nV(s0)\u0000V(s)i\n;\nwhere\u000bis a small positive fraction called the step-size parameter , which in-\n\ruences the rate of learning. This update rule is an example of a temporal-\ndi\u000berence learning method, so called because its changes are based on a dif-\nference,V(s0)\u0000V(s), between estimates at two di\u000berent times.\nThe method described above performs quite well on this task. For example,\nif the step-size parameter is reduced properly over time, this method converges,\nfor any \fxed opponent, to the true probabilities of winning from each state\ngiven optimal play by our player. Furthermore, the moves then taken (except\non exploratory moves) are in fact the optimal moves against the opponent. In\nother words, the method converges to an optimal policy for playing the game.\nIf the step-size parameter is not reduced all the way to zero over time, then\nthis player also plays well against opponents that slowly change their way of\nplaying.\nThis example illustrates the di\u000berences between evolutionary methods and\nthe methods that learn value functions. To evaluate a policy an evolutionary\nmethod holds the policy \fxed and plays many games against the opponent, or\nsimulate many games using a model of the opponent. The frequency of wins\ngives an unbiased estimate of the probability of winning with that policy, and\ncan be used to direct the next policy selection. But each policy change is made\nonly after many games, and only the \fnal outcome of each game is used: what\nhappens during the games is ignored. For example, if the player wins, then\nallof its behavior in the game is given credit, independently of how speci\fc\nmoves might have been critical to the win. Credit is even given to moves that\nnever occurred! Value function methods, in contrast, allow individual states\nto be evaluated. In the end, evolutionary and value function methods both\nsearch the space of policies, but learning a value function takes advantage of\ninformation available during the course of play.\nThis simple example illustrates some of the key features of reinforcement\nlearning methods. First, there is the emphasis on learning while interacting\n14 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nwith an environment, in this case with an opponent player. Second, there is a\nclear goal, and correct behavior requires planning or foresight that takes into\naccount delayed e\u000bects of one's choices. For example, the simple reinforce-\nment learning player would learn to set up multi-move traps for a shortsighted\nopponent. It is a striking feature of the reinforcement learning solution that it\ncan achieve the e\u000bects of planning and lookahead without using a model of the\nopponent and without conducting an explicit search over possible sequences\nof future states and actions.\nWhile this example illustrates some of the key features of reinforcement\nlearning, it is so simple that it might give the impression that reinforcement\nlearning is more limited than it really is. Although tic-tac-toe is a two-person\ngame, reinforcement learning also applies in the case in which there is no exter-\nnal adversary, that is, in the case of a \\game against nature.\" Reinforcement\nlearning also is not restricted to problems in which behavior breaks down into\nseparate episodes, like the separate games of tic-tac-toe, with reward only at\nthe end of each episode. It is just as applicable when behavior continues indef-\ninitely and when rewards of various magnitudes can be received at any time.\nReinforcement learning is also applicable to problems that do not even break\ndown into discrete time steps, like the plays of tic-tac-toe. The general princi-\nples apply to continuous-time problems as well, although the theory gets more\ncomplicated and we omit it from this introductory treatment.\nTic-tac-toe has a relatively small, \fnite state set, whereas reinforcement\nlearning can be used when the state set is very large, or even in\fnite. For\nexample, Gerry Tesauro (1992, 1995) combined the algorithm described above\nwith an arti\fcial neural network to learn to play backgammon, which has\napproximately 1020states. With this many states it is impossible ever to\nexperience more than a small fraction of them. Tesauro's program learned to\nplay far better than any previous program, and now plays at the level of the\nworld's best human players (see Chapter 15). The neural network provides\nthe program with the ability to generalize from its experience, so that in new\nstates it selects moves based on information saved from similar states faced\nin the past, as determined by its network. How well a reinforcement learning\nsystem can work in problems with such large state sets is intimately tied to\nhow appropriately it can generalize from past experience. It is in this role that\nwe have the greatest need for supervised learning methods with reinforcement\nlearning. Neural networks are not the only, or necessarily the best, way to do\nthis.\nIn this tic-tac-toe example, learning started with no prior knowledge be-\nyond the rules of the game, but reinforcement learning by no means entails a\ntabula rasa view of learning and intelligence. On the contrary, prior informa-\ntion can be incorporated into reinforcement learning in a variety of ways that\n1.6. SUMMARY 15\ncan be critical for e\u000ecient learning. We also had access to the true state in the\ntic-tac-toe example, whereas reinforcement learning can also be applied when\npart of the state is hidden, or when di\u000berent states appear to the learner to be\nthe same. That case, however, is substantially more di\u000ecult, and we do not\ncover it signi\fcantly in this book.\nFinally, the tic-tac-toe player was able to look ahead and know the states\nthat would result from each of its possible moves. To do this, it had to have\na model of the game that allowed it to \\think about\" how its environment\nwould change in response to moves that it might never make. Many problems\nare like this, but in others even a short-term model of the e\u000bects of actions\nis lacking. Reinforcement learning can be applied in either case. No model is\nrequired, but models can easily be used if they are available or can be learned.\nOn the other hand, there are reinforcement learning methods that do not\nneed any kind of environment model at all. Model-free systems cannot even\nthink about how their environments will change in response to a single action.\nThe tic-tac-toe player is model-free in this sense with respect to its opponent:\nit has no model of its opponent of any kind. Because models have to be\nreasonably accurate to be useful, model-free methods can have advantages over\nmore complex methods when the real bottleneck in solving a problem is the\ndi\u000eculty of constructing a su\u000eciently accurate environment model. Model-\nfree methods are also important building blocks for model-based methods. In\nthis book we devote several chapters to model-free methods before we discuss\nhow they can be used as components of more complex model-based methods.\nBut reinforcement learning can be used at both high and low levels in a sys-\ntem. Although the tic-tac-toe player learned only about the basic moves of the\ngame, nothing prevents reinforcement learning from working at higher levels\nwhere each of the \\actions\" may itself be the application of a possibly elabo-\nrate problem-solving method. In hierarchical learning systems, reinforcement\nlearning can work simultaneously on several levels.\n1.6 Summary\nReinforcement learning is a computational approach to understanding and au-\ntomating goal-directed learning and decision-making. It is distinguished from\nother computational approaches by its emphasis on learning by an agent from\ndirect interaction with its environment, without relying on exemplary super-\nvision or complete models of the environment. In our opinion, reinforcement\nlearning is the \frst \feld to seriously address the computational issues that\narise when learning from interaction with an environment in order to achieve\n16 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nlong-term goals.\nReinforcement learning uses a formal framework de\fning the interaction\nbetween a learning agent and its environment in terms of states, actions, and\nrewards. This framework is intended to be a simple way of representing es-\nsential features of the arti\fcial intelligence problem. These features include a\nsense of cause and e\u000bect, a sense of uncertainty and nondeterminism, and the\nexistence of explicit goals.\nThe concepts of value and value functions are the key features of most of\nthe reinforcement learning methods that we consider in this book. We take\nthe position that value functions are important for e\u000ecient search in the space\nof policies. Their use of value functions distinguishes reinforcement learning\nmethods from evolutionary methods that search directly in policy space guided\nby scalar evaluations of entire policies.\n1.7 History of Reinforcement Learning\nThe history of reinforcement learning has two main threads, both long and rich,\nthat were pursued independently before intertwining in modern reinforcement\nlearning. One thread concerns learning by trial and error that started in the\npsychology of animal learning. This thread runs through some of the earliest\nwork in arti\fcial intelligence and led to the revival of reinforcement learning in\nthe early 1980s. The other thread concerns the problem of optimal control and\nits solution using value functions and dynamic programming. For the most\npart, this thread did not involve learning. Although the two threads have\nbeen largely independent, the exceptions revolve around a third, less distinct\nthread concerning temporal-di\u000berence methods such as used in the tic-tac-toe\nexample in this chapter. All three threads came together in the late 1980s\nto produce the modern \feld of reinforcement learning as we present it in this\nbook.\nThe thread focusing on trial-and-error learning is the one with which we\nare most familiar and about which we have the most to say in this brief history.\nBefore doing that, however, we brie\ry discuss the optimal control thread.\nThe term \\optimal control\" came into use in the late 1950s to describe\nthe problem of designing a controller to minimize a measure of a dynamical\nsystem's behavior over time. One of the approaches to this problem was de-\nveloped in the mid-1950s by Richard Bellman and others through extending\na nineteenth century theory of Hamilton and Jacobi. This approach uses the\nconcepts of a dynamical system's state and of a value function, or \\optimal\nreturn function,\" to de\fne a functional equation, now often called the Bell-\n1.7. HISTORY OF REINFORCEMENT LEARNING 17\nman equation. The class of methods for solving optimal control problems by\nsolving this equation came to be known as dynamic programming (Bellman,\n1957a). Bellman (1957b) also introduced the discrete stochastic version of the\noptimal control problem known as Markovian decision processes (MDPs), and\nRonald Howard (1960) devised the policy iteration method for MDPs. All of\nthese are essential elements underlying the theory and algorithms of modern\nreinforcement learning.\nDynamic programming is widely considered the only feasible way of solv-\ning general stochastic optimal control problems. It su\u000bers from what Bell-\nman called \\the curse of dimensionality,\" meaning that its computational\nrequirements grow exponentially with the number of state variables, but it\nis still far more e\u000ecient and more widely applicable than any other general\nmethod. Dynamic programming has been extensively developed since the\nlate 1950s, including extensions to partially observable MDPs (surveyed by\nLovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), ap-\nproximation methods (surveyed by Rust, 1996), and asynchronous methods\n(Bertsekas, 1982, 1983). Many excellent modern treatments of dynamic pro-\ngramming are available (e.g., Bertsekas, 1995; Puterman, 1994; Ross, 1983;\nand Whittle, 1982, 1983). Bryson (1996) provides an authoritative history of\noptimal control.\nConnections between optimal control and dynamic programming, on the\none hand, and learning, on the other, were slow to be recognized. We can-\nnot be sure about what accounted for this separation, but its main cause was\nlikely the separation between the disciplines involved and their di\u000berent goals.\nAlso contributing may have been the prevalent view of dynamic programming\nas an o\u000b-line computation depending essentially on accurate system models\nand analytic solutions to the Bellman equation. Further, the simplest form\nof dynamic programming is a computation that proceeds backwards in time,\nmaking it di\u000ecult to see how it could be involved in a learning process that\nmust proceed in a forward direction. Perhaps the \frst to connect optimal con-\ntrol and dynamic programming with learning was Paul Werbos (1977), who\nproposed an approximate approach to dynamic programming that he called\n\\heuristic dynamic programming.\" He later argued for the need for greater\ninterrelation of dynamic programming and learning methods and its relevance\nto understanding neural and cognitive mechanisms (Werbos, 1987). For us\nthe full integration of dynamic programming methods with on-line learning\ndid not occur until the work of Chris Watkins in 1989, whose treatment of\nreinforcement learning using the MDP formalism has been widely adopted\n(Watkins, 1989). Since then these relationships have been extensively devel-\noped by many researchers, most particularly by Dimitri Bertsekas and John\nTsitsiklis (1996), who coined the term \\neurodynamic programming\" to refer\n18 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nto the combination of dynamic programming and neural networks. Another\nterm currently in use is \\approximate dynamic programming.\" These various\napproaches emphasize di\u000berent aspects of the subject, but they all share with\nreinforcement learning an interest in circumventing the classical shortcomings\nof dynamic programming.\nIn this book, we consider all of the work in optimal control also to be,\nin a sense, work in reinforcement learning. We de\fne a reinforcement learn-\ning method as any e\u000bective way of solving reinforcement learning problems,\nand it is now clear that these problems are closely related to optimal con-\ntrol problems, particularly stochastic optimal control problems such as those\nformulated as MDPs. Accordingly, we must consider the solution methods\nof optimal control, such as dynamic programming, also to be reinforcement\nlearning methods. Because almost all of the conventional methods require\ncomplete knowledge of the system to be controlled, it feels a little unnatural\nto say that they are part of reinforcement learning . On the other hand, many\ndynamic programming algorithms are incremental and iterative. Like learning\nmethods, they gradually reach the correct answer through successive approx-\nimations. As we show in the rest of this book, these similarities are far more\nthan super\fcial. The theories and solution methods for the cases of complete\nand incomplete knowledge are so closely related that we feel they must be\nconsidered together as part of the same subject matter.\nLet us return now to the other major thread leading to the modern \feld of\nreinforcement learning, that centered on the idea of trial-and-error learning.\nWe only touch on the major points of contact here, taking up this topic in more\ndetail in Chapter ??. According to American psychologist R. S. Woodworth\nthe idea of trial-and-error learning goes as far back as the 1850s to Alexander\nBain's discussion of learning by \\groping and experiment\" and more explicitly\nto the British ethologist and psychologist Conway Lloyd Morgan's 1894 use of\nthe term to describe his observations of animal behavior (Woodworth, 1938).\nPerhaps the \frst to succinctly express the essence of trial-and-error learning\nas a principle of learning was Edward Thorndike:\nOf several responses made to the same situation, those which are\naccompanied or closely followed by satisfaction to the animal will,\nother things being equal, be more \frmly connected with the sit-\nuation, so that, when it recurs, they will be more likely to recur;\nthose which are accompanied or closely followed by discomfort to\nthe animal will, other things being equal, have their connections\nwith that situation weakened, so that, when it recurs, they will be\nless likely to occur. The greater the satisfaction or discomfort, the\ngreater the strengthening or weakening of the bond. (Thorndike,\n1911, p. 244)\n1.7. HISTORY OF REINFORCEMENT LEARNING 19\nThorndike called this the \\Law of E\u000bect\" because it describes the e\u000bect of\nreinforcing events on the tendency to select actions. Thorndike later modi\fed\nthe law to better account for accumulating data on animal learning (such as\ndi\u000berences between the e\u000bects of reward and punishment), and the law in its\nvarious forms has generated considerable controversy among learning theorists\n(e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994).\nDespite this, the Law of E\u000bect|in one form or another|is widely regarded\nas a basic principle underlying much behavior (e.g., Hilgard and Bower, 1975;\nDennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the in\ruential\nlearning theories of Clark Hull and experimental methods of B. F. Skinner\n(e.g., Hull, 1943; Skinner, 1938).\nThe term \\reinforcement\" in the context of animal learning came into use\nwell after Thorndike's expression of the Law of E\u000bect, to the best of our knowl-\nedge \frst appearing in this context in the 1927 English translation of Pavlov's\nmonograph on conditioned re\rexes. Reinforcement is the strengthening of a\npattern of behavior as a result of an animal receiving a stimulus|a reinforcer|\nin an appropriate temporal relationship with another stimulus or with a re-\nsponse. Some psychologists extended its meaning to include the process of\nweakening in addition to strengthening, as well applying when the omission or\ntermination of an event changes behavior. Reinforcement produces changes in\nbehavior that persist after the reinforcer is withdrawn, so that a stimulus that\nattracts an animal's attention or that energizes its behavior without producing\nlasting changes is not considered to be a reinforcer.\nThe idea of implementing trial-and-error learning in a computer appeared\namong the earliest thoughts about the possibility of arti\fcial intelligence. In a\n1948 report, Alan Turing described a design for a \\pleasure-pain system\" that\nworked along the lines of the Law of E\u000bect:\nWhen a con\fguration is reached for which the action is undeter-\nmined, a random choice for the missing data is made and the appro-\npriate entry is made in the description, tentatively, and is applied.\nWhen a pain stimulus occurs all tentative entries are cancelled,\nand when a pleasure stimulus occurs they are all made permanent.\n(Turing, 1948)\nIn 1952 Claude Shannon demonstrated a maze-running mouse named Theseus\nthat used trial and error to \fnd its way to a goal location in a maze, with\nthe maze itself remembering the successful directions via magnets and relays\nunder its \roor (Shannon, 1952). Other early computational investigations of\ntrial-and-error learning were those of Minsky and of Farley and Clark, both\nin 1954. In his Ph.D. dissertation, Minsky discussed computational models of\nreinforcement learning and described his construction of an analog machine\n20 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\ncomposed of components he called SNARCs (Stochastic Neural-Analog Rein-\nforcement Calculators). Farley and Clark described another neural-network\nlearning machine designed to learn by trial and error. In the 1960s the terms\n\\reinforcement\" and \\reinforcement learning\" were used in the engineering lit-\nerature for the \frst time (e.g., Waltz and Fu, 1965; Mendel, 1966; Fu, 1970;\nMendel and McClaren, 1970). Particularly in\ruential was Minsky's paper\n\\Steps Toward Arti\fcial Intelligence\" (Minsky, 1961), which discussed several\nissues relevant to reinforcement learning, including what he called the credit\nassignment problem : How do you distribute credit for success among the many\ndecisions that may have been involved in producing it? All of the methods we\ndiscuss in this book are, in a sense, directed toward solving this problem.\nThe interests of Farley and Clark (1954; Clark and Farley, 1955) shifted\nfrom trial-and-error learning to generalization and pattern recognition, that\nis, from reinforcement learning to supervised learning. This began a pattern\nof confusion about the relationship between these types of learning. Many\nresearchers seemed to believe that they were studying reinforcement learning\nwhen they were actually studying supervised learning. For example, neural\nnetwork pioneers such as Rosenblatt (1962) and Widrow and Ho\u000b (1960) were\nclearly motivated by reinforcement learning|they used the language of re-\nwards and punishments|but the systems they studied were supervised learn-\ning systems suitable for pattern recognition and perceptual learning. Even to-\nday, some researchers and textbooks minimize or blur the distinction between\nthese types of learning. For example, some neural-network textbooks have\nused the term \\trial-and-error\" to describe networks that learn from training\nexamples. This is an understandable confusion because these networks use\nerror information to update connection weights, but this substantially misses\nthe essential selectional character of trial-and-error learning.\nPartly as a result of these confusions, research into genuine trial-and-error\nlearning became rare in the the 1960s and 1970s. In the next few paragraphs\nwe discuss some of the exceptions and partial exceptions to this trend.\nOne of these was the work by a New Zealand researcher named John An-\ndreae. Andreae (1963) developed a system called STeLLA that learned by trial\nand error in interaction with its environment. This system included an internal\nmodel of the world and, later, an \\internal monologue\" to deal with problems\nof hidden state (Andreae, 1969a). Andreae's later work (1977) placed more\nemphasis on learning from a teacher, but still included trial and error. Un-\nfortunately, his pioneering research was not well known, and did not greatly\nimpact subsequent reinforcement learning research.\nMore in\ruential was the work of Donald Michie. In 1961 and 1963 he\ndescribed a simple trial-and-error learning system for learning how to play\n1.7. HISTORY OF REINFORCEMENT LEARNING 21\ntic-tac-toe (or naughts and crosses) called MENACE (for Matchbox Educable\nNaughts and Crosses Engine). It consisted of a matchbox for each possible\ngame position, each matchbox containing a number of colored beads, a dif-\nferent color for each possible move from that position. By drawing a bead at\nrandom from the matchbox corresponding to the current game position, one\ncould determine MENACE's move. When a game was over, beads were added\nto or removed from the boxes used during play to reinforce or punish MEN-\nACE's decisions. Michie and Chambers (1968) described another tic-tac-toe\nreinforcement learner called GLEE (Game Learning Expectimaxing Engine)\nand a reinforcement learning controller called BOXES. They applied BOXES\nto the task of learning to balance a pole hinged to a movable cart on the basis\nof a failure signal occurring only when the pole fell or the cart reached the end\nof a track. This task was adapted from the earlier work of Widrow and Smith\n(1964), who used supervised learning methods, assuming instruction from a\nteacher already able to balance the pole. Michie and Chambers's version of\npole-balancing is one of the best early examples of a reinforcement learning\ntask under conditions of incomplete knowledge. It in\ruenced much later work\nin reinforcement learning, beginning with some of our own studies (Barto,\nSutton, and Anderson, 1983; Sutton, 1984). Michie has consistently empha-\nsized the role of trial and error and learning as essential aspects of arti\fcial\nintelligence (Michie, 1974).\nWidrow, Gupta, and Maitra (1973) modi\fed the Least-Mean-Square (LMS)\nalgorithm of Widrow and Ho\u000b (1960) to produce a reinforcement learning rule\nthat could learn from success and failure signals instead of from training exam-\nples. They called this form of learning \\selective bootstrap adaptation\" and\ndescribed it as \\learning with a critic\" instead of \\learning with a teacher.\"\nThey analyzed this rule and showed how it could learn to play blackjack. This\nwas an isolated foray into reinforcement learning by Widrow, whose contribu-\ntions to supervised learning were much more in\ruential. Our use of the term\n\\critic\" is derived from Widrow, Gupta, and Maitra's paper.\nResearch on learning automata had a more direct in\ruence on the trial-\nand-error thread leading to modern reinforcement learning research. These\nare methods for solving a nonassociative, purely selectional learning problem\nknown as the n-armed bandit by analogy to a slot machine, or \\one-armed\nbandit,\" except with nlevers (see Chapter 2). Learning automata are simple,\nlow-memory machines for improving the probability of reward in these prob-\nlems. Learning automata originated with work in the 1960s of the Russian\nmathematician and physicist M. L. Tsetlin and colleagues (published posthu-\nmously in Tsetlin, 1973) and has been extensively developed since then within\nengineering (see Narendra and Thathachar, 1974, 1989). These developments\nincluded the study of stochastic learning automata , which are methods for up-\n22 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\ndating action probabilities on the basis of reward signals. Stochastic learning\nautomata were foreshadowed by earlier work in psychology, beginning with\nWilliam Estes' 1950 e\u000bort toward a statistical theory of learning (Estes, 1950)\nand further developed by others, most famously by psychologist Robert Bush\nand statistician Frederick Mosteller (Bush and Mosteller, 1955).\nThe statistical learning theories developed in psychology were adopted by\nresearchers in economics, leading to a thread of research in that \feld devoted to\nreinforcement learning. This work began in 1973 with the application of Bush\nand Mosteller's learning theory to a collection of classical economic models\n(Cross, 1973). One goal of this research was to study arti\fcial agents that act\nmore like real people than do traditional idealized economic agents (Arthur,\n1991). This approach expanded to the study of reinforcement learning in the\ncontext of game theory. Although reinforcement learning in economics de-\nveloped largely independently of the early work in arti\fcial intelligence, rein-\nforcement learning and game theory is a topic of current interest in both \felds,\nbut one that is beyond the scope of this book. Camerer (2003) discusses the\nreinforcement learning tradition in economics, and Now\u0013 e et al. (2012) provide\nan overview of the subject from the point of view of multi-agent extensions\nto the approach that we introduce in this book. Reinforcement learning and\ngame theory is a much di\u000berent subject from reinforcement learning used in\nprograms to play tic-tac-toe, checkers, and other recreational games. See, for\nexample, Szita (2012) for an overview of this aspect of reinforcement learning\nand games.\nJohn Holland (1975) outlined a general theory of adaptive systems based\non selectional principles. His early work concerned trial and error primarily in\nits nonassociative form, as in evolutionary methods and the n-armed bandit.\nIn 1986 he introduced classi\fer systems , true reinforcement learning systems\nincluding association and value functions. A key component of Holland's clas-\nsi\fer systems was always a genetic algorithm , an evolutionary method whose\nrole was to evolve useful representations. Classi\fer systems have been exten-\nsively developed by many researchers to form a major branch of reinforcement\nlearning research (reviewed by Urbanowicz and Moore, 2009), but genetic\nalgorithms|which we do not consider to be reinforcement learning systems\nby themselves|have received much more attention, as have other approaches\nto evolutionary computation (e.g., Fogel, Owens and Walsh, 1966, and Koza,\n1992).\nThe individual most responsible for reviving the trial-and-error thread to\nreinforcement learning within arti\fcial intelligence was Harry Klopf (1972,\n1975, 1982). Klopf recognized that essential aspects of adaptive behavior\nwere being lost as learning researchers came to focus almost exclusively on\nsupervised learning. What was missing, according to Klopf, were the hedonic\n1.7. HISTORY OF REINFORCEMENT LEARNING 23\naspects of behavior, the drive to achieve some result from the environment, to\ncontrol the environment toward desired ends and away from undesired ends.\nThis is the essential idea of trial-and-error learning. Klopf's ideas were espe-\ncially in\ruential on the authors because our assessment of them (Barto and\nSutton, 1981a) led to our appreciation of the distinction between supervised\nand reinforcement learning, and to our eventual focus on reinforcement learn-\ning. Much of the early work that we and colleagues accomplished was directed\ntoward showing that reinforcement learning and supervised learning were in-\ndeed di\u000berent (Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b;\nBarto and Anandan, 1985). Other studies showed how reinforcement learning\ncould address important problems in neural network learning, in particular,\nhow it could produce learning algorithms for multilayer networks (Barto, An-\nderson, and Sutton, 1982; Barto and Anderson, 1985; Barto and Anandan,\n1985; Barto, 1985, 1986; Barto and Jordan, 1987). We say more about the\nreinforcement learning and neural networks in Chapter Y.\nWe turn now to the third thread to the history of reinforcement learn-\ning, that concerning temporal-di\u000berence learning. Temporal-di\u000berence learn-\ning methods are distinctive in being driven by the di\u000berence between tempo-\nrally successive estimates of the same quantity|for example, of the probability\nof winning in the tic-tac-toe example. This thread is smaller and less distinct\nthan the other two, but it has played a particularly important role in the \feld,\nin part because temporal-di\u000berence methods seem to be new and unique to\nreinforcement learning.\nThe origins of temporal-di\u000berence learning are in part in animal learning\npsychology, in particular, in the notion of secondary reinforcers . A secondary\nreinforcer is a stimulus that has been paired with a primary reinforcer such as\nfood or pain and, as a result, has come to take on similar reinforcing proper-\nties. Minsky (1954) may have been the \frst to realize that this psychological\nprinciple could be important for arti\fcial learning systems. Arthur Samuel\n(1959) was the \frst to propose and implement a learning method that included\ntemporal-di\u000berence ideas, as part of his celebrated checkers-playing program.\nSamuel made no reference to Minsky's work or to possible connections\nto animal learning. His inspiration apparently came from Claude Shannon's\n(1950) suggestion that a computer could be programmed to use an evalua-\ntion function to play chess, and that it might be able to improve its play by\nmodifying this function on-line. (It is possible that these ideas of Shannon's\nalso in\ruenced Bellman, but we know of no evidence for this.) Minsky (1961)\nextensively discussed Samuel's work in his \\Steps\" paper, suggesting the con-\nnection to secondary reinforcement theories, both natural and arti\fcial.\nAs we have discussed, in the decade following the work of Minsky and\n24 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\nSamuel, little computational work was done on trial-and-error learning, and\napparently no computational work at all was done on temporal-di\u000berence\nlearning. In 1972, Klopf brought trial-and-error learning together with an\nimportant component of temporal-di\u000berence learning. Klopf was interested\nin principles that would scale to learning in large systems, and thus was in-\ntrigued by notions of local reinforcement, whereby subcomponents of an overall\nlearning system could reinforce one another. He developed the idea of \\gen-\neralized reinforcement,\" whereby every component (nominally, every neuron)\nviews all of its inputs in reinforcement terms: excitatory inputs as rewards\nand inhibitory inputs as punishments. This is not the same idea as what we\nnow know as temporal-di\u000berence learning, and in retrospect it is farther from\nit than was Samuel's work. On the other hand, Klopf linked the idea with\ntrial-and-error learning and related it to the massive empirical database of\nanimal learning psychology.\nSutton (1978a, 1978b, 1978c) developed Klopf's ideas further, particu-\nlarly the links to animal learning theories, describing learning rules driven\nby changes in temporally successive predictions. He and Barto re\fned these\nideas and developed a psychological model of classical conditioning based on\ntemporal-di\u000berence learning (Sutton and Barto, 1981a; Barto and Sutton,\n1982). There followed several other in\ruential psychological models of classical\nconditioning based on temporal-di\u000berence learning (e.g., Klopf, 1988; Moore\net al., 1986; Sutton and Barto, 1987, 1990). Some neuroscience models devel-\noped at this time are well interpreted in terms of temporal-di\u000berence learning\n(Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990; Gelperin,\nHop\feld, and Tank, 1985; Tesauro, 1986; Friston et al., 1994), although in\nmost cases there was no historical connection.\nOur early work on temporal-di\u000berence learning was strongly in\ruenced\nby animal learning theories and by Klopf's work. Relationships to Minsky's\n\\Steps\" paper and to Samuel's checkers players appear to have been recognized\nonly afterward. By 1981, however, we were fully aware of all the prior work\nmentioned above as part of the temporal-di\u000berence and trial-and-error threads.\nAt this time we developed a method for using temporal-di\u000berence learning in\ntrial-and-error learning, known as the actor{critic architecture , and applied\nthis method to Michie and Chambers's pole-balancing problem (Barto, Sutton,\nand Anderson, 1983). This method was extensively studied in Sutton's (1984)\nPh.D. dissertation and extended to use backpropagation neural networks in\nAnderson's (1986) Ph.D. dissertation. Around this time, Holland (1986) incor-\nporated temporal-di\u000berence ideas explicitly into his classi\fer systems. A key\nstep was taken by Sutton in 1988 by separating temporal-di\u000berence learning\nfrom control, treating it as a general prediction method. That paper also in-\ntroduced the TD( \u0015) algorithm and proved some of its convergence properties.\n1.8. BIBLIOGRAPHICAL REMARKS 25\nAs we were \fnalizing our work on the actor{critic architecture in 1981, we\ndiscovered a paper by Ian Witten (1977) that contains the earliest known pub-\nlication of a temporal-di\u000berence learning rule. He proposed the method that\nwe now call tabular TD(0) for use as part of an adaptive controller for solving\nMDPs. Witten's work was a descendant of Andreae's early experiments with\nSTeLLA and other trial-and-error learning systems. Thus, Witten's 1977 pa-\nper spanned both major threads of reinforcement learning research|trial-and-\nerror learning and optimal control|while making a distinct early contribution\nto temporal-di\u000berence learning.\nThe temporal-di\u000berence and optimal control threads were fully brought\ntogether in 1989 with Chris Watkins's development of Q-learning. This work\nextended and integrated prior work in all three threads of reinforcement learn-\ning research. Paul Werbos (1987) contributed to this integration by arguing\nfor the convergence of trial-and-error learning and dynamic programming since\n1977. By the time of Watkins's work there had been tremendous growth in\nreinforcement learning research, primarily in the machine learning sub\feld\nof arti\fcial intelligence, but also in neural networks and arti\fcial intelligence\nmore broadly. In 1992, the remarkable success of Gerry Tesauro's backgammon\nplaying program, TD-Gammon, brought additional attention to the \feld.\nIn the time since publication of the \frst edition of this book, a \rourishing\nsub\feld of neuroscience developed that focuses on the relationship between\nreinforcement learning algorithms and reinforcement learning in the nervous\nsystem. Most responsible for this is an uncanny similarity between the behav-\nior of temporal-di\u000berence algorithms and the activity of dopamine producing\nneurons in the brain, as pointed out by a number of researchers (Friston et\nal., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague, Dayan,\nand Sejnowski, 1996; and Schultz, Dayan, and Montague,1997). Chapter Y\nprovides is an introduction to this exciting aspect of reinforcement leaning.\nOther important contributions made in the recent history of reinforcement\nlearning are too numerous to mention in this brief account; we cite many these\nat the end of the individual chapters in which they arise.\n1.8 Bibliographical Remarks\nFor additional general coverage of reinforcement learning, we refer the reader to\nthe books by Szepesv\u0013 ari (2010), Bertsekas and Tsitsiklis (1996), and Kaelbling\n(1993a). Books that take a control or operation research perspective are those\nof Si et al. (2004), Powell (2011), and Lewis and Liu (2012). Two special\nissues of the journal Machine Learning focus on reinforcement learning: Sutton\n26 CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM\n(1992) and Kaelbling (1996). Useful surveys are provided by Barto (1995b);\nKaelbling, Littman, and Moore (1996); and Keerthi and Ravindran (1997).\nThe volume edited by Weiring and van Otterlo (2012) provides an excellent\noverview of recent developments.\nThe example of Phil's breakfast in this chapter was inspired by Agre (1988).\nWe direct the reader to Chapter 6 for references to the kind of temporal-\ndi\u000berence method we used in the tic-tac-toe example.\nExercises\nExercise 1.1: Self-Play Suppose, instead of playing against a random\nopponent, the reinforcement learning algorithm described above played against\nitself. What do you think would happen in this case? Would it learn a di\u000berent\nway of playing?\nExercise 1.2: Symmetries Many tic-tac-toe positions appear di\u000berent but\nare really the same because of symmetries. How might we amend the reinforce-\nment learning algorithm described above to take advantage of this? In what\nways would this improve it? Now think again. Suppose the opponent did not\ntake advantage of symmetries. In that case, should we? Is it true, then, that\nsymmetrically equivalent positions should necessarily have the same value?\nExercise 1.3: Greedy Play Suppose the reinforcement learning player was\ngreedy , that is, it always played the move that brought it to the position that\nit rated the best. Would it learn to play better, or worse, than a nongreedy\nplayer? What problems might occur?\nExercise 1.4: Learning from Exploration Suppose learning updates occurred\nafter allmoves, including exploratory moves. If the step-size parameter is\nappropriately reduced over time, then the state values would converge to a\nset of probabilities. What are the two sets of probabilities computed when we\ndo, and when we do not, learn from exploratory moves? Assuming that we\ndo continue to make exploratory moves, which set of probabilities might be\nbetter to learn? Which would result in more wins?\nExercise 1.5: Other Improvements Can you think of other ways to improve\nthe reinforcement learning player? Can you think of any better way to solve\nthe tic-tac-toe problem as posed?\nPart I\nTabular Solution Methods\n27\n\n29\nIn this part of the book we describe almost all the core ideas of reinforce-\nment learning algorithms in their simplest forms, that in which the state and\naction spaces are small enough for the approximate action-value function to be\nrepresented as an array, or table. In this case, the methods can often \fnd exact\nsolutions, that is, they can often \fnd exactly the optimal value function and\nthe optimal policy. This contrasts with the approximate methods described in\nthe next part of the book, which only \fnd approximate solutions, but which\nin return can be applied e\u000bectively to much larger problems.\nThe \frst chapter of this part of the book describes solution methods for\nthe special of the reinforcement learning problem in which there is only a\nsingle state, called bandit problems. The second chapter describes the general\nproblem formulation that we treat throughout the rest of the book|\fnite\nmarkov decision processes|and its main ideas including Bellman equations\nand value functions.\nThe next three chapters describe three fundamental classes of methods\nfor solving \fnite Markov decision problems: dynamic programming, Monte\nCarlo methods, and temporal-di\u000berence learning. Each class of methods has\nits strengths and weaknesses. Dynamic programming methods are well de-\nveloped mathematically, but require a complete and accurate model of the\nenvironment. Monte Carlo methods don't require a model and are concep-\ntually simple, but are not suited for step-by-step incremental computation.\nFinally, temporal-di\u000berence methods require no model and are fully incremen-\ntal, but are more complex to analyze. The methods also di\u000ber in several ways\nwith respect to their e\u000eciency and speed of convergence.\nThe remaining two chapters describe how these three classes of methods\ncan be combined to obtain the best features of each of them. In one chapter we\ndescribe how the strengths of Monte Carlo methods can be combined with the\nstrengths of temporal-di\u000berence methods via the use of eligibility traces. In\nthe \fnal chapter of this part of the book we show these two learning methods\ncan be combined with model learning and planning methods (such as dynamic\nprogramming) for a complete and uni\fed solution to the tabular reinforcement\nlearning problem.\n30\nChapter 2\nMulti-arm Bandits\nThe most important feature distinguishing reinforcement learning from other\ntypes of learning is that it uses training information that evaluates the actions\ntaken rather than instructs by giving correct actions. This is what creates\nthe need for active exploration, for an explicit trial-and-error search for good\nbehavior. Purely evaluative feedback indicates how good the action taken is,\nbut not whether it is the best or the worst action possible. Evaluative feed-\nback is the basis of methods for function optimization, including evolutionary\nmethods. Purely instructive feedback, on the other hand, indicates the cor-\nrect action to take, independently of the action actually taken. This kind\nof feedback is the basis of supervised learning, which includes large parts of\npattern classi\fcation, arti\fcial neural networks, and system identi\fcation. In\ntheir pure forms, these two kinds of feedback are quite distinct: evaluative\nfeedback depends entirely on the action taken, whereas instructive feedback is\nindependent of the action taken. There are also interesting intermediate cases\nin which evaluation and instruction blend together.\nIn this chapter we study the evaluative aspect of reinforcement learning in\na simpli\fed setting, one that does not involve learning to act in more than\none situation. This nonassociative setting is the one in which most prior\nwork involving evaluative feedback has been done, and it avoids much of the\ncomplexity of the full reinforcement learning problem. Studying this case will\nenable us to see most clearly how evaluative feedback di\u000bers from, and yet can\nbe combined with, instructive feedback.\nThe particular nonassociative, evaluative feedback problem that we explore\nis a simple version of the n-armed bandit problem. We use this problem to\nintroduce a number of basic learning methods which we extend in later chapters\nto apply to the full reinforcement learning problem. At the end of this chapter,\nwe take a step closer to the full reinforcement learning problem by discussing\n31\n32 CHAPTER 2. MULTI-ARM BANDITS\nwhat happens when the bandit problem becomes associative, that is, when\nactions are taken in more than one situation.\n2.1 Ann-Armed Bandit Problem\nConsider the following learning problem. You are faced repeatedly with a\nchoice among ndi\u000berent options, or actions. After each choice you receive a\nnumerical reward chosen from a stationary probability distribution that de-\npends on the action you selected. Your objective is to maximize the expected\ntotal reward over some time period, for example, over 1000 action selections,\nortime steps .\nThis is the original form of the n-armed bandit problem , so named by anal-\nogy to a slot machine, or \\one-armed bandit,\" except that it has nlevers\ninstead of one. Each action selection is like a play of one of the slot machine's\nlevers, and the rewards are the payo\u000bs for hitting the jackpot. Through re-\npeated action selections you are to maximize your winnings by concentrating\nyour actions on the best levers. Another analogy is that of a doctor choosing\nbetween experimental treatments for a series of seriously ill patients. Each\naction selection is a treatment selection, and each reward is the survival or\nwell-being of the patient. Today the term \\ n-armed bandit problem\" is some-\ntimes used for a generalization of the problem described above, but in this\nbook we use it to refer just to this simple case.\nIn ourn-armed bandit problem, each action has an expected or mean\nreward given that that action is selected; let us call this the value of that\naction. If you knew the value of each action, then it would be trivial to solve\nthen-armed bandit problem: you would always select the action with highest\nvalue. We assume that you do not know the action values with certainty,\nalthough you may have estimates.\nIf you maintain estimates of the action values, then at any time step there\nis at least one action whose estimated value is greatest. We call this a greedy\naction. If you select a greedy action, we say that you are exploiting your\ncurrent knowledge of the values of the actions. If instead you select one of\nthe nongreedy actions, then we say you are exploring , because this enables\nyou to improve your estimate of the nongreedy action's value. Exploitation is\nthe right thing to do to maximize the expected reward on the one step, but\nexploration may produce the greater total reward in the long run. For example,\nsuppose the greedy action's value is known with certainty, while several other\nactions are estimated to be nearly as good but with substantial uncertainty.\nThe uncertainty is such that at least one of these other actions probably is\n2.2. ACTION-VALUE METHODS 33\nactually better than the greedy action, but you don't know which one. If\nyou have many time steps ahead on which to make action selections, then it\nmay be better to explore the nongreedy actions and discover which of them\nare better than the greedy action. Reward is lower in the short run, during\nexploration, but higher in the long run because after you have discovered the\nbetter actions, you can exploit them many times. Because it is not possible\nboth to explore and to exploit with any single action selection, one often refers\nto the \\con\rict\" between exploration and exploitation.\nIn any speci\fc case, whether it is better to explore or exploit depends in a\ncomplex way on the precise values of the estimates, uncertainties, and the num-\nber of remaining steps. There are many sophisticated methods for balancing\nexploration and exploitation for particular mathematical formulations of the\nn-armed bandit and related problems. However, most of these methods make\nstrong assumptions about stationarity and prior knowledge that are either\nviolated or impossible to verify in applications and in the full reinforcement\nlearning problem that we consider in subsequent chapters. The guarantees of\noptimality or bounded loss for these methods are of little comfort when the\nassumptions of their theory do not apply.\nIn this book we do not worry about balancing exploration and exploitation\nin a sophisticated way; we worry only about balancing them at all. In this\nchapter we present several simple balancing methods for the n-armed bandit\nproblem and show that they work much better than methods that always\nexploit. The need to balance exploration and exploitation is a distinctive\nchallenge that arises in reinforcement learning; the simplicity of the n-armed\nbandit problem enables us to show this in a particularly clear form.\n2.2 Action-Value Methods\nWe begin by looking more closely at some simple methods for estimating the\nvalues of actions and for using the estimates to make action selection decisions.\nIn this chapter, we denote the true (actual) value of action aasq(a), and the\nestimated value on the tth time step as Qt(a). Recall that the true value of an\naction is the mean reward received when that action is selected. One natural\nway to estimate this is by averaging the rewards actually received when the\naction was selected. In other words, if by the tth time step action ahas been\nchosenNt(a) times prior to t, yielding rewards R1;R2;:::;RNt(a), then its value\nis estimated to be\nQt(a) =R1+R2+\u0001\u0001\u0001+RNt(a)\nNt(a): (2.1)\n34 CHAPTER 2. MULTI-ARM BANDITS\nIfNt(a) = 0, then we de\fne Qt(a) instead as some default value, such as\nQ1(a) = 0. As Nt(a)!1 , by the law of large numbers, Qt(a) converges\ntoq(a). We call this the sample-average method for estimating action values\nbecause each estimate is a simple average of the sample of relevant rewards.\nOf course this is just one way to estimate action values, and not necessarily\nthe best one. Nevertheless, for now let us stay with this simple estimation\nmethod and turn to the question of how the estimates might be used to select\nactions.\nThe simplest action selection rule is to select the action (or one of the\nactions) with highest estimated action value, that is, to select at step tone\nof the greedy actions, A\u0003\nt, for which Qt(A\u0003\nt) = max aQt(a). This greedy action\nselection method can be written as\nAt= argmax\naQt(a); (2.2)\nwhere argmaxadenotes the value of aat which the expression that follows\nis maximized (with ties broken arbitrarily). Greedy action selection always\nexploits current knowledge to maximize immediate reward; it spends no time\nat all sampling apparently inferior actions to see if they might really be bet-\nter. A simple alternative is to behave greedily most of the time, but every\nonce in a while, say with small probability \", instead to select randomly from\namongst all the actions with equal probability independently of the action-\nvalue estimates. We call methods using this near-greedy action selection rule\n\"-greedy methods. An advantage of these methods is that, in the limit as the\nnumber of plays increases, every action will be sampled an in\fnite number\nof times, guaranteeing that Nt(a)!1 for alla, and thus ensuring that all\ntheQt(a) converge to q(a). This of course implies that the probability of se-\nlecting the optimal action converges to greater than 1 \u0000\", that is, to near\ncertainty. These are just asymptotic guarantees, however, and say little about\nthe practical e\u000bectiveness of the methods.\nTo roughly assess the relative e\u000bectiveness of the greedy and \"-greedy meth-\nods, we compared them numerically on a suite of test problems. This was a\nset of 2000 randomly generated n-armed bandit tasks with n= 10. For each\nbandit, the action values, q(a),a= 1;:::; 10, were selected according to a\nnormal (Gaussian) distribution with mean 0 and variance 1. On tth time step\nwith a given bandit, the actual reward Rtwas theq(At) for the bandit (where\nAtwas the action selected) plus a normally distributed noise term that was\nmean 0 and variance 1. Averaging over bandits, we can plot the performance\nand behavior of various methods as they improve with experience over 1000\nsteps, as in Figure 2.1. We call this suite of test tasks the 10-armed testbed .\nFigure 2.1 compares a greedy method with two \"-greedy methods ( \"= 0:01\nand\"= 0:1), as described above, on the 10-armed testbed. Both methods\n2.2. ACTION-VALUE METHODS 35\n = 0 (greedy) \ud835\udf00 = 0 (greedy)\n00.511.5Averagereward02505007501000Steps\n0%20%40%60%80%100%%Optimalaction02505007501000Steps = 0.01 = 0.1\ud835\udf00\ud835\udf00\n\ud835\udf00 = 0.1\ud835\udf00= 0.01\ud835\udf00\nFigure 2.1: Average performance of \"-greedy action-value methods on the\n10-armed testbed. These data are averages over 2000 tasks. All methods\nused sample averages as their action-value estimates. The detailed structure\nat the beginning of these curves depends on how actions are selected when\nmultiple actions have the same maximal action value. Here such ties were\nbroken randomly. An alternative that has a similar e\u000bect is to add a very\nsmall amount of randomness to each of the initial action values, so that ties\ne\u000bectively never happen.\n36 CHAPTER 2. MULTI-ARM BANDITS\nformed their action-value estimates using the sample-average technique. The\nupper graph shows the increase in expected reward with experience. The\ngreedy method improved slightly faster than the other methods at the very\nbeginning, but then leveled o\u000b at a lower level. It achieved a reward per step of\nonly about 1, compared with the best possible of about 1.55 on this testbed.\nThe greedy method performs signi\fcantly worse in the long run because it\noften gets stuck performing suboptimal actions. The lower graph shows that\nthe greedy method found the optimal action in only approximately one-third of\nthe tasks. In the other two-thirds, its initial samples of the optimal action were\ndisappointing, and it never returned to it. The \"-greedy methods eventually\nperform better because they continue to explore, and to improve their chances\nof recognizing the optimal action. The \"= 0:1 method explores more, and\nusually \fnds the optimal action earlier, but never selects it more than 91%\nof the time. The \"= 0:01 method improves more slowly, but eventually\nperforms better than the \"= 0:1 method on both performance measures. It\nis also possible to reduce \"over time to try to get the best of both high and\nlow values.\nThe advantage of \"-greedy over greedy methods depends on the task. For\nexample, suppose the reward variance had been larger, say 10 instead of 1.\nWith noisier rewards it takes more exploration to \fnd the optimal action, and\n\"-greedy methods should fare even better relative to the greedy method. On\nthe other hand, if the reward variances were zero, then the greedy method\nwould know the true value of each action after trying it once. In this case the\ngreedy method might actually perform best because it would soon \fnd the\noptimal action and then never explore. But even in the deterministic case,\nthere is a large advantage to exploring if we weaken some of the other as-\nsumptions. For example, suppose the bandit task were nonstationary, that is,\nthat the true values of the actions changed over time. In this case exploration\nis needed even in the deterministic case to make sure one of the nongreedy\nactions has not changed to become better than the greedy one. As we will see\nin the next few chapters, e\u000bective nonstationarity is the case most commonly\nencountered in reinforcement learning. Even if the underlying task is station-\nary and deterministic, the learner faces a set of banditlike decision tasks each\nof which changes over time due to the learning process itself. Reinforcement\nlearning requires a balance between exploration and exploitation.\n2.3 Incremental Implementation\nThe action-value methods we have discussed so far all estimate action values\nas sample averages of observed rewards. The obvious implementation is to\n2.3. INCREMENTAL IMPLEMENTATION 37\nmaintain, for each action a, a record of all the rewards that have followed the\nselection of that action. Then, when the estimate of the value of action a is\nneeded at time t, it can be computed according to (2.1), which we repeat here:\nQt(a) =R1+R2+\u0001\u0001\u0001+RNt(a)\nNt(a);\nwhere here R1;:::;RNt(a)are all the rewards received following all selections of\nactionaprior to play t. A problem with this straightforward implementation\nis that its memory and computational requirements grow over time without\nbound. That is, each additional reward following a selection of action are-\nquires more memory to store it and results in more computation being required\nto determine Qt(a).\nAs you might suspect, this is not really necessary. It is easy to devise in-\ncremental update formulas for computing averages with small, constant com-\nputation required to process each new reward. For some action, let Qkdenote\nthe estimate for its kth reward, that is, the average of its \frst k\u00001 rewards.\nGiven this average and a kth reward for the action, Rk, then the average of\nallkrewards can be computed by\nQk+1=1\nkkX\ni=1Ri\n=1\nk \nRk+k\u00001X\ni=1Ri!\n=1\nk\u0010\nRk+ (k\u00001)Qk+Qk\u0000Qk\u0011\n=1\nk\u0010\nRk+kQk\u0000Qk\u0011\n=Qk+1\nkh\nRk\u0000Qki\n; (2.3)\nwhich holds even for k= 1, obtaining Q2=R1for arbitrary Q1. This imple-\nmentation requires memory only for Qkandk, and only the small computation\n(2.3) for each new reward.\nThe update rule (2.3) is of a form that occurs frequently throughout this\nbook. The general form is\nNewEstimate OldEstimate +StepSizeh\nTarget\u0000OldEstimatei\n:(2.4)\nThe expression\u0002\nTarget\u0000OldEstimate\u0003\nis an error in the estimate. It is\nreduced by taking a step toward the \\Target.\" The target is presumed to\n38 CHAPTER 2. MULTI-ARM BANDITS\nindicate a desirable direction in which to move, though it may be noisy. In\nthe case above, for example, the target is the kth reward.\nNote that the step-size parameter ( StepSize ) used in the incremental\nmethod described above changes from time step to time step. In process-\ning thekth reward for action a, that method uses a step-size parameter of\n1\nk. In this book we denote the step-size parameter by the symbol \u000bor, more\ngenerally, by \u000bt(a). We sometimes use the informal shorthand \u000b=1\nkto refer\nto this case, leaving the dependence of kon the action implicit.\n2.4 Tracking a Nonstationary Problem\nThe averaging methods discussed so far are appropriate in a stationary envi-\nronment, but not if the bandit is changing over time. As noted earlier, we\noften encounter reinforcement learning problems that are e\u000bectively nonsta-\ntionary. In such cases it makes sense to weight recent rewards more heavily\nthan long-past ones. One of the most popular ways of doing this is to use a\nconstant step-size parameter. For example, the incremental update rule (2.3)\nfor updating an average Qkof thek\u00001 past rewards is modi\fed to be\nQk+1=Qk+\u000bh\nRk\u0000Qki\n; (2.5)\nwhere the step-size parameter \u000b2(0;1]1is constant. This results in Qk+1\nbeing a weighted average of past rewards and the initial estimate Q1:\nQk+1=Qk+\u000bh\nRk\u0000Qki\n=\u000bRk+ (1\u0000\u000b)Qk\n=\u000bRk+ (1\u0000\u000b) [\u000bRk\u00001+ (1\u0000\u000b)Qk\u00001]\n=\u000bRk+ (1\u0000\u000b)\u000bRk\u00001+ (1\u0000\u000b)2Qk\u00001\n=\u000bRk+ (1\u0000\u000b)\u000bRk\u00001+ (1\u0000\u000b)2\u000bRk\u00002+\n\u0001\u0001\u0001+ (1\u0000\u000b)k\u00001\u000bR1+ (1\u0000\u000b)kQ1\n= (1\u0000\u000b)kQ1+kX\ni=1\u000b(1\u0000\u000b)k\u0000iRi: (2.6)\nWe call this a weighted average because the sum of the weights is (1 \u0000\u000b)k+Pk\ni=1\u000b(1\u0000\u000b)k\u0000i= 1, as you can check yourself. Note that the weight, \u000b(1\u0000\n\u000b)k\u0000i, given to the reward Ridepends on how many rewards ago, k\u0000i, it was\n1The notation ( a;b] as a set denotes the real interval between aandbincludingbbut\nnot including a. Thus, here we are saying that 0 <\u000b\u00141.\n2.5. OPTIMISTIC INITIAL VALUES 39\nobserved. The quantity 1 \u0000\u000bis less than 1, and thus the weight given to Ri\ndecreases as the number of intervening rewards increases. In fact, the weight\ndecays exponentially according to the exponent on 1 \u0000\u000b. (If 1\u0000\u000b= 0, then\nall the weight goes on the very last reward, Rk, because of the convention that\n00= 1.) Accordingly, this is sometimes called an exponential, recency-weighted\naverage .\nSometimes it is convenient to vary the step-size parameter from step to\nstep. Let\u000bk(a) denote the step-size parameter used to process the reward\nreceived after the kth selection of action a. As we have noted, the choice\n\u000bk(a) =1\nkresults in the sample-average method, which is guaranteed to con-\nverge to the true action values by the law of large numbers. But of course\nconvergence is not guaranteed for all choices of the sequence f\u000bk(a)g. A well-\nknown result in stochastic approximation theory gives us the conditions re-\nquired to assure convergence with probability 1:\n1X\nk=1\u000bk(a) =1 and1X\nk=1\u000b2\nk(a)<1: (2.7)\nThe \frst condition is required to guarantee that the steps are large enough to\neventually overcome any initial conditions or random \ructuations. The second\ncondition guarantees that eventually the steps become small enough to assure\nconvergence.\nNote that both convergence conditions are met for the sample-average case,\n\u000bk(a) =1\nk, but not for the case of constant step-size parameter, \u000bk(a) =\u000b. In\nthe latter case, the second condition is not met, indicating that the estimates\nnever completely converge but continue to vary in response to the most re-\ncently received rewards. As we mentioned above, this is actually desirable in\na nonstationary environment, and problems that are e\u000bectively nonstationary\nare the norm in reinforcement learning. In addition, sequences of step-size\nparameters that meet the conditions (2.7) often converge very slowly or need\nconsiderable tuning in order to obtain a satisfactory convergence rate. Al-\nthough sequences of step-size parameters that meet these convergence condi-\ntions are often used in theoretical work, they are seldom used in applications\nand empirical research.\n2.5 Optimistic Initial Values\nAll the methods we have discussed so far are dependent to some extent on\nthe initial action-value estimates, Q1(a). In the language of statistics, these\nmethods are biased by their initial estimates. For the sample-average methods,\n40 CHAPTER 2. MULTI-ARM BANDITS\n0%20%40%60%80%100%%Optimalaction02004006008001000Playsoptimistic, greedyQ0 = 5,    = 0realistic, !-greedyQ0 = 0,    = 0.111\nSteps\ud835\udf00\ud835\udf00\nFigure 2.2: The e\u000bect of optimistic initial action-value estimates on the 10-\narmed testbed. Both methods used a constant step-size parameter, \u000b= 0:1.\nthe bias disappears once all actions have been selected at least once, but for\nmethods with constant \u000b, the bias is permanent, though decreasing over time\nas given by (2.6). In practice, this kind of bias is usually not a problem, and\ncan sometimes be very helpful. The downside is that the initial estimates\nbecome, in e\u000bect, a set of parameters that must be picked by the user, if only\nto set them all to zero. The upside is that they provide an easy way to supply\nsome prior knowledge about what level of rewards can be expected.\nInitial action values can also be used as a simple way of encouraging ex-\nploration. Suppose that instead of setting the initial action values to zero, as\nwe did in the 10-armed testbed, we set them all to +5. Recall that the q(a)\nin this problem are selected from a normal distribution with mean 0 and vari-\nance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism\nencourages action-value methods to explore. Whichever actions are initially\nselected, the reward is less than the starting estimates; the learner switches\nto other actions, being \\disappointed\" with the rewards it is receiving. The\nresult is that all actions are tried several times before the value estimates con-\nverge. The system does a fair amount of exploration even if greedy actions are\nselected all the time.\nFigure 2.2 shows the performance on the 10-armed bandit testbed of a\ngreedy method using Q1(a) = +5, for all a. For comparison, also shown is an\n\"-greedy method with Q1(a) = 0. Initially, the optimistic method performs\nworse because it explores more, but eventually it performs better because its\nexploration decreases with time. We call this technique for encouraging ex-\nploration optimistic initial values . We regard it as a simple trick that can be\nquite e\u000bective on stationary problems, but it is far from being a generally use-\nful approach to encouraging exploration. For example, it is not well suited to\n2.6. UPPER-CONFIDENCE-BOUND ACTION SELECTION 41\nnonstationary problems because its drive for exploration is inherently tempo-\nrary. If the task changes, creating a renewed need for exploration, this method\ncannot help. Indeed, any method that focuses on the initial state in any special\nway is unlikely to help with the general nonstationary case. The beginning\nof time occurs only once, and thus we should not focus on it too much. This\ncriticism applies as well to the sample-average methods, which also treat the\nbeginning of time as a special event, averaging all subsequent rewards with\nequal weights. Nevertheless, all of these methods are very simple, and one of\nthem or some simple combination of them is often adequate in practice. In the\nrest of this book we make frequent use of several of these simple exploration\ntechniques.\n2.6 Upper-Con\fdence-Bound Action Selection\nExploration is needed because the estimates of the action values are uncertain.\nThe greedy actions are those that look best at present, but some of the other\nactions may actually be better. \"-greedy action selection forces the non-greedy\nactions to be tried, but indiscriminately, with no preference for those that are\nnearly greedy or particularly uncertain. It would be better to select among\nthe non-greedy actions according to their potential for actually being optimal,\ntaking into account both how close their estimates are to being maximal and\nthe uncertainties in those estimates. One e\u000bective way of doing this is to select\nactions as\nAt= argmax\na\"\nQt(a) +cs\nlnt\nNt(a)#\n; (2.8)\nwhere lntdenotes the natural logarithm of t(the number that e\u00192:71828\nwould have to be raised to in order to equal t), and the number c>0 controls\nthe degree of exploration. If Nt(a) = 0, then ais considered to be a maximizing\naction.\nThe idea of this upper con\fdence bound (UCB) action selection is that the\nsquare-root term is a measure of the uncertainty or variance in the estimate\nofa's value. The quantity being max'ed over is thus a sort of upper bound\non the possible true value of action a, with thecparameter determining the\ncon\fdence level. Each time ais selected the uncertainty is presumably reduced;\nNt(a) is incremented and, as it appears in the denominator of the uncertainty\nterm, the term is decreased. On the other hand, each time an action other ais\nselectedtis increased; as it appears in the numerator the uncertainty estimate\nis increased. The use of the natural logarithm means that the increase gets\nsmaller over time, but is unbounded; all actions will eventually be selected, but\n42 CHAPTER 2. MULTI-ARM BANDITS\n\ud835\udf00-greedy  \ud835\udf00 = 0.1UCB  c = 2AveragerewardSteps\nFigure 2.3: Average performance of UCB action selection on the 10-armed\ntestbed. As shown, UCB generally performs better that \"-greedy action selec-\ntion, except in the \frst nplays, when it selects randomly among the as-yet-\nunplayed actions. UCB with c= 1 would perform even better but would not\nshow the prominent spike in performance on the 11th play. Can you think of\nan explanation of this spike?\nas time goes by it will be a longer wait, and thus a lower selection frequency,\nfor actions with a lower value estimate or that have already been selected more\ntimes.\nResults with UCB on the 10-armed testbed are shown in Figure 2.3. UCB\nwill often perform well, as shown here, but is more di\u000ecult than \"-greedy\nto extend beyond bandits to the more general reinforcement learning settings\nconsidered in the rest of this book. One di\u000eculty is in dealing with nonsta-\ntionary problems; something more complex than the methods presented in\nSection 2.4 would be needed. Another di\u000eculty is dealing with large state\nspaces, particularly function approximation as developed in Part III of this\nbook. In these more advanced settings there is currently no known practical\nway of utilizing the idea of UCB action selection.\n2.7 Gradient Bandits\nSo far in this chapter we have considered methods that estimate action values\nand use those estimates to select actions. This is often a good approach,\nbut it is not the only one possible. In this section we consider learning a\nnumerical preferenceHt(a) for each action a. The larger the preference, the\n2.7. GRADIENT BANDITS 43\nmore often that action is taken, but the preference has no interpretation in\nterms of reward. Only the relative preference of one action over another is\nimportant; if we add 1000 to all the preferences there is no a\u000bect on the action\nprobabilities, which are determined according to a soft-max distribution (i.e.,\nGibbs or Boltzmann distribution) as follows:\nPrfAt=ag=eHt(a))\nPn\nb=1eHt(b)=\u0019t(a); (2.9)\nwhere here we have also introduced a useful new notation \u0019t(a) for the proba-\nbility of taking action aat timet. Initially all preferences are the same (e.g.,\nH1(a) = 0;8a) so that all actions have an equal probability of being selected.\nThere is a natural learning algorithm for this setting based on the idea\nof stochastic gradient ascent. On each step, after selecting the action Atand\nreceiving the reward Rt, the preferences are updated by:\nHt+1(At) =Ht(At) +\u000b\u0000\nRt\u0000\u0016Rt\u0001\u0000\n1\u0000\u0019t(At)\u0001\n; and\nHt+1(a) =Ht(a)\u0000\u000b\u0000\nRt\u0000\u0016Rt\u0001\n\u0019t(a);8a6=At;(2.10)\nwhere\u000b > 0 is a step-size parameter, and \u0016Rt2Ris the average of all the\nrewards up through and including time t, which can be computed incrementally\nas described in Section 2.3 (or Section 2.4 if the problem is nonstationary).\nThe \u0016Rtterm serves as a baseline with which the reward is compared. If the\nreward is higher than the baseline, then the probability of taking Atin the\nfuture is increased, and if the reward is below baseline, then probability is\ndecreased. The non-selected actions move in the opposite direction.\nFigure 2.4 shows results with the gradient-bandit algorithm on a variant\nof the 10-armed testbed in which the true expected rewards were selected\naccording to a normal distribution with a mean of +4 instead of zero (and with\nunit variance as before). This shifting up of all the rewards has absolutely no\na\u000bect on the gradient-bandit algorithm because of the reward baseline term,\nwhich instantaneously adapts to the new level. But if the baseline were omitted\n(that is, if \u0016Rtwas taken to be constant zero in (2.10)), then performance would\nbe signi\fcantly degraded, as shown in the \fgure.\nOne can gain a deeper insight into this algorithm by understanding it as\na stochastic approximation to gradient ascent. In exact gradient ascent , each\npreferenceHt(a) would be incrementing proportional to the increment's e\u000bect\non performance:\nHt+1(a) =Ht(a) +\u000b@E[Rt]\n@Ht(a); (2.11)\nwhere the measure of performance here is the expected reward:\nE[Rt] =X\nb\u0019t(b)q(b):\n44 CHAPTER 2. MULTI-ARM BANDITS\n%OptimalactionSteps\u03b1 = 0.1100%80%60%40%20%0%\u03b1 = 0.4\u03b1 = 0.1\u03b1 = 0.4without baselinewith baseline\n02505007501000\nFigure 2.4: Average performance of the gradient-bandit algorithm with and\nwithout a reward baseline on the 10-armed testbed with E[q(a)] = 4.\nOf course, it is not possible to implement gradient ascent exactly in our case\nbecause by assumption we do not know the q(b), but in fact the updates of our\nalgorithm (2.10) are equal to (2.11) in expected value, making the algorithm\nan instance of stochastic gradient ascent .\nThe calculations showing this require only beginning calculus, but take\nseveral steps. If you are mathematically inclined, then you will enjoy the rest\nof this section in which we go through these steps. First we take a closer look\nat the exact performance gradient:\n@E[Rt]\n@Ht(a)=@\n@Ht(a)\"X\nb\u0019t(b)q(b)#\n=X\nbq(b)@\u0019t(b)\n@Ht(a)\n=X\nb\u0000\nq(b)\u0000Xt\u0001@\u0019t(b)\n@Ht(a);\nwhereXtcan be any scalar that does not depend on b. We can include it here\nbecause the gradient sums to zero over the all the actions,P\nb@\u0019t(b)\n@Ht(a)= 0. As\nHt(a) is changed, some actions' probabilities go up and some down, but the\nsum of the changes must be zero because the sum of the probabilities must\n2.7. GRADIENT BANDITS 45\nremain one.\n@E[Rt]\n@Ht(a)=X\nb\u0019t(b)\u0000\nq(b)\u0000Xt\u0001@\u0019t(b)\n@Ht(a)=\u0019t(b)\nThe equation is now in the form of an expectation, summing over all possible\nvaluesbof the random variable At, then multiplying by the probability of\ntaking those values. Thus:\n@E[Rt]\n@Ht(a)=E\u0014\u0000\nq(At)\u0000Xt\u0001@\u0019t(At)\n@Ht(a)=\u0019t(At)\u0015\n=E\u0014\u0000\nRt\u0000\u0016Rt\u0001@\u0019t(At)\n@Ht(a)=\u0019t(At)\u0015\n;\nwhere here we have chosen Xt=\u0016Rtand substituted Rtforq(At), which is\npermitted because E[Rt] =q(At) and because all the other factors are non-\nrandom. Shortly we will establish that@\u0019t(b)\n@Ht(a)=\u0019t(b)\u0000\nIa=b\u0000\u0019t(a)\u0001\n, where Ia=b\nis de\fned to be 1 if a=b, else 0. Assuming that for now we have\n@E[Rt]\n@Ht(a)=E\u0002\u0000\nRt\u0000\u0016Rt\u0001\n\u0019t(At)\u0000\nIa=At\u0000\u0019t(a)\u0001\n=\u0019t(At)\u0003\n=E\u0002\u0000\nRt\u0000\u0016Rt\u0001\u0000\nIa=At\u0000\u0019t(a)\u0001\u0003\n:\nRecall that our plan has been to write the performance gradient as an expecta-\ntion of something that we can sample on each step, as we have just done, and\nthen update on each step proportional to the sample. Substituting a sample\nof the expectation above for the performance gradient in (2.11) yields:\nHt+1(a) =Ht(a) +\u000b\u0000\nRt\u0000\u0016Rt\u0001\u0000\nIa=At\u0000\u0019t(a)\u0001\n;8a;\nwhich you will recognize as being equivalent to our original algorithm (2.10).\nThus it remains only to show that@\u0019t(b)\n@Ht(a)=\u0019t(b)\u0000\nIa=b\u0000\u0019t(a)\u0001\n, as we\nassumed earlier. Recall the standard quotient rule for derivatives:\n@\n@x\u0014f(x)\ng(x)\u0015\n=@f(x)\n@xg(x)\u0000f(x)@g(x)\n@x\ng(x)2:\n46 CHAPTER 2. MULTI-ARM BANDITS\nUsing this, we can write\n@\u0019t(b)\n@Ht(a)=@\n@Ht(a)\u0019t(b)\n=@\n@Ht(a)\u0014eHt(b)\nPn\nc=1eHt(c)\u0015\n=@eHt(b)\n@Ht(a)Pn\nc=1eHt(c)\u0000eHt(b)@Pn\nc=1eHt(c)\n@Ht(a)\n(Pn\nc=1eHt(c))2(by the quotient rule)\n=Ia=beHt(a)Pn\nc=1eHt(c)\u0000eHt(b)eHt(a)\n(Pn\nc=1eHt(c))2(because@ex\n@x=ex)\n=Ia=beHt(b)\nPn\nc=1eHt(c)\u0000eHt(b)eHt(a)\n(Pn\nc=1eHt(c))2\n= Ia=b\u0019t(b)\u0000\u0019t(b)\u0019t(a)\n=\u0019t(b)\u0000\nIa=b\u0000\u0019t(a)\u0001\n: Q.E.D.\nWe have just shown that the expected update of the gradient-bandit algo-\nrithm is equal to the gradient of expected reward, and thus that the algorithm\nis an instance of stochastic gradient ascent. This assures us that the algorithm\nhas robust convergence properties.\nNote that we did not require anything of the reward baseline other than\nthat it not depend on the selected action. For example, we could have set\nis to zero, or to 1000, and the algorithm would still have been an instance\nof stochastic gradient ascent. The choice of the baseline does not a\u000bect the\nexpected update of the algorithm, but it does a\u000bect the variance of the update\nand thus the rate of convergence (as shown, e.g., in Figure 2.4). Choosing it\nas the average of the rewards may not be the very best, but it is simple and\nworks well in practice.\n2.8 Associative Search (Contextual Bandits)\nSo far in this chapter we have considered only nonassociative tasks, in which\nthere is no need to associate di\u000berent actions with di\u000berent situations. In\nthese tasks the learner either tries to \fnd a single best action when the task is\nstationary, or tries to track the best action as it changes over time when the\ntask is nonstationary. However, in a general reinforcement learning task there\nis more than one situation, and the goal is to learn a policy: a mapping from\nsituations to the actions that are best in those situations. To set the stage for\n2.9. SUMMARY 47\nthe full problem, we brie\ry discuss the simplest way in which nonassociative\ntasks extend to the associative setting.\nAs an example, suppose there are several di\u000berent n-armed bandit tasks,\nand that on each play you confront one of these chosen at random. Thus, the\nbandit task changes randomly from play to play. This would appear to you as\na single, nonstationary n-armed bandit task whose true action values change\nrandomly from play to play. You could try using one of the methods described\nin this chapter that can handle nonstationarity, but unless the true action\nvalues change slowly, these methods will not work very well. Now suppose,\nhowever, that when a bandit task is selected for you, you are given some\ndistinctive clue about its identity (but not its action values). Maybe you are\nfacing an actual slot machine that changes the color of its display as it changes\nits action values. Now you can learn a policy associating each task, signaled\nby the color you see, with the best action to take when facing that task|for\ninstance, if red, play arm 1; if green, play arm 2. With the right policy you\ncan usually do much better than you could in the absence of any information\ndistinguishing one bandit task from another.\nThis is an example of an associative search task, so called because it in-\nvolves both trial-and-error learning in the form of search for the best actions\nand association of these actions with the situations in which they are best.2\nAssociative search tasks are intermediate between the n-armed bandit problem\nand the full reinforcement learning problem. They are like the full reinforce-\nment learning problem in that they involve learning a policy, but like our\nversion of the n-armed bandit problem in that each action a\u000bects only the\nimmediate reward. If actions are allowed to a\u000bect the next situation as well as\nthe reward, then we have the full reinforcement learning problem. We present\nthis problem in the next chapter and consider its rami\fcations throughout the\nrest of the book.\n2.9 Summary\nWe have presented in this chapter several simple ways of balancing exploration\nand exploitation. The \"-greedy methods choose randomly a small fraction of\nthe time, whereas UCB methods choose deterministically but achieve explo-\nration by subtly favoring at each step the actions that have so far received fewer\nsamples. Gradient-bandit algorithms estimate not action values, but action\npreferences, and favor the more preferred actions in a graded, probabalistic\nmanner using a soft-max distribution. The simple expedient of initializing\n2Associative search tasks are often now termed contextual bandits in the literature.\n48 CHAPTER 2. MULTI-ARM BANDITS\nAveragerewardover \ufb01rst 1000 steps1.51.41.31.21.11\ud835\udf00-greedyUCBgradientbanditgreedy withoptimisticinitialization\u03b1 = 0.1\u21b5/c/Q01241/21/41/81/161/321/641/128\nFigure 2.5: A parameter study of the various bandit algorithms presented in\nthis chapter. Each point is the average reward obtained over 1000 steps with\na particular algorithm at a particular setting of its parameter.\nestimates optimistically causes even greedy methods to explore signi\fcantly.\nIt is natural to ask which of these methods is best. Although this is a\ndi\u000ecult question to answer in general, we can certainly run them all on the\n10-armed testbed that we have used throughout this chapter and compare\ntheir performances. A complication is that they all have a parameter; to\nget a meaningful comparison we will have to consider their performance as\na function of their parameter. Our graphs so far have shown the course of\nlearning over time for each algorithm and parameter setting, but it would\nbe too visually confusing to show such a learning curve for each algorithm\nand parameter value. Instead we summarize a complete learning curve by its\naverage value over the 1000 steps; this value is proportional to the area under\nthe learning curves we have shown up to now. Figure 2.5 shows this measure\nfor the various bandit algorithms from this chapter, each as a function of its\nown parameter shown on a single scale on the x-axis. Note that the parameter\nvalues are varied by factors of two and presented on a log scale. Note also\nthe characteristic inverted-U shapes of each algorithm's performance; all the\nalgorithms perform best at an intermediate value of their parameter, neither\ntoo large nor too big. In assessing an method, we should attend not just to\nhow well it does at its best parameter setting, but also to how sensitive it is to\nits parameter value. All of these algorithms are fairly insensitive, performing\nwell over a range of parameter values varying by about an order of magnitude.\nOverall, on this problem, UCB seems to perform best.\n2.9. SUMMARY 49\nDespite their simplicity, in our opinion the methods presented in this chap-\nter can fairly be considered the state of the art. There are more sophisticated\nmethods, but their complexity and assumptions make them impractical for the\nfull reinforcement learning problem that is our real focus. Starting in Chap-\nter 5 we present learning methods for solving the full reinforcement learning\nproblem that use in part the simple methods explored in this chapter.\nAlthough the simple methods explored in this chapter may be the best we\ncan do at present, they are far from a fully satisfactory solution to the problem\nof balancing exploration and exploitation.\nThe classical solution to balancing exploration and exploitation in n-armed\nbandit problems is to compute special functions called Gittins indices . These\nprovide an optimal solution to a certain kind of bandit problem more general\nthan that considered here but that assumes the prior distribution of possible\nproblems is known. Unfortunately, neither the theory nor the computational\ntractability of this method appear to generalize to the full reinforcement learn-\ning problem that we consider in the rest of the book.\nThere is also a well-known algorithm for computing the Bayes optimal way\nto balance exploration and exploitation. This method is computationally in-\ntractable when done exactly, but there may be e\u000ecient ways to approximate it.\nIn this method we assume that we know the distribution of problem instances,\nthat is, the probability of each possible set of true action values. Given any\naction selection, we can then compute the probability of each possible imme-\ndiate reward and the resultant posterior probability distribution over action\nvalues. This evolving distribution becomes the information state of the prob-\nlem. Given a horizon, say 1000 plays, one can consider all possible actions, all\npossible resulting rewards, all possible next actions, all next rewards, and so\non for all 1000 plays. Given the assumptions, the rewards and probabilities\nof each possible chain of events can be determined, and one need only pick\nthe best. But the tree of possibilities grows extremely rapidly; even if there\nare only two actions and two rewards, the tree will have 22000leaves. This\napproach e\u000bectively turns the bandit problem into an instance of the full rein-\nforcement learning problem. In the end, we may be able to use reinforcement\nlearning methods to approximate this optimal solution. But that is a topic for\ncurrent research and beyond the scope of this book.\nBibliographical and Historical Remarks\n2.1 Bandit problems have been studied in statistics, engineering, and psy-\nchology. In statistics, bandit problems fall under the heading \\sequen-\ntial design of experiments,\" introduced by Thompson (1933, 1934) and\n50 CHAPTER 2. MULTI-ARM BANDITS\nRobbins (1952), and studied by Bellman (1956). Berry and Fristedt\n(1985) provide an extensive treatment of bandit problems from the\nperspective of statistics. Narendra and Thathachar (1989) treat bandit\nproblems from the engineering perspective, providing a good discussion\nof the various theoretical traditions that have focused on them. In psy-\nchology, bandit problems have played roles in statistical learning theory\n(e.g., Bush and Mosteller, 1955; Estes, 1950).\nThe term greedy is often used in the heuristic search literature (e.g.,\nPearl, 1984). The con\rict between exploration and exploitation is\nknown in control engineering as the con\rict between identi\fcation (or\nestimation) and control (e.g., Witten, 1976). Feldbaum (1965) called it\nthedual control problem, referring to the need to solve the two prob-\nlems of identi\fcation and control simultaneously when trying to control\na system under uncertainty. In discussing aspects of genetic algorithms,\nHolland (1975) emphasized the importance of this con\rict, referring to\nit as the con\rict between the need to exploit and the need for new\ninformation.\n2.2 Action-value methods for our n-armed bandit problem were \frst pro-\nposed by Thathachar and Sastry (1985). These are often called esti-\nmator algorithms in the learning automata literature. The term action\nvalue is due to Watkins (1989). The \frst to use \"-greedy methods may\nalso have been Watkins (1989, p. 187), but the idea is so simple that\nsome earlier use seems likely.\n2.3{4 This material falls under the general heading of stochastic iterative\nalgorithms, which is well covered by Bertsekas and Tsitsiklis (1996).\n2.5 Optimistic initialization was used in reinforcement learning by Sutton\n(1996).\n2.6 Early work on using estimates of the upper con\fdence bound to select\nactions was done by Lai and Robbins (1985), Kaelbling (1993b), and\nAgarwal (1995). The UCB algorithm we present here is called UCB1\nin the literature and was \frst developed by Auer, Cesa-Bianchi and\nFischer (2002).\n2.7 Gradient-bandit algorithms are a special case of the gradient-based\nreinforcement learning algorithms introduced by Williams (1992), and\nthat later developed into the actor{critic and policy-gradient algorithms\nthat we treat later in this book. Further discussion of the choice of\n2.9. SUMMARY 51\nbaseline is provided there and by Greensmith, Bartlett, and Baxter\n(2001, 2004) and Dick (2015).\nThe term softmax for the action selection rule (2.9) is due to Bridle\n(1990). This rule appears to have been \frst proposed by Luce (1959).\n2.8 The term associative search and the corresponding problem were in-\ntroduced by Barto, Sutton, and Brouwer (1981). The term associative\nreinforcement learning has also been used for associative search (Barto\nand Anandan, 1985), but we prefer to reserve that term as a synonym\nfor the full reinforcement learning problem (as in Sutton, 1984). (And,\nas we noted, the modern literature also uses the term \\contextual ban-\ndits\" for this problem.) We note that Thorndike's Law of E\u000bect (quoted\nin Chapter 1) describes associative search by referring to the formation\nof associative links between situations (states) and actions. Accord-\ning to the terminology of operant, or instrumental, conditioning (e.g.,\nSkinner, 1938), a discriminative stimulus is a stimulus that signals the\npresence of a particular reinforcement contingency. In our terms, dif-\nferent discriminative stimuli correspond to di\u000berent states.\n2.9 The Gittins index approach is due to Gittins and Jones (1974). Du\u000b\n(1995) showed how it is possible to learn Gittins indices for bandit\nproblems through reinforcement learning. Bellman (1956) was the \frst\nto show how dynamic programming could be used to compute the op-\ntimal balance between exploration and exploitation within a Bayesian\nformulation of the problem. The survey by Kumar (1985) provides\na good discussion of Bayesian and non-Bayesian approaches to these\nproblems. The term information state comes from the literature on\npartially observable MDPs; see, e.g., Lovejoy (1991).\nExercises\nExercise 2.1 In the comparison shown in Figure 2.1, which method will\nperform best in the long run in terms of cumulative reward and cumulative\nprobability of selecting the best action? How much better will it be? Express\nyour answer quantitatively.\nExercise 2.2 Give pseudocode for a complete algorithm for the n-armed\nbandit problem. Use greedy action selection and incremental computation of\naction values with \u000b=1\nkstep-size parameter. Assume a function bandit (a)\nthat takes an action and returns a reward. Use arrays and variables; do not\n52 CHAPTER 2. MULTI-ARM BANDITS\nsubscript anything by the time index t(for examples of this style of pseu-\ndocode, see Figures 4.1 and 4.3). Indicate how the action values are initialized\nand updated after each reward. Indicate how the step-size parameters are set\nfor each action as a function of how many times it has been tried.\nExercise 2.3 If the step-size parameters, \u000bk, are not constant, then the esti-\nmateQkis a weighted average of previously received rewards with a weighting\ndi\u000berent from that given by (2.6). What is the weighting on each prior reward\nfor the general case, analogous to (2.6), in terms of \u000bk?\nExercise 2.4 (programming) Design and conduct an experiment to demon-\nstrate the di\u000eculties that sample-average methods have for nonstationary\nproblems. Use a modi\fed version of the 10-armed testbed in which all the\nq(a) start out equal and then take independent random walks. Prepare plots\nlike Figure 2.1 for an action-value method using sample averages, incremen-\ntally computed by \u000b=1\nk, and another action-value method using a constant\nstep-size parameter, \u000b= 0:1. Use\"= 0:1 and, if necessary, runs longer than\n1000 plays.\nExercise 2.5 The results shown in Figure 2.2 should be quite reliable be-\ncause they are averages over 2000 individual, randomly chosen 10-armed ban-\ndit tasks. Why, then, are there oscillations and spikes in the early part of\nthe curve for the optimistic method? What might make this method perform\nparticularly better or worse, on average, on particular early plays?\nExercise 2.6 Suppose you face a binary bandit task whose true action values\nchange randomly from play to play. Speci\fcally, suppose that for any play the\ntrue values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5\n(case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to\ntell which case you face at any play, what is the best expectation of success\nyou can achieve and how should you behave to achieve it? Now suppose that\non each play you are told if you are facing case A or case B (although you still\ndon't know the true action values). This is an associative search task. What\nis the best expectation of success you can achieve in this task, and how should\nyou behave to achieve it?\nChapter 3\nFinite Markov Decision\nProcesses\nIn this chapter we introduce the problem that we try to solve in the rest of\nthe book. For us, this problem de\fnes the \feld of reinforcement learning: any\nmethod that is suited to solving this problem we consider to be a reinforcement\nlearning method.\nOur objective in this chapter is to describe the reinforcement learning prob-\nlem in a broad sense. We try to convey the wide range of possible applications\nthat can be framed as reinforcement learning tasks. We also describe math-\nematically idealized forms of the reinforcement learning problem for which\nprecise theoretical statements can be made. We introduce key elements of the\nproblem's mathematical structure, such as value functions and Bellman equa-\ntions. As in all of arti\fcial intelligence, there is a tension between breadth of\napplicability and mathematical tractability. In this chapter we introduce this\ntension and discuss some of the trade-o\u000bs and challenges that it implies.\n3.1 The Agent{Environment Interface\nThe reinforcement learning problem is meant to be a straightforward framing\nof the problem of learning from interaction to achieve a goal. The learner and\ndecision-maker is called the agent . The thing it interacts with, comprising\neverything outside the agent, is called the environment . These interact con-\ntinually, the agent selecting actions and the environment responding to those\nactions and presenting new situations to the agent.1The environment also\n1We use the terms agent ,environment , and action instead of the engineers' terms con-\ntroller ,controlled system (orplant ), and control signal because they are meaningful to a\n53\n54 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nAgentEnvironmentactionAtrewardRtstateStRt+1St+1\nFigure 3.1: The agent{environment interaction in reinforcement learning.\ngives rise to rewards, special numerical values that the agent tries to maximize\nover time. A complete speci\fcation of an environment de\fnes a task, one\ninstance of the reinforcement learning problem.\nMore speci\fcally, the agent and environment interact at each of a sequence\nof discrete time steps, t= 0;1;2;3;:::.2At each time step t, the agent receives\nsome representation of the environment's state,St2S, where Sis the set of\npossible states, and on that basis selects an action ,At2A(St), where A(St)\nis the set of actions available in state St. One time step later, in part as\na consequence of its action, the agent receives a numerical reward ,Rt+12\nR\u001aR, and \fnds itself in a new state, St+1.3Figure 3.1 diagrams the agent{\nenvironment interaction.\nAt each time step, the agent implements a mapping from states to prob-\nabilities of selecting each possible action. This mapping is called the agent's\npolicy and is denoted \u0019t, where\u0019t(ajs) is the probability that At=aifSt=s.\nReinforcement learning methods specify how the agent changes its policy as\na result of its experience. The agent's goal, roughly speaking, is to maximize\nthe total amount of reward it receives over the long run.\nThis framework is abstract and \rexible and can be applied to many di\u000berent\nproblems in many di\u000berent ways. For example, the time steps need not refer\nto \fxed intervals of real time; they can refer to arbitrary successive stages of\ndecision-making and acting. The actions can be low-level controls, such as the\nvoltages applied to the motors of a robot arm, or high-level decisions, such\nas whether or not to have lunch or to go to graduate school. Similarly, the\nstates can take a wide variety of forms. They can be completely determined\nby low-level sensations, such as direct sensor readings, or they can be more\nwider audience.\n2We restrict attention to discrete time to keep things as simple as possible, even though\nmany of the ideas can be extended to the continuous-time case (e.g., see Bertsekas and\nTsitsiklis, 1996; Werbos, 1992; Doya, 1996).\n3We useRt+1instead ofRtto denote the reward due to Atbecause it emphasizes that\nthe next reward and next state, Rt+1andSt+1, are jointly determined.\n3.1. THE AGENT{ENVIRONMENT INTERFACE 55\nhigh-level and abstract, such as symbolic descriptions of objects in a room.\nSome of what makes up a state could be based on memory of past sensations\nor even be entirely mental or subjective. For example, an agent could be in\nthe state of not being sure where an object is, or of having just been surprised\nin some clearly de\fned sense. Similarly, some actions might be totally mental\nor computational. For example, some actions might control what an agent\nchooses to think about, or where it focuses its attention. In general, actions\ncan be any decisions we want to learn how to make, and the states can be\nanything we can know that might be useful in making them.\nIn particular, the boundary between agent and environment is not often\nthe same as the physical boundary of a robot's or animal's body. Usually, the\nboundary is drawn closer to the agent than that. For example, the motors\nand mechanical linkages of a robot and its sensing hardware should usually be\nconsidered parts of the environment rather than parts of the agent. Similarly,\nif we apply the framework to a person or animal, the muscles, skeleton, and\nsensory organs should be considered part of the environment. Rewards, too,\npresumably are computed inside the physical bodies of natural and arti\fcial\nlearning systems, but are considered external to the agent.\nThe general rule we follow is that anything that cannot be changed ar-\nbitrarily by the agent is considered to be outside of it and thus part of its\nenvironment. We do not assume that everything in the environment is un-\nknown to the agent. For example, the agent often knows quite a bit about\nhow its rewards are computed as a function of its actions and the states in\nwhich they are taken. But we always consider the reward computation to be\nexternal to the agent because it de\fnes the task facing the agent and thus\nmust be beyond its ability to change arbitrarily. In fact, in some cases the\nagent may know everything about how its environment works and still face\na di\u000ecult reinforcement learning task, just as we may know exactly how a\npuzzle like Rubik's cube works, but still be unable to solve it. The agent{\nenvironment boundary represents the limit of the agent's absolute control , not\nof its knowledge.\nThe agent{environment boundary can be located at di\u000berent places for\ndi\u000berent purposes. In a complicated robot, many di\u000berent agents may be op-\nerating at once, each with its own boundary. For example, one agent may make\nhigh-level decisions which form part of the states faced by a lower-level agent\nthat implements the high-level decisions. In practice, the agent{environment\nboundary is determined once one has selected particular states, actions, and\nrewards, and thus has identi\fed a speci\fc decision-making task of interest.\nThe reinforcement learning framework is a considerable abstraction of the\nproblem of goal-directed learning from interaction. It proposes that whatever\n56 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nthe details of the sensory, memory, and control apparatus, and whatever ob-\njective one is trying to achieve, any problem of learning goal-directed behavior\ncan be reduced to three signals passing back and forth between an agent and\nits environment: one signal to represent the choices made by the agent (the\nactions), one signal to represent the basis on which the choices are made (the\nstates), and one signal to de\fne the agent's goal (the rewards). This frame-\nwork may not be su\u000ecient to represent all decision-learning problems usefully,\nbut it has proved to be widely useful and applicable.\nOf course, the particular states and actions vary greatly from task to task,\nand how they are represented can strongly a\u000bect performance. In reinforce-\nment learning, as in other kinds of learning, such representational choices are at\npresent more art than science. In this book we o\u000ber some advice and examples\nregarding good ways of representing states and actions, but our primary focus\nis on general principles for learning how to behave once the representations\nhave been selected.\nExample 3.1: Bioreactor Suppose reinforcement learning is being applied\nto determine moment-by-moment temperatures and stirring rates for a biore-\nactor (a large vat of nutrients and bacteria used to produce useful chemicals).\nThe actions in such an application might be target temperatures and target\nstirring rates that are passed to lower-level control systems that, in turn, di-\nrectly activate heating elements and motors to attain the targets. The states\nare likely to be thermocouple and other sensory readings, perhaps \fltered and\ndelayed, plus symbolic inputs representing the ingredients in the vat and the\ntarget chemical. The rewards might be moment-by-moment measures of the\nrate at which the useful chemical is produced by the bioreactor. Notice that\nhere each state is a list, or vector, of sensor readings and symbolic inputs,\nand each action is a vector consisting of a target temperature and a stirring\nrate. It is typical of reinforcement learning tasks to have states and actions\nwith such structured representations. Rewards, on the other hand, are always\nsingle numbers.\nExample 3.2: Pick-and-Place Robot Consider using reinforcement learn-\ning to control the motion of a robot arm in a repetitive pick-and-place task. If\nwe want to learn movements that are fast and smooth, the learning agent will\nhave to control the motors directly and have low-latency information about\nthe current positions and velocities of the mechanical linkages. The actions\nin this case might be the voltages applied to each motor at each joint, and\nthe states might be the latest readings of joint angles and velocities. The\nreward might be +1 for each object successfully picked up and placed. To\nencourage smooth movements, on each time step a small, negative reward can\nbe given as a function of the moment-to-moment \\jerkiness\" of the motion.\n3.2. GOALS AND REWARDS 57\nExample 3.3: Recycling Robot A mobile robot has the job of collecting\nempty soda cans in an o\u000ece environment. It has sensors for detecting cans, and\nan arm and gripper that can pick them up and place them in an onboard bin; it\nruns on a rechargeable battery. The robot's control system has components for\ninterpreting sensory information, for navigating, and for controlling the arm\nand gripper. High-level decisions about how to search for cans are made by a\nreinforcement learning agent based on the current charge level of the battery.\nThis agent has to decide whether the robot should (1) actively search for a\ncan for a certain period of time, (2) remain stationary and wait for someone\nto bring it a can, or (3) head back to its home base to recharge its battery.\nThis decision has to be made either periodically or whenever certain events\noccur, such as \fnding an empty can. The agent therefore has three actions,\nand its state is determined by the state of the battery. The rewards might be\nzero most of the time, but then become positive when the robot secures an\nempty can, or large and negative if the battery runs all the way down. In this\nexample, the reinforcement learning agent is not the entire robot. The states\nit monitors describe conditions within the robot itself, not conditions of the\nrobot's external environment. The agent's environment therefore includes the\nrest of the robot, which might contain other complex decision-making systems,\nas well as the robot's external environment.\n3.2 Goals and Rewards\nIn reinforcement learning, the purpose or goal of the agent is formalized in\nterms of a special reward signal passing from the environment to the agent.\nAt each time step, the reward is a simple number, Rt2R. Informally, the\nagent's goal is to maximize the total amount of reward it receives. This means\nmaximizing not immediate reward, but cumulative reward in the long run. We\ncan clearly state this informal idea as the reward hypothesis :\nThat all of what we mean by goals and purposes can be well\nthought of as the maximization of the expected value of the cu-\nmulative sum of a received scalar signal (called reward).\nThe use of a reward signal to formalize the idea of a goal is one of the most\ndistinctive features of reinforcement learning.\nAlthough formulating goals in terms of reward signals might at \frst appear\nlimiting, in practice it has proved to be \rexible and widely applicable. The best\nway to see this is to consider examples of how it has been, or could be, used.\nFor example, to make a robot learn to walk, researchers have provided reward\n58 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\non each time step proportional to the robot's forward motion. In making a\nrobot learn how to escape from a maze, the reward is often \u00001 for every time\nstep that passes prior to escape; this encourages the agent to escape as quickly\nas possible. To make a robot learn to \fnd and collect empty soda cans for\nrecycling, one might give it a reward of zero most of the time, and then a\nreward of +1 for each can collected. One might also want to give the robot\nnegative rewards when it bumps into things or when somebody yells at it. For\nan agent to learn to play checkers or chess, the natural rewards are +1 for\nwinning,\u00001 for losing, and 0 for drawing and for all nonterminal positions.\nYou can see what is happening in all of these examples. The agent always\nlearns to maximize its reward. If we want it to do something for us, we must\nprovide rewards to it in such a way that in maximizing them the agent will\nalso achieve our goals. It is thus critical that the rewards we set up truly\nindicate what we want accomplished. In particular, the reward signal is not\nthe place to impart to the agent prior knowledge about howto achieve what we\nwant it to do.4For example, a chess-playing agent should be rewarded only\nfor actually winning, not for achieving subgoals such taking its opponent's\npieces or gaining control of the center of the board. If achieving these sorts\nof subgoals were rewarded, then the agent might \fnd a way to achieve them\nwithout achieving the real goal. For example, it might \fnd a way to take the\nopponent's pieces even at the cost of losing the game. The reward signal is\nyour way of communicating to the robot what you want it to achieve, not how\nyou want it achieved.\nNewcomers to reinforcement learning are sometimes surprised that the\nrewards|which de\fne of the goal of learning|are computed in the environ-\nment rather than in the agent. Certainly most ultimate goals for animals\nare recognized by computations occurring inside their bodies, for example, by\nsensors for recognizing food, hunger, pain, and pleasure. Nevertheless, as we\ndiscussed in the previous section, one can redraw the agent{environment in-\nterface in such a way that these parts of the body are considered to be outside\nof the agent (and thus part of the agent's environment). For example, if the\ngoal concerns a robot's internal energy reservoirs, then these are considered to\nbe part of the environment; if the goal concerns the positions of the robot's\nlimbs, then these too are considered to be part of the environment|that is,\nthe agent's boundary is drawn at the interface between the limbs and their\ncontrol systems. These things are considered internal to the robot but external\nto the learning agent. For our purposes, it is convenient to place the boundary\nof the learning agent not at the limit of its physical body, but at the limit of\n4Better places for imparting this kind of prior knowledge are the initial policy or value\nfunction, or in in\ruences on these. See Lin (1992), Maclin and Shavlik (1994), and Clouse\n(1996).\n3.3. RETURNS 59\nits control.\nThe reason we do this is that the agent's ultimate goal should be something\nover which it has imperfect control: it should not be able, for example, to\nsimply decree that the reward has been received in the same way that it might\narbitrarily change its actions. Therefore, we place the reward source outside\nof the agent. This does not preclude the agent from de\fning for itself a kind\nof internal reward, or a sequence of internal rewards. Indeed, this is exactly\nwhat many reinforcement learning methods do.\n3.3 Returns\nSo far we have discussed the objective of learning informally. We have said that\nthe agent's goal is to maximize the cumulative reward it receives in the long\nrun. How might this be de\fned formally? If the sequence of rewards received\nafter time step tis denotedRt+1;Rt+2;Rt+3;:::, then what precise aspect of\nthis sequence do we wish to maximize? In general, we seek to maximize the\nexpected return , where the return Gtis de\fned as some speci\fc function of the\nreward sequence. In the simplest case the return is the sum of the rewards:\nGt=Rt+1+Rt+2+Rt+3+\u0001\u0001\u0001+RT; (3.1)\nwhereTis a \fnal time step. This approach makes sense in applications in\nwhich there is a natural notion of \fnal time step, that is, when the agent{\nenvironment interaction breaks naturally into subsequences, which we call\nepisodes ,5such as plays of a game, trips through a maze, or any sort of re-\npeated interactions. Each episode ends in a special state called the terminal\nstate, followed by a reset to a standard starting state or to a sample from a\nstandard distribution of starting states. Tasks with episodes of this kind are\ncalled episodic tasks . In episodic tasks we sometimes need to distinguish the\nset of all nonterminal states, denoted S, from the set of all states plus the\nterminal state, denoted S+.\nOn the other hand, in many cases the agent{environment interaction does\nnot break naturally into identi\fable episodes, but goes on continually without\nlimit. For example, this would be the natural way to formulate a continual\nprocess-control task, or an application to a robot with a long life span. We\ncall these continuing tasks . The return formulation (3.1) is problematic for\ncontinuing tasks because the \fnal time step would be T=1, and the return,\nwhich is what we are trying to maximize, could itself easily be in\fnite. (For\nexample, suppose the agent receives a reward of +1 at each time step.) Thus,\n5Episodes are sometimes called \\trials\" in the literature.\n60 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nFigure 3.2: The pole-balancing task.\nin this book we usually use a de\fnition of return that is slightly more complex\nconceptually but much simpler mathematically.\nThe additional concept that we need is that of discounting . According to\nthis approach, the agent tries to select actions so that the sum of the discounted\nrewards it receives over the future is maximized. In particular, it chooses At\nto maximize the expected discounted return :\nGt=Rt+1+\rRt+2+\r2Rt+3+\u0001\u0001\u0001=1X\nk=0\rkRt+k+1; (3.2)\nwhere\ris a parameter, 0 \u0014\r\u00141, called the discount rate .\nThe discount rate determines the present value of future rewards: a reward\nreceivedktime steps in the future is worth only \rk\u00001times what it would be\nworth if it were received immediately. If \r <1, the in\fnite sum has a \fnite\nvalue as long as the reward sequence fRkgis bounded. If \r= 0, the agent\nis \\myopic\" in being concerned only with maximizing immediate rewards: its\nobjective in this case is to learn how to choose Atso as to maximize only\nRt+1. If each of the agent's actions happened to in\ruence only the immediate\nreward, not future rewards as well, then a myopic agent could maximize (3.2)\nby separately maximizing each immediate reward. But in general, acting to\nmaximize immediate reward can reduce access to future rewards so that the\nreturn may actually be reduced. As \rapproaches 1, the objective takes future\nrewards into account more strongly: the agent becomes more farsighted.\nExample 3.4: Pole-Balancing Figure 3.2 shows a task that served as an\nearly illustration of reinforcement learning. The objective here is to apply\nforces to a cart moving along a track so as to keep a pole hinged to the cart\nfrom falling over. A failure is said to occur if the pole falls past a given angle\nfrom vertical or if the cart runs o\u000b the track. The pole is reset to vertical\nafter each failure. This task could be treated as episodic, where the natural\nepisodes are the repeated attempts to balance the pole. The reward in this\n3.4. UNIFIED NOTATION FOR EPISODIC AND CONTINUING TASKS 61\ncase could be +1 for every time step on which failure did not occur, so that the\nreturn at each time would be the number of steps until failure. Alternatively,\nwe could treat pole-balancing as a continuing task, using discounting. In this\ncase the reward would be \u00001 on each failure and zero at all other times. The\nreturn at each time would then be related to \u0000\rK, whereKis the number of\ntime steps before failure. In either case, the return is maximized by keeping\nthe pole balanced for as long as possible.\n3.4 Uni\fed Notation for Episodic and Contin-\nuing Tasks\nIn the preceding section we described two kinds of reinforcement learning tasks,\none in which the agent{environment interaction naturally breaks down into a\nsequence of separate episodes (episodic tasks), and one in which it does not\n(continuing tasks). The former case is mathematically easier because each\naction a\u000bects only the \fnite number of rewards subsequently received during\nthe episode. In this book we consider sometimes one kind of problem and\nsometimes the other, but often both. It is therefore useful to establish one\nnotation that enables us to talk precisely about both cases simultaneously.\nTo be precise about episodic tasks requires some additional notation. Rather\nthan one long sequence of time steps, we need to consider a series of episodes,\neach of which consists of a \fnite sequence of time steps. We number the time\nsteps of each episode starting anew from zero. Therefore, we have to refer not\njust toSt, the state representation at time t, but toSt;i, the state representa-\ntion at time tof episodei(and similarly for At;i,Rt;i,\u0019t;i,Ti, etc.). However,\nit turns out that, when we discuss episodic tasks we will almost never have to\ndistinguish between di\u000berent episodes. We will almost always be considering\na particular single episode, or stating something that is true for all episodes.\nAccordingly, in practice we will almost always abuse notation slightly by drop-\nping the explicit reference to episode number. That is, we will write Stto refer\ntoSt;i, and so on.\nWe need one other convention to obtain a single notation that covers both\nepisodic and continuing tasks. We have de\fned the return as a sum over a \fnite\nnumber of terms in one case (3.1) and as a sum over an in\fnite number of terms\nin the other (3.2). These can be uni\fed by considering episode termination to\nbe the entering of a special absorbing state that transitions only to itself and\nthat generates only rewards of zero. For example, consider the state transition\n62 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\ndiagram\nR1 = +1S0S1R2 = +1S2R3 = +1R4 = 0R5 = 0. . .\nHere the solid square represents the special absorbing state corresponding\nto the end of an episode. Starting from S0, we get the reward sequence\n+1;+1;+1;0;0;0;:::. Summing these, we get the same return whether we\nsum over the \frst Trewards (here T= 3) or over the full in\fnite sequence.\nThis remains true even if we introduce discounting. Thus, we can de\fne the\nreturn, in general, according to (3.2), using the convention of omitting episode\nnumbers when they are not needed, and including the possibility that \r= 1 if\nthe sum remains de\fned (e.g., because all episodes terminate). Alternatively,\nwe can also write the return as\nGt=T\u0000t\u00001X\nk=0\rkRt+k+1; (3.3)\nincluding the possibility that T=1or\r= 1 (but not both6). We use these\nconventions throughout the rest of the book to simplify notation and to express\nthe close parallels between episodic and continuing tasks.\n\u00033.5 The Markov Property\nIn the reinforcement learning framework, the agent makes its decisions as a\nfunction of a signal from the environment called the environment's state. In\nthis section we discuss what is required of the state signal, and what kind of\ninformation we should and should not expect it to provide. In particular, we\nformally de\fne a property of environments and their state signals that is of\nparticular interest, called the Markov property.\nIn this book, by \\the state\" we mean whatever information is available to\nthe agent. We assume that the state is given by some preprocessing system\nthat is nominally part of the environment. We do not address the issues of\nconstructing, changing, or learning the state signal in this book. We take this\napproach not because we consider state representation to be unimportant, but\n6Ways to formulate tasks that are both continuing and undiscounted are the subject of\ncurrent research (e.g., Mahadevan, 1996; Schwartz, 1993; Tadepalli and Ok, 1994). Some of\nthe ideas are discussed in Section 11.2.\n\u00033.5. THE MARKOV PROPERTY 63\nin order to focus fully on the decision-making issues. In other words, our main\nconcern is not with designing the state signal, but with deciding what action\nto take as a function of whatever state signal is available.\nCertainly the state signal should include immediate sensations such as sen-\nsory measurements, but it can contain much more than that. State represen-\ntations can be highly processed versions of original sensations, or they can be\ncomplex structures built up over time from the sequence of sensations. For ex-\nample, we can move our eyes over a scene, with only a tiny spot corresponding\nto the fovea visible in detail at any one time, yet build up a rich and detailed\nrepresentation of a scene. Or, more obviously, we can look at an object, then\nlook away, and know that it is still there. We can hear the word \\yes\" and\nconsider ourselves to be in totally di\u000berent states depending on the question\nthat came before and which is no longer audible. At a more mundane level, a\ncontrol system can measure position at two di\u000berent times to produce a state\nrepresentation including information about velocity. In all of these cases the\nstate is constructed and maintained on the basis of immediate sensations to-\ngether with the previous state or some other memory of past sensations. In\nthis book, we do not explore how that is done, but certainly it can be and has\nbeen done. There is no reason to restrict the state representation to immediate\nsensations; in typical applications we should expect the state representation\nto be able to inform the agent of more than that.\nOn the other hand, the state signal should not be expected to inform the\nagent of everything about the environment, or even everything that would be\nuseful to it in making decisions. If the agent is playing blackjack, we should not\nexpect it to know what the next card in the deck is. If the agent is answering\nthe phone, we should not expect it to know in advance who the caller is. If\nthe agent is a paramedic called to a road accident, we should not expect it\nto know immediately the internal injuries of an unconscious victim. In all\nof these cases there is hidden state information in the environment, and that\ninformation would be useful if the agent knew it, but the agent cannot know it\nbecause it has never received any relevant sensations. In short, we don't fault\nan agent for not knowing something that matters, but only for having known\nsomething and then forgotten it!\nWhat we would like, ideally, is a state signal that summarizes past sensa-\ntions compactly, yet in such a way that all relevant information is retained.\nThis normally requires more than the immediate sensations, but never more\nthan the complete history of all past sensations. A state signal that suc-\nceeds in retaining all relevant information is said to be Markov , or to have\nthe Markov property (we de\fne this formally below). For example, a check-\ners position|the current con\fguration of all the pieces on the board|would\nserve as a Markov state because it summarizes everything important about the\n64 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\ncomplete sequence of positions that led to it. Much of the information about\nthe sequence is lost, but all that really matters for the future of the game is\nretained. Similarly, the current position and velocity of a cannonball is all that\nmatters for its future \right. It doesn't matter how that position and velocity\ncame about. This is sometimes also referred to as an \\independence of path\"\nproperty because all that matters is in the current state signal; its meaning is\nindependent of the \\path,\" or history, of signals that have led up to it.\nWe now formally de\fne the Markov property for the reinforcement learning\nproblem. To keep the mathematics simple, we assume here that there are a\n\fnite number of states and reward values. This enables us to work in terms\nof sums and probabilities rather than integrals and probability densities, but\nthe argument can easily be extended to include continuous states and rewards.\nConsider how a general environment might respond at time t+ 1 to the action\ntaken at time t. In the most general, causal case this response may depend\non everything that has happened earlier. In this case the dynamics can be\nde\fned only by specifying the complete probability distribution:\nPrfRt+1=r;St+1=s0jS0;A0;R1;:::;St\u00001;At\u00001;Rt;St;Atg; (3.4)\nfor allr,s0, and all possible values of the past events: S0,A0,R1, ...,St\u00001,\nAt\u00001,Rt,St,At. If the state signal has the Markov property , on the other\nhand, then the environment's response at t+ 1 depends only on the state and\naction representations at t, in which case the environment's dynamics can be\nde\fned by specifying only\np(s0;rjs;a) = PrfRt+1=r;St+1=s0jSt;Atg; (3.5)\nfor allr,s0,St, andAt. In other words, a state signal has the Markov property,\nand is a Markov state, if and only if (3.5) is equal to (3.4) for all s0,r, and\nhistories,S0,A0,R1, ...,St\u00001,At\u00001,Rt,St,At. In this case, the environment\nand task as a whole are also said to have the Markov property.\nIf an environment has the Markov property, then its one-step dynamics\n(3.5) enable us to predict the next state and expected next reward given the\ncurrent state and action. One can show that, by iterating this equation, one\ncan predict all future states and expected rewards from knowledge only of the\ncurrent state as well as would be possible given the complete history up to the\ncurrent time. It also follows that Markov states provide the best possible basis\nfor choosing actions. That is, the best policy for choosing actions as a function\nof a Markov state is just as good as the best policy for choosing actions as a\nfunction of complete histories.\nEven when the state signal is non-Markov, it is still appropriate to think\nof the state in reinforcement learning as an approximation to a Markov state.\n\u00033.5. THE MARKOV PROPERTY 65\nIn particular, we always want the state to be a good basis for predicting\nfuture rewards and for selecting actions. In cases in which a model of the\nenvironment is learned (see Chapter 8), we also want the state to be a good\nbasis for predicting subsequent states. Markov states provide an unsurpassed\nbasis for doing all of these things. To the extent that the state approaches the\nability of Markov states in these ways, one will obtain better performance from\nreinforcement learning systems. For all of these reasons, it is useful to think of\nthe state at each time step as an approximation to a Markov state, although\none should remember that it may not fully satisfy the Markov property.\nThe Markov property is important in reinforcement learning because de-\ncisions and values are assumed to be a function only of the current state. In\norder for these to be e\u000bective and informative, the state representation must\nbe informative. All of the theory presented in this book assumes Markov state\nsignals. This means that not all the theory strictly applies to cases in which\nthe Markov property does not strictly apply. However, the theory developed\nfor the Markov case still helps us to understand the behavior of the algorithms,\nand the algorithms can be successfully applied to many tasks with states that\nare not strictly Markov. A full understanding of the theory of the Markov\ncase is an essential foundation for extending it to the more complex and real-\nistic non-Markov case. Finally, we note that the assumption of Markov state\nrepresentations is not unique to reinforcement learning but is also present in\nmost if not all other approaches to arti\fcial intelligence.\nExample 3.5: Pole-Balancing State In the pole-balancing task intro-\nduced earlier, a state signal would be Markov if it speci\fed exactly, or made\nit possible to reconstruct exactly, the position and velocity of the cart along\nthe track, the angle between the cart and the pole, and the rate at which this\nangle is changing (the angular velocity). In an idealized cart{pole system, this\ninformation would be su\u000ecient to exactly predict the future behavior of the\ncart and pole, given the actions taken by the controller. In practice, however,\nit is never possible to know this information exactly because any real sensor\nwould introduce some distortion and delay in its measurements. Furthermore,\nin any real cart{pole system there are always other e\u000bects, such as the bend-\ning of the pole, the temperatures of the wheel and pole bearings, and various\nforms of backlash, that slightly a\u000bect the behavior of the system. These factors\nwould cause violations of the Markov property if the state signal were only the\npositions and velocities of the cart and the pole.\nHowever, often the positions and velocities serve quite well as states. Some\nearly studies of learning to solve the pole-balancing task used a coarse state\nsignal that divided cart positions into three regions: right, left, and middle\n(and similar rough quantizations of the other three intrinsic state variables).\nThis distinctly non-Markov state was su\u000ecient to allow the task to be solved\n66 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\neasily by reinforcement learning methods. In fact, this coarse representation\nmay have facilitated rapid learning by forcing the learning agent to ignore \fne\ndistinctions that would not have been useful in solving the task.\nExample 3.6: Draw Poker In draw poker, each player is dealt a hand of\n\fve cards. There is a round of betting, in which each player exchanges some\nof his cards for new ones, and then there is a \fnal round of betting. At each\nround, each player must match or exceed the highest bets of the other players,\nor else drop out (fold). After the second round of betting, the player with the\nbest hand who has not folded is the winner and collects all the bets.\nThe state signal in draw poker is di\u000berent for each player. Each player\nknows the cards in his own hand, but can only guess at those in the other\nplayers' hands. A common mistake is to think that a Markov state signal\nshould include the contents of all the players' hands and the cards remaining\nin the deck. In a fair game, however, we assume that the players are in principle\nunable to determine these things from their past observations. If a player did\nknow them, then she could predict some future events (such as the cards one\ncould exchange for) better than by remembering all past observations.\nIn addition to knowledge of one's own cards, the state in draw poker should\ninclude the bets and the numbers of cards drawn by the other players. For\nexample, if one of the other players drew three new cards, you may suspect he\nretained a pair and adjust your guess of the strength of his hand accordingly.\nThe players' bets also in\ruence your assessment of their hands. In fact, much\nof your past history with these particular players is part of the Markov state.\nDoes Ellen like to blu\u000b, or does she play conservatively? Does her face or\ndemeanor provide clues to the strength of her hand? How does Joe's play\nchange when it is late at night, or when he has already won a lot of money?\nAlthough everything ever observed about the other players may have an\ne\u000bect on the probabilities that they are holding various kinds of hands, in\npractice this is far too much to remember and analyze, and most of it will have\nno clear e\u000bect on one's predictions and decisions. Very good poker players are\nadept at remembering just the key clues, and at sizing up new players quickly,\nbut no one remembers everything that is relevant. As a result, the state\nrepresentations people use to make their poker decisions are undoubtedly non-\nMarkov, and the decisions themselves are presumably imperfect. Nevertheless,\npeople still make very good decisions in such tasks. We conclude that the\ninability to have access to a perfect Markov state representation is probably\nnot a severe problem for a reinforcement learning agent.\n3.6. MARKOV DECISION PROCESSES 67\n3.6 Markov Decision Processes\nA reinforcement learning task that satis\fes the Markov property is called a\nMarkov decision process , or MDP . If the state and action spaces are \fnite,\nthen it is called a \fnite Markov decision process (\fnite MDP) . Finite MDPs\nare particularly important to the theory of reinforcement learning. We treat\nthem extensively throughout this book; they are all you need to understand\n90% of modern reinforcement learning.\nA particular \fnite MDP is de\fned by its state and action sets and by the\none-step dynamics of the environment. Given any state and action sanda,\nthe probability of each possible pair of next state and reward, s0;r, is denoted\np(s0;rjs;a) = PrfSt+1=s0;Rt+1=rjSt=s;At=ag: (3.6)\nThese quantities completely specify the dynamics of a \fnite MDP. Most of the\ntheory we present in the rest of this book implicitly assumes the environment\nis a \fnite MDP.\nGiven the dynamics as speci\fed by (3.6), one can compute anything else\none might want to know about the environment, such as the expected rewards\nfor state{action pairs,\nr(s;a) =E[Rt+1jSt=s;At=a] =X\nr2RrX\ns02Sp(s0;rjs;a); (3.7)\nthestate-transition probabilities ,\np(s0js;a) = PrfSt+1=s0jSt=s;At=ag=X\nr2Rp(s0;rjs;a); (3.8)\nand the expected rewards for state{action{next-state triples,\nr(s;a;s0) =E[Rt+1jSt=s;At=a;St+1=s0] =P\nr2Rrp(s0;rjs;a)\np(s0js;a):(3.9)\nIn the \frst edition of this book, the dynamics were expressed exclusively in\nterms of the latter two quantities, which were denote Pa\nss0andRa\nss0respectively.\nOne weakness of that notation is that it still did not fully characterize the\ndynamics of the rewards, giving only their expectations. Another weakness is\nthe excess of subscripts and superscripts. In this edition we will predominantly\nuse the explicit notation of (3.6), while sometimes referring directly to the\ntransition probabilities (3.8).\nExample 3.7: Recycling Robot MDP The recycling robot (Example\n3.3) can be turned into a simple example of an MDP by simplifying it and\n68 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nproviding some more details. (Our aim is to produce a simple example, not\na particularly realistic one.) Recall that the agent makes a decision at times\ndetermined by external events (or by other parts of the robot's control system).\nAt each such time the robot decides whether it should (1) actively search for\na can, (2) remain stationary and wait for someone to bring it a can, or (3) go\nback to home base to recharge its battery. Suppose the environment works\nas follows. The best way to \fnd cans is to actively search for them, but this\nruns down the robot's battery, whereas waiting does not. Whenever the robot\nis searching, the possibility exists that its battery will become depleted. In\nthis case the robot must shut down and wait to be rescued (producing a low\nreward).\nThe agent makes its decisions solely as a function of the energy level of\nthe battery. It can distinguish two levels, high andlow, so that the state set\nisS=fhigh;lowg. Let us call the possible decisions|the agent's actions|\nwait ,search , and recharge . When the energy level is high , recharging would\nalways be foolish, so we do not include it in the action set for this state. The\nagent's action sets are\nA(high ) =fsearch;waitg\nA(low) =fsearch;wait;rechargeg:\nIf the energy level is high , then a period of active search can always be\ncompleted without risk of depleting the battery. A period of searching that\nbegins with a high energy level leaves the energy level high with probability\n\u000band reduces it to lowwith probability 1 \u0000\u000b. On the other hand, a period of\nsearching undertaken when the energy level is lowleaves it lowwith probability\n\fand depletes the battery with probability 1 \u0000\f. In the latter case, the robot\nmust be rescued, and the battery is then recharged back to high . Each can\ncollected by the robot counts as a unit reward, whereas a reward of \u00003 results\nwhenever the robot has to be rescued. Let rsearch andrwait, withrsearch>r wait,\nrespectively denote the expected number of cans the robot will collect (and\nhence the expected reward) while searching and while waiting. Finally, to keep\nthings simple, suppose that no cans can be collected during a run home for\nrecharging, and that no cans can be collected on a step in which the battery\nis depleted. This system is then a \fnite MDP, and we can write down the\ntransition probabilities and the expected rewards, as in Table 3.1.\nAtransition graph is a useful way to summarize the dynamics of a \fnite\nMDP. Figure 3.3 shows the transition graph for the recycling robot example.\nThere are two kinds of nodes: state nodes and action nodes . There is a state\nnode for each possible state (a large open circle labeled by the name of the\nstate), and an action node for each state{action pair (a small solid circle labeled\n3.6. MARKOV DECISION PROCESSES 69\ns s0a p(s0js;a)r(s;a;s0)\nhigh high search \u000brsearch\nhigh low search 1\u0000\u000brsearch\nlow high search 1\u0000\f\u00003\nlow low search \frsearch\nhigh high wait 1rwait\nhigh low wait 0rwait\nlow high wait 0rwait\nlow low wait 1rwait\nlow high recharge 1 0\nlow low recharge 0 0.\nTable 3.1: Transition probabilities and expected rewards for the \fnite MDP\nof the recycling robot example. There is a row for each possible combination\nof current state, s, next state, s0, and action possible in the current state,\na2A(s).\nby the name of the action and connected by a line to the state node). Starting\nin statesand taking action amoves you along the line from state node sto\naction node ( s;a). Then the environment responds with a transition to the\nnext state's node via one of the arrows leaving action node ( s;a). Each arrow\ncorresponds to a triple ( s;s0;a), wheres0is the next state, and we label the\narrow with the transition probability, p(s0js;a), and the expected reward for\nthat transition, r(s;a;s0). Note that the transition probabilities labeling the\narrows leaving an action node always sum to 1.\nsearchhighlow1,  0 1\u2013! ,   \u20133\nsearchrechargewaitwait\nsearch1\u2013\" ,  R! ,  R search\n\", Rsearch1,  R wait\n1,  R wait3.6. MARKOV DECISION PROCESSES59ss0ap(s0|s, a)r(s, a, s0)high high search\u21b5rsearchhigh low search1\u0000\u21b5rsearchlow high search1\u0000\u0000\u00003low low search\u0000rsearchhigh high wait1rwaithigh low wait0rwaitlow high wait0rwaitlow low wait1rwaitlow high recharge10low low recharge00.Table 3.1: Transition probabilities and expected rewards for the \ufb01nite MDPof the recycling robot example. There is a row for each possible combinationof current state,s, next state,s0, and action possible in the current state,a2A(s).isS={high,low}. Let us call the possible decisions\u2014the agent\u2019s actions\u2014wait,search, andrecharge. When the energy level ishigh, recharging wouldalways be foolish, so we do not include it in the action set for this state. Theagent\u2019s action sets areA(high)={search,wait}A(low)={search,wait,recharge}.If the energy level ishigh, then a period of active search can always becompleted without risk of depleting the battery. A period of searching thatbegins with ahighenergy level leaves the energy levelhighwith probability\u21b5and reduces it tolowwith probability 1\u0000\u21b5. On the other hand, a period ofsearching undertaken when the energy level islowleaves itlowwith probability\u0000and depletes the battery with probability 1\u0000\u0000. In the latter case, the robotmust be rescued, and the battery is then recharged back tohigh. Each cancollected by the robot counts as a unit reward, whereas a reward of\u00003r e s u l t swhenever the robot has to be rescued. Letrsearchandrwait, withrsearch>rwait,respectively denote the expected number of cans the robot will collect (andhence the expected reward) while searching and while waiting. Finally, to keepthings simple, suppose that no cans can be collected during a run home forrecharging, and that no cans can be collected on a step in which the batteryis depleted. This system is then a \ufb01nite MDP, and we can write down thetransition probabilities and the expected rewards, as in Table 3.1.Atransition graphis a useful way to summarize the dynamics of a \ufb01nite3.6. MARKOV DECISION PROCESSES59ss0ap(s0|s, a)r(s, a, s0)high high search\u21b5rsearchhigh low search1\u0000\u21b5rsearchlow high search1\u0000\u0000\u00003low low search\u0000rsearchhigh high wait1rwaithigh low wait0rwaitlow high wait0rwaitlow low wait1rwaitlow high recharge10low low recharge00.Table 3.1: Transition probabilities and expected rewards for the \ufb01nite MDPof the recycling robot example. There is a row for each possible combinationof current state,s, next state,s0, and action possible in the current state,a2A(s).isS={high,low}. Let us call the possible decisions\u2014the agent\u2019s actions\u2014wait,search, andrecharge. When the energy level ishigh, recharging wouldalways be foolish, so we do not include it in the action set for this state. Theagent\u2019s action sets areA(high)={search,wait}A(low)={search,wait,recharge}.If the energy level ishigh, then a period of active search can always becompleted without risk of depleting the battery. A period of searching thatbegins with ahighenergy level leaves the energy levelhighwith probability\u21b5and reduces it tolowwith probability 1\u0000\u21b5. On the other hand, a period ofsearching undertaken when the energy level islowleaves itlowwith probability\u0000and depletes the battery with probability 1\u0000\u0000. In the latter case, the robotmust be rescued, and the battery is then recharged back tohigh. Each cancollected by the robot counts as a unit reward, whereas a reward of\u00003r e s u l t swhenever the robot has to be rescued. Letrsearchandrwait, withrsearch>rwait,respectively denote the expected number of cans the robot will collect (andhence the expected reward) while searching and while waiting. Finally, to keepthings simple, suppose that no cans can be collected during a run home forrecharging, and that no cans can be collected on a step in which the batteryis depleted. This system is then a \ufb01nite MDP, and we can write down thetransition probabilities and the expected rewards, as in Table 3.1.Atransition graphis a useful way to summarize the dynamics of a \ufb01nite3.6. MARKOV DECISION PROCESSES59ss0ap(s0|s, a)r(s, a, s0)high high search\u21b5rsearchhigh low search1\u0000\u21b5rsearchlow high search1\u0000\u0000\u00003low low search\u0000rsearchhigh high wait1rwaithigh low wait0rwaitlow high wait0rwaitlow low wait1rwaitlow high recharge10low low recharge00.Table 3.1: Transition probabilities and expected rewards for the \ufb01nite MDPof the recycling robot example. There is a row for each possible combinationof current state,s, next state,s0, and action possible in the current state,a2A(s).isS={high,low}. Let us call the possible decisions\u2014the agent\u2019s actions\u2014wait,search, andrecharge. When the energy level ishigh, recharging wouldalways be foolish, so we do not include it in the action set for this state. Theagent\u2019s action sets areA(high)={search,wait}A(low)={search,wait,recharge}.If the energy level ishigh, then a period of active search can always becompleted without risk of depleting the battery. A period of searching thatbegins with ahighenergy level leaves the energy levelhighwith probability\u21b5and reduces it tolowwith probability 1\u0000\u21b5. On the other hand, a period ofsearching undertaken when the energy level islowleaves itlowwith probability\u0000and depletes the battery with probability 1\u0000\u0000. In the latter case, the robotmust be rescued, and the battery is then recharged back tohigh. Each cancollected by the robot counts as a unit reward, whereas a reward of\u00003r e s u l t swhenever the robot has to be rescued. Letrsearchandrwait, withrsearch>rwait,respectively denote the expected number of cans the robot will collect (andhence the expected reward) while searching and while waiting. Finally, to keepthings simple, suppose that no cans can be collected during a run home forrecharging, and that no cans can be collected on a step in which the batteryis depleted. This system is then a \ufb01nite MDP, and we can write down thetransition probabilities and the expected rewards, as in Table 3.1.Atransition graphis a useful way to summarize the dynamics of a \ufb01nite3.6. MARKOV DECISION PROCESSES59ss0ap(s0|s, a)r(s, a, s0)high high search\u21b5rsearchhigh low search1\u0000\u21b5rsearchlow high search1\u0000\u0000\u00003low low search\u0000rsearchhigh high wait1rwaithigh low wait0rwaitlow high wait0rwaitlow low wait1rwaitlow high recharge10low low recharge00.Table 3.1: Transition probabilities and expected rewards for the \ufb01nite MDPof the recycling robot example. There is a row for each possible combinationof current state,s, next state,s0, and action possible in the current state,a2A(s).isS={high,low}. Let us call the possible decisions\u2014the agent\u2019s actions\u2014wait,search, andrecharge. When the energy level ishigh, recharging wouldalways be foolish, so we do not include it in the action set for this state. Theagent\u2019s action sets areA(high)={search,wait}A(low)={search,wait,recharge}.If the energy level ishigh, then a period of active search can always becompleted without risk of depleting the battery. A period of searching thatbegins with ahighenergy level leaves the energy levelhighwith probability\u21b5and reduces it tolowwith probability 1\u0000\u21b5. On the other hand, a period ofsearching undertaken when the energy level islowleaves itlowwith probability\u0000and depletes the battery with probability 1\u0000\u0000. In the latter case, the robotmust be rescued, and the battery is then recharged back tohigh. Each cancollected by the robot counts as a unit reward, whereas a reward of\u00003r e s u l t swhenever the robot has to be rescued. Letrsearchandrwait, withrsearch>rwait,respectively denote the expected number of cans the robot will collect (andhence the expected reward) while searching and while waiting. Finally, to keepthings simple, suppose that no cans can be collected during a run home forrecharging, and that no cans can be collected on a step in which the batteryis depleted. This system is then a \ufb01nite MDP, and we can write down thetransition probabilities and the expected rewards, as in Table 3.1.Atransition graphis a useful way to summarize the dynamics of a \ufb01nite3.6. MARKOV DECISION PROCESSES59ss0ap(s0|s, a)r(s, a, s0)high high search\u21b5rsearchhigh low search1\u0000\u21b5rsearchlow high search1\u0000\u0000\u00003low low search\u0000rsearchhigh high wait1rwaithigh low wait0rwaitlow high wait0rwaitlow low wait1rwaitlow high recharge10low low recharge00.Table 3.1: Transition probabilities and expected rewards for the \ufb01nite MDPof the recycling robot example. There is a row for each possible combinationof current state,s, next state,s0, and action possible in the current state,a2A(s).isS={high,low}. Let us call the possible decisions\u2014the agent\u2019s actions\u2014wait,search, andrecharge. When the energy level ishigh, recharging wouldalways be foolish, so we do not include it in the action set for this state. Theagent\u2019s action sets areA(high)={search,wait}A(low)={search,wait,recharge}.If the energy level ishigh, then a period of active search can always becompleted without risk of depleting the battery. A period of searching thatbegins with ahighenergy level leaves the energy levelhighwith probability\u21b5and reduces it tolowwith probability 1\u0000\u21b5. On the other hand, a period ofsearching undertaken when the energy level islowleaves itlowwith probability\u0000and depletes the battery with probability 1\u0000\u0000. In the latter case, the robotmust be rescued, and the battery is then recharged back tohigh. Each cancollected by the robot counts as a unit reward, whereas a reward of\u00003r e s u l t swhenever the robot has to be rescued. Letrsearchandrwait, withrsearch>rwait,respectively denote the expected number of cans the robot will collect (andhence the expected reward) while searching and while waiting. Finally, to keepthings simple, suppose that no cans can be collected during a run home forrecharging, and that no cans can be collected on a step in which the batteryis depleted. This system is then a \ufb01nite MDP, and we can write down thetransition probabilities and the expected rewards, as in Table 3.1.Atransition graphis a useful way to summarize the dynamics of a \ufb01nite\nFigure 3.3: Transition graph for the recycling robot example.\n70 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\n3.7 Value Functions\nAlmost all reinforcement learning algorithms involve estimating value func-\ntions |functions of states (or of state{action pairs) that estimate how good it\nis for the agent to be in a given state (or how good it is to perform a given\naction in a given state). The notion of \\how good\" here is de\fned in terms of\nfuture rewards that can be expected, or, to be precise, in terms of expected\nreturn. Of course the rewards the agent can expect to receive in the future\ndepend on what actions it will take. Accordingly, value functions are de\fned\nwith respect to particular policies.\nRecall that a policy, \u0019, is a mapping from each state, s2S, and action, a2\nA(s), to the probability \u0019(ajs) of taking action awhen in state s. Informally,\nthevalue of a statesunder a policy \u0019, denotedv\u0019(s), is the expected return\nwhen starting in sand following \u0019thereafter. For MDPs, we can de\fne v\u0019(s)\nformally as\nv\u0019(s) =E\u0019[GtjSt=s] =E\u0019\"1X\nk=0\rkRt+k+1\f\f\f\f\fSt=s#\n; (3.10)\nwhere E\u0019[\u0001] denotes the expected value of a random variable given that the\nagent follows policy \u0019, andtis any time step. Note that the value of the\nterminal state, if any, is always zero. We call the function v\u0019thestate-value\nfunction for policy \u0019.\nSimilarly, we de\fne the value of taking action ain statesunder a policy\n\u0019, denotedq\u0019(s;a), as the expected return starting from s, taking the action\na, and thereafter following policy \u0019:\nq\u0019(s;a) =E\u0019[GtjSt=s;At=a] =E\u0019\"1X\nk=0\rkRt+k+1\f\f\f\f\fSt=s;At=a#\n:(3.11)\nWe callq\u0019theaction-value function for policy \u0019.\nThe value functions v\u0019andq\u0019can be estimated from experience. For ex-\nample, if an agent follows policy \u0019and maintains an average, for each state\nencountered, of the actual returns that have followed that state, then the aver-\nage will converge to the state's value, v\u0019(s), as the number of times that state\nis encountered approaches in\fnity. If separate averages are kept for each ac-\ntion taken in a state, then these averages will similarly converge to the action\nvalues,q\u0019(s;a). We call estimation methods of this kind Monte Carlo methods\nbecause they involve averaging over many random samples of actual returns.\nThese kinds of methods are presented in Chapter 5. Of course, if there are very\nmany states, then it may not be practical to keep separate averages for each\n3.7. VALUE FUNCTIONS 71\nstate individually. Instead, the agent would have to maintain v\u0019andq\u0019as pa-\nrameterized functions and adjust the parameters to better match the observed\nreturns. This can also produce accurate estimates, although much depends on\nthe nature of the parameterized function approximator (Chapter 9).\nA fundamental property of value functions used throughout reinforcement\nlearning and dynamic programming is that they satisfy particular recursive\nrelationships. For any policy \u0019and any state s, the following consistency\ncondition holds between the value of sand the value of its possible successor\nstates:\nv\u0019(s) = E\u0019[GtjSt=s]\n=E\u0019\"1X\nk=0\rkRt+k+1\f\f\f\f\fSt=s#\n=E\u0019\"\nRt+1+\r1X\nk=0\rkRt+k+2\f\f\f\f\fSt=s#\n=X\na\u0019(ajs)X\ns0X\nrp(s0;rjs;a)\"\nr+\rE\u0019\"1X\nk=0\rkRt+k+2\f\f\f\f\fSt+1=s0##\n=X\na\u0019(ajs)X\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n; (3.12)\nwhere it is implicit that the actions, a, are taken from the set A(s), the next\nstates,s0, are taken from the set S(or from S+in the case of an episodic\nproblem), and the rewards, r, are taken from the set R. Note also how in the\nlast equation we have merged the two sums, one over all the values of s0and\nthe other over all values of r, into one sum over all possible values of both.\nWe will use this kind of merged sum often to simplify formulas. Note how\nthe \fnal expression can be read very easily as an expected value. It is really\na sum over all values of the three variables, a,s0, andr. For each triple, we\ncompute its probability, \u0019(ajs)p(s0;rjs;a), weight the quantity in brackets by\nthat probability, then sum over all possibilities to get an expected value.\nEquation (3.12) is the Bellman equation for v\u0019. It expresses a relationship\nbetween the value of a state and the values of its successor states. Think of\nlooking ahead from one state to its possible successor states, as suggested by\nFigure 3.4a. Each open circle represents a state and each solid circle represents\na state{action pair. Starting from state s, the root node at the top, the agent\ncould take any of some set of actions|three are shown in Figure 3.4a. From\neach of these, the environment could respond with one of several next states,\ns0, along with a reward, r. The Bellman equation (3.12) averages over all the\npossibilities, weighting each by its probability of occurring. It states that the\n72 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\ns,a s\na\ns'r\na's'r(b) (a)\nFigure 3.4: Backup diagrams for (a) v\u0019and (b)q\u0019.\nvalue of the start state must equal the (discounted) value of the expected next\nstate, plus the reward expected along the way.\nThe value function v\u0019is the unique solution to its Bellman equation. We\nshow in subsequent chapters how this Bellman equation forms the basis of a\nnumber of ways to compute, approximate, and learn v\u0019. We call diagrams\nlike those shown in Figure 3.4 backup diagrams because they diagram rela-\ntionships that form the basis of the update or backup operations that are at\nthe heart of reinforcement learning methods. These operations transfer value\ninformation back to a state (or a state{action pair) from its successor states (or\nstate{action pairs). We use backup diagrams throughout the book to provide\ngraphical summaries of the algorithms we discuss. (Note that unlike transi-\ntion graphs, the state nodes of backup diagrams do not necessarily represent\ndistinct states; for example, a state might be its own successor. We also omit\nexplicit arrowheads because time always \rows downward in a backup diagram.)\nExample 3.8: Gridworld Figure 3.5a uses a rectangular grid to illustrate\nvalue functions for a simple \fnite MDP. The cells of the grid correspond to\nthe states of the environment. At each cell, four actions are possible: north ,\nsouth ,east , and west , which deterministically cause the agent to move one\ncell in the respective direction on the grid. Actions that would take the agent\no\u000b the grid leave its location unchanged, but also result in a reward of \u00001.\nOther actions result in a reward of 0, except those that move the agent out\nof the special states A and B. From state A, all four actions yield a reward of\n+10 and take the agent to A0. From state B, all actions yield a reward of +5\nand take the agent to B0.\nSuppose the agent selects all four actions with equal probability in all\nstates. Figure 3.5b shows the value function, v\u0019, for this policy, for the dis-\ncounted reward case with \r= 0:9. This value function was computed by solv-\ning the system of equations (3.12). Notice the negative values near the lower\nedge; these are the result of the high probability of hitting the edge of the grid\nthere under the random policy. State A is the best state to be in under this pol-\n3.7. VALUE FUNCTIONS 73\n3.38.84.45.31.5\n1.53.02.31.90.5\n0.10.70.70.4-0.4\n-1.0-0.4-0.4-0.6-1.2\n-1.9-1.3-1.2-1.4-2.0A B\nA'B' +10+5\nActions\n(a) (b)\nFigure 3.5: Grid example: (a) exceptional reward dynamics; (b) state-value\nfunction for the equiprobable random policy.\nicy, but its expected return is less than 10, its immediate reward, because from\nA the agent is taken to A0, from which it is likely to run into the edge of the\ngrid. State B, on the other hand, is valued more than 5, its immediate reward,\nbecause from B the agent is taken to B0, which has a positive value. From B0the\nexpected penalty (negative reward) for possibly running into an edge is more\nthan compensated for by the expected gain for possibly stumbling onto A or B.\nExample 3.9: Golf To formulate playing a hole of golf as a reinforcement\nlearning task, we count a penalty (negative reward) of \u00001 for each stroke until\nwe hit the ball into the hole. The state is the location of the ball. The value of\na state is the negative of the number of strokes to the hole from that location.\nOur actions are how we aim and swing at the ball, of course, and which club\nwe select. Let us take the former as given and consider just the choice of club,\nwhich we assume is either a putter or a driver. The upper part of Figure 3.6\nshows a possible state-value function, vputt(s), for the policy that always uses\nthe putter. The terminal state in-the-hole has a value of 0. From anywhere\non the green we assume we can make a putt; these states have value \u00001. O\u000b\nthe green we cannot reach the hole by putting, and the value is greater. If\nwe can reach the green from a state by putting, then that state must have\nvalue one less than the green's value, that is, \u00002. For simplicity, let us assume\nwe can putt very precisely and deterministically, but with a limited range.\nThis gives us the sharp contour line labeled \u00002 in the \fgure; all locations\nbetween that line and the green require exactly two strokes to complete the\nhole. Similarly, any location within putting range of the \u00002 contour line\nmust have a value of \u00003, and so on to get all the contour lines shown in the\n\fgure. Putting doesn't get us out of sand traps, so they have a value of \u00001.\nOverall, it takes us six strokes to get from the tee to the hole by putting.\n74 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nQ*(s,driver)Vputtsandgreen!1sand!2!2!3!4!1!5!6!4!3\n!3!2!4\nsandgreen!1sand!2!3!20\n0!\"!\"vputt\nq*(s,driver)\nFigure 3.6: A golf example: the state-value function for putting (above) and\nthe optimal action-value function for using the driver (below).\n3.8. OPTIMAL VALUE FUNCTIONS 75\n3.8 Optimal Value Functions\nSolving a reinforcement learning task means, roughly, \fnding a policy that\nachieves a lot of reward over the long run. For \fnite MDPs, we can precisely\nde\fne an optimal policy in the following way. Value functions de\fne a partial\nordering over policies. A policy \u0019is de\fned to be better than or equal to a\npolicy\u00190if its expected return is greater than or equal to that of \u00190for all\nstates. In other words, \u0019\u0015\u00190if and only if v\u0019(s)\u0015v\u00190(s) for alls2S. There\nis always at least one policy that is better than or equal to all other policies.\nThis is an optimal policy . Although there may be more than one, we denote\nall the optimal policies by \u0019\u0003. They share the same state-value function, called\ntheoptimal state-value function , denotedv\u0003, and de\fned as\nv\u0003(s) = max\n\u0019v\u0019(s); (3.13)\nfor alls2S.\nOptimal policies also share the same optimal action-value function , denoted\nq\u0003, and de\fned as\nq\u0003(s;a) = max\n\u0019q\u0019(s;a); (3.14)\nfor alls2Sanda2A(s). For the state{action pair ( s;a), this function gives\nthe expected return for taking action ain statesand thereafter following an\noptimal policy. Thus, we can write q\u0003in terms of v\u0003as follows:\nq\u0003(s;a) =E[Rt+1+\rv\u0003(St+1)jSt=s;At=a]: (3.15)\nExample 3.10: Optimal Value Functions for Golf The lower part\nof Figure 3.6 shows the contours of a possible optimal action-value function\nq\u0003(s;driver ). These are the values of each state if we \frst play a stroke with\nthe driver and afterward select either the driver or the putter, whichever is\nbetter. The driver enables us to hit the ball farther, but with less accuracy.\nWe can reach the hole in one shot using the driver only if we are already very\nclose; thus the\u00001 contour for q\u0003(s;driver ) covers only a small portion of\nthe green. If we have two strokes, however, then we can reach the hole from\nmuch farther away, as shown by the \u00002 contour. In this case we don't have\nto drive all the way to within the small \u00001 contour, but only to anywhere\non the green; from there we can use the putter. The optimal action-value\nfunction gives the values after committing to a particular \frst action, in this\ncase, to the driver, but afterward using whichever actions are best. The \u00003\ncontour is still farther out and includes the starting tee. From the tee, the best\nsequence of actions is two drives and one putt, sinking the ball in three strokes.\n76 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nBecausev\u0003is the value function for a policy, it must satisfy the self-\nconsistency condition given by the Bellman equation for state values (3.12).\nBecause it is the optimal value function, however, v\u0003's consistency condition\ncan be written in a special form without reference to any speci\fc policy. This\nis the Bellman equation for v\u0003, or the Bellman optimality equation . Intuitively,\nthe Bellman optimality equation expresses the fact that the value of a state\nunder an optimal policy must equal the expected return for the best action\nfrom that state:\nv\u0003(s) = max\na2A(s)q\u0019\u0003(s;a)\n= max\naE\u0019\u0003[GtjSt=s;At=a]\n= max\naE\u0019\u0003\"1X\nk=0\rkRt+k+1\f\f\f\f\fSt=s;At=a#\n= max\naE\u0019\u0003\"\nRt+1+\r1X\nk=0\rkRt+k+2\f\f\f\f\fSt=s;At=a#\n= max\naE[Rt+1+\rv\u0003(St+1)jSt=s;At=a] (3.16)\n= max\na2A(s)X\ns0;rp(s0;rjs;a)\u0002\nr+\rv\u0003(s0)\u0003\n: (3.17)\nThe last two equations are two forms of the Bellman optimality equation for\nv\u0003. The Bellman optimality equation for q\u0003is\nq\u0003(s;a) = Eh\nRt+1+\rmax\na0q\u0003(St+1;a0)\f\f\fSt=s;At=ai\n=X\ns0;rp(s0;rjs;a)h\nr+\rmax\na0q\u0003(s0;a0)i\n:\nThe backup diagrams in Figure 3.7 show graphically the spans of future\nstates and actions considered in the Bellman optimality equations for v\u0003and\nq\u0003. These are the same as the backup diagrams for v\u0019andq\u0019except that arcs\nhave been added at the agent's choice points to represent that the maximum\nover that choice is taken rather than the expected value given some policy.\nFigure 3.7a graphically represents the Bellman optimality equation (3.17).\nFor \fnite MDPs, the Bellman optimality equation (3.17) has a unique so-\nlution independent of the policy. The Bellman optimality equation is actually\na system of equations, one for each state, so if there are Nstates, then there\nareNequations in Nunknowns. If the dynamics of the environment are\nknown (p(s0;rjs;a)), then in principle one can solve this system of equations\n3.8. OPTIMAL VALUE FUNCTIONS 77\ns,a s\na\ns'r\na's'r(b) (a)\nmax\nmax\nFigure 3.7: Backup diagrams for (a) v\u0003and (b)q\u0003\nforv\u0003using any one of a variety of methods for solving systems of nonlinear\nequations. One can solve a related set of equations for q\u0003.\nOnce one has v\u0003, it is relatively easy to determine an optimal policy. For\neach states, there will be one or more actions at which the maximum is ob-\ntained in the Bellman optimality equation. Any policy that assigns nonzero\nprobability only to these actions is an optimal policy. You can think of this\nas a one-step search. If you have the optimal value function, v\u0003, then the\nactions that appear best after a one-step search will be optimal actions. An-\nother way of saying this is that any policy that is greedy with respect to\nthe optimal evaluation function v\u0003is an optimal policy. The term greedy is\nused in computer science to describe any search or decision procedure that\nselects alternatives based only on local or immediate considerations, without\nconsidering the possibility that such a selection may prevent future access to\neven better alternatives. Consequently, it describes policies that select actions\nbased only on their short-term consequences. The beauty of v\u0003is that if one\nuses it to evaluate the short-term consequences of actions|speci\fcally, the\none-step consequences|then a greedy policy is actually optimal in the long-\nterm sense in which we are interested because v\u0003already takes into account\nthe reward consequences of all possible future behavior. By means of v\u0003, the\noptimal expected long-term return is turned into a quantity that is locally and\nimmediately available for each state. Hence, a one-step-ahead search yields\nthe long-term optimal actions.\nHavingq\u0003makes choosing optimal actions still easier. With q\u0003, the agent\ndoes not even have to do a one-step-ahead search: for any state s, it can simply\n\fnd any action that maximizes q\u0003(s;a). The action-value function e\u000bectively\ncaches the results of all one-step-ahead searches. It provides the optimal ex-\npected long-term return as a value that is locally and immediately available\nfor each state{action pair. Hence, at the cost of representing a function of\nstate{action pairs, instead of just of states, the optimal action-value function\nallows optimal actions to be selected without having to know anything about\npossible successor states and their values, that is, without having to know\n78 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nanything about the environment's dynamics.\nExample 3.11: Bellman Optimality Equations for the Recycling\nRobot Using (3.17), we can explicitly give the Bellman optimality equation\nfor the recycling robot example. To make things more compact, we abbre-\nviate the states high and low, and the actions search ,wait , and recharge\nrespectively by h,l,s,w, and re. Since there are only two states, the Bellman\noptimality equation consists of two equations. The equation for v\u0003(h) can be\nwritten as follows:\nv\u0003(h) = max\u001ap(hjh;s)[r(h;s;h) +\rv\u0003(h)] +p(ljh;s)[r(h;s;l) +\rv\u0003(l)];\np(hjh;w)[r(h;w;h) +\rv\u0003(h)] +p(ljh;w)[r(h;w;l) +\rv\u0003(l)]\u001b\n= max\u001a\u000b[rs+\rv\u0003(h)] + (1\u0000\u000b)[rs+\rv\u0003(l)];\n1[rw+\rv\u0003(h)] + 0[rw+\rv\u0003(l)]\u001b\n= max\u001ars+\r[\u000bv\u0003(h) + (1\u0000\u000b)v\u0003(l)];\nrw+\rv\u0003(h)\u001b\n:\nFollowing the same procedure for v\u0003(l) yields the equation\nv\u0003(l) = max8\n<\n:\frs\u00003(1\u0000\f) +\r[(1\u0000\f)v\u0003(h) +\fv\u0003(l)]\nrw+\rv\u0003(l);\n\rv\u0003(h)9\n=\n;:\nFor any choice of rs,rw,\u000b,\f, and\r, with 0\u0014\r <1, 0\u0014\u000b;\f\u00141, there is\nexactly one pair of numbers, v\u0003(h) andv\u0003(l), that simultaneously satisfy these\ntwo nonlinear equations.\nExample 3.12: Solving the Gridworld Suppose we solve the Bellman\nequation for v\u0003for the simple grid task introduced in Example 3.8 and shown\nagain in Figure 3.8a. Recall that state A is followed by a reward of +10 and\ntransition to state A0, while state B is followed by a reward of +5 and transition\nto state B0. Figure 3.8b shows the optimal value function, and Figure 3.8c\nshows the corresponding optimal policies. Where there are multiple arrows in\na cell, any of the corresponding actions is optimal.\nExplicitly solving the Bellman optimality equation provides one route to\n\fnding an optimal policy, and thus to solving the reinforcement learning prob-\nlem. However, this solution is rarely directly useful. It is akin to an exhaustive\nsearch, looking ahead at all possibilities, computing their probabilities of oc-\ncurrence and their desirabilities in terms of expected rewards. This solution\nrelies on at least three assumptions that are rarely true in practice: (1) we\naccurately know the dynamics of the environment; (2) we have enough com-\nputational resources to complete the computation of the solution; and (3) the\nMarkov property. For the kinds of tasks in which we are interested, one is\n3.9. OPTIMALITY AND APPROXIMATION 79\na) gridworldb) V*c) !*22.024.422.019.417.519.822.019.817.816.017.819.817.816.014.416.017.816.014.413.014.416.014.413.011.7ABA'B'+10+5v*\u03c0*\nFigure 3.8: Optimal solutions to the gridworld example.\ngenerally not able to implement this solution exactly because various com-\nbinations of these assumptions are violated. For example, although the \frst\nand third assumptions present no problems for the game of backgammon, the\nsecond is a major impediment. Since the game has about 1020states, it would\ntake thousands of years on today's fastest computers to solve the Bellman\nequation for v\u0003, and the same is true for \fnding q\u0003. In reinforcement learning\none typically has to settle for approximate solutions.\nMany di\u000berent decision-making methods can be viewed as ways of ap-\nproximately solving the Bellman optimality equation. For example, heuristic\nsearch methods can be viewed as expanding the right-hand side of (3.17) sev-\neral times, up to some depth, forming a \\tree\" of possibilities, and then using\na heuristic evaluation function to approximate v\u0003at the \\leaf\" nodes. (Heuris-\ntic search methods such as A\u0003are almost always based on the episodic case.)\nThe methods of dynamic programming can be related even more closely to\nthe Bellman optimality equation. Many reinforcement learning methods can\nbe clearly understood as approximately solving the Bellman optimality equa-\ntion, using actual experienced transitions in place of knowledge of the expected\ntransitions. We consider a variety of such methods in the following chapters.\n3.9 Optimality and Approximation\nWe have de\fned optimal value functions and optimal policies. Clearly, an\nagent that learns an optimal policy has done very well, but in practice this\nrarely happens. For the kinds of tasks in which we are interested, optimal\npolicies can be generated only with extreme computational cost. A well-de\fned\nnotion of optimality organizes the approach to learning we describe in this book\nand provides a way to understand the theoretical properties of various learning\nalgorithms, but it is an ideal that agents can only approximate to varying\ndegrees. As we discussed above, even if we have a complete and accurate\nmodel of the environment's dynamics, it is usually not possible to simply\n80 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\ncompute an optimal policy by solving the Bellman optimality equation. For\nexample, board games such as chess are a tiny fraction of human experience,\nyet large, custom-designed computers still cannot compute the optimal moves.\nA critical aspect of the problem facing the agent is always the computational\npower available to it, in particular, the amount of computation it can perform\nin a single time step.\nThe memory available is also an important constraint. A large amount\nof memory is often required to build up approximations of value functions,\npolicies, and models. In tasks with small, \fnite state sets, it is possible to\nform these approximations using arrays or tables with one entry for each state\n(or state{action pair). This we call the tabular case, and the corresponding\nmethods we call tabular methods. In many cases of practical interest, however,\nthere are far more states than could possibly be entries in a table. In these\ncases the functions must be approximated, using some sort of more compact\nparameterized function representation.\nOur framing of the reinforcement learning problem forces us to settle for\napproximations. However, it also presents us with some unique opportunities\nfor achieving useful approximations. For example, in approximating opti-\nmal behavior, there may be many states that the agent faces with such a low\nprobability that selecting suboptimal actions for them has little impact on the\namount of reward the agent receives. Tesauro's backgammon player, for exam-\nple, plays with exceptional skill even though it might make very bad decisions\non board con\fgurations that never occur in games against experts. In fact, it\nis possible that TD-Gammon makes bad decisions for a large fraction of the\ngame's state set. The on-line nature of reinforcement learning makes it possi-\nble to approximate optimal policies in ways that put more e\u000bort into learning\nto make good decisions for frequently encountered states, at the expense of\nless e\u000bort for infrequently encountered states. This is one key property that\ndistinguishes reinforcement learning from other approaches to approximately\nsolving MDPs.\n3.10 Summary\nLet us summarize the elements of the reinforcement learning problem that\nwe have presented in this chapter. Reinforcement learning is about learning\nfrom interaction how to behave in order to achieve a goal. The reinforcement\nlearning agent and its environment interact over a sequence of discrete time\nsteps. The speci\fcation of their interface de\fnes a particular task: the actions\nare the choices made by the agent; the states are the basis for making the\nchoices; and the rewards are the basis for evaluating the choices. Everything\n3.10. SUMMARY 81\ninside the agent is completely known and controllable by the agent; everything\noutside is incompletely controllable but may or may not be completely known.\nApolicy is a stochastic rule by which the agent selects actions as a function of\nstates. The agent's objective is to maximize the amount of reward it receives\nover time.\nThe return is the function of future rewards that the agent seeks to max-\nimize. It has several di\u000berent de\fnitions depending upon the nature of the\ntask and whether one wishes to discount delayed reward. The undiscounted\nformulation is appropriate for episodic tasks , in which the agent{environment\ninteraction breaks naturally into episodes ; the discounted formulation is appro-\npriate for continuing tasks , in which the interaction does not naturally break\ninto episodes but continues without limit.\nAn environment satis\fes the Markov property if its state signal compactly\nsummarizes the past without degrading the ability to predict the future. This\nis rarely exactly true, but often nearly so; the state signal should be chosen or\nconstructed so that the Markov property holds as nearly as possible. In this\nbook we assume that this has already been done and focus on the decision-\nmaking problem: how to decide what to do as a function of whatever state\nsignal is available. If the Markov property does hold, then the environment is\ncalled a Markov decision process (MDP). A \fnite MDP is an MDP with \fnite\nstate and action sets. Most of the current theory of reinforcement learning is\nrestricted to \fnite MDPs, but the methods and ideas apply more generally.\nA policy's value functions assign to each state, or state{action pair, the\nexpected return from that state, or state{action pair, given that the agent uses\nthe policy. The optimal value functions assign to each state, or state{action\npair, the largest expected return achievable by any policy. A policy whose value\nfunctions are optimal is an optimal policy . Whereas the optimal value functions\nfor states and state{action pairs are unique for a given MDP, there can be many\noptimal policies. Any policy that is greedy with respect to the optimal value\nfunctions must be an optimal policy. The Bellman optimality equations are\nspecial consistency condition that the optimal value functions must satisfy and\nthat can, in principle, be solved for the optimal value functions, from which\nan optimal policy can be determined with relative ease.\nA reinforcement learning problem can be posed in a variety of di\u000berent ways\ndepending on assumptions about the level of knowledge initially available to\nthe agent. In problems of complete knowledge , the agent has a complete and\naccurate model of the environment's dynamics. If the environment is an MDP,\nthen such a model consists of the one-step transition probabilities andexpected\nrewards for all states and their allowable actions. In problems of incomplete\nknowledge , a complete and perfect model of the environment is not available.\n82 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nEven if the agent has a complete and accurate environment model, the\nagent is typically unable to perform enough computation per time step to fully\nuse it. The memory available is also an important constraint. Memory may\nbe required to build up accurate approximations of value functions, policies,\nand models. In most cases of practical interest there are far more states than\ncould possibly be entries in a table, and approximations must be made.\nA well-de\fned notion of optimality organizes the approach to learning we\ndescribe in this book and provides a way to understand the theoretical prop-\nerties of various learning algorithms, but it is an ideal that reinforcement\nlearning agents can only approximate to varying degrees. In reinforcement\nlearning we are very much concerned with cases in which optimal solutions\ncannot be found but must be approximated in some way.\nBibliographical and Historical Remarks\nThe reinforcement learning problem is deeply indebted to the idea of Markov\ndecision processes (MDPs) from the \feld of optimal control. These histor-\nical in\ruences and other major in\ruences from psychology are described in\nthe brief history given in Chapter 1. Reinforcement learning adds to MDPs a\nfocus on approximation and incomplete information for realistically large prob-\nlems. MDPs and the reinforcement learning problem are only weakly linked\nto traditional learning and decision-making problems in arti\fcial intelligence.\nHowever, arti\fcial intelligence is now vigorously exploring MDP formulations\nfor planning and decision-making from a variety of perspectives. MDPs are\nmore general than previous formulations used in arti\fcial intelligence in that\nthey permit more general kinds of goals and uncertainty.\nOur presentation of the reinforcement learning problem was in\ruenced by\nWatkins (1989).\n3.1 The bioreactor example is based on the work of Ungar (1990) and Miller\nand Williams (1992). The recycling robot example was inspired by the\ncan-collecting robot built by Jonathan Connell (1989).\n3.3{4 The terminology of episodic andcontinuing tasks is di\u000berent from that\nusually used in the MDP literature. In that literature it is common\nto distinguish three types of tasks: (1) \fnite-horizon tasks, in which\ninteraction terminates after a particular \fxed number of time steps; (2)\ninde\fnite-horizon tasks, in which interaction can last arbitrarily long\nbut must eventually terminate; and (3) in\fnite-horizon tasks, in which\ninteraction does not terminate. Our episodic and continuing tasks are\n3.10. SUMMARY 83\nsimilar to inde\fnite-horizon and in\fnite-horizon tasks, respectively, but\nwe prefer to emphasize the di\u000berence in the nature of the interaction.\nThis di\u000berence seems more fundamental than the di\u000berence in the ob-\njective functions emphasized by the usual terms. Often episodic tasks\nuse an inde\fnite-horizon objective function and continuing tasks an\nin\fnite-horizon objective function, but we see this as a common coin-\ncidence rather than a fundamental di\u000berence.\nThe pole-balancing example is from Michie and Chambers (1968) and\nBarto, Sutton, and Anderson (1983).\n3.5 For further discussion of the concept of state, see Minsky (1967).\n3.6 The theory of MDPs is treated by, e.g., Bertsekas (1995), Ross (1983),\nWhite (1969), and Whittle (1982, 1983). This theory is also studied\nunder the heading of stochastic optimal control, where adaptive optimal\ncontrol methods are most closely related to reinforcement learning (e.g.,\nKumar, 1985; Kumar and Varaiya, 1986).\nThe theory of MDPs evolved from e\u000borts to understand the problem\nof making sequences of decisions under uncertainty, where each deci-\nsion can depend on the previous decisions and their outcomes. It is\nsometimes called the theory of multistage decision processes, or se-\nquential decision processes, and has roots in the statistical literature\non sequential sampling beginning with the papers by Thompson (1933,\n1934) and Robbins (1952) that we cited in Chapter 2 in connection\nwith bandit problems (which are prototypical MDPs if formulated as\nmultiple-situation problems).\nThe earliest instance of which we are aware in which reinforcement\nlearning was discussed using the MDP formalism is Andreae's (1969b)\ndescription of a uni\fed view of learning machines. Witten and Corbin\n(1973) experimented with a reinforcement learning system later ana-\nlyzed by Witten (1977) using the MDP formalism. Although he did not\nexplicitly mention MDPs, Werbos (1977) suggested approximate solu-\ntion methods for stochastic optimal control problems that are related to\nmodern reinforcement learning methods (see also Werbos, 1982, 1987,\n1988, 1989, 1992). Although Werbos's ideas were not widely recognized\nat the time, they were prescient in emphasizing the importance of ap-\nproximately solving optimal control problems in a variety of domains,\nincluding arti\fcial intelligence. The most in\ruential integration of rein-\nforcement learning and MDPs is due to Watkins (1989). His treatment\nof reinforcement learning using the MDP formalism has been widely\nadopted.\n84 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nOur characterization of the dynamics of an MDP in terms of p(s0;rjs;a)\nis slightly unusual. It is more common in the MDP literature to describe\nthe dynamics in terms of the state transition probabilities p(s0js;a) and\nexpected next rewards r(s;a). In reinforcement learning, however, we\nmore often have to refer to individual actual or sample rewards (rather\nthan just their expected values). Our notation also makes it plainer that\nStandRtare in general jointly determined, and thus must have the\nsame time index. In teaching reinforcement learning, we have found\nour notation to be more straightforward conceptually and easier to\nunderstand.\n3.7{8 Assigning value on the basis of what is good or bad in the long run has\nancient roots. In control theory, mapping states to numerical values\nrepresenting the long-term consequences of control decisions is a key\npart of optimal control theory, which was developed in the 1950s by ex-\ntending nineteenth century state-function theories of classical mechan-\nics (see, e.g., Schultz and Melsa, 1967). In describing how a computer\ncould be programmed to play chess, Shannon (1950) suggested using\nan evaluation function that took into account the long-term advantages\nand disadvantages of chess positions.\nWatkins's (1989) Q-learning algorithm for estimating q\u0003(Chapter 6)\nmade action-value functions an important part of reinforcement learn-\ning, and consequently these functions are often called Q-functions . But\nthe idea of an action-value function is much older than this. Shannon\n(1950) suggested that a function h(P;M ) could be used by a chess-\nplaying program to decide whether a move Min position Pis worth\nexploring. Michie's (1961, 1963) MENACE system and Michie and\nChambers's (1968) BOXES system can be understood as estimating\naction-value functions. In classical physics, Hamilton's principal func-\ntion is an action-value function; Newtonian dynamics are greedy with\nrespect to this function (e.g., Goldstein, 1957). Action-value functions\nalso played a central role in Denardo's (1967) theoretical treatment of\nDP in terms of contraction mappings.\nWhat we call the Bellman equation for v\u0003was \frst introduced by\nRichard Bellman (1957a), who called it the \\basic functional equa-\ntion.\" The counterpart of the Bellman optimality equation for continu-\nous time and state problems is known as the Hamilton{Jacobi{Bellman\nequation (or often just the Hamilton{Jacobi equation), indicating its\nroots in classical physics (e.g., Schultz and Melsa, 1967).\nThe golf example was suggested by Chris Watkins.\n3.10. SUMMARY 85\nExercises\nExercise 3.1 Devise three example tasks of your own that \ft into the re-\ninforcement learning framework, identifying for each its states, actions, and\nrewards. Make the three examples as di\u000berent from each other as possible.\nThe framework is abstract and \rexible and can be applied in many di\u000berent\nways. Stretch its limits in some way in at least one of your examples.\nExercise 3.2 Is the reinforcement learning framework adequate to usefully\nrepresent allgoal-directed learning tasks? Can you think of any clear excep-\ntions?\nExercise 3.3 Consider the problem of driving. You could de\fne the actions\nin terms of the accelerator, steering wheel, and brake, that is, where your\nbody meets the machine. Or you could de\fne them farther out|say, where\nthe rubber meets the road, considering your actions to be tire torques. Or\nyou could de\fne them farther in|say, where your brain meets your body, the\nactions being muscle twitches to control your limbs. Or you could go to a\nreally high level and say that your actions are your choices of where to drive.\nWhat is the right level, the right place to draw the line between agent and\nenvironment? On what basis is one location of the line to be preferred over\nanother? Is there any fundamental reason for preferring one location over\nanother, or is it a free choice?\nExercise 3.4 Suppose you treated pole-balancing as an episodic task but\nalso used discounting, with all rewards zero except for \u00001 upon failure. What\nthen would the return be at each time? How does this return di\u000ber from that\nin the discounted, continuing formulation of this task?\nExercise 3.5 Imagine that you are designing a robot to run a maze. You de-\ncide to give it a reward of +1 for escaping from the maze and a reward of zero\nat all other times. The task seems to break down naturally into episodes|the\nsuccessive runs through the maze|so you decide to treat it as an episodic task,\nwhere the goal is to maximize expected total reward (3.1). After running the\nlearning agent for a while, you \fnd that it is showing no improvement in escap-\ning from the maze. What is going wrong? Have you e\u000bectively communicated\nto the agent what you want it to achieve?\nExercise 3.6: Broken Vision System Imagine that you are a vision\nsystem. When you are \frst turned on for the day, an image \roods into your\ncamera. You can see lots of things, but not all things. You can't see objects\nthat are occluded, and of course you can't see objects that are behind you.\nAfter seeing that \frst scene, do you have access to the Markov state of the\nenvironment? Suppose your camera was broken that day and you received no\n86 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nimages at all, all day. Would you have access to the Markov state then?\nExercise 3.7 There is no exercise 3.7.\nExercise 3.8 What is the Bellman equation for action values, that is, for q\u0019?\nIt must give the action value q\u0019(s;a) in terms of the action values, q\u0019(s0;a0),\nof possible successors to the state{action pair ( s;a). As a hint, the backup\ndiagram corresponding to this equation is given in Figure 3.4b. Show the\nsequence of equations analogous to (3.12), but for action values.\nExercise 3.9 The Bellman equation (3.12) must hold for each state for the\nvalue function v\u0019shown in Figure 3.5b. As an example, show numerically that\nthis equation holds for the center state, valued at +0 :7, with respect to its four\nneighboring states, valued at +2 :3, +0:4,\u00000:4, and +0:7. (These numbers are\naccurate only to one decimal place.)\nExercise 3.10 In the gridworld example, rewards are positive for goals,\nnegative for running into the edge of the world, and zero the rest of the time.\nAre the signs of these rewards important, or only the intervals between them?\nProve, using (3.2), that adding a constant cto all the rewards adds a constant,\nvc, to the values of all states, and thus does not a\u000bect the relative values of\nany states under any policies. What is vcin terms of cand\r?\nExercise 3.11 Now consider adding a constant cto all the rewards in an\nepisodic task, such as maze running. Would this have any e\u000bect, or would it\nleave the task unchanged as in the continuing task above? Why or why not?\nGive an example.\nExercise 3.12 The value of a state depends on the the values of the actions\npossible in that state and on how likely each action is to be taken under the\ncurrent policy. We can think of this in terms of a small backup diagram rooted\nat the state and considering each possible action:\nsa1a2a3V!(s)Q!(s,a)taken withprobability !(s,a)v\u03c0(s)q\u03c0(s,a)\u03c0(a|s)\nGive the equation corresponding to this intuition and diagram for the value at\nthe root node, v\u0019(s), in terms of the value at the expected leaf node, q\u0019(s;a),\ngivenSt=s. This expectation depends on the policy, \u0019. Then give a second\nequation in which the expected value is written out explicitly in terms of \u0019(ajs)\nsuch that no expected value notation appears in the equation.\nExercise 3.13 The value of an action, q\u0019(s;a), depends on the expected\nnext reward and the expected sum of the remaining rewards. Again we can\n3.10. SUMMARY 87\nthink of this in terms of a small backup diagram, this one rooted at an action\n(state{action pair) and branching to the possible next states:\ns,aV!(s)Q!(s,a)s1's2s3r1r2r3''v\u03c0(s)q\u03c0(s,a)expectedrewards\nGive the equation corresponding to this intuition and diagram for the action\nvalue,q\u0019(s;a), in terms of the expected next reward, Rt+1, and the expected\nnext state value, v\u0019(St+1), given that St=sandAt=a. Then give a second\nequation, writing out the expected value explicitly in terms of p(s0;rjs;a)\nde\fned by (3.6), such that no expected value notation appears in the equation.\nExercise 3.14 Draw or describe the optimal state-value function for the golf\nexample.\nExercise 3.15 Draw or describe the contours of the optimal action-value\nfunction for putting, q\u0003(s;putter ), for the golf example.\nExercise 3.16 Give the Bellman equation for q\u0003for the recycling robot.\nExercise 3.17 Figure 3.8 gives the optimal value of the best state of the\ngridworld as 24.4, to one decimal place. Use your knowledge of the optimal\npolicy and (3.2) to express this value symbolically, and then to compute it to\nthree decimal places.\nExercise 3.18 Give a de\fnition of v\u0003in terms of q\u0003.\nExercise 3.19 Give a de\fnition of q\u0003in terms of v\u0003.\nExercise 3.20 Give a de\fnition of \u0019\u0003in terms of q\u0003.\nExercise 3.21 Give a de\fnition of \u0019\u0003in terms of v\u0003.\n88 CHAPTER 3. FINITE MARKOV DECISION PROCESSES\nChapter 4\nDynamic Programming\nThe term dynamic programming (DP) refers to a collection of algorithms that\ncan be used to compute optimal policies given a perfect model of the envi-\nronment as a Markov decision process (MDP). Classical DP algorithms are of\nlimited utility in reinforcement learning both because of their assumption of\na perfect model and because of their great computational expense, but they\nare still important theoretically. DP provides an essential foundation for the\nunderstanding of the methods presented in the rest of this book. In fact, all\nof these methods can be viewed as attempts to achieve much the same e\u000bect\nas DP, only with less computation and without assuming a perfect model of\nthe environment.\nStarting with this chapter, we usually assume that the environment is a\n\fnite MDP. That is, we assume that its state, action, and reward sets, S,\nA(s), and R, fors2S, are \fnite, and that its dynamics are given by a set of\nprobabilities p(s0;rjs;a), for alls2S,a2A(s),r2R, ands02S+(S+isS\nplus a terminal state if the problem is episodic). Although DP ideas can be\napplied to problems with continuous state and action spaces, exact solutions\nare possible only in special cases. A common way of obtaining approximate\nsolutions for tasks with continuous states and actions is to quantize the state\nand action spaces and then apply \fnite-state DP methods. The methods we\nexplore in Chapter 9 are applicable to continuous problems and are a signi\fcant\nextension of that approach.\nThe key idea of DP, and of reinforcement learning generally, is the use of\nvalue functions to organize and structure the search for good policies. In this\nchapter we show how DP can be used to compute the value functions de\fned\nin Chapter 3. As discussed there, we can easily obtain optimal policies once\nwe have found the optimal value functions, v\u0003orq\u0003, which satisfy the Bellman\n89\n90 CHAPTER 4. DYNAMIC PROGRAMMING\noptimality equations:\nv\u0003(s) = max\naE[Rt+1+\rv\u0003(St+1)jSt=s;At=a]\n= max\naX\ns0;rp(s0;rjs;a)h\nr+\rv\u0003(s0)i\n(4.1)\nor\nq\u0003(s;a) = Eh\nRt+1+\rmax\na0q\u0003(St+1;a0)\f\f\fSt=s;At=ai\n=X\ns0;rp(s0;rjs;a)h\nr+\rmax\na0q\u0003(s0;a0)i\n; (4.2)\nfor alls2S,a2A(s), ands02S+. As we shall see, DP algorithms are\nobtained by turning Bellman equations such as these into assignments, that is,\ninto update rules for improving approximations of the desired value functions.\n4.1 Policy Evaluation\nFirst we consider how to compute the state-value function v\u0019for an arbitrary\npolicy\u0019. This is called policy evaluation in the DP literature. We also refer\nto it as the prediction problem . Recall from Chapter 3 that, for all s2S,\nv\u0019(s) = E\u0019\u0002\nRt+1+\rRt+2+\r2Rt+3+\u0001\u0001\u0001\f\fSt=s\u0003\n=E\u0019[Rt+1+\rv\u0019(St+1)jSt=s] (4.3)\n=X\na\u0019(ajs)X\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n; (4.4)\nwhere\u0019(ajs) is the probability of taking action ain statesunder policy \u0019, and\nthe expectations are subscripted by \u0019to indicate that they are conditional on\n\u0019being followed. The existence and uniqueness of v\u0019are guaranteed as long\nas either\r <1 or eventual termination is guaranteed from all states under the\npolicy\u0019.\nIf the environment's dynamics are completely known, then (4.4) is a system\nofjSjsimultaneous linear equations in jSjunknowns (the v\u0019(s),s2S). In\nprinciple, its solution is a straightforward, if tedious, computation. For our\npurposes, iterative solution methods are most suitable. Consider a sequence of\napproximate value functions v0;v1;v2;:::, each mapping S+toR. The initial\napproximation, v0, is chosen arbitrarily (except that the terminal state, if any,\n4.1. POLICY EVALUATION 91\nmust be given value 0), and each successive approximation is obtained by using\nthe Bellman equation for v\u0019(3.12) as an update rule:\nvk+1(s) = E\u0019[Rt+1+\rvk(St+1)jSt=s]\n=X\na\u0019(ajs)X\ns0;rp(s0;rjs;a)h\nr+\rvk(s0)i\n; (4.5)\nfor alls2S. Clearly,vk=v\u0019is a \fxed point for this update rule because\nthe Bellman equation for v\u0019assures us of equality in this case. Indeed, the\nsequencefvkgcan be shown in general to converge to v\u0019ask!1 under the\nsame conditions that guarantee the existence of v\u0019. This algorithm is called\niterative policy evaluation .\nTo produce each successive approximation, vk+1fromvk, iterative policy\nevaluation applies the same operation to each state s: it replaces the old value\nofswith a new value obtained from the old values of the successor states\nofs, and the expected immediate rewards, along all the one-step transitions\npossible under the policy being evaluated. We call this kind of operation a\nfull backup . Each iteration of iterative policy evaluation backs up the value of\nevery state once to produce the new approximate value function vk+1. There\nare several di\u000berent kinds of full backups, depending on whether a state (as\nhere) or a state{action pair is being backed up, and depending on the precise\nway the estimated values of the successor states are combined. All the backups\ndone in DP algorithms are called fullbackups because they are based on all\npossible next states rather than on a sample next state. The nature of a\nbackup can be expressed in an equation, as above, or in a backup diagram\nlike those introduced in Chapter 3. For example, Figure 3.4a is the backup\ndiagram corresponding to the full backup used in iterative policy evaluation.\nTo write a sequential computer program to implement iterative policy eval-\nuation, as given by (4.5), you would have to use two arrays, one for the old\nvalues,vk(s), and one for the new values, vk+1(s). This way, the new values\ncan be computed one by one from the old values without the old values being\nchanged. Of course it is easier to use one array and update the values \\in\nplace,\" that is, with each new backed-up value immediately overwriting the\nold one. Then, depending on the order in which the states are backed up,\nsometimes new values are used instead of old ones on the right-hand side of\n(4.5). This slightly di\u000berent algorithm also converges to v\u0019; in fact, it usually\nconverges faster than the two-array version, as you might expect, since it uses\nnew data as soon as they are available. We think of the backups as being done\nin a sweep through the state space. For the in-place algorithm, the order in\nwhich states are backed up during the sweep has a signi\fcant in\ruence on the\nrate of convergence. We usually have the in-place version in mind when we\nthink of DP algorithms.\n92 CHAPTER 4. DYNAMIC PROGRAMMING\nInput\u0019, the policy to be evaluated\nInitialize an array V(s) = 0, for all s2S+\nRepeat\n\u0001 0\nFor eachs2S:\nv V(s)\nV(s) P\na\u0019(ajs)P\ns0;rp(s0;rjs;a)\u0002\nr+\rV(s0)\u0003\n\u0001 max(\u0001;jv\u0000V(s)j)\nuntil \u0001<\u0012(a small positive number)\nOutputV\u0019v\u0019\nFigure 4.1: Iterative policy evaluation.\nAnother implementation point concerns the termination of the algorithm.\nFormally, iterative policy evaluation converges only in the limit, but in practice\nit must be halted short of this. A typical stopping condition for iterative policy\nevaluation is to test the quantity max s2Sjvk+1(s)\u0000vk(s)jafter each sweep and\nstop when it is su\u000eciently small. Figure 4.1 gives a complete algorithm for\niterative policy evaluation with this stopping criterion.\nExample 4.1 Consider the 4\u00024 gridworld shown below.\nactionsr  =  !1on all transitions1234567891011121314R\nThe nonterminal states are S=f1;2;:::; 14g. There are four actions pos-\nsible in each state, A=fup,down ,right ,leftg, which deterministically\ncause the corresponding state transitions, except that actions that would take\nthe agent o\u000b the grid in fact leave the state unchanged. Thus, for instance,\np(6j5;right ) = 1,p(10j5;right ) = 0, andp(7j7;right ) = 1. This is an undis-\ncounted, episodic task. The reward is \u00001 on all transitions until the terminal\nstate is reached. The terminal state is shaded in the \fgure (although it is\nshown in two places, it is formally one state). The expected reward function is\nthusr(s;a;s0) =\u00001 for all states s;s0and actions a. Suppose the agent follows\nthe equiprobable random policy (all actions equally likely). The left side of\nFigure 4.2 shows the sequence of value functions fvkgcomputed by iterative\npolicy evaluation. The \fnal estimate is in fact v\u0019, which in this case gives for\neach state the negation of the expected number of steps from that state until\n4.1. POLICY EVALUATION 93\n 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.7-2.0-2.0-1.7-2.0-2.0-2.0-2.0-2.0-2.0-1.7-2.0-2.0-1.7-2.4-2.9-3.0-2.4-2.9-3.0-2.9-2.9-3.0-2.9-2.4-3.0-2.9-2.4-6.1-8.4-9.0-6.1-7.7-8.4-8.4-8.4-8.4-7.7-6.1-9.0-8.4-6.1-14.-20.-22.-14.-18.-20.-20.-20.-20.-18.-14.-22.-20.-14.Vk  for theRandom PolicyGreedy Policyw.r.t. Vkk = 0k = 1k = 2\nk = 10k = !k = 3optimal policyrandom policy 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0vkvk\nFigure 4.2: Convergence of iterative policy evaluation on a small gridworld.\nThe left column is the sequence of approximations of the state-value function\nfor the random policy (all actions equal). The right column is the sequence\nof greedy policies corresponding to the value function estimates (arrows are\nshown for all actions achieving the maximum). The last policy is guaranteed\nonly to be an improvement over the random policy, but in this case it, and all\npolicies after the third iteration, are optimal.\n94 CHAPTER 4. DYNAMIC PROGRAMMING\ntermination.\n4.2 Policy Improvement\nOur reason for computing the value function for a policy is to help \fnd better\npolicies. Suppose we have determined the value function v\u0019for an arbitrary\ndeterministic policy \u0019. For some state swe would like to know whether or not\nwe should change the policy to deterministically choose an action a6=\u0019(s).\nWe know how good it is to follow the current policy from s|that isv\u0019(s)|but\nwould it be better or worse to change to the new policy? One way to answer\nthis question is to consider selecting ainsand thereafter following the existing\npolicy,\u0019. The value of this way of behaving is\nq\u0019(s;a) = E\u0019[Rt+1+\rv\u0019(St+1)jSt=s;At=a] (4.6)\n=X\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n:\nThe key criterion is whether this is greater than or less than v\u0019(s). If it is\ngreater|that is, if it is better to select aonce insand thereafter follow \u0019\nthan it would be to follow \u0019all the time|then one would expect it to be\nbetter still to select aevery time sis encountered, and that the new policy\nwould in fact be a better one overall.\nThat this is true is a special case of a general result called the policy\nimprovement theorem . Let\u0019and\u00190be any pair of deterministic policies such\nthat, for all s2S,\nq\u0019(s;\u00190(s))\u0015v\u0019(s): (4.7)\nThen the policy \u00190must be as good as, or better than, \u0019. That is, it must\nobtain greater or equal expected return from all states s2S:\nv\u00190(s)\u0015v\u0019(s): (4.8)\nMoreover, if there is strict inequality of (4.7) at any state, then there must be\nstrict inequality of (4.8) at at least one state. This result applies in particular\nto the two policies that we considered in the previous paragraph, an original\ndeterministic policy, \u0019, and a changed policy, \u00190, that is identical to \u0019except\nthat\u00190(s) =a6=\u0019(s). Obviously, (4.7) holds at all states other than s. Thus,\nifq\u0019(s;a)>v\u0019(s), then the changed policy is indeed better than \u0019.\nThe idea behind the proof of the policy improvement theorem is easy to\nunderstand. Starting from (4.7), we keep expanding the q\u0019side and reapplying\n4.2. POLICY IMPROVEMENT 95\n(4.7) until we get v\u00190(s):\nv\u0019(s)\u0014q\u0019(s;\u00190(s))\n=E\u00190[Rt+1+\rv\u0019(St+1)jSt=s]\n\u0014E\u00190[Rt+1+\rq\u0019(St+1;\u00190(St+1))jSt=s]\n=E\u00190[Rt+1+\rE\u00190[Rt+2+\rv\u0019(St+2)]jSt=s]\n=E\u00190\u0002\nRt+1+\rRt+2+\r2v\u0019(St+2)\f\fSt=s\u0003\n\u0014E\u00190\u0002\nRt+1+\rRt+2+\r2Rt+3+\r3v\u0019(St+3)\f\fSt=s\u0003\n...\n\u0014E\u00190\u0002\nRt+1+\rRt+2+\r2Rt+3+\r3Rt+4+\u0001\u0001\u0001\f\fSt=s\u0003\n=v\u00190(s):\nSo far we have seen how, given a policy and its value function, we can easily\nevaluate a change in the policy at a single state to a particular action. It is a\nnatural extension to consider changes at allstates and to allpossible actions,\nselecting at each state the action that appears best according to q\u0019(s;a). In\nother words, to consider the new greedy policy,\u00190, given by\n\u00190(s) = argmax\naq\u0019(s;a)\n= argmax\naE[Rt+1+\rv\u0019(St+1)jSt=s;At=a] (4.9)\n= argmax\naX\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n;\nwhere argmaxadenotes the value of aat which the expression that follows is\nmaximized (with ties broken arbitrarily). The greedy policy takes the action\nthat looks best in the short term|after one step of lookahead|according to\nv\u0019. By construction, the greedy policy meets the conditions of the policy\nimprovement theorem (4.7), so we know that it is as good as, or better than,\nthe original policy. The process of making a new policy that improves on an\noriginal policy, by making it greedy with respect to the value function of the\noriginal policy, is called policy improvement .\nSuppose the new greedy policy, \u00190, is as good as, but not better than, the\nold policy\u0019. Thenv\u0019=v\u00190, and from (4.9) it follows that for all s2S:\nv\u00190(s) = max\naE[Rt+1+\rv\u00190(St+1)jSt=s;At=a]\n= max\naX\ns0;rp(s0;rjs;a)h\nr+\rv\u00190(s0)i\n:\n96 CHAPTER 4. DYNAMIC PROGRAMMING\nBut this is the same as the Bellman optimality equation (4.1), and therefore,\nv\u00190must bev\u0003, and both \u0019and\u00190must be optimal policies. Policy improve-\nment thus must give us a strictly better policy except when the original policy\nis already optimal.\nSo far in this section we have considered the special case of deterministic\npolicies. In the general case, a stochastic policy \u0019speci\fes probabilities, \u0019(ajs),\nfor taking each action, a, in each state, s. We will not go through the details,\nbut in fact all the ideas of this section extend easily to stochastic policies. In\nparticular, the policy improvement theorem carries through as stated for the\nstochastic case, under the natural de\fnition:\nq\u0019(s;\u00190(s)) =X\na\u00190(ajs)q\u0019(s;a):\nIn addition, if there are ties in policy improvement steps such as (4.9)|that\nis, if there are several actions at which the maximum is achieved|then in the\nstochastic case we need not select a single action from among them. Instead,\neach maximizing action can be given a portion of the probability of being\nselected in the new greedy policy. Any apportioning scheme is allowed as long\nas all submaximal actions are given zero probability.\nThe last row of Figure 4.2 shows an example of policy improvement for\nstochastic policies. Here the original policy, \u0019, is the equiprobable random\npolicy, and the new policy, \u00190, is greedy with respect to v\u0019. The value function\nv\u0019is shown in the bottom-left diagram and the set of possible \u00190is shown in\nthe bottom-right diagram. The states with multiple arrows in the \u00190diagram\nare those in which several actions achieve the maximum in (4.9); any appor-\ntionment of probability among these actions is permitted. The value function\nof any such policy, v\u00190(s), can be seen by inspection to be either \u00001,\u00002, or\u00003\nat all states, s2S, whereasv\u0019(s) is at most\u000014. Thus,v\u00190(s)\u0015v\u0019(s), for all\ns2S, illustrating policy improvement. Although in this case the new policy\n\u00190happens to be optimal, in general only an improvement is guaranteed.\n4.3 Policy Iteration\nOnce a policy, \u0019, has been improved using v\u0019to yield a better policy, \u00190, we can\nthen compute v\u00190and improve it again to yield an even better \u001900. We can thus\nobtain a sequence of monotonically improving policies and value functions:\n\u00190E\u0000!v\u00190I\u0000!\u00191E\u0000!v\u00191I\u0000!\u00192E\u0000!\u0001\u0001\u0001I\u0000!\u0019\u0003E\u0000!v\u0003;\nwhereE\u0000!denotes a policy evaluation andI\u0000!denotes a policy improvement .\nEach policy is guaranteed to be a strict improvement over the previous one\n4.3. POLICY ITERATION 97\n1. Initialization\nV(s)2Rand\u0019(s)2A(s) arbitrarily for all s2S\n2. Policy Evaluation\nRepeat\n\u0001 0\nFor eachs2S:\nv V(s)\nV(s) P\ns0;rp(s0;rjs;\u0019(s))\u0002\nr+\rV(s0)\u0003\n\u0001 max(\u0001;jv\u0000V(s)j)\nuntil \u0001<\u0012 (a small positive number)\n3. Policy Improvement\npolicy-stable true\nFor eachs2S:\na \u0019(s)\n\u0019(s) argmaxaP\ns0;rp(s0;rjs;a)\u0002\nr+\rV(s0)\u0003\nIfa6=\u0019(s), then policy-stable false\nIfpolicy-stable , then stop and return Vand\u0019; else go to 2\nFigure 4.3: Policy iteration (using iterative policy evaluation) for v\u0003. This\nalgorithm has a subtle bug, in that it may never terminate if the policy con-\ntinually switches between two or more policies that are equally good. The bug\ncan be \fxed by adding additional \rags, but it makes the pseudocode so ugly\nthat it is not worth it. :-)\n(unless it is already optimal). Because a \fnite MDP has only a \fnite number\nof policies, this process must converge to an optimal policy and optimal value\nfunction in a \fnite number of iterations.\nThis way of \fnding an optimal policy is called policy iteration . A complete\nalgorithm is given in Figure 4.3. Note that each policy evaluation, itself an\niterative computation, is started with the value function for the previous policy.\nThis typically results in a great increase in the speed of convergence of policy\nevaluation (presumably because the value function changes little from one\npolicy to the next).\nPolicy iteration often converges in surprisingly few iterations. This is illus-\ntrated by the example in Figure 4.2. The bottom-left diagram shows the value\nfunction for the equiprobable random policy, and the bottom-right diagram\nshows a greedy policy for this value function. The policy improvement theo-\nrem assures us that these policies are better than the original random policy.\nIn this case, however, these policies are not just better, but optimal, proceed-\n98 CHAPTER 4. DYNAMIC PROGRAMMING\ning to the terminal states in the minimum number of steps. In this example,\npolicy iteration would \fnd the optimal policy after just one iteration.\nExample 4.2: Jack's Car Rental Jack manages two locations for a na-\ntionwide car rental company. Each day, some number of customers arrive at\neach location to rent cars. If Jack has a car available, he rents it out and is\ncredited $10 by the national company. If he is out of cars at that location,\nthen the business is lost. Cars become available for renting the day after they\nare returned. To help ensure that cars are available where they are needed,\nJack can move them between the two locations overnight, at a cost of $2 per\ncar moved. We assume that the number of cars requested and returned at\neach location are Poisson random variables, meaning that the probability that\nthe number is nis\u0015n\nn!e\u0000\u0015, where\u0015is the expected number. Suppose \u0015is 3\nand 4 for rental requests at the \frst and second locations and 3 and 2 for\nreturns. To simplify the problem slightly, we assume that there can be no\nmore than 20 cars at each location (any additional cars are returned to the\nnationwide company, and thus disappear from the problem) and a maximum\nof \fve cars can be moved from one location to the other in one night. We take\nthe discount rate to be \r= 0:9 and formulate this as a continuing \fnite MDP,\nwhere the time steps are days, the state is the number of cars at each location\nat the end of the day, and the actions are the net numbers of cars moved\nbetween the two locations overnight. Figure 4.4 shows the sequence of policies\nfound by policy iteration starting from the policy that never moves any cars.\n4.4 Value Iteration\nOne drawback to policy iteration is that each of its iterations involves policy\nevaluation, which may itself be a protracted iterative computation requiring\nmultiple sweeps through the state set. If policy evaluation is done iteratively,\nthen convergence exactly to v\u0019occurs only in the limit. Must we wait for\nexact convergence, or can we stop short of that? The example in Figure 4.2\ncertainly suggests that it may be possible to truncate policy evaluation. In\nthat example, policy evaluation iterations beyond the \frst three have no e\u000bect\non the corresponding greedy policy.\nIn fact, the policy evaluation step of policy iteration can be truncated in\nseveral ways without losing the convergence guarantees of policy iteration.\nOne important special case is when policy evaluation is stopped after just one\nsweep (one backup of each state). This algorithm is called value iteration . It\ncan be written as a particularly simple backup operation that combines the\n4.4. VALUE ITERATION 99\n4V612#Cars at second location042020020#Cars at first location115\n!1!2-4432432!3005\n!1!2!3!412340\"1\"0\"2\n!3!4!201234\n!1\"32\n!4!3!201345\n!1\"4\n#Cars at second location#Cars at first location5\n200020v4\nFigure 4.4: The sequence of policies found by policy iteration on Jack's car\nrental problem, and the \fnal state-value function. The \frst \fve diagrams show,\nfor each number of cars at each location at the end of the day, the number\nof cars to be moved from the \frst location to the second (negative numbers\nindicate transfers from the second location to the \frst). Each successive policy\nis a strict improvement over the previous policy, and the last policy is optimal.\n100 CHAPTER 4. DYNAMIC PROGRAMMING\npolicy improvement and truncated policy evaluation steps:\nvk+1(s) = max\naE[Rt+1+\rvk(St+1)jSt=s;At=a] (4.10)\n= max\naX\ns0;rp(s0;rjs;a)h\nr+\rvk(s0)i\n;\nfor alls2S. For arbitrary v0, the sequencefvkgcan be shown to converge to\nv\u0003under the same conditions that guarantee the existence of v\u0003.\nAnother way of understanding value iteration is by reference to the Bellman\noptimality equation (4.1). Note that value iteration is obtained simply by\nturning the Bellman optimality equation into an update rule. Also note how\nthe value iteration backup is identical to the policy evaluation backup (4.5)\nexcept that it requires the maximum to be taken over all actions. Another\nway of seeing this close relationship is to compare the backup diagrams for\nthese algorithms: Figure 3.4a shows the backup diagram for policy evaluation\nand Figure 3.7a shows the backup diagram for value iteration. These two are\nthe natural backup operations for computing v\u0019andv\u0003.\nFinally, let us consider how value iteration terminates. Like policy eval-\nuation, value iteration formally requires an in\fnite number of iterations to\nconverge exactly to v\u0003. In practice, we stop once the value function changes\nby only a small amount in a sweep. Figure 4.5 gives a complete value iteration\nalgorithm with this kind of termination condition.\nValue iteration e\u000bectively combines, in each of its sweeps, one sweep of\npolicy evaluation and one sweep of policy improvement. Faster convergence is\noften achieved by interposing multiple policy evaluation sweeps between each\npolicy improvement sweep. In general, the entire class of truncated policy\niteration algorithms can be thought of as sequences of sweeps, some of which\nuse policy evaluation backups and some of which use value iteration backups.\nSince the max operation in (4.10) is the only di\u000berence between these backups,\nthis just means that the max operation is added to some sweeps of policy\nevaluation. All of these algorithms converge to an optimal policy for discounted\n\fnite MDPs.\nExample 4.3: Gambler's Problem A gambler has the opportunity to\nmake bets on the outcomes of a sequence of coin \rips. If the coin comes up\nheads, he wins as many dollars as he has staked on that \rip; if it is tails, he\nloses his stake. The game ends when the gambler wins by reaching his goal\nof $100, or loses by running out of money. On each \rip, the gambler must\ndecide what portion of his capital to stake, in integer numbers of dollars. This\nproblem can be formulated as an undiscounted, episodic, \fnite MDP. The\nstate is the gambler's capital, s2f1;2;:::; 99gand the actions are stakes,\n4.5. ASYNCHRONOUS DYNAMIC PROGRAMMING 101\nInitialize array Varbitrarily (e.g., V(s) = 0 for all s2S+)\nRepeat\n\u0001 0\nFor eachs2S:\nv V(s)\nV(s) maxaP\ns0;rp(s0;rjs;a)\u0002\nr+\rV(s0)\u0003\n\u0001 max(\u0001;jv\u0000V(s)j)\nuntil \u0001<\u0012(a small positive number)\nOutput a deterministic policy, \u0019, such that\n\u0019(s) = argmaxaP\ns0;rp(s0;rjs;a)\u0002\nr+\rV(s0)\u0003\nFigure 4.5: Value iteration.\na2f0;1;:::; min(s;100\u0000s)g. The reward is zero on all transitions except\nthose on which the gambler reaches his goal, when it is +1. The state-value\nfunction then gives the probability of winning from each state. A policy is a\nmapping from levels of capital to stakes. The optimal policy maximizes the\nprobability of reaching the goal. Let phdenote the probability of the coin\ncoming up heads. If phis known, then the entire problem is known and it can\nbe solved, for instance, by value iteration. Figure 4.6 shows the change in the\nvalue function over successive sweeps of value iteration, and the \fnal policy\nfound, for the case of ph= 0:4. This policy is optimal, but not unique. In\nfact, there is a whole family of optimal policies, all corresponding to ties for\nthe argmax action selection with respect to the optimal value function. Can\nyou guess what the entire family looks like?\n4.5 Asynchronous Dynamic Programming\nA major drawback to the DP methods that we have discussed so far is that\nthey involve operations over the entire state set of the MDP, that is, they\nrequire sweeps of the state set. If the state set is very large, then even a single\nsweep can be prohibitively expensive. For example, the game of backgammon\nhas over 1020states. Even if we could perform the value iteration backup on\na million states per second, it would take over a thousand years to complete a\nsingle sweep.\nAsynchronous DP algorithms are in-place iterative DP algorithms that are\nnot organized in terms of systematic sweeps of the state set. These algorithms\nback up the values of states in any order whatsoever, using whatever values of\n102 CHAPTER 4. DYNAMIC PROGRAMMING\n99 75 50 25 111020304050100.20.40.60.81\n25 50 75 99\nCapitalCapitalValue\nestimates\nFinal\npolicy\n(stake)sweep 1\nsweep 2\nsweep 3sweep 32\nFigure 4.6: The solution to the gambler's problem for ph= 0:4. The upper\ngraph shows the value function found by successive sweeps of value iteration.\nThe lower graph shows the \fnal policy.\n4.5. ASYNCHRONOUS DYNAMIC PROGRAMMING 103\nother states happen to be available. The values of some states may be backed\nup several times before the values of others are backed up once. To converge\ncorrectly, however, an asynchronous algorithm must continue to backup the\nvalues of all the states: it can't ignore any state after some point in the\ncomputation. Asynchronous DP algorithms allow great \rexibility in selecting\nstates to which backup operations are applied.\nFor example, one version of asynchronous value iteration backs up the\nvalue, in place, of only one state, sk, on each step, k, using the value iteration\nbackup (4.10). If 0 \u0014\r <1, asymptotic convergence to v\u0003is guaranteed given\nonly that all states occur in the sequence fskgan in\fnite number of times\n(the sequence could even be stochastic). (In the undiscounted episodic case,\nit is possible that there are some orderings of backups that do not result in\nconvergence, but it is relatively easy to avoid these.) Similarly, it is possible\nto intermix policy evaluation and value iteration backups to produce a kind\nof asynchronous truncated policy iteration. Although the details of this and\nother more unusual DP algorithms are beyond the scope of this book, it is clear\nthat a few di\u000berent backups form building blocks that can be used \rexibly in\na wide variety of sweepless DP algorithms.\nOf course, avoiding sweeps does not necessarily mean that we can get away\nwith less computation. It just means that an algorithm does not need to get\nlocked into any hopelessly long sweep before it can make progress improving a\npolicy. We can try to take advantage of this \rexibility by selecting the states\nto which we apply backups so as to improve the algorithm's rate of progress.\nWe can try to order the backups to let value information propagate from state\nto state in an e\u000ecient way. Some states may not need their values backed up\nas often as others. We might even try to skip backing up some states entirely\nif they are not relevant to optimal behavior. Some ideas for doing this are\ndiscussed in Chapter 8.\nAsynchronous algorithms also make it easier to intermix computation with\nreal-time interaction. To solve a given MDP, we can run an iterative DP\nalgorithm at the same time that an agent is actually experiencing the MDP .\nThe agent's experience can be used to determine the states to which the DP\nalgorithm applies its backups. At the same time, the latest value and policy\ninformation from the DP algorithm can guide the agent's decision-making. For\nexample, we can apply backups to states as the agent visits them. This makes\nit possible to focus the DP algorithm's backups onto parts of the state set that\nare most relevant to the agent. This kind of focusing is a repeated theme in\nreinforcement learning.\n104 CHAPTER 4. DYNAMIC PROGRAMMING\n4.6 Generalized Policy Iteration\nPolicy iteration consists of two simultaneous, interacting processes, one making\nthe value function consistent with the current policy (policy evaluation), and\nthe other making the policy greedy with respect to the current value function\n(policy improvement). In policy iteration, these two processes alternate, each\ncompleting before the other begins, but this is not really necessary. In value\niteration, for example, only a single iteration of policy evaluation is performed\nin between each policy improvement. In asynchronous DP methods, the eval-\nuation and improvement processes are interleaved at an even \fner grain. In\nsome cases a single state is updated in one process before returning to the\nother. As long as both processes continue to update all states, the ultimate\nresult is typically the same|convergence to the optimal value function and an\noptimal policy.\nWe use the term generalized policy iteration (GPI) to refer to the general\nidea of letting policy evaluation and policy improvement processes interact,\nindependent of the granularity and other details of the two processes. Almost\nall reinforcement learning methods are well described as GPI. That is, all\nhave identi\fable policies and value functions, with the policy always being\nimproved with respect to the value function and the value function always\nbeing driven toward the value function for the policy. This overall schema for\nGPI is illustrated in Figure 4.7.\nIt is easy to see that if both the evaluation process and the improvement\nprocess stabilize, that is, no longer produce changes, then the value function\nand policy must be optimal. The value function stabilizes only when it is con-\nsistent with the current policy, and the policy stabilizes only when it is greedy\nwith respect to the current value function. Thus, both processes stabilize only\nwhen a policy has been found that is greedy with respect to its own evaluation\nfunction. This implies that the Bellman optimality equation (4.1) holds, and\nthus that the policy and the value function are optimal.\nThe evaluation and improvement processes in GPI can be viewed as both\ncompeting and cooperating. They compete in the sense that they pull in op-\nposing directions. Making the policy greedy with respect to the value function\ntypically makes the value function incorrect for the changed policy, and mak-\ning the value function consistent with the policy typically causes that policy no\nlonger to be greedy. In the long run, however, these two processes interact to\n\fnd a single joint solution: the optimal value function and an optimal policy.\nOne might also think of the interaction between the evaluation and im-\nprovement processes in GPI in terms of two constraints or goals|for example,\nas two lines in two-dimensional space:\n4.6. GENERALIZED POLICY ITERATION 105\n\u03c0vevaluationv \u2192 v\u03c0improvement\u03c0\u2192greedy(v)v*\u03c0*\nFigure 4.7: Generalized policy iteration: Value and policy functions interact\nuntil they are optimal and thus consistent with each other.\nv0  \u03c00  v  =  v\u03c0\u03c0  =  greedy(v)v*  \u03c0*  \nAlthough the real geometry is much more complicated than this, the diagram\nsuggests what happens in the real case. Each process drives the value function\nor policy toward one of the lines representing a solution to one of the two\ngoals. The goals interact because the two lines are not orthogonal. Driving\ndirectly toward one goal causes some movement away from the other goal.\nInevitably, however, the joint process is brought closer to the overall goal of\noptimality. The arrows in this diagram correspond to the behavior of policy\niteration in that each takes the system all the way to achieving one of the two\ngoals completely. In GPI one could also take smaller, incomplete steps toward\neach goal. In either case, the two processes together achieve the overall goal\n106 CHAPTER 4. DYNAMIC PROGRAMMING\nof optimality even though neither is attempting to achieve it directly.\n4.7 E\u000eciency of Dynamic Programming\nDP may not be practical for very large problems, but compared with other\nmethods for solving MDPs, DP methods are actually quite e\u000ecient. If we\nignore a few technical details, then the (worst case) time DP methods take to\n\fnd an optimal policy is polynomial in the number of states and actions. If n\nandmdenote the number of states and actions, this means that a DP method\ntakes a number of computational operations that is less than some polynomial\nfunction of nandm. A DP method is guaranteed to \fnd an optimal policy in\npolynomial time even though the total number of (deterministic) policies is mn.\nIn this sense, DP is exponentially faster than any direct search in policy space\ncould be, because direct search would have to exhaustively examine each policy\nto provide the same guarantee. Linear programming methods can also be used\nto solve MDPs, and in some cases their worst-case convergence guarantees are\nbetter than those of DP methods. But linear programming methods become\nimpractical at a much smaller number of states than do DP methods (by a\nfactor of about 100). For the largest problems, only DP methods are feasible.\nDP is sometimes thought to be of limited applicability because of the curse\nof dimensionality (Bellman, 1957a), the fact that the number of states often\ngrows exponentially with the number of state variables. Large state sets do\ncreate di\u000eculties, but these are inherent di\u000eculties of the problem, not of DP\nas a solution method. In fact, DP is comparatively better suited to handling\nlarge state spaces than competing methods such as direct search and linear\nprogramming.\nIn practice, DP methods can be used with today's computers to solve\nMDPs with millions of states. Both policy iteration and value iteration are\nwidely used, and it is not clear which, if either, is better in general. In practice,\nthese methods usually converge much faster than their theoretical worst-case\nrun times, particularly if they are started with good initial value functions or\npolicies.\nOn problems with large state spaces, asynchronous DP methods are of-\nten preferred. To complete even one sweep of a synchronous method requires\ncomputation and memory for every state. For some problems, even this much\nmemory and computation is impractical, yet the problem is still potentially\nsolvable because only a relatively few states occur along optimal solution tra-\njectories. Asynchronous methods and other variations of GPI can be applied in\nsuch cases and may \fnd good or optimal policies much faster than synchronous\n4.8. SUMMARY 107\nmethods can.\n4.8 Summary\nIn this chapter we have become familiar with the basic ideas and algorithms of\ndynamic programming as they relate to solving \fnite MDPs. Policy evaluation\nrefers to the (typically) iterative computation of the value functions for a\ngiven policy. Policy improvement refers to the computation of an improved\npolicy given the value function for that policy. Putting these two computations\ntogether, we obtain policy iteration and value iteration , the two most popular\nDP methods. Either of these can be used to reliably compute optimal policies\nand value functions for \fnite MDPs given complete knowledge of the MDP.\nClassical DP methods operate in sweeps through the state set, performing\nafull backup operation on each state. Each backup updates the value of one\nstate based on the values of all possible successor states and their probabilities\nof occurring. Full backups are closely related to Bellman equations: they are\nlittle more than these equations turned into assignment statements. When the\nbackups no longer result in any changes in value, convergence has occurred to\nvalues that satisfy the corresponding Bellman equation. Just as there are\nfour primary value functions ( v\u0019,v\u0003,q\u0019, andq\u0003), there are four corresponding\nBellman equations and four corresponding full backups. An intuitive view of\nthe operation of backups is given by backup diagrams .\nInsight into DP methods and, in fact, into almost all reinforcement learn-\ning methods, can be gained by viewing them as generalized policy iteration\n(GPI). GPI is the general idea of two interacting processes revolving around\nan approximate policy and an approximate value function. One process takes\nthe policy as given and performs some form of policy evaluation, changing the\nvalue function to be more like the true value function for the policy. The other\nprocess takes the value function as given and performs some form of policy\nimprovement, changing the policy to make it better, assuming that the value\nfunction is its value function. Although each process changes the basis for the\nother, overall they work together to \fnd a joint solution: a policy and value\nfunction that are unchanged by either process and, consequently, are optimal.\nIn some cases, GPI can be proved to converge, most notably for the classical\nDP methods that we have presented in this chapter. In other cases convergence\nhas not been proved, but still the idea of GPI improves our understanding of\nthe methods.\nIt is not necessary to perform DP methods in complete sweeps through the\nstate set. Asynchronous DP methods are in-place iterative methods that back\n108 CHAPTER 4. DYNAMIC PROGRAMMING\nup states in an arbitrary order, perhaps stochastically determined and using\nout-of-date information. Many of these methods can be viewed as \fne-grained\nforms of GPI.\nFinally, we note one last special property of DP methods. All of them\nupdate estimates of the values of states based on estimates of the values of\nsuccessor states. That is, they update estimates on the basis of other estimates.\nWe call this general idea bootstrapping . Many reinforcement learning methods\nperform bootstrapping, even those that do not require, as DP requires, a\ncomplete and accurate model of the environment. In the next chapter we\nexplore reinforcement learning methods that do not require a model and do\nnot bootstrap. In the chapter after that we explore methods that do not require\na model but do bootstrap. These key features and properties are separable,\nyet can be mixed in interesting combinations.\nBibliographical and Historical Remarks\nThe term \\dynamic programming\" is due to Bellman (1957a), who showed\nhow these methods could be applied to a wide range of problems. Extensive\ntreatments of DP can be found in many texts, including Bertsekas (1995),\nBertsekas and Tsitsiklis (1996), Dreyfus and Law (1977), Ross (1983), White\n(1969), and Whittle (1982, 1983). Our interest in DP is restricted to its use\nin solving MDPs, but DP also applies to other types of problems. Kumar and\nKanal (1988) provide a more general look at DP.\nTo the best of our knowledge, the \frst connection between DP and rein-\nforcement learning was made by Minsky (1961) in commenting on Samuel's\ncheckers player. In a footnote, Minsky mentioned that it is possible to ap-\nply DP to problems in which Samuel's backing-up process can be handled in\nclosed analytic form. This remark may have misled arti\fcial intelligence re-\nsearchers into believing that DP was restricted to analytically tractable prob-\nlems and therefore largely irrelevant to arti\fcial intelligence. Andreae (1969b)\nmentioned DP in the context of reinforcement learning, speci\fcally policy iter-\nation, although he did not make speci\fc connections between DP and learning\nalgorithms. Werbos (1977) suggested an approach to approximating DP called\n\\heuristic dynamic programming\" that emphasizes gradient-descent methods\nfor continuous-state problems (Werbos, 1982, 1987, 1988, 1989, 1992). These\nmethods are closely related to the reinforcement learning algorithms that we\ndiscuss in this book. Watkins (1989) was explicit in connecting reinforcement\nlearning to DP, characterizing a class of reinforcement learning methods as\n\\incremental dynamic programming.\"\n4.8. SUMMARY 109\n4.1{4 These sections describe well-established DP algorithms that are covered\nin any of the general DP references cited above. The policy improve-\nment theorem and the policy iteration algorithm are due to Bellman\n(1957a) and Howard (1960). Our presentation was in\ruenced by the\nlocal view of policy improvement taken by Watkins (1989). Our discus-\nsion of value iteration as a form of truncated policy iteration is based\non the approach of Puterman and Shin (1978), who presented a class\nof algorithms called modi\fed policy iteration , which includes policy it-\neration and value iteration as special cases. An analysis showing how\nvalue iteration can be made to \fnd an optimal policy in \fnite time is\ngiven by Bertsekas (1987).\nIterative policy evaluation is an example of a classical successive ap-\nproximation algorithm for solving a system of linear equations. The\nversion of the algorithm that uses two arrays, one holding the old val-\nues while the other is updated, is often called a Jacobi-style algorithm,\nafter Jacobi's classical use of this method. It is also sometimes called\nasynchronous algorithm because it can be performed in parallel, with\nseparate processors simultaneously updating the values of individual\nstates using input from other processors. The second array is needed\nto simulate this parallel computation sequentially. The in-place version\nof the algorithm is often called a Gauss{Seidel-style algorithm after\nthe classical Gauss{Seidel algorithm for solving systems of linear equa-\ntions. In addition to iterative policy evaluation, other DP algorithms\ncan be implemented in these di\u000berent versions. Bertsekas and Tsit-\nsiklis (1989) provide excellent coverage of these variations and their\nperformance di\u000berences.\n4.5 Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who\nalso called them distributed DP algorithms. The original motivation for\nasynchronous DP was its implementation on a multiprocessor system\nwith communication delays between processors and no global synchro-\nnizing clock. These algorithms are extensively discussed by Bertsekas\nand Tsitsiklis (1989). Jacobi-style and Gauss{Seidel-style DP algo-\nrithms are special cases of the asynchronous version. Williams and\nBaird (1990) presented DP algorithms that are asynchronous at a \fner\ngrain than the ones we have discussed: the backup operations them-\nselves are broken into steps that can be performed asynchronously.\n4.7 This section, written with the help of Michael Littman, is based on\nLittman, Dean, and Kaelbling (1995).\n110 CHAPTER 4. DYNAMIC PROGRAMMING\nExercises\nExercise 4.1 If\u0019is the equiprobable random policy, what is q\u0019(11;down )?\nWhat isq\u0019(7;down )?\nExercise 4.2 Suppose a new state 15 is added to the gridworld just below\nstate 13, and its actions, left ,up,right , and down , take the agent to states\n12, 13, 14, and 15, respectively. Assume that the transitions from the original\nstates are unchanged. What, then, is v\u0019(15) for the equiprobable random\npolicy? Now suppose the dynamics of state 13 are also changed, such that\naction down from state 13 takes the agent to the new state 15. What is v\u0019(15)\nfor the equiprobable random policy in this case?\nExercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for\nthe action-value function q\u0019and its successive approximation by a sequence of\nfunctionsq0;q1;q2;:::?\nExercise 4.4 In some undiscounted episodic tasks there may be policies\nfor which eventual termination is not guaranteed. For example, in the grid\nproblem above it is possible to go back and forth between two states forever.\nIn a task that is otherwise perfectly sensible, v\u0019(s) may be negative in\fnity\nfor some policies and states, in which case the algorithm for iterative policy\nevaluation given in Figure 4.1 will not terminate. As a purely practical matter,\nhow might we amend this algorithm to assure termination even in this case?\nAssume that eventual termination isguaranteed under the optimal policy.\nExercise 4.5 (programming) Write a program for policy iteration and\nre-solve Jack's car rental problem with the following changes. One of Jack's\nemployees at the \frst location rides a bus home each night and lives near\nthe second location. She is happy to shuttle one car to the second location\nfor free. Each additional car still costs $2, as do all cars moved in the other\ndirection. In addition, Jack has limited parking space at each location. If\nmore than 10 cars are kept overnight at a location (after any moving of cars),\nthen an additional cost of $4 must be incurred to use a second parking lot\n(independent of how many cars are kept there). These sorts of nonlinearities\nand arbitrary dynamics often occur in real problems and cannot easily be\nhandled by optimization methods other than dynamic programming. To check\nyour program, \frst replicate the results given for the original problem. If your\ncomputer is too slow for the full problem, cut all the numbers of cars in half.\nExercise 4.6 How would policy iteration be de\fned for action values? Give\na complete algorithm for computing q\u0003, analogous to Figure 4.3 for computing\nv\u0003. Please pay special attention to this exercise, because the ideas involved\nwill be used throughout the rest of the book.\n4.8. SUMMARY 111\nExercise 4.7 Suppose you are restricted to considering only policies that are\n\u000f-soft, meaning that the probability of selecting each action in each state, s,\nis at least\u000f=jA(s)j. Describe qualitatively the changes that would be required\nin each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm\nforv\u0003(Figure 4.3).\nExercise 4.8 Why does the optimal policy for the gambler's problem have\nsuch a curious form? In particular, for capital of 50 it bets it all on one \rip,\nbut for capital of 51 it does not. Why is this a good policy?\nExercise 4.9 (programming) Implement value iteration for the gambler's\nproblem and solve it for ph= 0:25 andph= 0:55. In programming, you may\n\fnd it convenient to introduce two dummy states corresponding to termination\nwith capital of 0 and 100, giving them values of 0 and 1 respectively. Show\nyour results graphically, as in Figure 4.6. Are your results stable as \u0012!0?\nExercise 4.10 What is the analog of the value iteration backup (4.10) for\naction values, qk+1(s;a)?\n112 CHAPTER 4. DYNAMIC PROGRAMMING\nChapter 5\nMonte Carlo Methods\nIn this chapter we consider our \frst learning methods for estimating value func-\ntions and discovering optimal policies. Unlike the previous chapter, here we\ndo not assume complete knowledge of the environment. Monte Carlo methods\nrequire only experience |sample sequences of states, actions, and rewards from\nactual or simulated interaction with an environment. Learning from actual ex-\nperience is striking because it requires no prior knowledge of the environment's\ndynamics, yet can still attain optimal behavior. Learning from simulated ex-\nperience is also powerful. Although a model is required, the model need only\ngenerate sample transitions, not the complete probability distributions of all\npossible transitions that is required for dynamic programming (DP). In sur-\nprisingly many cases it is easy to generate experience sampled according to\nthe desired probability distributions, but infeasible to obtain the distributions\nin explicit form.\nMonte Carlo methods are ways of solving the reinforcement learning prob-\nlem based on averaging sample returns. To ensure that well-de\fned returns are\navailable, here we de\fne Monte Carlo methods only for episodic tasks. That is,\nwe assume experience is divided into episodes, and that all episodes eventually\nterminate no matter what actions are selected. Only on the completion of an\nepisode are value estimates and policies changed. Monte Carlo methods can\nthus be incremental in an episode-by-episode sense, but not in a step-by-step\n(online) sense. The term \\Monte Carlo\" is often used more broadly for any\nestimation method whose operation involves a signi\fcant random component.\nHere we use it speci\fcally for methods based on averaging complete returns\n(as opposed to methods that learn from partial returns, considered in the next\nchapter).\nMonte Carlo methods sample and average returns for each state{action pair\nmuch like the bandit methods we explored in Chapter 2 sample and average\n113\n114 CHAPTER 5. MONTE CARLO METHODS\nrewards for each action. The main di\u000berence is that now there are multiple\nstates, each acting like a di\u000berent bandit problem (like an associative-search\nor contextual bandit) and that the di\u000berent bandit problems are interrelated.\nThat is, the return after taking an action in one state depends on the actions\ntaken in later states in the same episode. Because all the action selections\nare undergoing learning, the problem becomes nonstationary from the point\nof view of the earlier state.\nTo handle the nonstationarity, we adapt the idea of general policy iteration\n(GPI) developed in Chapter 4 for DP. Whereas there we computed value func-\ntions from knowledge of the MDP, here we learn value functions from sample\nreturns with the MDP. The value functions and corresponding policies still\ninteract to attain optimality in essentially the same way (GPI). As in the DP\nchapter, \frst we consider the prediction problem (the computation of v\u0019and\nq\u0019for a \fxed arbitrary policy \u0019) then policy improvement, and, \fnally, the\ncontrol problem and its solution by GPI. Each of these ideas taken from DP is\nextended to the Monte Carlo case in which only sample experience is available.\n5.1 Monte Carlo Prediction\nWe begin by considering Monte Carlo methods for learning the state-value\nfunction for a given policy. Recall that the value of a state is the expected\nreturn|expected cumulative future discounted reward|starting from that\nstate. An obvious way to estimate it from experience, then, is simply to\naverage the returns observed after visits to that state. As more returns are\nobserved, the average should converge to the expected value. This idea under-\nlies all Monte Carlo methods.\nIn particular, suppose we wish to estimate v\u0019(s), the value of a state s\nunder policy \u0019, given a set of episodes obtained by following \u0019and passing\nthroughs. Each occurrence of state sin an episode is called a visit tos. Of\ncourse,smay be visited multiple times in the same episode; let us call the \frst\ntime it is visited in an episode the \frst visit tos. The \frst-visit MC method\nestimatesv\u0019(s) as the average of the returns following \frst visits to s, whereas\ntheevery-visit MC method averages the returns following all visits to s. These\ntwo Monte Carlo (MC) methods are very similar but have slightly di\u000berent\ntheoretical properties. First-visit MC has been most widely studied, dating\nback to the 1940s, and is the one we focus on in this chapter. Every-visit\nMC extends more naturally to function approximation and eligibility traces,\nas discussed in Chapters 9 and 7. First-visit MC is shown in procedural form\nin Figure 5.1.\n5.1. MONTE CARLO PREDICTION 115\nInitialize:\n\u0019 policy to be evaluated\nV an arbitrary state-value function\nReturns (s) an empty list, for all s2S\nRepeat forever:\nGenerate an episode using \u0019\nFor each state sappearing in the episode:\nG return following the \frst occurrence of s\nAppendGtoReturns (s)\nV(s) average(Returns (s))\nFigure 5.1: The \frst-visit MC method for estimating v\u0019. Note that we use a\ncapital letter Vfor the approximate value function because, after initialization,\nit soon becomes a random variable.\nBoth \frst-visit MC and every-visit MC converge to v\u0019(s) as the number\nof visits (or \frst visits) to sgoes to in\fnity. This is easy to see for the\ncase of \frst-visit MC. In this case each return is an independent, identically\ndistributed estimate of v\u0019(s) with \fnite variance. By the law of large numbers\nthe sequence of averages of these estimates converges to their expected value.\nEach average is itself an unbiased estimate, and the standard deviation of its\nerror falls as 1 =pn, wherenis the number of returns averaged. Every-visit\nMC is less straightforward, but its estimates also converge asymptotically to\nv\u0019(s) (Singh and Sutton, 1996).\nThe use of Monte Carlo methods is best illustrated through an example.\nExample 5.1: Blackjack The object of the popular casino card game of\nblackjack is to obtain cards the sum of whose numerical values is as great as\npossible without exceeding 21. All face cards count as 10, and an ace can count\neither as 1 or as 11. We consider the version in which each player competes\nindependently against the dealer. The game begins with two cards dealt to\nboth dealer and player. One of the dealer's cards is face up and the other is\nface down. If the player has 21 immediately (an ace and a 10-card), it is called\nanatural . He then wins unless the dealer also has a natural, in which case the\ngame is a draw. If the player does not have a natural, then he can request\nadditional cards, one by one ( hits), until he either stops ( sticks ) or exceeds 21\n(goes bust ). If he goes bust, he loses; if he sticks, then it becomes the dealer's\nturn. The dealer hits or sticks according to a \fxed strategy without choice:\nhe sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes\nbust, then the player wins; otherwise, the outcome|win, lose, or draw|is\ndetermined by whose \fnal sum is closer to 21.\n116 CHAPTER 5. MONTE CARLO METHODS\n+1\n!1\nA\nDealer showing1012Player sum21After 500,000 episodes After 10,000 episodes\nUsable\nace\nNo\nusable\nace\nFigure 5.2: Approximate state-value functions for the blackjack policy that\nsticks only on 20 or 21, computed by Monte Carlo policy evaluation.\nPlaying blackjack is naturally formulated as an episodic \fnite MDP. Each\ngame of blackjack is an episode. Rewards of +1, \u00001, and 0 are given for\nwinning, losing, and drawing, respectively. All rewards within a game are\nzero, and we do not discount ( \r= 1); therefore these terminal rewards are\nalso the returns. The player's actions are to hit or to stick. The states depend\non the player's cards and the dealer's showing card. We assume that cards\nare dealt from an in\fnite deck (i.e., with replacement) so that there is no\nadvantage to keeping track of the cards already dealt. If the player holds an\nace that he could count as 11 without going bust, then the ace is said to be\nusable . In this case it is always counted as 11 because counting it as 1 would\nmake the sum 11 or less, in which case there is no decision to be made because,\nobviously, the player should always hit. Thus, the player makes decisions on\nthe basis of three variables: his current sum (12{21), the dealer's one showing\ncard (ace{10), and whether or not he holds a usable ace. This makes for a\ntotal of 200 states.\nConsider the policy that sticks if the player's sum is 20 or 21, and other-\nwise hits. To \fnd the state-value function for this policy by a Monte Carlo\napproach, one simulates many blackjack games using the policy and averages\nthe returns following each state. Note that in this task the same state never\nrecurs within one episode, so there is no di\u000berence between \frst-visit and\nevery-visit MC methods. In this way, we obtained the estimates of the state-\nvalue function shown in Figure 5.2. The estimates for states with a usable ace\nare less certain and less regular because these states are less common. In any\nevent, after 500,000 games the value function is very well approximated.\n5.1. MONTE CARLO PREDICTION 117\nAlthough we have complete knowledge of the environment in this task, it\nwould not be easy to apply DP methods to compute the value function. DP\nmethods require the distribution of next events|in particular, they require\nthe quantities p(s0;rjs;a)|and it is not easy to determine these for blackjack.\nFor example, suppose the player's sum is 14 and he chooses to stick. What is\nhis expected reward as a function of the dealer's showing card? All of these\nexpected rewards and transition probabilities must be computed before DP\ncan be applied, and such computations are often complex and error-prone. In\ncontrast, generating the sample games required by Monte Carlo methods is\neasy. This is the case surprisingly often; the ability of Monte Carlo methods\nto work with sample episodes alone can be a signi\fcant advantage even when\none has complete knowledge of the environment's dynamics.\nCan we generalize the idea of backup diagrams to Monte Carlo algorithms?\nThe general idea of a backup diagram is to show at the top the root node to be\nupdated and to show below all the transitions and leaf nodes whose rewards\nand estimated values contribute to the update. For Monte Carlo estimation of\nv\u0019, the root is a state node, and below it is the entire trajectory of transitions\nalong a particular single episode, ending at the terminal state, as in Figure 5.3.\nWhereas the DP diagram (Figure 3.4a) shows all possible transitions, the\nMonte Carlo diagram shows only those sampled on the one episode. Whereas\nthe DP diagram includes only one-step transitions, the Monte Carlo diagram\ngoes all the way to the end of the episode. These di\u000berences in the diagrams\naccurately re\rect the fundamental di\u000berences between the algorithms.\nAn important fact about Monte Carlo methods is that the estimates for\neach state are independent. The estimate for one state does not build upon\nthe estimate of any other state, as is the case in DP. In other words, Monte\nCarlo methods do not bootstrap as we de\fned it in the previous chapter.\nIn particular, note that the computational expense of estimating the value\nof a single state is independent of the number of states. This can make Monte\nCarlo methods particularly attractive when one requires the value of only one\nor a subset of states. One can generate many sample episodes starting from\nthe states of interest, averaging returns from only these states ignoring all\nothers. This is a third advantage Monte Carlo methods can have over DP\nmethods (after the ability to learn from actual experience and from simulated\nexperience).\n118 CHAPTER 5. MONTE CARLO METHODS\nterminal state\nFigure 5.3: The backup diagram for Monte Carlo estimation of v\u0019.\nA bubble on a wire loopExample 5.2: Soap Bubble\nSuppose a wire frame forming a closed\nloop is dunked in soapy water to form a\nsoap surface or bubble conforming at its\nedges to the wire frame. If the geometry of\nthe wire frame is irregular but known, how\ncan you compute the shape of the surface?\nThe shape has the property that the total\nforce on each point exerted by neighbor-\ning points is zero (or else the shape would\nchange). This means that the surface's\nheight at any point is the average of its heights at points in a small circle\naround that point. In addition, the surface must meet at its boundaries with\nthe wire frame. The usual approach to problems of this kind is to put a grid\nover the area covered by the surface and solve for its height at the grid points\nby an iterative computation. Grid points at the boundary are forced to the\nwire frame, and all others are adjusted toward the average of the heights of\ntheir four nearest neighbors. This process then iterates, much like DP's iter-\native policy evaluation, and ultimately converges to a close approximation to\nthe desired surface.\nThis is similar to the kind of problem for which Monte Carlo methods\nwere originally designed. Instead of the iterative computation described above,\nimagine standing on the surface and taking a random walk, stepping randomly\nfrom grid point to neighboring grid point, with equal probability, until you\n5.2. MONTE CARLO ESTIMATION OF ACTION VALUES 119\nreach the boundary. It turns out that the expected value of the height at the\nboundary is a close approximation to the height of the desired surface at the\nstarting point (in fact, it is exactly the value computed by the iterative method\ndescribed above). Thus, one can closely approximate the height of the surface\nat a point by simply averaging the boundary heights of many walks started at\nthe point. If one is interested in only the value at one point, or any \fxed small\nset of points, then this Monte Carlo method can be far more e\u000ecient than the\niterative method based on local consistency.\n5.2 Monte Carlo Estimation of Action Values\nIf a model is not available, then it is particularly useful to estimate action\nvalues (the values of state{action pairs) rather than state values. With a\nmodel, state values alone are su\u000ecient to determine a policy; one simply looks\nahead one step and chooses whichever action leads to the best combination\nof reward and next state, as we did in the chapter on DP. Without a model,\nhowever, state values alone are not su\u000ecient. One must explicitly estimate the\nvalue of each action in order for the values to be useful in suggesting a policy.\nThus, one of our primary goals for Monte Carlo methods is to estimate q\u0003. To\nachieve this, we \frst consider the policy evaluation problem for action values.\nThe policy evaluation problem for action values is to estimate q\u0019(s;a),\nthe expected return when starting in state s, taking action a, and thereafter\nfollowing policy \u0019. The Monte Carlo methods for this are essentially the same\nas just presented for state values, except now we talk about visits to a state{\naction pair rather than to a state. A state{action pair s;ais said to be visited\nin an episode if ever the state sis visited and action ais taken in it. The every-\nvisit MC method estimates the value of a state{action pair as the average of the\nreturns that have followed visits all the visits to it. The \frst-visit MC method\naverages the returns following the \frst time in each episode that the state was\nvisited and the action was selected. These methods converge quadratically, as\nbefore, to the true expected values as the number of visits to each state{action\npair approaches in\fnity.\nThe only complication is that many state{action pairs may never be visited.\nIf\u0019is a deterministic policy, then in following \u0019one will observe returns only\nfor one of the actions from each state. With no returns to average, the Monte\nCarlo estimates of the other actions will not improve with experience. This is\na serious problem because the purpose of learning action values is to help in\nchoosing among the actions available in each state. To compare alternatives\nwe need to estimate the value of allthe actions from each state, not just the\none we currently favor.\n120 CHAPTER 5. MONTE CARLO METHODS\nThis is the general problem of maintaining exploration , as discussed in the\ncontext of the n-armed bandit problem in Chapter 2. For policy evaluation\nto work for action values, we must assure continual exploration. One way\nto do this is by specifying that the episodes start in a state{action pair , and\nthat every pair has a nonzero probability of being selected as the start. This\nguarantees that all state{action pairs will be visited an in\fnite number of times\nin the limit of an in\fnite number of episodes. We call this the assumption of\nexploring starts .\nThe assumption of exploring starts is sometimes useful, but of course it\ncannot be relied upon in general, particularly when learning directly from\nactual interaction with an environment. In that case the starting conditions are\nunlikely to be so helpful. The most common alternative approach to assuring\nthat all state{action pairs are encountered is to consider only policies that are\nstochastic with a nonzero probability of selecting all actions in each state. We\ndiscuss two important variants of this approach in later sections. For now, we\nretain the assumption of exploring starts and complete the presentation of a\nfull Monte Carlo control method.\n5.3 Monte Carlo Control\nWe are now ready to consider how Monte Carlo estimation can be used in\ncontrol, that is, to approximate optimal policies. The overall idea is to pro-\nceed according to the same pattern as in the DP chapter, that is, according to\nthe idea of generalized policy iteration (GPI). In GPI one maintains both an\napproximate policy and an approximate value function. The value function is\nrepeatedly altered to more closely approximate the value function for the cur-\nrent policy, and the policy is repeatedly improved with respect to the current\nvalue function:\n\u03c0qevaluationq \u2192 q\u03c0improvement\u03c0\u2192greedy(q)\nThese two kinds of changes work against each other to some extent, as each\ncreates a moving target for the other, but together they cause both policy and\n5.3. MONTE CARLO CONTROL 121\nvalue function to approach optimality.\nTo begin, let us consider a Monte Carlo version of classical policy iteration.\nIn this method, we perform alternating complete steps of policy evaluation and\npolicy improvement, beginning with an arbitrary policy \u00190and ending with\nthe optimal policy and optimal action-value function:\n\u00190E\u0000!q\u00190I\u0000!\u00191E\u0000!q\u00191I\u0000!\u00192E\u0000!\u0001\u0001\u0001I\u0000!\u0019\u0003E\u0000!q\u0003;\nwhereE\u0000!denotes a complete policy evaluation andI\u0000!denotes a complete\npolicy improvement. Policy evaluation is done exactly as described in the pre-\nceding section. Many episodes are experienced, with the approximate action-\nvalue function approaching the true function asymptotically. For the moment,\nlet us assume that we do indeed observe an in\fnite number of episodes and\nthat, in addition, the episodes are generated with exploring starts. Under\nthese assumptions, the Monte Carlo methods will compute each q\u0019kexactly,\nfor arbitrary \u0019k.\nPolicy improvement is done by making the policy greedy with respect to\nthe current value function. In this case we have an action -value function, and\ntherefore no model is needed to construct the greedy policy. For any action-\nvalue function q, the corresponding greedy policy is the one that, for each\ns2S, deterministically chooses an action with maximal action-value:\n\u0019(s) = arg max\naq(s;a): (5.1)\nPolicy improvement then can be done by constructing each \u0019k+1as the greedy\npolicy with respect to q\u0019k. The policy improvement theorem (Section 4.2) then\napplies to\u0019kand\u0019k+1because, for all s2S,\nq\u0019k(s;\u0019k+1(s)) =q\u0019k(s;argmax\naq\u0019k(s;a))\n= max\naq\u0019k(s;a)\n\u0015q\u0019k(s;\u0019k(s))\n=v\u0019k(s):\nAs we discussed in the previous chapter, the theorem assures us that each \u0019k+1\nis uniformly better than \u0019k, or just as good as \u0019k, in which case they are both\noptimal policies. This in turn assures us that the overall process converges\nto the optimal policy and optimal value function. In this way Monte Carlo\nmethods can be used to \fnd optimal policies given only sample episodes and\nno other knowledge of the environment's dynamics.\nWe made two unlikely assumptions above in order to easily obtain this\nguarantee of convergence for the Monte Carlo method. One was that the\n122 CHAPTER 5. MONTE CARLO METHODS\nepisodes have exploring starts, and the other was that policy evaluation could\nbe done with an in\fnite number of episodes. To obtain a practical algorithm\nwe will have to remove both assumptions. We postpone consideration of the\n\frst assumption until later in this chapter.\nFor now we focus on the assumption that policy evaluation operates on an\nin\fnite number of episodes. This assumption is relatively easy to remove. In\nfact, the same issue arises even in classical DP methods such as iterative policy\nevaluation, which also converge only asymptotically to the true value function.\nIn both DP and Monte Carlo cases there are two ways to solve the problem.\nOne is to hold \frm to the idea of approximating q\u0019kin each policy evaluation.\nMeasurements and assumptions are made to obtain bounds on the magnitude\nand probability of error in the estimates, and then su\u000ecient steps are taken\nduring each policy evaluation to assure that these bounds are su\u000eciently small.\nThis approach can probably be made completely satisfactory in the sense of\nguaranteeing correct convergence up to some level of approximation. However,\nit is also likely to require far too many episodes to be useful in practice on any\nbut the smallest problems.\nThe second approach to avoiding the in\fnite number of episodes nominally\nrequired for policy evaluation is to forgo trying to complete policy evaluation\nbefore returning to policy improvement. On each evaluation step we move the\nvalue function towardq\u0019k, but we do not expect to actually get close except\nover many steps. We used this idea when we \frst introduced the idea of GPI in\nSection 4.6. One extreme form of the idea is value iteration, in which only one\niteration of iterative policy evaluation is performed between each step of policy\nimprovement. The in-place version of value iteration is even more extreme;\nthere we alternate between improvement and evaluation steps for single states.\nFor Monte Carlo policy evaluation it is natural to alternate between eval-\nuation and improvement on an episode-by-episode basis. After each episode,\nthe observed returns are used for policy evaluation, and then the policy is\nimproved at all the states visited in the episode. A complete simple algorithm\nalong these lines is given in Figure 5.4. We call this algorithm Monte Carlo\nES, for Monte Carlo with Exploring Starts.\nIn Monte Carlo ES, all the returns for each state{action pair are accumu-\nlated and averaged, irrespective of what policy was in force when they were\nobserved. It is easy to see that Monte Carlo ES cannot converge to any sub-\noptimal policy. If it did, then the value function would eventually converge to\nthe value function for that policy, and that in turn would cause the policy to\nchange. Stability is achieved only when both the policy and the value func-\ntion are optimal. Convergence to this optimal \fxed point seems inevitable as\nthe changes to the action-value function decrease over time, but has not yet\n5.3. MONTE CARLO CONTROL 123\nInitialize, for all s2S,a2A(s):\nQ(s;a) arbitrary\n\u0019(s) arbitrary\nReturns (s;a) empty list\nRepeat forever:\nChooseS02SandA02A(S0) s.t. all pairs have probability >0\nGenerate an episode starting from S0;A0, following\u0019\nFor each pair s;aappearing in the episode:\nG return following the \frst occurrence of s;a\nAppendGtoReturns (s;a)\nQ(s;a) average(Returns (s;a))\nFor eachsin the episode:\n\u0019(s) argmaxaQ(s;a)\nFigure 5.4: Monte Carlo ES: A Monte Carlo control algorithm assuming ex-\nploring starts and that episodes always terminate for all policies.\nbeen formally proved. In our opinion, this is one of the most fundamental\nopen theoretical questions in reinforcement learning (for a partial solution, see\nTsitsiklis, 2002).\nExample 5.3: Solving Blackjack It is straightforward to apply Monte\nCarlo ES to blackjack. Since the episodes are all simulated games, it is easy\nto arrange for exploring starts that include all possibilities. In this case one\nsimply picks the dealer's cards, the player's sum, and whether or not the\nplayer has a usable ace, all at random with equal probability. As the initial\npolicy we use the policy evaluated in the previous blackjack example, that\nwhich sticks only on 20 or 21. The initial action-value function can be zero\nfor all state{action pairs. Figure 5.5 shows the optimal policy for blackjack\nfound by Monte Carlo ES. This policy is the same as the \\basic\" strategy\nof Thorp (1966) with the sole exception of the leftmost notch in the policy\nfor a usable ace, which is not present in Thorp's strategy. We are uncertain\nof the reason for this discrepancy, but con\fdent that what is shown here is\nindeed the optimal policy for the version of blackjack we have described.\n124 CHAPTER 5. MONTE CARLO METHODS\nUsableaceNousableace20\n10A23456789Dealer showingPlayer sumHITSTICK1921\n1112131415161718!*\n10A23456789HITSTICK201921\n1112131415161718V*211012A\nDealer showingPlayer sum10A1221+1\"1v*\nUsableaceNousableace20\n10A23456789Dealer showingPlayer sumHITSTICK1921\n1112131415161718!*\n10A23456789HITSTICK201921\n1112131415161718V*211012A\nDealer showingPlayer sum10A1221+1\"1v*UsableaceNousableace20\n10A23456789Dealer showingPlayer sumHITSTICK1921\n1112131415161718!*\n10A23456789HITSTICK201921\n1112131415161718V*211012A\nDealer showingPlayer sum10A1221+1\"1v*\nDealer showingPlayer sum**\nFigure 5.5: The optimal policy and state-value function for blackjack, found by\nMonte Carlo ES (Figure 5.4). The state-value function shown was computed\nfrom the action-value function found by Monte Carlo ES.\n5.4 Monte Carlo Control without Exploring\nStarts\nHow can we avoid the unlikely assumption of exploring starts? The only\ngeneral way to ensure that all actions are selected in\fnitely often is for the\nagent to continue to select them. There are two approaches to ensuring this,\nresulting in what we call on-policy methods and o\u000b-policy methods. On-\npolicy methods attempt to evaluate or improve the policy that is used to make\ndecisions, whereas o\u000b-policy methods evaluate or improve a policy di\u000berent\nfrom that used to generate the data. The Monte Carlo ES method developed\nabove is an example of an on-policy method. In this section we show how an\non-policy Monte Carlo control method can be designed that does not use the\nunrealistic assumption of exploring starts. O\u000b-policy methods are considered\nin the next section.\nIn on-policy control methods the policy is generally soft, meaning that\n\u0019(ajs)>0 for alls2Sand alla2A(s), but gradually shifted closer and\ncloser to a deterministic optimal policy. Many of the methods discussed in\nChapter 2 provide mechanisms for this. The on-policy method we present in\nthis section uses \"-greedy policies, meaning that most of the time they choose\nan action that has maximal estimated action value, but with probability \"\n5.4. MONTE CARLO CONTROL WITHOUT EXPLORING STARTS 125\nthey instead select an action at random. That is, all nongreedy actions are\ngiven the minimal probability of selection,\u000f\njA(s)j, and the remaining bulk of the\nprobability, 1\u0000\"+\u000f\njA(s)j, is given to the greedy action. The \"-greedy policies\nare examples of \"-soft policies, de\fned as policies for which \u0019(ajs)\u0015\u000f\njA(s)jfor\nall states and actions, for some \">0. Among\"-soft policies, \"-greedy policies\nare in some sense those that are closest to greedy.\nThe overall idea of on-policy Monte Carlo control is still that of GPI. As in\nMonte Carlo ES, we use \frst-visit MC methods to estimate the action-value\nfunction for the current policy. Without the assumption of exploring starts,\nhowever, we cannot simply improve the policy by making it greedy with respect\nto the current value function, because that would prevent further exploration\nof nongreedy actions. Fortunately, GPI does not require that the policy be\ntaken all the way to a greedy policy, only that it be moved toward a greedy\npolicy. In our on-policy method we will move it only to an \"-greedy policy.\nFor any\"-soft policy, \u0019, any\"-greedy policy with respect to q\u0019is guaranteed\nto be better than or equal to \u0019.\nThat any\"-greedy policy with respect to q\u0019is an improvement over any\n\"-soft policy \u0019is assured by the policy improvement theorem. Let \u00190be the\n\"-greedy policy. The conditions of the policy improvement theorem apply\nbecause for any s2S:\nq\u0019(s;\u00190(s)) =X\na\u00190(ajs)q\u0019(s;a)\n=\u000f\njA(s)jX\naq\u0019(s;a) + (1\u0000\") max\naq\u0019(s;a) (5.2)\n\u0015\u000f\njA(s)jX\naq\u0019(s;a) + (1\u0000\")X\na\u0019(ajs)\u0000\u000f\njA(s)j\n1\u0000\"q\u0019(s;a)\n(the sum is a weighted average with nonnegative weights summing to 1, and\nas such it must be less than or equal to the largest number averaged)\n=\u000f\njA(s)jX\naq\u0019(s;a)\u0000\u000f\njA(s)jX\naq\u0019(s;a) +X\na\u0019(ajs)q\u0019(s;a)\n=v\u0019(s):\nThus, by the policy improvement theorem, \u00190\u0015\u0019(i.e.,v\u00190(s)\u0015v\u0019(s), for all\ns2S). We now prove that equality can hold only when both \u00190and\u0019are\noptimal among the \"-soft policies, that is, when they are better than or equal\nto all other \"-soft policies.\nConsider a new environment that is just like the original environment, ex-\ncept with the requirement that policies be \"-soft \\moved inside\" the environ-\nment. The new environment has the same action and state set as the original\n126 CHAPTER 5. MONTE CARLO METHODS\nand behaves as follows. If in state sand taking action a, then with probability\n1\u0000\"the new environment behaves exactly like the old environment. With\nprobability \"it repicks the action at random, with equal probabilities, and\nthen behaves like the old environment with the new, random action. The best\none can do in this new environment with general policies is the same as the\nbest one could do in the original environment with \"-soft policies. Let ev\u0003and\neq\u0003denote the optimal value functions for the new environment. Then a policy\n\u0019is optimal among \"-soft policies if and only if v\u0019=ev\u0003. From the de\fnition\nofev\u0003we know that it is the unique solution to\nev\u0003(s) = (1\u0000\") max\naeq\u0003(s;a) +\u000f\njA(s)jX\naeq\u0003(s;a)\n= (1\u0000\") max\naX\ns0;rp(s0;rjs;a)h\nr+\rev\u0003(s0)i\n+\u000f\njA(s)jX\naX\ns0;rp(s0;rjs;a)h\nr+\rev\u0003(s0)i\n:\nWhen equality holds and the \"-soft policy \u0019is no longer improved, then we\nalso know, from (5.2), that\nv\u0019(s) = (1\u0000\") max\naq\u0019(s;a) +\u000f\njA(s)jX\naq\u0019(s;a)\n= (1\u0000\") max\naX\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n+\u000f\njA(s)jX\naX\ns0;rp(s0;rjs;a)h\nr+\rv\u0019(s0)i\n:\nHowever, this equation is the same as the previous one, except for the substi-\ntution ofv\u0019forev\u0003. Sinceev\u0003is the unique solution, it must be that v\u0019=ev\u0003.\nIn essence, we have shown in the last few pages that policy iteration works\nfor\"-soft policies. Using the natural notion of greedy policy for \"-soft policies,\none is assured of improvement on every step, except when the best policy has\nbeen found among the \"-soft policies. This analysis is independent of how\nthe action-value functions are determined at each stage, but it does assume\nthat they are computed exactly. This brings us to roughly the same point\nas in the previous section. Now we only achieve the best policy among the\n\"-soft policies, but on the other hand, we have eliminated the assumption of\nexploring starts. The complete algorithm is given in Figure 5.6.\n5.5. OFF-POLICY PREDICTION VIA IMPORTANCE SAMPLING 127\nInitialize, for all s2S,a2A(s):\nQ(s;a) arbitrary\nReturns (s;a) empty list\n\u0019(ajs) an arbitrary \"-soft policy\nRepeat forever:\n(a) Generate an episode using \u0019\n(b) For each pair s;aappearing in the episode:\nG return following the \frst occurrence of s;a\nAppendGtoReturns (s;a)\nQ(s;a) average(Returns (s;a))\n(c) For each sin the episode:\na\u0003 arg maxaQ(s;a)\nFor alla2A(s):\n\u0019(ajs) \u001a1\u0000\"+\"=jA(s)jifa=a\u0003\n\"=jA(s)j ifa6=a\u0003\nFigure 5.6: An on-policy \frst-visit MC control algorithm for \"-soft policies.\n5.5 O\u000b-policy Prediction via Importance Sam-\npling\nSo far we have considered methods for estimating the value functions for a\npolicy given an in\fnite supply of episodes generated using that policy. Suppose\nnow that all we have are episodes generated from a di\u000berent policy. That is,\nsuppose we wish to estimate v\u0019orq\u0019, but all we have are episodes following\nanother policy \u0016, where\u00166=\u0019. We call\u0019thetarget policy because learning its\nvalue function is the target of the learning process, and we call \u0016thebehavior\npolicy because it is the policy controlling the agent and generating behavior.\nThe overall problem is called o\u000b-policy learning because it is learning about a\npolicy given only experience \\o\u000b\" (not following) that policy.\nIn order to use episodes from \u0016to estimate values for \u0019, we must require\nthat every action taken under \u0019is also taken, at least occasionally, under \u0016.\nThat is, we require that \u0019(ajs)>0 implies\u0016(ajs)>0. This is called the\nassumption of coverage . It follows from coverage that \u0016must be stochastic\nin states where it is not identical to \u0019. The target policy \u0019, on the other\nhand, may be deterministic, and, in fact, this is a case of particular interest.\nTypically the target policy is the deterministic greedy policy with respect to\nthe current action-value function estimate. This policy we hope becomes a\ndeterministic optimal policy while the behavior policy remains stochastic and\nmore exploratory, for example, an \"-greedy policy.\n128 CHAPTER 5. MONTE CARLO METHODS\nImportance sampling is a general technique for estimating expected values\nunder one distribution given samples from another. We apply this technique\nto o\u000b-policy learning by weighting returns according to the relative probability\nof their trajectories occurring under the target and behavior policies, called\ntheimportance-sampling ratio . Given a starting state St, the probability of\nthe subsequent state{action trajectory, At;St+1;At+1;:::;ST, occurring under\nany policy\u0019is\nT\u00001Y\nk=t\u0019(AkjSk)p(Sk+1jSk;Ak);\nwherepis the state-transition probability function de\fned by (3.8). Thus, the\nrelative probability of the trajectory under the target and behavior policies\n(the importance-sampling ratio) is\n\u001aT\nt=QT\u00001\nk=t\u0019(AkjSk)p(Sk+1jSk;Ak)QT\u00001\nk=t\u0016(AkjSk)p(Sk+1jSk;Ak)=T\u00001Y\nk=t\u0019(AkjSk)\n\u0016(AkjSk): (5.3)\nNote that although the trajectory probabilities depend on the MDP's transi-\ntion probabilities, which are generally unknown, all the transition probabilities\ncancel and drop out. The importance sampling ratio ends up depending only\non the two policies and not at all on the MDP.\nNow we are ready to give a Monte Carto algorithm that uses a batch of\nobserved episodes following policy \u0016to estimate v\u0019(s). It is convenient here to\nnumber time steps in a way that increases across episode boundaries. That is,\nif the \frst episode of the batch ends in a terminal state at time 100, then the\nnext episode begins at time t= 101. This enables us to use time-step numbers\nto refer to particular steps in particular episodes. In particular, we can de\fne\nthe set of all time steps in which state sis visited, denoted T(s). This is for\nan every-visit method; for a \frst-visit method, T(s) would only include time\nsteps that were \frst visits to swithin their episode. Also, let T(t) denote the\n\frst time of termination following time t, andGtdenote the return after t\nup through T(t). ThenfGtgt2T(s)are the returns that pertain to state s, and\nf\u001aT(t)\ntgt2T(s)are the corresponding importance-sampling ratios. To estimate\nv\u0019(s), we simply scale the returns by the ratios and average the results:\nV(s) =P\nt2T(s)\u001aT(t)\ntGt\njT(s)j: (5.4)\nWhen importance sampling is done as a simple average in this way it is called\nordinary importance sampling .\n5.5. OFF-POLICY PREDICTION VIA IMPORTANCE SAMPLING 129\nAn important alternative is weighted importance sampling , which uses a\nweighted average, de\fned as\nV(s) =P\nt2T(s)\u001aT(t)\ntGt\nP\nt2T(s)\u001aT(t)\nt; (5.5)\nor zero if the denominator is zero. To understand these two varieties of im-\nportance sampling, consider their estimates after observing a single return. In\nthe weighted-average estimate, the ratio \u001aT(t)\ntfor the single return cancels in\nthe numerator and denominator, so that the estimate is equal to the observed\nreturn independent of the ratio (assuming the ratio is nonzero). Given that\nthis return was the only one observed, this is a reasonable estimate, but of\ncourse its expectation is v\u0016(s) rather than v\u0019(s), and in this statistical sense it\nis biased. In contrast, the simple average (5.4) is always v\u0019(s) in expectation\n(it is unbiased), but it can be extreme. Suppose the ratio were ten, indicating\nthat the trajectory observed is ten times as likely under the target policy as\nunder the behavior policy. In this case the ordinary importance-sampling es-\ntimate would be ten times the observed return. That is, it would be quite far\nfrom the observed return even though the episode's trajectory is considered\nvery representative of the target policy.\nFormally, the di\u000berence between the two kinds of importance sampling\nis expressed in their variances. The variance of the ordinary importance-\nsampling estimator is in general unbounded because the variance of the ratios\nis unbounded, whereas in the weighted estimator the largest weight on any\nsingle return is one. In fact, assuming bounded returns, the variance of the\nweighted importance-sampling estimator converges to zero even if the variance\nof the ratios themselves is in\fnite (Precup, Sutton, and Dasgupta 2001). In\npractice, the weighted estimator usually has dramatically lower variance and is\nstrongly preferred. A complete every-visit MC algorithm for o\u000b-policy policy\nevaluation using weighted importance sampling is given at the end of the next\nsection in Figure 5.9.\n130 CHAPTER 5. MONTE CARLO METHODS\nExample 5.4: O\u000b-policy Estimation of a Blackjack State Value\nWe applied both ordinary and weighted importance-sampling methods to es-\ntimate the value of a single blackjack state from o\u000b-policy data. Recall that\none of the advantages of Monte Carlo methods is that they can be used to\nevaluate a single state without forming estimates for any other states. In this\nexample, we evaluated the state in which the dealer is showing a deuce, the\nsum of the player's cards is 13, and the player has a usable ace (that is, the\nplayer holds an ace and a deuce, or equivalently three aces). The data was\ngenerated by starting in this state then choosing to hit or stick at random with\nequal probability (the behavior policy). The target policy was to stick only\non a sum of 20 or 21, as in Example 5.1. The value of this state under the\ntarget policy is approximately \u00000:27726 (this was determined by separately\ngenerating one-hundred million episodes using the target policy and averaging\ntheir returns). Both o\u000b-policy methods closely approximated this value after\n1000 o\u000b-policy episodes using the random policy. Figure 5.7 shows the mean\nsquared error (estimated from 100 independent runs) for each method as a\nfunction of number of episodes. The weighted importance-sampling method\nhas much lower overall error in this example, as is typical in practice.\nOrdinary importance samplingWeighted importance samplingEpisodes (log scale)010100100010,000Meansquareerror(average over100 runs)024\nFigure 5.7: Weighted importance sampling produces lower error estimates of\nthe value of a single blackjack state from o\u000b-policy episodes (see Example 5.4).\n5.5. OFF-POLICY PREDICTION VIA IMPORTANCE SAMPLING 131\n1100,0001,000,00010,000,000100,000,00020.10.9R=+ 1s\u21e1(back|s)=1\u00b5(back|s)=12backendv\u21e1(s)Monte-Carlo estimate of           with ordinaryimportance sampling(ten runs)Episodes (log scale)110100100010,0000\nFigure 5.8: Ordinary importance sampling produces surprisingly unstable esti-\nmates on the one-state MDP shown inset (Example 5.5). The correct estimate\nhere is 1, and, even though this is the expected value of a sample return (after\nimportance sampling), the variance of the samples is in\fnite, and the estimates\ndo not convergence to this value. These results are for o\u000b-policy \frst-visit MC.\nExample 5.5: In\fnite Variance\nThe estimates of ordinary importance sampling will typically have in\fnite\nvariance, and thus unsatisfactory convergence properties, whenever the scaled\nreturns have in\fnite variance|and this can easily happen in o\u000b-policy learning\nwhen trajectories contain loops. A simple example is shown inset in Figure 5.8.\nThere is only one nonterminal state sand two actions, endandback. The\nendaction causes a deterministic transition to termination, whereas the back\naction transitions, with probability 0.9, back to sor, with probability 0.1, on\nto termination. The rewards are +1 on the latter transition and otherwise\nzero. Consider the target policy that always selects back. All episodes under\nthis policy consist of some number (possibly zero) of transitions back to s\nfollowed by termination with a reward and return of +1. Thus the value of\nsunder the target policy is thus 1. Suppose we are estimating this value\nfrom o\u000b-policy data using the behavior policy that selects endandback with\nequal probability. The lower part of Figure 5.8 shows ten independent runs\nof the \frst-visit MC algorithm using ordinary importance sampling. Even\nafter millions of episodes, the estimates fail to converge to the correct value\nof 1. In contrast, the weighted importance-sampling algorithm would give an\nestimate of exactly 1 everafter the \frst episode that was consistent with the\ntarget policy (i.e., that ended with the back action). This is clear because\n132 CHAPTER 5. MONTE CARLO METHODS\nthat algorithm produces a weighted average of the returns consistent with the\ntarget policy, all of which would be exactly 1.\nWe can verify that the variance of the importance-sampling-scaled returns\nis in\fnite in this example by a simple calculation. The variance of any random\nvariableXis the expected value of the deviation from its mean \u0016X, which can\nbe written\nVar[X] =Eh\u0000\nX\u0000\u0016X\u00012i\n=E\u0002\nX2\u00002X\u0016X+\u0016X2\u0003\n=E\u0002\nX2\u0003\n\u0000\u0016X2:\nThus, if the mean is \fnite, as it is in our case, the variance is in\fnite if and\nonly if the expectation of the square of the random variable is in\fnite. Thus,\nwe need only show that the expected square of the importance-sampling-scaled\nreturn is in\fnite:\nE2\n4 T\u00001Y\nt=0\u0019(AtjSt)\n\u0016(AtjSt)G0!23\n5:\nTo compute this expectation, we break it down into cases based on episode\nlength and termination. First note that, for any episode ending with the\nendaction, the importance sampling ratio is zero, because the target policy\nwould never take this action; these episodes thus contribute nothing to the\nexpectation (the quantity in parenthesis will be zero) and can be ignored. We\nneed only consider episodes that involve some number (possibly zero) of back\nactions that transition back to the nonterminal state, followed by a back action\ntransitioning to termination. All of these episodes have a return of 1, so the\nG0factor can be ignored. To get the expected square we need only consider\neach length of episode, multiplying the probability of the episode's occurrence\nby the square of its importance-sampling ratio, and add these up:\n=1\n2\u00010:1\u00121\n0:5\u00132\n(the length 1 episode)\n+1\n2\u00010:9\u00011\n2\u00010:1\u00121\n0:51\n0:5\u00132\n(the length 2 episode)\n+1\n2\u00010:9\u00011\n2\u00010:9\u00011\n2\u00010:1\u00121\n0:51\n0:51\n0:5\u00132\n(the length 3 episode)\n+\u0001\u0001\u0001\n= 0:11X\nk=00:9k\u00012k\u00012\n= 0:21X\nk=01:8k\n=1:\n5.6. INCREMENTAL IMPLEMENTATION 133\n5.6 Incremental Implementation\nMonte Carlo prediction methods can be implemented incrementally, on an\nepisode-by-episode basis, using extensions of the techniques described in Chap-\nter 2. Whereas in Chapter 2 we averaged rewards , in Monte Carlo methods\nwe average returns . In all other respects exactly the same methods as used\nin Chapter 2 can be used for on-policy Monte Carlo methods. For o\u000b-policy\nMonte Carlo methods, we need to separately consider those that use ordinary\nimportance sampling and those that use weighted importance sampling.\nIn ordinary importance sampling, the returns are scaled by the importance\nsampling ratio \u001aT(t)\nt(5.3), then simply averaged. For these methods we can\nagain use the incremental methods of Chapter 2, but using the scaled returns in\nplace of the rewards of that chapter. This leaves the case of o\u000b-policy methods\nusing weighted importance sampling. Here we have to form a weighted average\nof the returns, and a slightly di\u000berent incremental algorithm is required.\nSuppose we have a sequence of returns G1;G2;:::;Gn\u00001, all starting in the\nsame state and each with a corresponding random weight Wi(e.g.,Wi=\u001aT(t)\nt).\nWe wish to form the estimate\nVn=Pn\u00001\nk=1WkGkPn\u00001\nk=1Wk; n\u00152; (5.6)\nand keep it up-to-date as we obtain a single additional return Gn. In addition\nto keeping track of Vn, we must maintain for each state the cumulative sum\nCnof the weights given to the \frst nreturns. The update rule for Vnis\nVn+1=Vn+Wn\nCnh\nGn\u0000Vni\n; n\u00151; (5.7)\nand\nCn+1=Cn+Wn+1;\nwhereC0= 0 (andV1is arbitrary and thus need not be speci\fed). Figure 5.9\ngives a complete episode-by-episode incremental algorithm for Monte Carlo\npolicy evaluation. The algorithm is nominally for the o\u000b-policy case, using\nweighted importance sampling, but applies as well to the on-policy case just\nby choosing the target and behavior policies as the same.\n134 CHAPTER 5. MONTE CARLO METHODS\nInitialize, for all s2S,a2A(s):\nQ(s;a) arbitrary\nC(s;a) 0\n\u0016(ajs) an arbitrary soft behavior policy\n\u0019(ajs) an arbitrary target policy\nRepeat forever:\nGenerate an episode using \u0016:\nS0;A0;R1;:::;ST\u00001;AT\u00001;RT;ST\nG 0\nW 1\nFort=T\u00001;T\u00002;::: downto 0:\nG \rG+Rt+1\nC(St;At) C(St;At) +W\nQ(St;At) Q(St;At) +W\nC(St;At)[G\u0000Q(St;At)]\nW W\u0019(AtjSt)\n\u0016(AtjSt)\nIfW= 0 then ExitForLoop\nFigure 5.9: An incremental every-visit MC policy-evaluation algorithm, using\nweighted importance sampling. The approximation Qconverges to q\u0019(for all\nencountered state{action pairs) even though all actions are selected according\nto a potentially di\u000berent policy, \u0016. In the on-policy case ( \u0019=\u0016),Wis always\n1.\n5.7. OFF-POLICY MONTE CARLO CONTROL 135\n5.7 O\u000b-Policy Monte Carlo Control\nWe are now ready to present an example of the second class of learning control\nmethods we consider in this book: o\u000b-policy methods. Recall that the distin-\nguishing feature of on-policy methods is that they estimate the value of a\npolicy while using it for control. In o\u000b-policy methods these two functions are\nseparated. The policy used to generate behavior, called the behavior policy,\nmay in fact be unrelated to the policy that is evaluated and improved, called\nthetarget policy. An advantage of this separation is that the target policy\nmay be deterministic (e.g., greedy), while the behavior policy can continue to\nsample all possible actions.\nO\u000b-policy Monte Carlo control methods use one of the techniques presented\nin the preceding two sections. They follow the behavior policy while learning\nabout and improving the target policy. These techniques requires that the\nbehavior policy has a nonzero probability of selecting all actions that might\nbe selected by the target policy (coverage). To explore all possibilities, we\nrequire that the behavior policy be soft (i.e., that it select all actions in all\nstates with nonzero probability).\nFigure 5.10 shows an o\u000b-policy Monte Carlo method, based on GPI and\nweighted importance sampling, for estimating q\u0003. The target policy \u0019is the\ngreedy policy with respect to Q, which is an estimate of q\u0019. The behavior\npolicy\u0016can be anything, but in order to assure convergence of \u0019to the\noptimal policy, an in\fnite number of returns must be obtained for each pair\nof state and action. This can be assured by choosing \u0016to be\"-soft.\nA potential problem is that this method learns only from the tails of\nepisodes, after the last nongreedy action. If nongreedy actions are frequent,\nthen learning will be slow, particularly for states appearing in the early por-\ntions of long episodes. Potentially, this could greatly slow learning. There\nhas been insu\u000ecient experience with o\u000b-policy Monte Carlo methods to assess\nhow serious this problem is. If it is serious, the most important way to address\nit is probably by incorporating temporal-di\u000berence learning, the algorithmic\nidea developed in the next chapter. Alternatively, if \ris less than 1, then the\nidea developed in the next section may also help signi\fcantly.\n136 CHAPTER 5. MONTE CARLO METHODS\nInitialize, for all s2S,a2A(s):\nQ(s;a) arbitrary\nC(s;a) 0\n\u0019(s) a deterministic policy that is greedy with respect to Q\nRepeat forever:\nGenerate an episode using any soft policy \u0016:\nS0;A0;R1;:::;ST\u00001;AT\u00001;RT;ST\nG 0\nW 1\nFort=T\u00001;T\u00002;::: downto 0:\nG \rG+Rt+1\nC(St;At) C(St;At) +W\nQ(St;At) Q(St;At) +W\nC(St;At)[G\u0000Q(St;At)]\n\u0019(St) argmaxaQ(St;a) (with ties broken arbitrarily)\nW W1\n\u0016(AtjSt)\nIfW= 0 then ExitForLoop\nFigure 5.10: An o\u000b-policy every-visit MC control algorithm, using weighted\nimportance sampling. The policy \u0019converges to optimal at all encountered\nstates even though actions are selected according to a di\u000berent soft policy \u0016,\nwhich may change between or even within episodes.\n\u00035.8 Importance Sampling on Truncated Re-\nturns\nSo far our o\u000b-policy methods have formed importance-sampling ratios for\nreturns considered as unitary wholes. This is clearly the right thing for a\nMonte Carlo method to do in the absence of discounting (i.e., if \r= 1),\nbut if\r < 1 then there may be something better. Consider the case where\nepisodes are long and \ris signi\fcantly less than 1. For concreteness, say that\nepisodes last 100 steps and that \r= 0. The return from time 0 will then be\nG0=R1, and its importance sampling ratio will be a product of 100 factors,\n\u0019(A0jS0)\n\u0016(A0jS0)\u0019(A1jS1)\n\u0016(A1jS1)\u0001\u0001\u0001\u0019(A99jS99)\n\u0016(A99jS99). In ordinary importance sampling, the return will\nbe scaled by the entire product, but it is really only necessary to scale by the\n\frst factor, by\u0019(A0jS0)\n\u0016(A0jS0). The other 99 factors\u0019(A1jS1)\n\u0016(A1jS1)\u0001\u0001\u0001\u0019(A99jS99)\n\u0016(A99jS99)are irrelevant\nbecause after the \frst reward the return has already been determined. These\nlater factors are all independent of the return and of expected value 1; they\ndo not change the expected update, but they add enormously to its variance.\nIn some cases they could even make the variance in\fnite. Let us now consider\nan idea for avoiding this large extraneous variance.\n\u00035.8. IMPORTANCE SAMPLING ON TRUNCATED RETURNS 137\nThe essence of the idea is to think of discounting as determining a proba-\nbility of termination or, equivalently, a degree of partial termination. For any\n\r2[0;1), we can think of the return G0as partly terminating in one step,\nto the degree 1\u0000\r, producing a return of just the \frst reward, R1, and as\npartly terminating after two steps, to the degree (1 \u0000\r)\r, producing a return\nofR1+R2, and so on. The latter degree corresponds to terminating on the\nsecond step, 1\u0000\r, and not having already terminated on the \frst step, \r. The\ndegree of termination on the third step is thus (1 \u0000\r)\r2, with the\r2re\recting\nthat termination did not occur on either of the \frst two steps. The partial\nreturns here are called \rat partial returns :\n\u0016Gh\nt=Rt+1+Rt+2+\u0001\u0001\u0001+Rh; 0\u0014t<h\u0014T;\nwhere \\\rat\" denotes the absence of discounting, and \\partial\" denotes that\nthese returns do not extend all the way to termination but instead stop at\nh, called the horizon (andTis the time of termination of the episode). The\nconventional full return Gtcan be viewed as a sum of \rat partial returns as\nsuggested above as follows:\nGt=Rt+1+\rRt+2+\r2Rt+3+\u0001\u0001\u0001+\rT\u0000t\u00001RT\n= (1\u0000\r)Rt+1\n+ (1\u0000\r)\r(Rt+1+Rt+2)\n+ (1\u0000\r)\r2(Rt+1+Rt+2+Rt+3)\n...\n+ (1\u0000\r)\rT\u0000t\u00002(Rt+1+Rt+2+\u0001\u0001\u0001+RT\u00001)\n+\rT\u0000t\u00001(Rt+1+Rt+2+\u0001\u0001\u0001+RT)\n=\rT\u0000t\u00001\u0016GT\nt+ (1\u0000\r)T\u00001X\nh=t+1\rh\u0000t\u00001\u0016Gh\nt\nNow we need to scale the \rat partial returns by an importance sampling\nratio that is similarly truncated. As Gh\ntonly involves rewards up to a horizon\nh, we only need the ratio of the probabilities up to h. We de\fne an ordinary\nimportance-sampling estimator, analogous to (5.4), as\nV(s) =P\nt2T(s)\u0010\n\rT(t)\u0000t\u00001\u001aT(t)\nt\u0016GT(t)\nt+ (1\u0000\r)PT(t)\u00001\nh=t+1\rh\u0000t\u00001\u001ah\nt\u0016Gh\nt\u0011\njT(s)j;(5.8)\nand a weighted importance-sampling estimator, analogous to (5.5), as\nV(s) =P\nt2T(s)\u0010\n\rT(t)\u0000t\u00001\u001aT(t)\nt\u0016GT(t)\nt+ (1\u0000\r)PT(t)\u00001\nh=t+1\rh\u0000t\u00001\u001ah\nt\u0016Gh\nt\u0011\nP\nt2T(s)\u0010\n\rT(t)\u0000t\u00001\u001aT(t)\nt+ (1\u0000\r)PT(t)\u00001\nh=t+1\rh\u0000t\u00001\u001ah\nt\u0011:(5.9)\n138 CHAPTER 5. MONTE CARLO METHODS\n5.9 Summary\nThe Monte Carlo methods presented in this chapter learn value functions and\noptimal policies from experience in the form of sample episodes . This gives\nthem at least three kinds of advantages over DP methods. First, they can be\nused to learn optimal behavior directly from interaction with the environment,\nwith no model of the environment's dynamics. Second, they can be used with\nsimulation or sample models . For surprisingly many applications it is easy to\nsimulate sample episodes even though it is di\u000ecult to construct the kind of\nexplicit model of transition probabilities required by DP methods. Third, it\nis easy and e\u000ecient to focus Monte Carlo methods on a small subset of the\nstates. A region of special interest can be accurately evaluated without going\nto the expense of accurately evaluating the rest of the state set (we explore\nthis further in Chapter 8).\nA fourth advantage of Monte Carlo methods, which we discuss later in the\nbook, is that they may be less harmed by violations of the Markov property.\nThis is because they do not update their value estimates on the basis of the\nvalue estimates of successor states. In other words, it is because they do not\nbootstrap.\nIn designing Monte Carlo control methods we have followed the overall\nschema of generalized policy iteration (GPI) introduced in Chapter 4. GPI\ninvolves interacting processes of policy evaluation and policy improvement.\nMonte Carlo methods provide an alternative policy evaluation process. Rather\nthan use a model to compute the value of each state, they simply average many\nreturns that start in the state. Because a state's value is the expected return,\nthis average can become a good approximation to the value. In control meth-\nods we are particularly interested in approximating action-value functions,\nbecause these can be used to improve the policy without requiring a model of\nthe environment's transition dynamics. Monte Carlo methods intermix policy\nevaluation and policy improvement steps on an episode-by-episode basis, and\ncan be incrementally implemented on an episode-by-episode basis.\nMaintaining su\u000ecient exploration is an issue in Monte Carlo control meth-\nods. It is not enough just to select the actions currently estimated to be best,\nbecause then no returns will be obtained for alternative actions, and it may\nnever be learned that they are actually better. One approach is to ignore this\nproblem by assuming that episodes begin with state{action pairs randomly\nselected to cover all possibilities. Such exploring starts can sometimes be ar-\nranged in applications with simulated episodes, but are unlikely in learning\nfrom real experience. In on-policy methods, the agent commits to always\nexploring and tries to \fnd the best policy that still explores. In o\u000b-policy\nmethods, the agent also explores, but learns a deterministic optimal policy\n5.9. SUMMARY 139\nthat may be unrelated to the policy followed.\nO\u000b-policy Monte Carlo prediction refers to learning the value function of\natarget policy from data generated by a di\u000berent behavior policy . Such learn-\ning methods are all based on some form of importance sampling , that is, on\nweighting returns by the ratio of the probabilities of taking the observed ac-\ntions under the two policies. Ordinary importance sampling uses a simple\naverage of the weighted returns, whereas weighted importance sampling uses\na weighted average. Ordinary importance sampling produces unbiased esti-\nmates, but has larger, possibly in\fnite, variance, whereas weighted importance\nsampling always has \fnite variance and are preferred in practice. Despite their\nconceptual simplicity, o\u000b-policy Monte Carlo methods for both prediction and\ncontrol remain unsettled and a subject of ongoing research.\nThe Monte Carlo methods treated in this chapter di\u000ber from the DP meth-\nods treated in the previous chapter in two major ways. First, they operate on\nsample experience, and thus can be used for direct learning without a model.\nSecond, they do not bootstrap. That is, they do not update their value es-\ntimates on the basis of other value estimates. These two di\u000berences are not\ntightly linked, and can be separated. In the next chapter we consider methods\nthat learn from experience, like Monte Carlo methods, but also bootstrap, like\nDP methods.\nBibliographical and Historical Remarks\nThe term \\Monte Carlo\" dates from the 1940s, when physicists at Los Alamos\ndevised games of chance that they could study to help understand complex\nphysical phenomena relating to the atom bomb. Coverage of Monte Carlo\nmethods in this sense can be found in several textbooks (e.g., Kalos and Whit-\nlock, 1986; Rubinstein, 1981).\nAn early use of Monte Carlo methods to estimate action values in a re-\ninforcement learning context was by Michie and Chambers (1968). In pole\nbalancing (Example 3.4), they used averages of episode durations to assess\nthe worth (expected balancing \\life\") of each possible action in each state,\nand then used these assessments to control action selections. Their method is\nsimilar in spirit to Monte Carlo ES with every-visit MC estimates. Narendra\nand Wheeler (1986) studied a Monte Carlo method for ergodic \fnite Markov\nchains that used the return accumulated from one visit to a state to the next\nas a reward for adjusting a learning automaton's action probabilities.\nBarto and Du\u000b (1994) discussed policy evaluation in the context of classi-\ncal Monte Carlo algorithms for solving systems of linear equations. They used\n140 CHAPTER 5. MONTE CARLO METHODS\nthe analysis of Curtiss (1954) to point out the computational advantages of\nMonte Carlo policy evaluation for large problems. Singh and Sutton (1996)\ndistinguished between every-visit and \frst-visit MC methods and proved re-\nsults relating these methods to reinforcement learning algorithms.\nThe blackjack example is based on an example used by Widrow, Gupta,\nand Maitra (1973). The soap bubble example is a classical Dirichlet problem\nwhose Monte Carlo solution was \frst proposed by Kakutani (1945; see Hersh\nand Griego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted\nfrom Barto, Bradtke, and Singh (1995), and from Gardner (1973).\nMonte Carlo ES was introduced in the 1998 edition of this book. That\nmay have been the \frst explicit connection between Monte Carlo estimation\nand control methods based on policy iteration.\nE\u000ecient o\u000b-policy learning has become recognized as an important chal-\nlenge that arises in several \felds. For example, it is closely related to the idea\nof \\interventions\" and \\counterfactuals\" in probabalistic graphical (Bayesian)\nmodels (e.g., Pearl, 1995; Balke and Pearl, 1994). O\u000b-policy methods using\nimportance sampling have a long history and yet still are not well understood.\nWeighted importance sampling, which is also sometimes called normalized im-\nportance sampling (e.g., Koller and Friedman, 2009), is discussed by, for ex-\nample, Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu (2001).\nCombining o\u000b-policy learning with temporal-di\u000berence learning and approxi-\nmation methods introduces subtle issues that we consider in later chapters.\nThe target policy in o\u000b-policy learning is sometimes referred to in the\nliterature as the \\estimation\" policy, as it was in the \frst edition of this book.\nOur treatment of the idea of importance sampling based on truncated re-\nturns is based on the analysis and \\forward view\" of Sutton, Mahmood, Pre-\ncup, and van Hasselt (2014). A related idea is that of per-decision importance\nsampling (Precup, Sutton and Singh, 2000).\nExercises\nExercise 5.1 Consider the diagrams on the right in Figure 5.2. Why does\nthe estimated value function jump up for the last two rows in the rear? Why\ndoes it drop o\u000b for the whole last row on the left? Why are the frontmost\nvalues higher in the upper diagrams than in the lower?\nExercise 5.2 What is the backup diagram for Monte Carlo estimation of q\u0019?\nExercise 5.3 What is the Monte Carlo estimate analogous to (5.5) for action\n5.9. SUMMARY 141\nStarting lineFinish\nline\nStarting lineFinish\nline\nFigure 5.11: A couple of right turns for the racetrack task.\nvalues, given returns generated using \u0016?\nExercise 5.4 What is the equation analogous to (5.5) for action valuesQ(s;a)\ninstead of state values V(s)?\nExercise 5.5 In learning curves such as those shown in Figure 5.7 error gener-\nally decreases with training, as indeed happened for the ordinary importance-\nsampling method. But for the weighted importance-sampling method error\n\frst increased and then decreased. Why do you think this happened?\nExercise 5.6 The results with Example 5.5 and shown in Figure 5.8 used\na \frst-visit MC method. Suppose that instead an every-visit MC method\nwas used on the same problem. Would the variance of the estimator still be\nin\fnite? Why or why not?\nExercise 5.7 Modify the algorithm for \frst-visit MC policy evaluation (Fig-\nure 5.1) to use the incremental implementation for sample averages described\nin Section 2.4.\nExercise 5.8 Derive the weighted-average update rule (5.7) from (5.6). Fol-\nlow the pattern of the derivation of the unweighted rule (2.3).\nExercise 5.9: Racetrack (programming) Consider driving a race car\naround a turn like those shown in Figure 5.11. You want to go as fast as\npossible, but not so fast as to run o\u000b the track. In our simpli\fed racetrack,\nthe car is at one of a discrete set of grid positions, the cells in the diagram. The\nvelocity is also discrete, a number of grid cells moved horizontally and vertically\nper time step. The actions are increments to the velocity components. Each\nmay be changed by +1, \u00001, or 0 in one step, for a total of nine actions.\n142 CHAPTER 5. MONTE CARLO METHODS\nBoth velocity components are restricted to be nonnegative and less than 5,\nand they cannot both be zero. Each episode begins in one of the randomly\nselected start states and ends when the car crosses the \fnish line. The rewards\nare\u00001 for each step that stays on the track, and \u00005 if the agent tries to drive\no\u000b the track. Actually leaving the track is not allowed, but the position is\nalways advanced by at least one cell along either the horizontal or vertical\naxes. With these restrictions and considering only right turns, such as shown\nin the \fgure, all episodes are guaranteed to terminate, yet the optimal policy\nis unlikely to be excluded. To make the task more challenging, we assume that\non half of the time steps the position is displaced forward or to the right by\none additional cell beyond that speci\fed by the velocity. Apply a Monte Carlo\ncontrol method to this task to compute the optimal policy from each starting\nstate. Exhibit several trajectories following the optimal policy.\n\u0003Exercise 5.10 Modify the algorithm for o\u000b-policy Monte Carlo control (Fig-\nure 5.10) to use the idea of the truncated weighted-average estimator (5.9).\nNote that you will \frst need to convert this equation to action values.\nChapter 6\nTemporal-Di\u000berence Learning\nIf one had to identify one idea as central and novel to reinforcement learning,\nit would undoubtedly be temporal-di\u000berence (TD) learning. TD learning is\na combination of Monte Carlo ideas and dynamic programming (DP) ideas.\nLike Monte Carlo methods, TD methods can learn directly from raw experience\nwithout a model of the environment's dynamics. Like DP, TD methods update\nestimates based in part on other learned estimates, without waiting for a \fnal\noutcome (they bootstrap). The relationship between TD, DP, and Monte\nCarlo methods is a recurring theme in the theory of reinforcement learning.\nThis chapter is the beginning of our exploration of it. Before we are done,\nwe will see that these ideas and methods blend into each other and can be\ncombined in many ways. In particular, in Chapter 7 we introduce the TD( \u0015)\nalgorithm, which seamlessly integrates TD and Monte Carlo methods.\nAs usual, we start by focusing on the policy evaluation or prediction prob-\nlem, that of estimating the value function v\u0019for a given policy \u0019. For the\ncontrol problem (\fnding an optimal policy), DP, TD, and Monte Carlo meth-\nods all use some variation of generalized policy iteration (GPI). The di\u000berences\nin the methods are primarily di\u000berences in their approaches to the prediction\nproblem.\n6.1 TD Prediction\nBoth TD and Monte Carlo methods use experience to solve the prediction\nproblem. Given some experience following a policy \u0019, both methods update\ntheir estimate vofv\u0019for the nonterminal states Stoccurring in that experience.\nRoughly speaking, Monte Carlo methods wait until the return following the\nvisit is known, then use that return as a target for V(St). A simple every-visit\n143\n144 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nMonte Carlo method suitable for nonstationary environments is\nV(St) V(St) +\u000bh\nGt\u0000V(St)i\n; (6.1)\nwhereGtis the actual return following time t, and\u000bis a constant step-\nsize parameter (c.f., Equation 2.4). Let us call this method constant-\u000bMC.\nWhereas Monte Carlo methods must wait until the end of the episode to\ndetermine the increment to V(St) (only then is Gtknown), TD methods need\nwait only until the next time step. At time t+1 they immediately form a target\nand make a useful update using the observed reward Rt+1and the estimate\nV(St+1). The simplest TD method, known as TD(0) , is\nV(St) V(St) +\u000bh\nRt+1+\rV(St+1)\u0000V(St)i\n: (6.2)\nIn e\u000bect, the target for the Monte Carlo update is Gt, whereas the target for\nthe TD update is Rt+1+\rV(St+1).\nBecause the TD method bases its update in part on an existing estimate,\nwe say that it is a bootstrapping method, like DP. We know from Chapter 3\nthat\nv\u0019(s) = E\u0019[GtjSt=s] (6.3)\n=E\u0019\"1X\nk=0\rkRt+k+1\f\f\f\f\fSt=s#\n=E\u0019\"\nRt+1+\r1X\nk=0\rkRt+k+2\f\f\f\f\fSt=s#\n=E\u0019[Rt+1+\rv\u0019(St+1)jSt=s]: (6.4)\nRoughly speaking, Monte Carlo methods use an estimate of (6.3) as a target,\nwhereas DP methods use an estimate of (6.4) as a target. The Monte Carlo\ntarget is an estimate because the expected value in (6.3) is not known; a\nsample return is used in place of the real expected return. The DP target\nis an estimate not because of the expected values, which are assumed to be\ncompletely provided by a model of the environment, but because v\u0019(St+1) is\nnot known and the current estimate, V(St+1), is used instead. The TD target\nis an estimate for both reasons: it samples the expected values in (6.4) andit\nuses the current estimate Vinstead of the true v\u0019. Thus, TD methods combine\nthe sampling of Monte Carlo with the bootstrapping of DP. As we shall see,\nwith care and imagination this can take us a long way toward obtaining the\nadvantages of both Monte Carlo and DP methods.\nFigure 6.1 speci\fes TD(0) completely in procedural form, and Figure 6.2\nshows its backup diagram. The value estimate for the state node at the top of\n6.1. TD PREDICTION 145\nInput: the policy \u0019to be evaluated\nInitializeV(s) arbitrarily (e.g., V(s) = 0;8s2S+)\nRepeat (for each episode):\nInitializeS\nRepeat (for each step of episode):\nA action given by \u0019forS\nTake action A; observe reward, R, and next state, S0\nV(S) V(S) +\u000b\u0002\nR+\rV(S0)\u0000V(S)\u0003\nS S0\nuntilSis terminal\nFigure 6.1: Tabular TD(0) for estimating v\u0019.\nFigure 6.2: The backup diagram for TD(0).\nthe backup diagram is updated on the basis of the one sample transition from\nit to the immediately following state. We refer to TD and Monte Carlo updates\nassample backups because they involve looking ahead to a sample successor\nstate (or state{action pair), using the value of the successor and the reward\nalong the way to compute a backed-up value, and then changing the value\nof the original state (or state{action pair) accordingly. Sample backups di\u000ber\nfrom the fullbackups of DP methods in that they are based on a single sample\nsuccessor rather than on a complete distribution of all possible successors.\nExample 6.1: Driving Home Each day as you drive home from work, you\ntry to predict how long it will take to get home. When you leave your o\u000ece,\nyou note the time, the day of week, and anything else that might be relevant.\nSay on this Friday you are leaving at exactly 6 o'clock, and you estimate that\nit will take 30 minutes to get home. As you reach your car it is 6:05, and\nyou notice it is starting to rain. Tra\u000ec is often slower in the rain, so you\nreestimate that it will take 35 minutes from then, or a total of 40 minutes.\nFifteen minutes later you have completed the highway portion of your journey\nin good time. As you exit onto a secondary road you cut your estimate of total\ntravel time to 35 minutes. Unfortunately, at this point you get stuck behind\na slow truck, and the road is too narrow to pass. You end up having to follow\nthe truck until you turn onto the side street where you live at 6:40. Three\nminutes later you are home. The sequence of states, times, and predictions is\n146 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nroad30354045\nPredicted\ntotal\ntravel\ntime\nleaving\nofficeexiting\nhighway2ndaryhomearrive\nSituationactual outcome\nreach\ncar streethome\nFigure 6.3: Changes recommended by Monte Carlo methods in the driving\nhome example.\nthus as follows:\nElapsed Time Predicted Predicted\nState (minutes) Time to Go Total Time\nleaving o\u000ece, friday at 6 0 30 30\nreach car, raining 5 35 40\nexiting highway 20 15 35\n2ndary road, behind truck 30 10 40\nentering home street 40 3 43\narrive home 43 0 43\nThe rewards in this example are the elapsed times on each leg of the journey.1\nWe are not discounting ( \r= 1), and thus the return for each state is the actual\ntime to go from that state. The value of each state is the expected time to go.\nThe second column of numbers gives the current estimated value for each state\nencountered.\nA simple way to view the operation of Monte Carlo methods is to plot the\npredicted total time (the last column) over the sequence, as in Figure 6.3. The\narrows show the changes in predictions recommended by the constant- \u000bMC\nmethod (6.1), for \u000b= 1. These are exactly the errors between the estimated\nvalue (predicted time to go) in each state and the actual return (actual time\nto go). For example, when you exited the highway you thought it would take\nonly 15 minutes more to get home, but in fact it took 23 minutes. Equation\n6.1 applies at this point and determines an increment in the estimate of time\nto go after exiting the highway. The error, Gt\u0000V(St), at this time is eight\n1If this were a control problem with the objective of minimizing travel time, then we\nwould of course make the rewards the negative of the elapsed time. But since we are\nconcerned here only with prediction (policy evaluation), we can keep things simple by using\npositive numbers.\n6.1. TD PREDICTION 147\nactual\noutcome\nSituation30354045\nPredicted\ntotal\ntravel\ntime\nroadleaving\nofficeexiting\nhighway2ndaryhomearrive reach\ncar streethome\nFigure 6.4: Changes recommended by TD methods in the driving home ex-\nample.\nminutes. Suppose the step-size parameter, \u000b, is 1=2. Then the predicted time\nto go after exiting the highway would be revised upward by four minutes as a\nresult of this experience. This is probably too large a change in this case; the\ntruck was probably just an unlucky break. In any event, the change can only\nbe made o\u000b-line, that is, after you have reached home. Only at this point do\nyou know any of the actual returns.\nIs it necessary to wait until the \fnal outcome is known before learning can\nbegin? Suppose on another day you again estimate when leaving your o\u000ece\nthat it will take 30 minutes to drive home, but then you become stuck in a\nmassive tra\u000ec jam. Twenty-\fve minutes after leaving the o\u000ece you are still\nbumper-to-bumper on the highway. You now estimate that it will take another\n25 minutes to get home, for a total of 50 minutes. As you wait in tra\u000ec, you\nalready know that your initial estimate of 30 minutes was too optimistic. Must\nyou wait until you get home before increasing your estimate for the initial\nstate? According to the Monte Carlo approach you must, because you don't\nyet know the true return.\nAccording to a TD approach, on the other hand, you would learn immedi-\nately, shifting your initial estimate from 30 minutes toward 50. In fact, each\nestimate would be shifted toward the estimate that immediately follows it.\nReturning to our \frst day of driving, Figure 6.4 shows the same predictions as\nFigure 6.3, except with the changes recommended by the TD rule (6.2) (these\nare the changes made by the rule if \u000b= 1). Each error is proportional to\nthe change over time of the prediction, that is, to the temporal di\u000berences in\npredictions.\nBesides giving you something to do while waiting in tra\u000ec, there are several\ncomputational reasons why it is advantageous to learn based on your current\npredictions rather than waiting until termination when you know the actual\n148 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nreturn. We brie\ry discuss some of these next.\n6.2 Advantages of TD Prediction Methods\nTD methods learn their estimates in part on the basis of other estimates.\nThey learn a guess from a guess|they bootstrap . Is this a good thing to do?\nWhat advantages do TD methods have over Monte Carlo and DP methods?\nDeveloping and answering such questions will take the rest of this book and\nmore. In this section we brie\ry anticipate some of the answers.\nObviously, TD methods have an advantage over DP methods in that they\ndo not require a model of the environment, of its reward and next-state prob-\nability distributions.\nThe next most obvious advantage of TD methods over Monte Carlo meth-\nods is that they are naturally implemented in an on-line, fully incremental\nfashion. With Monte Carlo methods one must wait until the end of an episode,\nbecause only then is the return known, whereas with TD methods one need\nwait only one time step. Surprisingly often this turns out to be a critical\nconsideration. Some applications have very long episodes, so that delaying all\nlearning until an episode's end is too slow. Other applications are continuing\ntasks and have no episodes at all. Finally, as we noted in the previous chap-\nter, some Monte Carlo methods must ignore or discount episodes on which\nexperimental actions are taken, which can greatly slow learning. TD meth-\nods are much less susceptible to these problems because they learn from each\ntransition regardless of what subsequent actions are taken.\nBut are TD methods sound? Certainly it is convenient to learn one guess\nfrom the next, without waiting for an actual outcome, but can we still guar-\nantee convergence to the correct answer? Happily, the answer is yes. For any\n\fxed policy \u0019, the TD algorithm described above has been proved to converge\ntov\u0019, in the mean for a constant step-size parameter if it is su\u000eciently small,\nand with probability 1 if the step-size parameter decreases according to the\nusual stochastic approximation conditions (2.7). Most convergence proofs ap-\nply only to the table-based case of the algorithm presented above (6.2), but\nsome also apply to the case of general linear function approximation. These\nresults are discussed in a more general setting in the next two chapters.\nIf both TD and Monte Carlo methods converge asymptotically to the cor-\nrect predictions, then a natural next question is \\Which gets there \frst?\" In\nother words, which method learns faster? Which makes the more e\u000ecient use\nof limited data? At the current time this is an open question in the sense\nthat no one has been able to prove mathematically that one method converges\n6.2. ADVANTAGES OF TD PREDICTION METHODS 149\nA B C D E1 0 0 0 0 0\nstart\nFigure 6.5: A small Markov process for generating random walks.\nfaster than the other. In fact, it is not even clear what is the most appro-\npriate formal way to phrase this question! In practice, however, TD methods\nhave usually been found to converge faster than constant- \u000bMC methods on\nstochastic tasks, as illustrated in the following example.\nExample 6.2: Random Walk In this example we empirically compare the\nprediction abilities of TD(0) and constant- \u000bMC applied to the small Markov\nprocess shown in Figure 6.5. All episodes start in the center state, C, and\nproceed either left or right by one state on each step, with equal probabil-\nity. This behavior is presumably due to the combined e\u000bect of a \fxed policy\nand an environment's state-transition probabilities, but we do not care which;\nwe are concerned only with predicting returns however they are generated.\nEpisodes terminate either on the extreme left or the extreme right. When an\nepisode terminates on the right a reward of +1 occurs; all other rewards are\nzero. For example, a typical walk might consist of the following state-and-\nreward sequence: C;0;B;0;C;0;D;0;E;1. Because this task is undiscounted\nand episodic, the true value of each state is the probability of terminating\non the right if starting from that state. Thus, the true value of the cen-\nter state is v\u0019(C) = 0:5. The true values of all the states, Athrough E, are\n1\n6;2\n6;3\n6;4\n6, and5\n6. Figure 6.6 shows the values learned by TD(0) approaching the\ntrue values as more episodes are experienced. Averaging over many episode\nsequences, Figure 6.7 shows the average error in the predictions found by\nTD(0) and constant- \u000bMC, for a variety of values of \u000b, as a function of num-\nber of episodes. In all cases the approximate value function was initialized\nto the intermediate value V(s) = 0:5, for alls. The TD method is consis-\ntently better than the MC method on this task over this number of episodes.\n150 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\n0.8\n00.20.40.6\nA B C D E010\n1100\nStateEstimated\nvalue\ntrue \nvalues\nFigure 6.6: Values learned by TD(0) after various numbers of episodes. The\n\fnal estimate is about as close as the estimates ever get to the true values.\nWith a constant step-size parameter ( \u000b= 0:1 in this example), the values\n\ructuate inde\fnitely in response to the outcomes of the most recent episodes.\n00.050.10.150.20.25\n0 25 50 75 100\nWalks / EpisodesTDMC\n!=.05!=.01\n!=.1!=.15!=.02\n!=.04\n!=.03RMS error,\naveraged\nover states\nFigure 6.7: Learning curves for TD(0) and constant- \u000bMC methods, for various\nvalues of\u000b, on the prediction problem for the random walk. The performance\nmeasure shown is the root mean-squared (RMS) error between the value func-\ntion learned and the true value function, averaged over the \fve states. These\ndata are averages over 100 di\u000berent sequences of episodes.\n6.3. OPTIMALITY OF TD(0) 151\n6.3 Optimality of TD(0)\nSuppose there is available only a \fnite amount of experience, say 10 episodes\nor 100 time steps. In this case, a common approach with incremental learning\nmethods is to present the experience repeatedly until the method converges\nupon an answer. Given an approximate value function, V, the increments\nspeci\fed by (6.1) or (6.2) are computed for every time step tat which a\nnonterminal state is visited, but the value function is changed only once, by\nthe sum of all the increments. Then all the available experience is processed\nagain with the new value function to produce a new overall increment, and\nso on, until the value function converges. We call this batch updating because\nupdates are made only after processing each complete batch of training data.\nUnder batch updating, TD(0) converges deterministically to a single an-\nswer independent of the step-size parameter, \u000b, as long as \u000bis chosen to be\nsu\u000eciently small. The constant- \u000bMC method also converges deterministically\nunder the same conditions, but to a di\u000berent answer. Understanding these two\nanswers will help us understand the di\u000berence between the two methods. Un-\nder normal updating the methods do not move all the way to their respective\nbatch answers, but in some sense they take steps in these directions. Before\ntrying to understand the two answers in general, for all possible tasks, we \frst\nlook at a few examples.\nExample 6.3 Random walk under batch updating . Batch-updating versions\nof TD(0) and constant- \u000bMC were applied as follows to the random walk pre-\ndiction example (Example 6.2). After each new episode, all episodes seen so\nfar were treated as a batch. They were repeatedly presented to the algorithm,\neither TD(0) or constant- \u000bMC, with\u000bsu\u000eciently small that the value func-\ntion converged. The resulting value function was then compared with v\u0019, and\nthe average root mean-squared error across the \fve states (and across 100\nindependent repetitions of the whole experiment) was plotted to obtain the\nlearning curves shown in Figure 6.8. Note that the batch TD method was\nconsistently better than the batch Monte Carlo method.\nUnder batch training, constant- \u000bMC converges to values, V(s), that are\nsample averages of the actual returns experienced after visiting each state s.\nThese are optimal estimates in the sense that they minimize the mean-squared\nerror from the actual returns in the training set. In this sense it is surprising\nthat the batch TD method was able to perform better according to the root\nmean-squared error measure shown in Figure 6.8. How is it that batch TD\nwas able to perform better than this optimal method? The answer is that the\nMonte Carlo method is optimal only in a limited way, and that TD is optimal\nin a way that is more relevant to predicting returns. But \frst let's develop our\n152 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\n.0.05.1.15.2.25\n0 25 50 75 100TDMCBATCH TRAINING\nWalks / EpisodesRMS error,\naveraged\nover states\nFigure 6.8: Performance of TD(0) and constant- \u000bMC under batch training\non the random walk task.\nintuitions about di\u000berent kinds of optimality through another example.\nExample 6.4: You are the Predictor Place yourself now in the role of\nthe predictor of returns for an unknown Markov reward process. Suppose you\nobserve the following eight episodes:\nA;0;B;0 B;1\nB;1 B;1\nB;1 B;1\nB;1 B;0\nThis means that the \frst episode started in state A, transitioned to Bwith\na reward of 0, and then terminated from Bwith a reward of 0. The other\nseven episodes were even shorter, starting from Band terminating immediately.\nGiven this batch of data, what would you say are the optimal predictions, the\nbest values for the estimates V(A) andV(B)? Everyone would probably agree\nthat the optimal value for V(B) is3\n4, because six out of the eight times in state\nBthe process terminated immediately with a return of 1, and the other two\ntimes in Bthe process terminated immediately with a return of 0.\nBut what is the optimal value for the estimate V(A) given this data? Here\nthere are two reasonable answers. One is to observe that 100% of the times\nthe process was in state Ait traversed immediately to B(with a reward of 0);\nand since we have already decided that Bhas value3\n4, therefore Amust have\nvalue3\n4as well. One way of viewing this answer is that it is based on \frst\nmodeling the Markov process, in this case as\n6.3. OPTIMALITY OF TD(0) 153\nA Br = 1\n100%75%\n25%r = 0r = 0\nand then computing the correct estimates given the model, which indeed in\nthis case gives V(A) =3\n4. This is also the answer that batch TD(0) gives.\nThe other reasonable answer is simply to observe that we have seen Aonce\nand the return that followed it was 0; we therefore estimate V(A) as 0. This\nis the answer that batch Monte Carlo methods give. Notice that it is also the\nanswer that gives minimum squared error on the training data. In fact, it gives\nzero error on the data. But still we expect the \frst answer to be better. If\nthe process is Markov, we expect that the \frst answer will produce lower error\nonfuture data, even though the Monte Carlo answer is better on the existing\ndata.\nThe above example illustrates a general di\u000berence between the estimates\nfound by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo\nmethods always \fnd the estimates that minimize mean-squared error on the\ntraining set, whereas batch TD(0) always \fnds the estimates that would be\nexactly correct for the maximum-likelihood model of the Markov process. In\ngeneral, the maximum-likelihood estimate of a parameter is the parameter\nvalue whose probability of generating the data is greatest. In this case, the\nmaximum-likelihood estimate is the model of the Markov process formed in the\nobvious way from the observed episodes: the estimated transition probability\nfromitojis the fraction of observed transitions from ithat went to j, and\nthe associated expected reward is the average of the rewards observed on\nthose transitions. Given this model, we can compute the estimate of the value\nfunction that would be exactly correct if the model were exactly correct. This\nis called the certainty-equivalence estimate because it is equivalent to assuming\nthat the estimate of the underlying process was known with certainty rather\nthan being approximated. In general, batch TD(0) converges to the certainty-\nequivalence estimate.\nThis helps explain why TD methods converge more quickly than Monte\nCarlo methods. In batch form, TD(0) is faster than Monte Carlo methods\nbecause it computes the true certainty-equivalence estimate. This explains\nthe advantage of TD(0) shown in the batch results on the random walk task\n(Figure 6.8). The relationship to the certainty-equivalence estimate may also\nexplain in part the speed advantage of nonbatch TD(0) (e.g., Figure 6.7). Al-\nthough the nonbatch methods do not achieve either the certainty-equivalence\n154 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nor the minimum squared-error estimates, they can be understood as moving\nroughly in these directions. Nonbatch TD(0) may be faster than constant- \u000b\nMC because it is moving toward a better estimate, even though it is not get-\nting all the way there. At the current time nothing more de\fnite can be said\nabout the relative e\u000eciency of on-line TD and Monte Carlo methods.\nFinally, it is worth noting that although the certainty-equivalence estimate\nis in some sense an optimal solution, it is almost never feasible to compute\nit directly. If Nis the number of states, then just forming the maximum-\nlikelihood estimate of the process may require N2memory, and computing the\ncorresponding value function requires on the order of N3computational steps\nif done conventionally. In these terms it is indeed striking that TD methods\ncan approximate the same solution using memory no more than Nand re-\npeated computations over the training set. On tasks with large state spaces,\nTD methods may be the only feasible way of approximating the certainty-\nequivalence solution.\n6.4 Sarsa: On-Policy TD Control\nWe turn now to the use of TD prediction methods for the control problem.\nAs usual, we follow the pattern of generalized policy iteration (GPI), only this\ntime using TD methods for the evaluation or prediction part. As with Monte\nCarlo methods, we face the need to trade o\u000b exploration and exploitation, and\nagain approaches fall into two main classes: on-policy and o\u000b-policy. In this\nsection we present an on-policy TD control method.\nThe \frst step is to learn an action-value function rather than a state-value\nfunction. In particular, for an on-policy method we must estimate q\u0019(s;a) for\nthe current behavior policy \u0019and for all states sand actions a. This can\nbe done using essentially the same TD method described above for learning\nv\u0019. Recall that an episode consists of an alternating sequence of states and\nstate{action pairs:\nAtRt+1StAt+1Rt+2St+1At+2Rt+3St+2At+3St+3. . .. . .\nIn the previous section we considered transitions from state to state and\nlearned the values of states. Now we consider transitions from state{action\npair to state{action pair, and learn the value of state{action pairs. Formally\nthese cases are identical: they are both Markov chains with a reward process.\nThe theorems assuring the convergence of state values under TD(0) also apply\n6.4. SARSA: ON-POLICY TD CONTROL 155\nInitializeQ(s;a);8s2S;a2A(s);arbitrarily, and Q(terminal-state ;\u0001) = 0\nRepeat (for each episode):\nInitializeS\nChooseAfromSusing policy derived from Q(e.g.,\u000f-greedy)\nRepeat (for each step of episode):\nTake action A, observeR,S0\nChooseA0fromS0using policy derived from Q(e.g.,\u000f-greedy)\nQ(S;A) Q(S;A) +\u000b\u0002\nR+\rQ(S0;A0)\u0000Q(S;A)\u0003\nS S0;A A0;\nuntilSis terminal\nFigure 6.9: Sarsa: An on-policy TD control algorithm.\nto the corresponding algorithm for action values:\nQ(St;At) Q(St;At) +\u000bh\nRt+1+\rQ(St+1;At+1)\u0000Q(St;At)i\n:(6.5)\nThis update is done after every transition from a nonterminal state St. IfSt+1\nis terminal, then Q(St+1;At+1) is de\fned as zero. This rule uses every element\nof the quintuple of events, ( St;At;Rt+1;St+1;At+1), that make up a transition\nfrom one state{action pair to the next. This quintuple gives rise to the name\nSarsa for the algorithm.\nIt is straightforward to design an on-policy control algorithm based on the\nSarsa prediction method. As in all on-policy methods, we continually estimate\nq\u0019for the behavior policy \u0019, and at the same time change \u0019toward greediness\nwith respect to q\u0019. The general form of the Sarsa control algorithm is given\nin Figure 6.9.\nThe convergence properties of the Sarsa algorithm depend on the nature\nof the policy's dependence on q. For example, one could use \"-greedy or \"-\nsoft policies. According to Satinder Singh (personal communication), Sarsa\nconverges with probability 1 to an optimal policy and action-value function as\nlong as all state{action pairs are visited an in\fnite number of times and the\npolicy converges in the limit to the greedy policy (which can be arranged, for\nexample, with \"-greedy policies by setting \"= 1=t), but this result has not yet\nbeen published in the literature.\nExample 6.5: Windy Gridworld Figure 6.10 shows a standard gridworld,\nwith start and goal states, but with one di\u000berence: there is a crosswind upward\nthrough the middle of the grid. The actions are the standard four| up, down,\nright , and left |but in the middle region the resultant next states are shifted\nupward by a \\wind,\" the strength of which varies from column to column. The\nstrength of the wind is given below each column, in number of cells shifted\n156 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nS G\n000 0 111 1 22standard\nmovesking's\nmoves\nFigure 6.10: Gridworld in which movement is altered by a location-dependent,\nupward \\wind.\"\n0 1000 2000 3000 4000 5000 6000 7000 8000050100150170\nEpisodes\nTime stepsS G\n000 0 111 1 22\nFigure 6.11: Results of Sarsa applied to the windy gridworld.\n6.5. Q-LEARNING: OFF-POLICY TD CONTROL 157\nupward. For example, if you are one cell to the right of the goal, then the\naction left takes you to the cell just above the goal. Let us treat this as an\nundiscounted episodic task, with constant rewards of \u00001 until the goal state\nis reached. Figure 6.11 shows the result of applying \"-greedy Sarsa to this\ntask, with\"= 0:1,\u000b= 0:5, and the initial values Q(s;a) = 0 for all s;a. The\nincreasing slope of the graph shows that the goal is reached more and more\nquickly over time. By 8000 time steps, the greedy policy (shown inset) was\nlong since optimal; continued \"-greedy exploration kept the average episode\nlength at about 17 steps, two more than the minimum of 15. Note that Monte\nCarlo methods cannot easily be used on this task because termination is not\nguaranteed for all policies. If a policy was ever found that caused the agent to\nstay in the same state, then the next episode would never end. Step-by-step\nlearning methods such as Sarsa do not have this problem because they quickly\nlearn during the episode that such policies are poor, and switch to something\nelse.\n6.5 Q-Learning: O\u000b-Policy TD Control\nOne of the most important breakthroughs in reinforcement learning was the de-\nvelopment of an o\u000b-policy TD control algorithm known as Q-learning (Watkins,\n1989). Its simplest form, one-step Q-learning , is de\fned by\nQ(St;At) Q(St;At) +\u000bh\nRt+1+\rmax\naQ(St+1;a)\u0000Q(St;At)i\n:(6.6)\nIn this case, the learned action-value function, Q, directly approximates q\u0003,\nthe optimal action-value function, independent of the policy being followed.\nThis dramatically simpli\fes the analysis of the algorithm and enabled early\nconvergence proofs. The policy still has an e\u000bect in that it determines which\nstate{action pairs are visited and updated. However, all that is required for\ncorrect convergence is that all pairs continue to be updated. As we observed\nin Chapter 5, this is a minimal requirement in the sense that any method\nguaranteed to \fnd optimal behavior in the general case must require it. Under\nthis assumption and a variant of the usual stochastic approximation conditions\non the sequence of step-size parameters, Qhas been shown to converge with\nprobability 1 to q\u0003. The Q-learning algorithm is shown in procedural form in\nFigure 6.12.\nWhat is the backup diagram for Q-learning? The rule (6.6) updates a\nstate{action pair, so the top node, the root of the backup, must be a small,\n\flled action node. The backup is also from action nodes, maximizing over all\nthose actions possible in the next state. Thus the bottom nodes of the backup\ndiagram should be all these action nodes. Finally, remember that we indicate\n158 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nInitializeQ(s;a);8s2S;a2A(s);arbitrarily, and Q(terminal-state ;\u0001) = 0\nRepeat (for each episode):\nInitializeS\nRepeat (for each step of episode):\nChooseAfromSusing policy derived from Q(e.g.,\u000f-greedy)\nTake action A, observeR,S0\nQ(S;A) Q(S;A) +\u000b\u0002\nR+\rmaxaQ(S0;a)\u0000Q(S;A)\u0003\nS S0;\nuntilSis terminal\nFigure 6.12: Q-learning: An o\u000b-policy TD control algorithm.\ntaking the maximum of these \\next action\" nodes with an arc across them\n(Figure 3.7). Can you guess now what the diagram is? If so, please do make\na guess before turning to the answer in Figure 6.14.\nExample 6.6: Cli\u000b Walking This gridworld example compares Sarsa\nand Q-learning, highlighting the di\u000berence between on-policy (Sarsa) and o\u000b-\npolicy (Q-learning) methods. Consider the gridworld shown in the upper part\nof Figure 6.13. This is a standard undiscounted, episodic task, with start and\ngoal states, and the usual actions causing movement up, down, right, and left.\nReward is\u00001 on all transitions except those into the the region marked \\The\nCli\u000b.\" Stepping into this region incurs a reward of \u0000100 and sends the agent\ninstantly back to the start. The lower part of the \fgure shows the performance\nof the Sarsa and Q-learning methods with \"-greedy action selection, \"= 0:1.\nAfter an initial transient, Q-learning learns values for the optimal policy, that\nwhich travels right along the edge of the cli\u000b. Unfortunately, this results in\nits occasionally falling o\u000b the cli\u000b because of the \"-greedy action selection.\nSarsa, on the other hand, takes the action selection into account and learns\nthe longer but safer path through the upper part of the grid. Although Q-\nlearning actually learns the values of the optimal policy, its on-line performance\nis worse than that of Sarsa, which learns the roundabout policy. Of course, if\n\"were gradually reduced, then both methods would asymptotically converge\nto the optimal policy.\n6.5. Q-LEARNING: OFF-POLICY TD CONTROL 159\nRewardperepsiode!100!75!50!25\n0100200300400500EpisodesSarsaQ-learningSGr = !100The Cliffr = !1safe pathoptimal pathR\nR\nFigure 6.13: The cli\u000b-walking task. The results are from a single run, but\nsmoothed.\nFigure 6.14: The backup diagram for Q-learning.\n160 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\n6.6 Games, Afterstates, and Other Special Cases\nIn this book we try to present a uniform approach to a wide class of tasks,\nbut of course there are always exceptional tasks that are better treated in a\nspecialized way. For example, our general approach involves learning an ac-\ntion-value function, but in Chapter 1 we presented a TD method for learning\nto play tic-tac-toe that learned something much more like a state -value func-\ntion. If we look closely at that example, it becomes apparent that the function\nlearned there is neither an action-value function nor a state-value function in\nthe usual sense. A conventional state-value function evaluates states in which\nthe agent has the option of selecting an action, but the state-value function\nused in tic-tac-toe evaluates board positions after the agent has made its move.\nLet us call these afterstates , and value functions over these, afterstate value\nfunctions . Afterstates are useful when we have knowledge of an initial part\nof the environment's dynamics but not necessarily of the full dynamics. For\nexample, in games we typically know the immediate e\u000bects of our moves. We\nknow for each possible chess move what the resulting position will be, but not\nhow our opponent will reply. Afterstate value functions are a natural way to\ntake advantage of this kind of knowledge and thereby produce a more e\u000ecient\nlearning method.\nThe reason it is more e\u000ecient to design algorithms in terms of afterstates is\napparent from the tic-tac-toe example. A conventional action-value function\nwould map from positions andmoves to an estimate of the value. But many\nposition{move pairs produce the same resulting position, as in this example:\nX\nOXX\nO+ XO+XX\nIn such cases the position{move pairs are di\u000berent but produce the same \\af-\nterposition,\" and thus must have the same value. A conventional action-value\nfunction would have to separately assess both pairs, whereas an afterstate\nvalue function would immediately assess both equally. Any learning about the\nposition{move pair on the left would immediately transfer to the pair on the\nright.\nAfterstates arise in many tasks, not just games. For example, in queuing\n6.7. SUMMARY 161\ntasks there are actions such as assigning customers to servers, rejecting cus-\ntomers, or discarding information. In such cases the actions are in fact de\fned\nin terms of their immediate e\u000bects, which are completely known. For exam-\nple, in the access-control queuing example described in the previous section,\na more e\u000ecient learning method could be obtained by breaking the environ-\nment's dynamics into the immediate e\u000bect of the action, which is deterministic\nand completely known, and the unknown random processes having to do with\nthe arrival and departure of customers. The afterstates would be the number\nof free servers after the action but before the random processes had produced\nthe next conventional state. Learning an afterstate value function over the\nafterstates would enable all actions that produced the same number of free\nservers to share experience. This should result in a signi\fcant reduction in\nlearning time.\nIt is impossible to describe all the possible kinds of specialized problems\nand corresponding specialized learning algorithms. However, the principles\ndeveloped in this book should apply widely. For example, afterstate methods\nare still aptly described in terms of generalized policy iteration, with a policy\nand (afterstate) value function interacting in essentially the same way. In many\ncases one will still face the choice between on-policy and o\u000b-policy methods\nfor managing the need for persistent exploration.\n6.7 Summary\nIn this chapter we introduced a new kind of learning method, temporal-\ndi\u000berence (TD) learning, and showed how it can be applied to the reinforce-\nment learning problem. As usual, we divided the overall problem into a pre-\ndiction problem and a control problem. TD methods are alternatives to Monte\nCarlo methods for solving the prediction problem. In both cases, the extension\nto the control problem is via the idea of generalized policy iteration (GPI) that\nwe abstracted from dynamic programming. This is the idea that approximate\npolicy and value functions should interact in such a way that they both move\ntoward their optimal values.\nOne of the two processes making up GPI drives the value function to accu-\nrately predict returns for the current policy; this is the prediction problem. The\nother process drives the policy to improve locally (e.g., to be \"-greedy) with\nrespect to the current value function. When the \frst process is based on expe-\nrience, a complication arises concerning maintaining su\u000ecient exploration. We\nhave grouped the TD control methods according to whether they deal with\nthis complication by using an on-policy or o\u000b-policy approach. Sarsa and\nactor{critic methods are on-policy methods, and Q-learning and R-learning\n162 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nare o\u000b-policy methods.\nThe methods presented in this chapter are today the most widely used re-\ninforcement learning methods. This is probably due to their great simplicity:\nthey can be applied on-line, with a minimal amount of computation, to expe-\nrience generated from interaction with an environment; they can be expressed\nnearly completely by single equations that can be implemented with small\ncomputer programs. In the next few chapters we extend these algorithms,\nmaking them slightly more complicated and signi\fcantly more powerful. All\nthe new algorithms will retain the essence of those introduced here: they will\nbe able to process experience on-line, with relatively little computation, and\nthey will be driven by TD errors. The special cases of TD methods introduced\nin the present chapter should rightly be called one-step, tabular, modelfree TD\nmethods. In the next three chapters we extend them to multistep forms (a\nlink to Monte Carlo methods), forms using function approximation rather than\ntables (a link to arti\fcial neural networks), and forms that include a model of\nthe environment (a link to planning and dynamic programming).\nFinally, in this chapter we have discussed TD methods entirely within\nthe context of reinforcement learning problems, but TD methods are actually\nmore general than this. They are general methods for learning to make long-\nterm predictions about dynamical systems. For example, TD methods may\nbe relevant to predicting \fnancial data, life spans, election outcomes, weather\npatterns, animal behavior, demands on power stations, or customer purchases.\nIt was only when TD methods were analyzed as pure prediction methods, inde-\npendent of their use in reinforcement learning, that their theoretical properties\n\frst came to be well understood. Even so, these other potential applications\nof TD learning methods have not yet been extensively explored.\nBibliographical and Historical Remarks\nAs we outlined in Chapter 1, the idea of TD learning has its early roots in\nanimal learning psychology and arti\fcial intelligence, most notably the work\nof Samuel (1959) and Klopf (1972). Samuel's work is described as a case\nstudy in Section 15.2. Also related to TD learning are Holland's (1975, 1976)\nearly ideas about consistency among value predictions. These in\ruenced one\nof the authors (Barto), who was a graduate student from 1970 to 1975 at the\nUniversity of Michigan, where Holland was teaching. Holland's ideas led to a\nnumber of TD-related systems, including the work of Booker (1982) and the\nbucket brigade of Holland (1986), which is related to Sarsa as discussed below.\n6.1{2 Most of the speci\fc material from these sections is from Sutton (1988),\n6.7. SUMMARY 163\nincluding the TD(0) algorithm, the random walk example, and the term\n\\temporal-di\u000berence learning.\" The characterization of the relationship\nto dynamic programming and Monte Carlo methods was in\ruenced\nby Watkins (1989), Werbos (1987), and others. The use of backup\ndiagrams here and in other chapters is new to this book. Example 6.4\nis due to Sutton, but has not been published before.\nTabular TD(0) was proved to converge in the mean by Sutton (1988)\nand with probability 1 by Dayan (1992), based on the work of Watkins\nand Dayan (1992). These results were extended and strengthened by\nJaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994) by using ex-\ntensions of the powerful existing theory of stochastic approximation.\nOther extensions and generalizations are covered in the next two chap-\nters.\n6.3 The optimality of the TD algorithm under batch training was estab-\nlished by Sutton (1988). The term certainty equivalence is from the\nadaptive control literature (e.g., Goodwin and Sin, 1984). Illuminating\nthis result is Barnard's (1993) derivation of the TD algorithm as a com-\nbination of one step of an incremental method for learning a model of\nthe Markov chain and one step of a method for computing predictions\nfrom the model.\n6.4 The Sarsa algorithm was \frst explored by Rummery and Niranjan\n(1994), who called it modi\fed Q-learning . The name \\Sarsa\" was in-\ntroduced by Sutton (1996). The convergence of one-step tabular Sarsa\n(the form treated in this chapter) has been proved by Satinder Singh\n(personal communication). The \\windy gridworld\" example was sug-\ngested by Tom Kalt.\nHolland's (1986) bucket brigade idea evolved into an algorithm closely\nrelated to Sarsa. The original idea of the bucket brigade involved chains\nof rules triggering each other; it focused on passing credit back from\nthe current rule to the rules that triggered it. Over time, the bucket\nbrigade came to be more like TD learning in passing credit back to\nany temporally preceding rule, not just to the ones that triggered the\ncurrent rule. The modern form of the bucket brigade, when simpli\fed\nin various natural ways, is nearly identical to one-step Sarsa, as detailed\nby Wilson (1994).\n6.5 Q-learning was introduced by Watkins (1989), whose outline of a con-\nvergence proof was later made rigorous by Watkins and Dayan (1992).\nMore general convergence results were proved by Jaakkola, Jordan, and\nSingh (1994) and Tsitsiklis (1994).\n164 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\n6.6 R-learning is due to Schwartz (1993). Mahadevan (1996), Tadepalli\nand Ok (1994), and Bertsekas and Tsitsiklis (1996) have studied rein-\nforcement learning for undiscounted continuing tasks. In the literature,\nthe undiscounted continuing case is often called the case of maximiz-\ning \\average reward per time step\" or the \\average-reward case.\" The\nname R-learning was probably meant to be the alphabetic successor to\nQ-learning, but we prefer to think of it as a reference to the learning of\nrelative values. The access-control queuing example was suggested by\nthe work of Carlstr\u007f om and Nordstr\u007f om (1997).\nExercises\nExercise 6.1 This is an exercise to help develop your intuition about why\nTD methods are often more e\u000ecient than Monte Carlo methods. Consider\nthe driving home example and how it is addressed by TD and Monte Carlo\nmethods. Can you imagine a scenario in which a TD update would be better on\naverage than an Monte Carlo update? Give an example scenario|a description\nof past experience and a current state|in which you would expect the TD\nupdate to be better. Here's a hint: Suppose you have lots of experience\ndriving home from work. Then you move to a new building and a new parking\nlot (but you still enter the highway at the same place). Now you are starting\nto learn predictions for the new building. Can you see why TD updates are\nlikely to be much better, at least initially, in this case? Might the same sort\nof thing happen in the original task?\nExercise 6.2 From Figure 6.6, it appears that the \frst episode results in a\nchange in only V(A). What does this tell you about what happened on the\n\frst episode? Why was only the estimate for this one state changed? By\nexactly how much was it changed?\nExercise 6.3 The speci\fc results shown in Figure 6.7 are dependent on the\nvalue of the step-size parameter, \u000b. Do you think the conclusions about which\nalgorithm is better would be a\u000bected if a wider range of \u000bvalues were used?\nIs there a di\u000berent, \fxed value of \u000bat which either algorithm would have\nperformed signi\fcantly better than shown? Why or why not?\nExercise 6.4 In Figure 6.7, the RMS error of the TD method seems to go\ndown and then up again, particularly at high \u000b's. What could have caused\nthis? Do you think this always occurs, or might it be a function of how the\napproximate value function was initialized?\nExercise 6.5 Above we stated that the true values for the random walk task\n6.7. SUMMARY 165\nare1\n6;2\n6;3\n6;4\n6, and5\n6, for states Athrough E. Describe at least two di\u000berent\nways that these could have been computed. Which would you guess we actually\nused? Why?\nExercise 6.6: Windy Gridworld with King's Moves Re-solve the\nwindy gridworld task assuming eight possible actions, including the diagonal\nmoves, rather than the usual four. How much better can you do with the extra\nactions? Can you do even better by including a ninth action that causes no\nmovement at all other than that caused by the wind?\nExercise 6.7: Stochastic Wind Resolve the windy gridworld task with\nKing's moves, assuming that the e\u000bect of the wind, if there is any, is stochastic,\nsometimes varying by 1 from the mean values given for each column. That\nis, a third of the time you move exactly according to these values, as in the\nprevious exercise, but also a third of the time you move one cell above that,\nand another third of the time you move one cell below that. For example, if\nyou are one cell to the right of the goal and you move left , then one-third\nof the time you move one cell above the goal, one-third of the time you move\ntwo cells above the goal, and one-third of the time you move to the goal.\nExercise 6.8 What is the backup diagram for Sarsa?\nExercise 6.9 Why is Q-learning considered an o\u000b-policy control method?\nExercise 6.10 Consider the learning algorithm that is just like Q-learning\nexcept that instead of the maximum over next state{action pairs it uses the\nexpected value, taking into account how likely each action is under the current\npolicy. That is, consider the algorithm otherwise like Q-learning except with\nthe update rule\nQ(St;At) Q(St;At) +\u000bh\nRt+1+\rE[Q(St+1;At+1)jSt+1]\u0000Q(St;At)i\n Q(St;At) +\u000bh\nRt+1+\rX\na\u0019(ajSt+1)Q(St+1;a)\u0000Q(St;At)i\n:\nIs this new method an on-policy or o\u000b-policy method? What is the backup\ndiagram for this algorithm? Given the same amount of experience, would\nyou expect this method to work better or worse than Sarsa? What other\nconsiderations might impact the comparison of this method with Sarsa?\nExercise 6.11 Describe how the task of Jack's Car Rental (Example 4.2)\ncould be reformulated in terms of afterstates. Why, in terms of this speci\fc\ntask, would such a reformulation be likely to speed convergence?\n166 CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING\nChapter 7\nEligibility Traces\nEligibility traces are one of the basic mechanisms of reinforcement learning.\nFor example, in the popular TD( \u0015) algorithm, the \u0015refers to the use of an\neligibility trace. Almost any temporal-di\u000berence (TD) method, such as Q-\nlearning or Sarsa, can be combined with eligibility traces to obtain a more\ngeneral method that may learn more e\u000eciently.\nThere are two ways to view eligibility traces. The more theoretical view,\nwhich we emphasize here, is that they are a bridge from TD to Monte Carlo\nmethods. When TD methods are augmented with eligibility traces, they pro-\nduce a family of methods spanning a spectrum that has Monte Carlo methods\nat one end and one-step TD methods at the other. In between are intermediate\nmethods that are often better than either extreme method. In this sense eli-\ngibility traces unify TD and Monte Carlo methods in a valuable and revealing\nway.\nThe other way to view eligibility traces is more mechanistic. From this\nperspective, an eligibility trace is a temporary record of the occurrence of an\nevent, such as the visiting of a state or the taking of an action. The trace\nmarks the memory parameters associated with the event as eligible for un-\ndergoing learning changes. When a TD error occurs, only the eligible states\nor actions are assigned credit or blame for the error. Thus, eligibility traces\nhelp bridge the gap between events and training information. Like TD meth-\nods themselves, eligibility traces are a basic mechanism for temporal credit\nassignment.\nFor reasons that will become apparent shortly, the more theoretical view of\neligibility traces is called the forward view, and the more mechanistic view is\ncalled the backward view. The forward view is most useful for understanding\nwhat is computed by methods using eligibility traces, whereas the backward\nview is more appropriate for developing intuition about the algorithms them-\n167\n168 CHAPTER 7. ELIGIBILITY TRACES\nselves. In this chapter we present both views and then establish senses in\nwhich they are equivalent, that is, in which they describe the same algorithms\nfrom two points of view. As usual, we \frst consider the prediction problem\nand then the control problem. That is, we \frst consider how eligibility traces\nare used to help in predicting returns as a function of state for a \fxed pol-\nicy (i.e., in estimating v\u0019). Only after exploring the two views of eligibility\ntraces within this prediction setting do we extend the ideas to action values\nand control methods.\n7.1n-Step TD Prediction\nWhat is the space of methods lying between Monte Carlo and TD methods?\nConsider estimating v\u0019from sample episodes generated using \u0019. Monte Carlo\nmethods perform a backup for each state based on the entire sequence of\nobserved rewards from that state until the end of the episode. The backup of\nsimple TD methods, on the other hand, is based on just the one next reward,\nusing the value of the state one step later as a proxy for the remaining rewards.\nOne kind of intermediate method, then, would perform a backup based on an\nintermediate number of rewards: more than one, but less than all of them\nuntil termination. For example, a two-step backup would be based on the \frst\ntwo rewards and the estimated value of the state two steps later. Similarly,\nwe could have three-step backups, four-step backups, and so on. Figure 7.1\ndiagrams the spectrum of n-step backups forv\u0019, with the one-step, simple TD\nbackup on the left and the up-until-termination Monte Carlo backup on the\nright.\nThe methods that use n-step backups are still TD methods because they\nstill change an earlier estimate based on how it di\u000bers from a later estimate.\nNow the later estimate is not one step later, but nsteps later. Methods\nin which the temporal di\u000berence extends over nsteps are called n-step TD\nmethods . The TD methods introduced in the previous chapter all use one-step\nbackups, and henceforth we call them one-step TD methods .\nMore formally, consider the backup applied to state Stas a result of the\nstate{reward sequence, St;Rt+1;St+1;Rt+2;:::;RT;ST(omitting the actions\nfor simplicity). We know that in Monte Carlo backups the estimate of v\u0019(St)\nis updated in the direction of the complete return:\nGt=Rt+1+\rRt+2+\r2Rt+3+\u0001\u0001\u0001+\rT\u0000t\u00001RT;\nwhereTis the last time step of the episode. Let us call this quantity the target\nof the backup. Whereas in Monte Carlo backups the target is the return, in\n7.1.N-STEP TD PREDICTION 169\nTD (1-step)2-step 3-step n-step Monte Carlo\nFigure 7.1: The spectrum ranging from the one-step backups of simple TD\nmethods to the up-until-termination backups of Monte Carlo methods. In\nbetween are the n-step backups, based on nsteps of real rewards and the\nestimated value of the nth next state, all appropriately discounted.\none-step backups the target is the \frst reward plus the discounted estimated\nvalue of the next state:\nRt+1+\rVt(St+1);\nwhereVt:S!Rhere is the estimate at time tofv\u0019, in which case it makes\nsense that \rVt(St+1) should take the place of the remaining terms \rRt+2+\n\r2Rt+3+\u0001\u0001\u0001+\rT\u0000t\u00001RT, as we discussed in the previous chapter. Our point\nnow is that this idea makes just as much sense after two steps as it does after\none. The target for a two-step backup might be\nRt+1+\rRt+2+\r2Vt(St+2);\nwhere now \r2Vt(St+2) corrects for the absence of the terms \r2Rt+3+\r3Rt+4+\n\u0001\u0001\u0001+\rT\u0000t\u00001RT. Similarly, the target for an arbitrary n-step backup might be\nRt+1+\rRt+2+\r2+\u0001\u0001\u0001+\rn\u00001Rt+n+\rnVt(St+n);8n\u00151: (7.1)\nAll of these can be considered approximate returns, truncated after nsteps and\nthen corrected for the remaining missing terms, in the above case by Vt(St+n).\nNotationally, it is useful to retain full generality for the correction term. We\nde\fne the general n-step return as\nGt+n\nt(c) =Rt+1+\rRt+2+\u0001\u0001\u0001+\rn\u00001Rh+\rnc;\n170 CHAPTER 7. ELIGIBILITY TRACES\nfor anyn\u00151 and any scalar correction c2R. The time h=t+nis called\nthehorizon of then-step return.\nIf the episode ends before the horizon is reached, then the truncation in\nann-step return e\u000bectively occurs at the episode's end, resulting in the con-\nventional complete return. In other words, if h\u0015T, thenGh\nt(c) =Gt. Thus,\nthe lastn n-step returns of an episode are always complete returns, and an\nin\fnite-step return is always a complete return. This de\fnition enables us\nto treat Monte Carlo methods as the special case of in\fnite-step targets. All\nof this is consistent with the tricks for treating episodic and continuing tasks\nequivalently that we introduced in Section 3.4. There we chose to treat the\nterminal state as a state that always transitions to itself with zero reward.\nUnder this trick, all n-step returns that last up to or past termination have\nthe same value as the complete return.\nAnn-step backup is de\fned to be a backup toward the n-step return. In the\ntabular, state-value case, the n-step backup at time tproduces the following\nincrement \u0001 t(St) in the estimated value V(St):\n\u0001t(St) =\u000bh\nGt+n\nt(Vt(St+n))\u0000Vt(St)i\n; (7.2)\nwhere\u000bis a positive step-size parameter, as usual. The increments to the\nestimated values of the other states are de\fned to be zero (\u0001 t(s) = 0;8s6=St).\nWe de\fne the n-step backup in terms of an increment, rather than as a\ndirect update rule as we did in the previous chapter, in order to allow di\u000berent\nways of making the updates. In on-line updating , the updates are made during\nthe episode, as soon as the increment is computed. In this case we write\nVt+1(s) =Vt(s) + \u0001t(s);8s2S: (7.3)\nOn-line updating is what we have implicitly assumed in most of the previous\ntwo chapters. In o\u000b-line updating , on the other hand, the increments are\naccumulated \\on the side\" and are not used to change value estimates until\nthe end of the episode. In this case, the approximate values Vt(s);8s2S, do\nnot change during an episode and can be denoted simpty V(s). At the end\nof the episode, the new value (for the next episode) is obtained by summing\nall the increments during the episode. That is, for an episode starting at time\nstep 0 and terminating at step T, for alls2S:\nVt+1(s) =Vt(s);8t<T;\nVT(s) =VT\u00001(s) +T\u00001X\nt=0\u0001t(s);(7.4)\nwith of course V0of the next episode being the VTof this one. You may\nrecall how in Section 6.3 we carried this idea one step further, deferring the\n7.1.N-STEP TD PREDICTION 171\nincrements until they could be summed over a whole set of episodes, in batch\nupdating .\nFor any value function v:S!R, the expected value of the n-step return\nusingvis guaranteed to be a better estimate of v\u0019thanvis, in a worst-state\nsense. That is, the worst error under the new estimate is guaranteed to be less\nthan or equal to \rntimes the worst error under v:\nmax\ns\f\fE\u0019\u0002\nGt+n\nt(v(St+n))\f\fSt=s\u0003\n\u0000v\u0019(s)\f\f\u0014\rnmax\nsjv(s)\u0000v\u0019(s)j;(7.5)\nfor alln\u00151. This is called the error reduction property ofn-step returns.\nBecause of the error reduction property, one can show formally that on-line and\no\u000b-line TD prediction methods using n-step backups converge to the correct\npredictions under appropriate technical conditions. The n-step TD methods\nthus form a family of valid methods, with one-step TD methods and Monte\nCarlo methods as extreme members.\nNevertheless, n-step TD methods are rarely used because they are incon-\nvenient to implement. Computing n-step returns requires waiting nsteps to\nobserve the resultant rewards and states. For large n, this can become prob-\nlematic, particularly in control applications. The signi\fcance of n-step TD\nmethods is primarily for theory and for understanding related methods that\nare more conveniently implemented. In the next few sections we use the idea\nofn-step TD methods to explain and justify eligibility trace methods.\nExample 7.1: n-step TD Methods on the Random Walk Consider\nusingn-step TD methods on the random walk task described in Example 6.2\nand shown in Figure 6.5. Suppose the \frst episode progressed directly from the\ncenter state, C, to the right, through DandE, and then terminated on the right\nwith a return of 1. Recall that the estimated values of all the states started\nat an intermediate value, V0(s) = 0:5. As a result of this experience, a one-\nstep method would change only the estimate for the last state, V(E), which\nwould be incremented toward 1, the observed return. A two-step method,\non the other hand, would increment the values of the two states preceding\ntermination: V(D) andV(E) both would be incremented toward 1. A three-\nstep method, or any n-step method for n > 2, would increment the values\nof all three of the visited states toward 1, all by the same amount. Which\nnis better? Figure 7.2 shows the results of a simple empirical assessment\nfor a larger random walk process, with 19 states (and with a \u00001 outcome on\nthe left, all values initialized to 0). Results are shown for on-line and o\u000b-line\nn-step TD methods with a range of values for nand\u000b. The performance\nmeasure for each algorithm and parameter setting, shown on the vertical axis,\nis the square-root of the average squared error between its predictions at the\nend of the episodenfor the 19 states and their true values, then averaged over\n172 CHAPTER 7. ELIGIBILITY TRACES\nOn-line n-step TD methodsOff-line n-step TD methods\n\u21b5\u21b5RMS errorover \ufb01rst10 episodesn=1n=2n=4n=8n=16n=32n=64256128512\nn=3n=64n=1n=2n=4n=8n=16n=32n=32n=64128512256\nFigure 7.2: Performance of n-step TD methods as a function of \u000b, for various\nvalues ofn, on a 19-state random walk task (Example 7.1).\nthe \frst 10 episodes and 100 repetitions of the whole experiment (the same\nsets of walks were used for all methods). First note that the on-line methods\ngenerally worked best on this task, both reaching lower levels of absolute error\nand doing so over a larger range of the step-size parameter \u000b(in fact, all\nthe o\u000b-line methods were unstable for \u000bmuch above 0.3). Second, note that\nmethods with an intermediate value of nworked best. This illustrates how\nthe generalization of TD and Monte Carlo methods to n-step methods can\npotentially perform better than either of the two extreme methods.\n7.2 The Forward View of TD( \u0015)\nBackups can be done not just toward any n-step return, but toward any aver-\nageofn-step returns. For example, a backup can be done toward a target that\nis half of a two-step return and half of a four-step return:1\n2Gt+2\nt(Vt(St+2)) +\n1\n2Gt+4\nt(Vt(St+4)). Any set of returns can be averaged in this way, even an in-\n\fnite set, as long as the weights on the component returns are positive and\nsum to 1. The composite return possesses an error reduction property similar\nto that of individual n-step returns (7.5) and thus can be used to construct\nbackups with guaranteed convergence properties. Averaging produces a sub-\nstantial new range of algorithms. For example, one could average one-step\nand in\fnite-step returns to obtain another way of interrelating TD and Monte\nCarlo methods. In principle, one could even average experience-based back-\nups with DP backups to get a simple combination of experience-based and\nmodel-based methods (see Chapter 8).\n7.2. THE FORWARD VIEW OF TD( \u0015) 173\nA backup that averages simpler component backups is called a complex\nbackup . The backup diagram for a complex backup consists of the backup\ndiagrams for each of the component backups with a horizontal line above\nthem and the weighting fractions below. For example, the complex backup\nfor the case mentioned at the start of this section, mixing half of a two-step\nbackup and half of a four-step backup, has the diagram:\n1\n2\n1\n2\nThe TD(\u0015) algorithm can be understood as one particular way of averaging\nn-step backups. This average contains all the n-step backups, each weighted\nproportional to \u0015n\u00001, where\u00152[0;1], and normalized by a factor of 1 \u0000\u0015\nto ensure that the weights sum to 1 (see Figure 7.3). The resulting backup is\ntoward a return, called the \u0015-return , de\fned by\nLt= (1\u0000\u0015)1X\nn=1\u0015n\u00001Gt+n\nt(Vt(St+n)):\nFigure 7.4 further illustrates the weighting on the sequence of n-step returns\nin the\u0015-return. The one-step return is given the largest weight, 1 \u0000\u0015; the\ntwo-step return is given the next largest weight, (1 \u0000\u0015)\u0015; the three-step return\nis given the weight (1 \u0000\u0015)\u00152; and so on. The weight fades by \u0015with each\nadditional step. After a terminal state has been reached, all subsequent n-step\nreturns are equal to Gt. If we want, we can separate these post-termination\nterms from the main sum, yielding\nLt= (1\u0000\u0015)T\u0000t\u00001X\nn=1\u0015n\u00001Gt+n\nt(Vt(St+n)) +\u0015T\u0000t\u00001Gt; (7.6)\nas indicated in the \fgures. This equation makes it clearer what happens when\n\u0015= 1. In this case the main sum goes to zero, and the remaining term reduces\nto the conventional return, Gt. Thus, for \u0015= 1, backing up according to the\n174 CHAPTER 7. ELIGIBILITY TRACES\n1!\"\n(1!\") \"\n(1!\") \"2\n#= 1TD(\"), \"-return\n\"T-t-1\nFigure 7.3: The backup digram for TD( \u0015). If\u0015= 0, then the overall backup\nreduces to its \frst component, the one-step TD backup, whereas if \u0015= 1, then\nthe overall backup reduces to its last component, the Monte Carlo backup.\n\u0015-return is the same as the Monte Carlo algorithm that we called constant- \u000b\nMC (6.1) in the previous chapter. On the other hand, if \u0015= 0, then the\n\u0015-return reduces to Gt+1\nt(Vt(St+1)), the one-step return. Thus, for \u0015= 0,\nbacking up according to the \u0015-return is the same as the one-step TD method,\nTD(0) from the previous chapter (6.2).\nWe de\fne the \u0015-return algorithm as the method that performs backups\ntowards the \u0015-return as target. On each step, t, it computes an increment,\n1!\"weight given tothe 3-step returndecay by \"weight given toactual, final returntTTimeWeighttotal area = 1is(1\u0000\u0000)\u00002is\u0000T\u0000t\u00001\nFigure 7.4: Weighting given in the \u0015-return to each of the n-step returns.\n7.2. THE FORWARD VIEW OF TD( \u0015) 175\nTimert+3rt+2rt+1rTst+1st+2st+3stStSt+1St+2St+3RRRR\nFigure 7.5: The forward or theoretical view. We decide how to update each\nstate by looking forward to future rewards and states.\n\u0001t(St), to the value of the state occurring on that step:\n\u0001t(St) =\u000bh\nLt\u0000Vt(St)i\n: (7.7)\n(The increments for other states are of course \u0001 t(s) = 0, for all s6=St.)\nAs withn-step TD methods, the updating can be either on-line or o\u000b-line.\nThe upper row of Figure 7.6 shows the performance of the on-line and o\u000b-\nline\u0015-return algorithms on the 19-state random walk task (Example 7.1).\nThe experiment was just as in the n-step case (Figure 7.2) except that here we\nvaried\u0015instead ofn. Note that overall performance of the \u0015-return algorithms\nis comparable to that of the n-step algorithms. In both cases we get best\nperformance with an intermediate value of the truncation parameter, nor\u0015.\nThe approach that we have been taking so far is what we call the theoret-\nical, or forward , view of a learning algorithm. For each state visited, we look\nforward in time to all the future rewards and decide how best to combine them.\nWe might imagine ourselves riding the stream of states, looking forward from\neach state to determine its update, as suggested by Figure 7.5. After looking\nforward from and updating one state, we move on to the next and never have\nto work with the preceding state again. Future states, on the other hand,\nare viewed and processed repeatedly, once from each vantage point preceding\nthem.\nThe\u0015-return algorithm is the basis for the forward view of eligibility traces\nas used in the TD( \u0015) method. In fact, we show in a later section that, in the\no\u000b-line case, the \u0015-return algorithm isthe TD(\u0015) algorithm. The \u0015-return\nand TD(\u0015) methods use the \u0015parameter to shift from one-step TD methods\nto Monte Carlo methods. The speci\fc way this shift is done is interesting,\nbut not obviously better or worse than the way it is done with simple n-step\nmethods by varying n. Ultimately, the most compelling motivation for the \u0015\nway of mixing n-step backups is that there in a simple algorithm|TD( \u0015)|for\nachieving it. This is a mechanism issue rather than a theoretical one. In the\n176 CHAPTER 7. ELIGIBILITY TRACES\nOn-line TD(\u03bb), accumulating tracesOn-line TD(\u03bb), dutch traces\nOn-line \u03bb-returnOff-line \u03bb-return = off-line TD(\u03bb), accumulating tracesRMS error over \ufb01rst 10 episodes on 19-state random walk\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95.975.991\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95\u03bb=.975\u03bb=.99\u03bb=1\u03bb=.95\nOn-line TD(\u03bb), replacing tracesTrue on-line TD(\u03bb)= real-time \u03bb-return\n\u21b5\u21b5\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95\u03bb=.975\u03bb=.99\u03bb=1\u03bb=.95\n\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95\u03bb=.975\u03bb=.99\u03bb=1\u03bb=.95\u03bb=.975\u03bb=1\u03bb=.99\u03bb=0\u03bb=.4\u03bb=.8\u03bb=.9\u03bb=.95\u03bb=.975\u03bb=.99\u03bb=1\u03bb=.99\n\u03bb=.975\nFigure 7.6: Performance of all \u0015-based algorithms on the 19-state random walk\n(Example 7.1). The \u0015= 0 line is the same for all \fve on-line algorithms.\n7.3. THE BACKWARD VIEW OF TD( \u0015) 177\nnext few sections we develop the mechanistic, or backward, view of eligibility\ntraces as used in TD( \u0015).\n7.3 The Backward View of TD( \u0015)\nIn the previous section we presented the forward or theoretical view of the\ntabular TD( \u0015) algorithm as a way of mixing backups that parametrically shifts\nfrom a TD method to a Monte Carlo method. In this section we instead\nde\fne TD(\u0015) mechanistically and show that it can closely approximate the\nforward view. The mechanistic, or backward , view of TD( \u0015) is useful because\nit is simple conceptually and computationally. In particular, the forward view\nitself is not directly implementable because it is acausal , using at each step\nknowledge of what will happen many steps later. The backward view provides\na causal, incremental mechanism for approximating the forward view and, in\nthe o\u000b-line case, for achieving it exactly.\nIn the backward view of TD( \u0015), there is an additional memory variable\nassociated with each state, its eligibility trace . The eligibility trace for state\nsat timetis a random variable denoted Et(s)2R+. On each step, the\neligibility traces of all non-visited states decay by \r\u0015:\nEt(s) =\r\u0015Et\u00001(s);8s2S;s6=St; (7.8)\nwhere\ris the discount rate and \u0015is the parameter introduced in the previous\nsection. Henceforth we refer to \u0015as the trace-decay parameter . What about\nthe trace for St, the one state visited at time t? The classical eligibility trace\nforStdecays just like for any state, but is then incremented by 1:\nEt(St) =\r\u0015Et\u00001(St) + 1: (7.9)\nThis kind of eligibility trace is called an accumulating trace because it accu-\nmulates each time the state is visited, then fades away gradually when the\nstate is not visited, as illustrated as illustrated below.\naccumulating eligibility trace\ntimes of visits to a state\nEligibility traces keep a simple record of which states have recently been\nvisited, where \\recently\" is de\fned in terms of \r\u0015. The traces are said to indi-\ncate the degree to which each state is eligible for undergoing learning changes\nshould a reinforcing event occur. The reinforcing events we are concerned with\n178 CHAPTER 7. ELIGIBILITY TRACES\nare the moment-by-moment one-step TD errors. For example, the TD error\nfor state-value prediction is\n\u000et=Rt+1+\rVt(St+1)\u0000Vt(St): (7.10)\nIn the backward view of TD( \u0015), the global TD error signal triggers propor-\ntional updates to all recently visited states, as signaled by their nonzero traces:\n\u0001Vt(s) =\u000b\u000etEt(s); for alls2S: (7.11)\nAs always, these increments could be done on each step to form an on-line\nalgorithm, or saved until the end of the episode to produce an o\u000b-line algo-\nrithm. In either case, equations (7.8{7.11) provide the mechanistic de\fnition\nof the TD(\u0015) algorithm. A complete algorithm for on-line TD( \u0015) is given in\nFigure 7.7.\nThe backward view of TD( \u0015) is oriented backward in time. At each mo-\nment we look at the current TD error and assign it backward to each prior\nstate according to the state's eligibility trace at that time. We might imagine\nourselves riding along the stream of states, computing TD errors, and shouting\nthem back to the previously visited states, as suggested by Figure 7.8. Where\nthe TD error and traces come together, we get the update given by (7.11).\nTo better understand the backward view, consider what happens at various\nvalues of\u0015. If\u0015= 0, then by (7.9) all traces are zero at texcept for the trace\ncorresponding to St. Thus the TD( \u0015) update (7.11) reduces to the simple TD\nInitializeV(s) arbitrarily (but set to 0 if sis terminal)\nRepeat (for each episode):\nInitializeE(s) = 0, for all s2S\nInitializeS\nRepeat (for each step of episode):\nA action given by \u0019forS\nTake action A, observe reward, R, and next state, S0\n\u000e R+\rV(S0)\u0000V(S)\nE(S) E(S) + 1 (accumulating traces)\norE(S) (1\u0000\u000b)E(S) + 1 (dutch traces)\norE(S) 1 (replacing traces)\nFor alls2S:\nV(s) V(s) +\u000b\u000eE (s)\nE(s) \r\u0015E (s)\nS S0\nuntilSis terminal\nFigure 7.7: On-line tabular TD( \u0015).\n7.3. THE BACKWARD VIEW OF TD( \u0015) 179\n!tetetetetTimestst+1st-1st-2st-3\ud835\udefftSt+1StSt-1St-2St-3xivSUMMARY OF NOTATION\u0000ttemporal-di\u21b5erence error att(a random variable, even though not upper case)Et(s)e l i g i b i l i t y t r a c e f o r s t a t esattEt(s, a)e l i g i b i l i t y t r a c e f o r a s t a t e \u2013 a c t i o n p a i reteligibility trace vector att\u0000discount-rate parameter\"probability of random action in\"-greedy policy\u21b5,\u0000step-size parameters\u0000decay-rate parameter for eligibility tracesxivSUMMARY OF NOTATION\u0000ttemporal-di\u21b5erence error att(a random variable, even though not upper case)Et(s)e l i g i b i l i t y t r a c e f o r s t a t esattEt(s, a)e l i g i b i l i t y t r a c e f o r a s t a t e \u2013 a c t i o n p a i reteligibility trace vector att\u0000discount-rate parameter\"probability of random action in\"-greedy policy\u21b5,\u0000step-size parameters\u0000decay-rate parameter for eligibility tracesxivSUMMARY OF NOTATION\u0000ttemporal-di\u21b5erence error att(a random variable, even though not upper case)Et(s)e l i g i b i l i t y t r a c e f o r s t a t esattEt(s, a)e l i g i b i l i t y t r a c e f o r a s t a t e \u2013 a c t i o n p a i reteligibility trace vector att\u0000discount-rate parameter\"probability of random action in\"-greedy policy\u21b5,\u0000step-size parameters\u0000decay-rate parameter for eligibility tracesxivSUMMARY OF NOTATION\u0000ttemporal-di\u21b5erence error att(a random variable, even though not upper case)Et(s)e l i g i b i l i t y t r a c e f o r s t a t esattEt(s, a)e l i g i b i l i t y t r a c e f o r a s t a t e \u2013 a c t i o n p a i reteligibility trace vector att\u0000discount-rate parameter\"probability of random action in\"-greedy policy\u21b5,\u0000step-size parameters\u0000decay-rate parameter for eligibility traces\nFigure 7.8: The backward or mechanistic view. Each update depends on the\ncurrent TD error combined with eligibility traces of past events.\nrule (6.2), which we henceforth call TD(0). In terms of Figure 7.8, TD(0) is\nthe case in which only the one state preceding the current one is changed by\nthe TD error. For larger values of \u0015, but still\u0015 < 1, more of the preceding\nstates are changed, but each more temporally distant state is changed less\nbecause its eligibility trace is smaller, as suggested in the \fgure. We say that\nthe earlier states are given less credit for the TD error.\nIf\u0015= 1, then the credit given to earlier states falls only by \rper step. This\nturns out to be just the right thing to do to achieve Monte Carlo behavior.\nFor example, remember that the TD error, \u000et, includes an undiscounted term\nofRt+1. In passing this back ksteps it needs to be discounted, like any reward\nin a return, by \rk, which is just what the falling eligibility trace achieves. If\n\u0015= 1 and\r= 1, then the eligibility traces do not decay at all with time. In\nthis case the method behaves like a Monte Carlo method for an undiscounted,\nepisodic task. If \u0015= 1, the algorithm is also known as TD(1).\nTD(1) is a way of implementing Monte Carlo algorithms that is more gen-\neral than those presented earlier and that signi\fcantly increases their range\nof applicability. Whereas the earlier Monte Carlo methods were limited to\nepisodic tasks, TD(1) can be applied to discounted continuing tasks as well.\nMoreover, TD(1) can be performed incrementally and on-line. One disadvan-\ntage of Monte Carlo methods is that they learn nothing from an episode until\nit is over. For example, if a Monte Carlo control method does something that\nproduces a very poor reward but does not end the episode, then the agent's\ntendency to do that will be undiminished during the episode. On-line TD(1),\non the other hand, learns in an n-step TD way from the incomplete ongoing\nepisode, where the nsteps are all the way up to the current step. If something\nunusually good or bad happens during an episode, control methods based on\n180 CHAPTER 7. ELIGIBILITY TRACES\nTD(1) can learn immediately and alter their behavior on that same episode.\nIt is revealing to revisit the 19-state random walk example (Example 7.1)\nto see how well the backward-view TD( \u0015) algorithm does in approximating\nthe ideal of the forward-view \u0015-return algorithm. The performances of o\u000b-line\nand on-line TD( \u0015) with accumulating traces are shown in the upper-right and\nmiddle-right panels of Figure 7.6. In the o\u000b-line case it has been proven that\nthe\u0015-return algorithm and TD( \u0015) are identical in their overall updates at the\nend of the episode. Thus, the one set of results in the upper-right panel is\nsu\u000ecient for both of these algorithms. However, recall that the o\u000b-line case\nis not our main focus, as all of its performance levels are generally lower and\nobtained over a narrower range of parameter values than can be obtained with\non-line methods, as we saw earlier for n-step methods in Figure 7.2 and for\n\u0015-return methods in the upper two panels of Figure 7.6.\nIn the on-line case, the performances of TD( \u0015) with accumulating traces\n(middle-right panel) are indeed much better and closer to that of the on-line\n\u0015-return algorithm (upper-left panel). If \u0015= 0, then in fact it is the identical\nalgorithm at all \u000b, and if\u000bis small, then for all \u0015it is a close approximation to\nthe\u0015-return algorithm by the end of each episode. However, if both parameters\nare larger, for example \u0015 > 0:9 and\u000b > 0:5, then the algorithms perform\nsubstantially di\u000berently: the \u0015-return algorithm performs a little less well\nwhereas TD( \u0015) is likely to be unstable. This is not a terrible problem, as\nthese parameter values are higher than one would want to use anyway, but it\nis a weakness of the method.\nTwo alternative variations of eligibility traces have been proposed to ad-\ndress these limitations of the accumulating trace. On each step, all three trace\ntypes decay the traces of the non-visited states in the same way, that is, ac-\ncording to (7.8), but they di\u000ber in how the visited state is incremented. The\n\frst trace variation is the replacing trace . Suppose a state is visited and then\nrevisited before the trace due to the \frst visit has fully decayed to zero. With\naccumulating traces the revisit causes a further increment in the trace (7.9),\ndriving it greater than 1, whereas, with replacing traces, the trace is simply\nreset to 1:\nEt(St) = 1: (7.12)\nIn the special case of \u0015= 1, TD(\u0015) with replacing traces is closely related\nto \frst-visit Monte Carlo methods. The second trace variation, called the\ndutch trace , is sort of intermediate between accumulating and replacing traces,\ndepending on the step-size parameter \u000b. Dutch traces are de\fned by\nEt(St) = (1\u0000\u000b)\r\u0015Et\u00001(St) + 1: (7.13)\nNote that as \u000bapproaches zero, the dutch trace becomes the accumulating\n7.4. EQUIVALENCES OF FORWARD AND BACKWARD VIEWS 181\ntimes of state visitsaccumulating tracesdutch traces (\u03b1 = 0.5)replacing traces\nFigure 7.9: The three di\u000berent kinds of traces. Accumulating traces add up\neach time a state is visited, whereas replacing traces are reset to one, and\ndutch traces do something inbetween, depending on \u000b(here we show them for\n\u000b= 0:5). In all cases the traces decay at a rate of \r\u0015per step; here we show\n\r\u0015= 0:8 such that the traces have a time constant of approximately 5 steps\n(the last four visits are on successive steps).\ntrace, and, if \u000b= 1, the dutch trace becomes the replacing trace. Figure 7.9\ncontrasts the three kinds of traces, showing the behavior of the dutch trace for\n\u000b= 1=2. The performances of TD( \u0015) with these two kinds of traces are shown\nas additional panels in (7.6). In both cases, performance is more robust to the\nparameter values than it is with accumulating traces. The performance with\ndutch traces in particular achieves our goal of an on-line causal algorithm that\nclosely approximates the \u0015-return algorithm.\n7.4 Equivalences of Forward and Backward Views\nIt is sometimes possible to prove that two learning methods originating in\ndi\u000berent ways are in fact equivalent in the strong sense that the value functions\nthey produce are exactly the same on every time step. A simple case of this is\nthat one-step methods and all \u0015-based methods are equivalent if \u0015= 0. This\nfollows immediately from the fact that their backup targets are all the same.\nAnother easy-to-see example is the equivalence at \u0015= 1 of o\u000b-line TD( \u0015) and\nthe constant- \u000bMC methods, as noted in the previous section. Of particular\ninterest are equivalences between forward-view algorithms, which are often\nmore intuitive and clearer conceptually, and backward-view algorithms that\nare e\u000ecient and causal. The best example of this that we have encountered so\nfar is the equivalence at all \u0015of the o\u000b-line \u0015-return algorithm (forward view)\nand o\u000b-line TD( \u0015) with accumulating traces (backward view). That was an\nequivalence of value functions at the end of episodes and, because they are o\u000b-\nline methods which don't change values within an episode, it is a step-by-step\nequivalence as well. This equivalence was proved formally in the \frst edition\n182 CHAPTER 7. ELIGIBILITY TRACES\nof this book, and was veri\fed empirically here on the 19-state random-walk\nexample in producing the upper-left panel of Figure 7.6.\nFor on-line methods (and \u0015>0) the \frst edition of this book established\nonly approximate episode-by-episode equivalences between the \u0015-return algo-\nrithm and TD( \u0015). In the random-walk problem, at the end of episodes, TD( \u0015)\nwith accumulating traces is almost the same as the \u0015-return algorithm, but\nonly for small \u000band\u0015. With dutch traces the approximation is closer, but it\nis still not exact even on an episode-by-episode basis (compare the upper-left\nand middle left panels of Figure 7.6). Only recently has an interesting exact\nequivalence been established between a \u0015-based forward view and an e\u000ecient\nbackward-view implementation, in particular, between a \\real-time\" \u0015-return\nalgorithm and the \\true online TD( \u0015)\" algorithm (van Seijen and Sutton,\n2014). This is a striking and revealing result, but a little technical. The best\nway to present it is using the notation of linear function approximation, which\nwe develop in Chapter 9. We postpone development of the real-time \u0015-return\nalgorithm until then and present here only the backward-view algorithm.\nTrue online TD( \u0015)is de\fned by the dutch trace (Eqs. 7.13 and 7.8) and\nthe following value function update:\nVt+1(s) =Vt(s) +\u000b\u0002\n\u000et+Vt(St)\u0000Vt\u00001(St)\u0003\nEt(s)\u0000\u000bIsSt\u0002\nVt(St)\u0000Vt\u00001(St)\u0003\n;\nfor alls2S, whereIxyis an identity-indicator function, equal to 1 if x=y\nand 0 otherwise. An e\u000ecient implementation is given as a boxed algorithm in\nFigure 7.10.\nResults on the 19-state random-walk example for true online TD( \u0015) are\ngiven in the lower-left panel of Figure 7.6. We see that in this example true\non-line TD( \u0015) appears to perform slightly better than the on-line \u0015-return\nalgorithm, but not necessarly better than TD( \u0015) with dutch traces; most of\nthe performance improvement seems to come from the dutch traces rather\nthan the slightly di\u000berent or extra terms in the equations above. Of course,\nthis is just one example; bene\fts of the exact equivalence may appear on other\nproblems. One thing we can say in that these slight di\u000berences enable true\non-line TD( \u0015) with\u0015= 1 to be exactly equivalent by the end of the episode\nto the constant- \u000bMC method, while making updates on-line and in real-time.\nThe same cannot be said for any of the other methods.\n7.5. SARSA( \u0015) 183\nInitializeV(s) arbitrarily (but set to 0 if sis terminal)\nVold 0\nRepeat (for each episode):\nInitializeE(s) = 0, for all s2S\nInitializeS\nRepeat (for each step of episode):\nA action given by \u0019forS\nTake action A, observe reward, R, and next state, S0\n\u0001 V(S)\u0000Vold\nVold V(S0)\n\u000e R+\rV(S0)\u0000V(S)\nE(S) (1\u0000\u000b)E(S) + 1\nFor alls2S:\nV(s) V(s) +\u000b(\u000e+ \u0001)E(s)\nE(s) \r\u0015E (s)\nV(S) V(S)\u0000\u000b\u0001\nS S0\nuntilSis terminal\nFigure 7.10: Tabular true on-line TD( \u0015).\n7.5 Sarsa( \u0015)\nHow can eligibility traces be used not just for prediction, as in TD( \u0015), but for\ncontrol? As usual, the main idea of one popular approach is simply to learn\naction values, Qt(s;a), rather than state values, Vt(s). In this section we show\nhow eligibility traces can be combined with Sarsa in a straightforward way to\nproduce an on-policy TD control method. The eligibility trace version of Sarsa\nwe call Sarsa (\u0015), and the original version presented in the previous chapter\nwe henceforth call one-step Sarsa .\nThe idea in Sarsa( \u0015) is to apply the TD( \u0015) prediction method to state{\naction pairs rather than to states. Obviously, then, we need a trace not just\nfor each state, but for each state{action pair. Let Et(s;a) denote the trace for\nstate{action pair s;a. The traces can be any of the three types|accumulating,\nreplace, or dutch|and are updated in essentially the same way as before except\nof course being triggered by visiting the state{action pair (here given using the\nidentity-indicator notation):\nEt(s;a) =\r\u0015Et\u00001(s;a) +IsStIaAt (accumulating)\nEt(s;a) = (1\u0000\u000b)\r\u0015Et\u00001(s;a) +IsStIaAt (dutch)\nEt(s;a) = (1\u0000IsStIaAt)\r\u0015Et\u00001(s;a) +IsStIaAt (replacing)\nfor alls2S;a2A. Otherwise Sarsa( \u0015) is just like TD( \u0015), substituting\n184 CHAPTER 7. ELIGIBILITY TRACES\n\u03bbT-t-1s, at1\u2212\u03bb(1\u2212\u03bb) \u03bb(1\u2212\u03bb) \u03bb2\u03a3= 1t\nsTSarsa(\u03bb)St\nST, At\nFigure 7.11: Sarsa( \u0015)'s backup diagram.\nstate{action variables for state variables| Qt(s;a) forVt(s) andEt(s;a) for\nEt(s):\nQt+1(s;a) =Qt(s;a) +\u000b\u000etEt(s;a); for alls;a\nwhere\n\u000et=Rt+1+\rQt(St+1;At+1)\u0000Qt(St;At):\nFigure 7.11 shows the backup diagram for Sarsa( \u0015). Notice the similarity to\nthe diagram of the TD( \u0015) algorithm (Figure 7.3). The \frst backup looks ahead\none full step, to the next state{action pair, the second looks ahead two steps,\nand so on. A \fnal backup is based on the complete return. The weighting of\neach backup is just as in TD( \u0015) and the\u0015-return algorithm.\nOne-step Sarsa and Sarsa( \u0015) are on-policy algorithms, meaning that they\napproximate q\u0019(s;a), the action values for the current policy, \u0019, then improve\nthe policy gradually based on the approximate values for the current policy.\nThe policy improvement can be done in many di\u000berent ways, as we have seen\nthroughout this book. For example, the simplest approach is to use the \"-\ngreedy policy with respect to the current action-value estimates. Figure 7.12\nshows the complete Sarsa( \u0015) algorithm for this case.\nExample 7.2: Traces in Gridworld The use of eligibility traces can\nsubstantially increase the e\u000eciency of control algorithms. The reason for this\n7.5. SARSA( \u0015) 185\nInitializeQ(s;a) arbitrarily, for all s2S;a2A(s)\nRepeat (for each episode):\nE(s;a) = 0, for all s2S;a2A(s)\nInitializeS,A\nRepeat (for each step of episode):\nTake action A, observeR,S0\nChooseA0fromS0using policy derived from Q(e.g.,\"-greedy)\n\u000e R+\rQ(S0;A0)\u0000Q(S;A)\nE(S;A) E(S;A) + 1 (accumulating traces)\norE(S;A) (1\u0000\u000b)E(S;A) + 1 (dutch traces)\norE(S;A) 1 (replacing traces)\nFor alls2S;a2A(s):\nQ(s;a) Q(s;a) +\u000b\u000eE (s;a)\nE(s;a) \r\u0015E (s;a)\nS S0;A A0\nuntilSis terminal\nFigure 7.12: Tabular Sarsa( \u0015).\nPath takenAction values increased\nby one-step SarsaAction values increased\nby Sarsa(!) with !=0.9\nFigure 7.13: Gridworld example of the speedup of policy learning due to the\nuse of eligibility traces.\n186 CHAPTER 7. ELIGIBILITY TRACES\nis illustrated by the gridworld example in Figure 7.13. The \frst panel shows\nthe path taken by an agent in a single episode, ending at a location of high\nreward, marked by the *. In this example the values were all initially 0, and\nall rewards were zero except for a positive reward at the * location. The\narrows in the other two panels show which action values were strengthened as\na result of this path by one-step Sarsa and Sarsa( \u0015) methods. The one-step\nmethod strengthens only the last action of the sequence of actions that led to\nthe high reward, whereas the trace method strengthens many actions of the\nsequence. The degree of strengthening (indicated by the size of the arrows)\nfalls o\u000b (according to \r\u0015or (1\u0000\u000b)\r\u0015) with steps from the reward. In this\nexample, the fall o\u000b is 0.9 per step.\n7.6 Watkins's Q( \u0015)\nWhen Chris Watkins (1989) \frst proposed Q-learning, he also proposed a\nsimple way to combine it with eligibility traces. Recall that Q-learning is an\no\u000b-policy method, meaning that the policy learned about need not be the same\nas the one used to select actions. In particular, Q-learning learns about the\ngreedy policy while it typically follows a policy involving exploratory actions|\noccasional selections of actions that are suboptimal according to Q. Because\nof this, special care is required when introducing eligibility traces.\nSuppose we are backing up the state{action pair St;Atat timet. Suppose\nthat on the next two time steps the agent selects the greedy action, but on\nthe third, at time t+ 3, the agent selects an exploratory, nongreedy action. In\nlearning about the value of the greedy policy at St;Atwe can use subsequent\nexperience only as long as the greedy policy is being followed. Thus, we can use\nthe one-step and two-step returns, but not, in this case, the three-step return.\nThen-step returns for all n\u00153 no longer have any necessary relationship to\nthe greedy policy.\nThus, unlike TD( \u0015) or Sarsa(\u0015), Watkins's Q( \u0015) does not look ahead all\nthe way to the end of the episode in its backup. It only looks ahead as far\nas the next exploratory action. Aside from this di\u000berence, however, Watkins's\nQ(\u0015) is much like TD( \u0015) and Sarsa( \u0015). Their lookahead stops at episode's end,\nwhereas Q( \u0015)'s lookahead stops at the \frst exploratory action, or at episode's\nend if there are no exploratory actions before that. Actually, to be more\nprecise, one-step Q-learning and Watkins's Q( \u0015) both look one action past the\n\frst exploration, using their knowledge of the action values. For example,\nsuppose the \frst action, At+1, is exploratory. Watkins's Q( \u0015) would still do\nthe one-step update of Qt(St;At) towardRt+1+\rmaxaQt(St+1;a). In general,\n7.6. WATKINS'S Q( \u0015) 187\n1!\"(1!\") \"(1!\") \"2Watkins's Q(\")\nOR\nfirstnon-greedyaction\"n!1s, att\nst+n\"T-t-1St\nSt-n, At\nFigure 7.14: The backup diagram for Watkins's Q( \u0015). The series of component\nbackups ends either with the end of the episode or with the \frst nongreedy\naction, whichever comes \frst.\nifAt+nis the \frst exploratory action, then the longest backup is toward\nRt+1+\rRt+2+\u0001\u0001\u0001+\rn\u00001Rt+n+\rnmax\naQt(St+n;a);\nwhere we assume o\u000b-line updating. The backup diagram in Figure 7.14 illus-\ntrates the forward view of Watkins's Q( \u0015), showing all the component backups.\nThe mechanistic or backward view of Watkins's Q( \u0015) is also very simple.\nEligibility traces are used just as in Sarsa( \u0015), except that they are set to zero\nwhenever an exploratory (nongreedy) action is taken. The trace update is\nbest thought of as occurring in two steps. First, the traces for all state{action\npairs are either decayed by \r\u0015or, if an exploratory action was taken, set to 0.\nSecond, the trace corresponding to the current state and action is incremented\nby 1. The overall result is\nEt(s;a) =\u001a\r\u0015Et\u00001(s;a) +IsSt\u0001IaAtifQt\u00001(St;At) = max aQt\u00001(St;a);\nIsSt\u0001IaAt otherwise.\nOne could also use analogous dutch or replacing traces here. The rest of the\nalgorithm is de\fned by\nQt+1(s;a) =Qt(s;a) +\u000b\u000etEt(s;a);8s2S;a2A(s)\n188 CHAPTER 7. ELIGIBILITY TRACES\nInitializeQ(s;a) arbitrarily, for all s2S;a2A(s)\nRepeat (for each episode):\nE(s;a) = 0, for all s2S;a2A(s)\nInitializeS,A\nRepeat (for each step of episode):\nTake action A, observeR,S0\nChooseA0fromS0using policy derived from Q(e.g.,\"-greedy)\nA\u0003 argmaxaQ(S0;a) (ifA0ties for the max, then A\u0003 A0)\n\u000e R+\rQ(S0;A\u0003)\u0000Q(S;A)\nE(S;A) E(S;A) + 1 (accumulating traces)\norE(S;A) (1\u0000\u000b)E(S;A) + 1 (dutch traces)\norE(S;A) 1 (replacing traces)\nFor alls2S;a2A(s):\nQ(s;a) Q(s;a) +\u000b\u000eE (s;a)\nIfA0=A\u0003, thenE(s;a) \r\u0015E (s;a)\nelseE(s;a) 0\nS S0;A A0\nuntilSis terminal\nFigure 7.15: Tabular version of Watkins's Q( \u0015) algorithm.\nwhere\n\u000et=Rt+1+\rmax\na0Qt(St+1;a0)\u0000Qt(St;At):\nFigure 7.15 shows the complete algorithm in pseudocode.\nUnfortunately, cutting o\u000b traces every time an exploratory action is taken\nloses much of the advantage of using eligibility traces. If exploratory actions\nare frequent, as they often are early in learning, then only rarely will backups\nof more than one or two steps be done, and learning may be little faster than\none-step Q-learning.\n7.7 O\u000b-policy Eligibility Traces using Impor-\ntance Sampling\nThe eligibility traces in Watkins's Q( \u0015) are a crude way to deal with o\u000b-\npolicy training. First, they treat the o\u000b-policy aspect as binary; either the\ntarget policy is followed and traces continue normally, or it is deviated from\nand traces are cut o\u000b completely; there is nothing inbetween. But the target\npolicy may take di\u000berent actions with di\u000berent positive probabilities, as may\nthe behavior policy, in which case following and deviating will be a matter of\n7.8. IMPLEMENTATION ISSUES 189\ndegree. In Chapter 5 we saw how to use the ratio of the two probabilities of\ntaking the action to more precisely assign credit to a single action, and the\nproduct of ratios to assign credit to a sequence.\nSecond, Watkins's Q( \u0015) confounds bootstrapping and o\u000b-policy deviation.\nBootstrapping refers to the degree to which an algorithm builds its estimates\nfrom other estimates, like TD and DP, or does not, like MC methods. In\nTD(\u0015) and Sarsa( \u0015), the\u0015parameter controls the degree of bootstrapping,\nwith the value \u0015= 1 denoting no bootstrapping, turning these TD methods\ninto MC methods. But the same cannot be said for Q( \u0015). As soon as there is a\ndeviation from the target policy Q( \u0015) cuts the trace and uses its value estimate\nrather than waiting for the actual rewards|it bootstraps even if \u0015= 1. Ideally\nwe would like to totally de-couple bootstrapping from the o\u000b-policy aspect, to\nuse\u0015to specify the degree of bootstrapping while using importance sampling\nto correct independently for the degree of o\u000b-policy deviation.\n7.8 Implementation Issues\nIt might at \frst appear that methods using eligibility traces are much more\ncomplex than one-step methods. A naive implementation would require every\nstate (or state{action pair) to update both its value estimate and its eligibility\ntrace on every time step. This would not be a problem for implementations\non single-instruction, multiple-data parallel computers or in plausible neural\nimplementations, but it is a problem for implementations on conventional serial\ncomputers. Fortunately, for typical values of \u0015and\rthe eligibility traces of\nalmost all states are almost always nearly zero; only those that have recently\nbeen visited will have traces signi\fcantly greater than zero. Only these few\nstates really need to be updated because the updates at the others will have\nessentially no e\u000bect.\nIn practice, then, implementations on conventional computers keep track\nof and update only the few states with nonzero traces. Using this trick, the\ncomputational expense of using traces is typically a few times that of a one-\nstep method. The exact multiple of course depends on \u0015and\rand on the\nexpense of the other computations. Cichosz (1995) has demonstrated a fur-\nther implementation technique that further reduces complexity to a constant\nindependent of \u0015and\r. Finally, it should be noted that the tabular case is\nin some sense a worst case for the computational complexity of traces. When\nfunction approximation is used (Chapter 9), the computational advantages of\nnot using traces generally decrease. For example, if arti\fcial neural networks\nand backpropagation are used, then traces generally cause only a doubling of\nthe required memory and computation per step.\n190 CHAPTER 7. ELIGIBILITY TRACES\n\u00037.9 Variable \u0015\nThe\u0015-return can be signi\fcantly generalized beyond what we have described\nso far by allowing \u0015to vary from step to step, that is, by rede\fning the trace\nupdate as\nEt(s) =\u001a\r\u0015tEt\u00001(s) ifs6=St;\n\r\u0015tEt\u00001(s) + 1 ifs=St;\nwhere\u0015tdenotes the value of \u0015at timet. This is an advanced topic because\nthe added generality has never been used in practical applications, but it is\ninteresting theoretically and may yet prove useful. For example, one idea is to\nvary\u0015as a function of state: \u0015t=\u0015(St). If a state's value estimate is believed\nto be known with high certainty, then it makes sense to use that estimate fully,\nignoring whatever states and rewards are received after it. This corresponds to\ncutting o\u000b all the traces once this state has been reached, that is, to choosing\nthe\u0015for the certain state to be zero or very small. Similarly, states whose\nvalue estimates are highly uncertain, perhaps because even the state estimate\nis unreliable, can be given \u0015s near 1. This causes their estimated values to\nhave little e\u000bect on any updates. They are \\skipped over\" until a state that is\nknown better is encountered. Some of these ideas were explored formally by\nSutton and Singh (1994).\nThe eligibility trace equation above is the backward view of variable \u0015s.\nThe corresponding forward view is a more general de\fnition of the \u0015-return:\nG\u0015\nt=1X\nn=1G(n)\nt(1\u0000\u0015t+n)t+n\u00001Y\ni=t+1\u0015i\n=T\u00001X\nk=t+1G(k\u0000t)\nt(1\u0000\u0015k)k\u00001Y\ni=t+1\u0015i+GtT\u00001Y\ni=t+1\u0015i:\n7.10 Conclusions\nEligibility traces in conjunction with TD errors provide an e\u000ecient, incremen-\ntal way of shifting and choosing between Monte Carlo and TD methods. Traces\ncan be used without TD errors to achieve a similar e\u000bect, but only awkwardly.\nA method such as TD( \u0015) enables this to be done from partial experiences and\nwith little memory and little nonmeaningful variation in predictions.\nAs we mentioned in Chapter 5, Monte Carlo methods may have advan-\ntages in non-Markov tasks because they do not bootstrap. Because eligibility\n7.10. CONCLUSIONS 191\ntraces make TD methods more like Monte Carlo methods, they also can have\nadvantages in these cases. If one wants to use TD methods because of their\nother advantages, but the task is at least partially non-Markov, then the use\nof an eligibility trace method is indicated. Eligibility traces are the \frst line\nof defense against both long-delayed rewards and non-Markov tasks.\nBy adjusting \u0015, we can place eligibility trace methods anywhere along\na continuum from Monte Carlo to one-step TD methods. Where shall we\nplace them? We do not yet have a good theoretical answer to this question,\nbut a clear empirical answer appears to be emerging. On tasks with many\nsteps per episode, or many steps within the half-life of discounting, it appears\nsigni\fcantly better to use eligibility traces than not to (e.g., see Figure 9.12).\nOn the other hand, if the traces are so long as to produce a pure Monte Carlo\nmethod, or nearly so, then performance degrades sharply. An intermediate\nmixture appears to be the best choice. Eligibility traces should be used to\nbring us toward Monte Carlo methods, but not all the way there. In the\nfuture it may be possible to vary the trade-o\u000b between TD and Monte Carlo\nmethods more \fnely by using variable \u0015, but at present it is not clear how this\ncan be done reliably and usefully.\nMethods using eligibility traces require more computation than one-step\nmethods, but in return they o\u000ber signi\fcantly faster learning, particularly\nwhen rewards are delayed by many steps. Thus it often makes sense to use\neligibility traces when data are scarce and cannot be repeatedly processed, as\nis often the case in on-line applications. On the other hand, in o\u000b-line appli-\ncations in which data can be generated cheaply, perhaps from an inexpensive\nsimulation, then it often does not pay to use eligibility traces. In these cases\nthe objective is not to get more out of a limited amount of data, but simply\nto process as much data as possible as quickly as possible. In these cases the\nspeedup per datum due to traces is typically not worth their computational\ncost, and one-step methods are favored.\nBibliographical and Historical Remarks\n7.1{2 The forward view of eligibility traces in terms of n-step returns and the\n\u0015-return is due to Watkins (1989), who also \frst discussed the error\nreduction property of n-step returns. Our presentation is based on\nthe slightly modi\fed treatment by Jaakkola, Jordan, and Singh (1994).\nThe results in the random walk examples were made for this text based\non work of Sutton (1988) and Singh and Sutton (1996). The use of\nbackup diagrams to describe these and other algorithms in this chapter\nis new, as are the terms \\forward view\" and \\backward view.\"\n192 CHAPTER 7. ELIGIBILITY TRACES\nTD(\u0015) was proved to converge in the mean by Dayan (1992), and with\nprobability 1 by many researchers, including Peng (1993), Dayan and\nSejnowski (1994), and Tsitsiklis (1994). Jaakkola, Jordan, and Singh\n(1994), in addition, \frst proved convergence of TD( \u0015) under on-line\nupdating. Gurvits, Lin, and Hanson (1994) proved convergence of a\nmore general class of eligibility trace methods.\n7.3 TD(\u0015) with accumulating traces was introduced by Sutton (1988, 1984).\nReplacing traces are due to Singh and Sutton (1996). Dutch traces are\ndue to van Seijen and Sutton (2014, in prep).\nEligibility traces came into reinforcement learning via the fecund ideas\nof Klopf (1972). Our use of eligibility traces was based on Klopf's\nwork (Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b;\nSutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983; Sutton,\n1984). We may have been the \frst to use the term \\eligibility trace\"\n(Sutton and Barto, 1981). The idea that stimuli produce aftere\u000bects\nin the nervous system that are important for learning is very old. See\nSection 14.??.\n7.4 The episode-by-episode equivalence of forward and backward views,\nand the relationships to Monte Carlo methods, were proved by Sutton\n(1988) for undiscounted episodic tasks, then extended to the general\ncase in the \frst edition of this book (1989). We see these as now\nsuperceded by the analyses and step-by-step equivalences in Section\n9.??.\n7.5 Sarsa(\u0015) was \frst explored as a control method by Rummery and Ni-\nranjan (1994; Rummery, 1995). Our presentation of replacing traces\nomits a subtlety which is sometimes found to be bene\fcial: clearing\n(setting to zero) the traces of all the actions nottaken in the state that\nis visited, as suggested by Singh and Sutton (1996). This can also be\ndone in Q(\u0015). Nowadays we would recommend just using dutch traces,\nwhich generalize better to function approximation.\n7.6 Watkins's Q( \u0015) is due to Watkins (1989). Peng's Q( \u0015) is due to Peng\nand Williams (Peng, 1993; Peng and Williams, 1994, 1996). Rummery\n(1995) made extensive comparative studies of these algorithms.\nConvergence has still not been proved for any control method for 0 <\n\u0015<1.\n7.10. CONCLUSIONS 193\n7.8-9 The ideas in these two sections were generally known for many years,\nbut beyond what is in the sources cited in the sections themselves, this\ntext may be the \frst place they have been described. Perhaps the \frst\npublished discussion of variable \u0015was by Watkins (1989), who pointed\nout that the cutting o\u000b of the backup sequence (Figure 7.14) in his\nQ(\u0015) when a nongreedy action was selected could be implemented by\ntemporarily setting \u0015to 0.\nExercises\nExercise 7.1 Why do you think a larger random walk task (19 states instead\nof 5) was used in the examples of this chapter? Would a smaller walk have\nshifted the advantage to a di\u000berent value of n? How about the change in left-\nside outcome from 0 to \u00001? Would that have made any di\u000berence in the best\nvalue ofn?\nExercise 7.2 Why do you think on-line methods worked better than o\u000b-line\nmethods on the example task?\n\u0003Exercise 7.3 In the lower part of Figure 7.2, notice that the plot for n= 3 is\ndi\u000berent from the others, dropping to low performance at a much lower value of\n\u000bthan similar methods. In fact, the same was observed for n= 5,n= 7, and\nn= 9. Can you explain why this might have been so? In fact, we are not sure\nourselves. See http://www.cs.utexas.edu/~ikarpov/Classes/RL/RandomWalk/\nfor an attempt at a thorough answer by Igor Karpov.\nExercise 7.4 The parameter \u0015characterizes how fast the exponential weight-\ning in Figure 7.4 falls o\u000b, and thus how far into the future the \u0015-return algo-\nrithm looks in determining its backup. But a rate factor such as \u0015is sometimes\nan awkward way of characterizing the speed of the decay. For some purposes it\nis better to specify a time constant, or half-life. What is the equation relating\n\u0015and the half-life, \u001c\u0015, the time by which the weighting sequence will have\nfallen to half of its initial value?\nExercise 7.5 (programming) Draw a backup diagram for Sarsa( \u0015) with\nreplacing traces.\nExercise 7.6 Write pseudocode for an implementation of TD( \u0015) that up-\ndates only value estimates for states whose traces are greater than some small\npositive constant.\nExercise 7.7 Write equations or pseudocode for Sarsa( \u0015) and/or Q( \u0015) with\ndutch traces. Do the same for a true-on-line version.\n194 CHAPTER 7. ELIGIBILITY TRACES\nChapter 8\nPlanning and Learning with\nTabular Methods\nIn this chapter we develop a uni\fed view of methods that require a model\nof the environment, such as dynamic programming and heuristic search, and\nmethods that can be used without a model, such as Monte Carlo and temporal-\ndi\u000berence methods. We think of the former as planning methods and of the\nlatter as learning methods. Although there are real di\u000berences between these\ntwo kinds of methods, there are also great similarities. In particular, the heart\nof both kinds of methods is the computation of value functions. Moreover, all\nthe methods are based on looking ahead to future events, computing a backed-\nup value, and then using it to update an approximate value function. Earlier\nin this book we presented Monte Carlo and temporal-di\u000berence methods as\ndistinct alternatives, then showed how they can be seamlessly integrated by\nusing eligibility traces such as in TD( \u0015). Our goal in this chapter is a similar\nintegration of planning and learning methods. Having established these as\ndistinct in earlier chapters, we now explore the extent to which they can be\nintermixed.\n8.1 Models and Planning\nBy a model of the environment we mean anything that an agent can use to\npredict how the environment will respond to its actions. Given a state and an\naction, a model produces a prediction of the resultant next state and next re-\nward. If the model is stochastic, then there are several possible next states and\nnext rewards, each with some probability of occurring. Some models produce\na description of all possibilities and their probabilities; these we call distri-\nbution models . Other models produce just one of the possibilities, sampled\n195\n196CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\naccording to the probabilities; these we call sample models . For example, con-\nsider modeling the sum of a dozen dice. A distribution model would produce\nall possible sums and their probabilities of occurring, whereas a sample model\nwould produce an individual sum drawn according to this probability distribu-\ntion. The kind of model assumed in dynamic programming|estimates of the\nstate transition probabilities and expected rewards, p(s0js;a) andr(s;a;s0)|\nis a distribution model. The kind of model used in the blackjack example in\nChapter 5 is a sample model. Distribution models are stronger than sample\nmodels in that they can always be used to produce samples. However, in sur-\nprisingly many applications it is much easier to obtain sample models than\ndistribution models.\nModels can be used to mimic or simulate experience. Given a starting state\nand action, a sample model produces a possible transition, and a distribution\nmodel generates all possible transitions weighted by their probabilities of oc-\ncurring. Given a starting state and a policy, a sample model could produce an\nentire episode, and a distribution model could generate all possible episodes\nand their probabilities. In either case, we say the model is used to simulate\nthe environment and produce simulated experience .\nThe word planning is used in several di\u000berent ways in di\u000berent \felds. We\nuse the term to refer to any computational process that takes a model as\ninput and produces or improves a policy for interacting with the modeled\nenvironment:\nplanningmodel policy\nWithin arti\fcial intelligence, there are two distinct approaches to planning ac-\ncording to our de\fnition. In state-space planning , which includes the approach\nwe take in this book, planning is viewed primarily as a search through the state\nspace for an optimal policy or path to a goal. Actions cause transitions from\nstate to state, and value functions are computed over states. In what we\ncallplan-space planning , planning is instead viewed as a search through the\nspace of plans. Operators transform one plan into another, and value func-\ntions, if any, are de\fned over the space of plans. Plan-space planning includes\nevolutionary methods and partial-order planning , a popular kind of planning\nin arti\fcial intelligence in which the ordering of steps is not completely de-\ntermined at all stages of planning. Plan-space methods are di\u000ecult to apply\ne\u000eciently to the stochastic optimal control problems that are the focus in rein-\nforcement learning, and we do not consider them further (but see Section 15.6\nfor one application of reinforcement learning within plan-space planning).\nThe uni\fed view we present in this chapter is that all state-space planning\nmethods share a common structure, a structure that is also present in the\n8.1. MODELS AND PLANNING 197\nlearning methods presented in this book. It takes the rest of the chapter to\ndevelop this view, but there are two basic ideas: (1) all state-space planning\nmethods involve computing value functions as a key intermediate step toward\nimproving the policy, and (2) they compute their value functions by backup\noperations applied to simulated experience. This common structure can be\ndiagrammed as follows:\nvaluesbackupsmodelsimulated\nexperiencepolicy\nDynamic programming methods clearly \ft this structure: they make sweeps\nthrough the space of states, generating for each state the distribution of pos-\nsible transitions. Each distribution is then used to compute a backed-up value\nand update the state's estimated value. In this chapter we argue that vari-\nous other state-space planning methods also \ft this structure, with individual\nmethods di\u000bering only in the kinds of backups they do, the order in which\nthey do them, and in how long the backed-up information is retained.\nViewing planning methods in this way emphasizes their relationship to the\nlearning methods that we have described in this book. The heart of both\nlearning and planning methods is the estimation of value functions by backup\noperations. The di\u000berence is that whereas planning uses simulated experience\ngenerated by a model, learning methods use real experience generated by the\nenvironment. Of course this di\u000berence leads to a number of other di\u000berences,\nfor example, in how performance is assessed and in how \rexibly experience\ncan be generated. But the common structure means that many ideas and\nalgorithms can be transferred between planning and learning. In particular,\nin many cases a learning algorithm can be substituted for the key backup step\nof a planning method. Learning methods require only experience as input, and\nin many cases they can be applied to simulated experience just as well as to real\nexperience. Figure 8.1 shows a simple example of a planning method based on\none-step tabular Q-learning and on random samples from a sample model. This\nmethod, which we call random-sample one-step tabular Q-planning , converges\nto the optimal policy for the model under the same conditions that one-step\ntabular Q-learning converges to the optimal policy for the real environment\n(each state{action pair must be selected an in\fnite number of times in Step\n1, and\u000bmust decrease appropriately over time).\nIn addition to the uni\fed view of planning and learning methods, a second\ntheme in this chapter is the bene\fts of planning in small, incremental steps.\nThis enables planning to be interrupted or redirected at any time with lit-\ntle wasted computation, which appears to be a key requirement for e\u000eciently\nintermixing planning with acting and with learning of the model. More sur-\nprisingly, later in this chapter we present evidence that planning in very small\n198CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nDo forever:\n1. Select a state, S2S, and an action, A2A(s), at random\n2. SendS;A to a sample model, and obtain\na sample next reward, R, and a sample next state, S0\n3. Apply one-step tabular Q-learning to S;A;R;S0:\nQ(S;A) Q(S;A) +\u000b\u0002\nR+\rmaxaQ(S0;a)\u0000Q(S;A)\u0003\nFigure 8.1: Random-sample one-step tabular Q-planning\nsteps may be the most e\u000ecient approach even on pure planning problems if\nthe problem is too large to be solved exactly.\n8.2 Integrating Planning, Acting, and Learn-\ning\nWhen planning is done on-line, while interacting with the environment, a num-\nber of interesting issues arise. New information gained from the interaction\nmay change the model and thereby interact with planning. It may be desirable\nto customize the planning process in some way to the states or decisions cur-\nrently under consideration, or expected in the near future. If decision-making\nand model-learning are both computation-intensive processes, then the avail-\nable computational resources may need to be divided between them. To begin\nexploring these issues, in this section we present Dyna-Q, a simple architec-\nture integrating the major functions needed in an on-line planning agent. Each\nfunction appears in Dyna-Q in a simple, almost trivial, form. In subsequent\nsections we elaborate some of the alternate ways of achieving each function\nand the trade-o\u000bs between them. For now, we seek merely to illustrate the\nideas and stimulate your intuition.\nWithin a planning agent, there are at least two roles for real experience: it\ncan be used to improve the model (to make it more accurately match the real\nenvironment) and it can be used to directly improve the value function and\npolicy using the kinds of reinforcement learning methods we have discussed in\nprevious chapters. The former we call model-learning , and the latter we call\ndirect reinforcement learning (direct RL). The possible relationships between\nexperience, model, values, and policy are summarized in Figure 8.2. Each\narrow shows a relationship of in\ruence and presumed improvement. Note how\nexperience can improve value and policy functions either directly or indirectly\nvia the model. It is the latter, which is sometimes called indirect reinforcement\nlearning , that is involved in planning.\n8.2. INTEGRATING PLANNING, ACTING, AND LEARNING 199\nplanningvalue/policy\nexperience model\nmodel\nlearningacting\ndirect\nRL\nFigure 8.2: Relationships among learning, planning, and acting.\nBoth direct and indirect methods have advantages and disadvantages. In-\ndirect methods often make fuller use of a limited amount of experience and\nthus achieve a better policy with fewer environmental interactions. On the\nother hand, direct methods are much simpler and are not a\u000bected by biases\nin the design of the model. Some have argued that indirect methods are al-\nways superior to direct ones, while others have argued that direct methods\nare responsible for most human and animal learning. Related debates in psy-\nchology and AI concern the relative importance of cognition as opposed to\ntrial-and-error learning, and of deliberative planning as opposed to reactive\ndecision-making. Our view is that the contrast between the alternatives in\nall these debates has been exaggerated, that more insight can be gained by\nrecognizing the similarities between these two sides than by opposing them.\nFor example, in this book we have emphasized the deep similarities between\ndynamic programming and temporal-di\u000berence methods, even though one was\ndesigned for planning and the other for modelfree learning.\nDyna-Q includes all of the processes shown in Figure 8.2|planning, act-\ning, model-learning, and direct RL|all occurring continually. The planning\nmethod is the random-sample one-step tabular Q-planning method given in\nFigure 8.1. The direct RL method is one-step tabular Q-learning. The model-\nlearning method is also table-based and assumes the world is deterministic.\nAfter each transition St;At Rt+1;St+1, the model records in its table entry\nforSt;Atthe prediction that Rt+1;St+1will deterministically follow. Thus, if\nthe model is queried with a state{action pair that has been experienced before,\nit simply returns the last-observed next state and next reward as its predic-\ntion. During planning, the Q-planning algorithm randomly samples only from\nstate{action pairs that have previously been experienced (in Step 1), so the\nmodel is never queried with a pair about which it has no information.\nThe overall architecture of Dyna agents, of which the Dyna-Q algorithm is\n200CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nrealdirect RL\nupdate\nModelplanning update\nsearch\ncontrolPolicy/value functions\nexperience\nmodel\nlearning\nEnvironmentsimulated\nexperience\nFigure 8.3: The general Dyna Architecture\none example, is shown in Figure 8.3. The central column represents the basic\ninteraction between agent and environment, giving rise to a trajectory of real\nexperience. The arrow on the left of the \fgure represents direct reinforcement\nlearning operating on real experience to improve the value function and the\npolicy. On the right are model-based processes. The model is learned from real\nexperience and gives rise to simulated experience. We use the term search con-\ntrolto refer to the process that selects the starting states and actions for the\nsimulated experiences generated by the model. Finally, planning is achieved\nby applying reinforcement learning methods to the simulated experiences just\nas if they had really happened. Typically, as in Dyna-Q, the same reinforce-\nment learning method is used both for learning from real experience and for\nplanning from simulated experience. The reinforcement learning method is\nthus the \\\fnal common path\" for both learning and planning. Learning and\nplanning are deeply integrated in the sense that they share almost all the same\nmachinery, di\u000bering only in the source of their experience.\nConceptually, planning, acting, model-learning, and direct RL occur simul-\ntaneously and in parallel in Dyna agents. For concreteness and implementation\non a serial computer, however, we fully specify the order in which they occur\nwithin a time step. In Dyna-Q, the acting, model-learning, and direct RL pro-\ncesses require little computation, and we assume they consume just a fraction\nof the time. The remaining time in each step can be devoted to the planning\nprocess, which is inherently computation-intensive. Let us assume that there\nis time in each step, after acting, model-learning, and direct RL, to complete\nniterations (Steps 1{3) of the Q-planning algorithm. Figure 8.4 shows the\ncomplete algorithm for Dyna-Q.\n8.2. INTEGRATING PLANNING, ACTING, AND LEARNING 201\nInitializeQ(s;a) andModel (s;a) for alls2Sanda2A(s)\nDo forever:\n(a)S current (nonterminal) state\n(b)A \u000f-greedy(S;Q)\n(c) Execute action A; observe resultant reward, R, and state, S0\n(d)Q(S;A) Q(S;A) +\u000b\u0002\nR+\rmaxaQ(S0;a)\u0000Q(S;A)\u0003\n(e)Model (S;A) R;S0(assuming deterministic environment)\n(f) Repeat ntimes:\nS random previously observed state\nA random action previously taken in S\nR;S0 Model (S;A)\nQ(S;A) Q(S;A) +\u000b\u0002\nR+\rmaxaQ(S0;a)\u0000Q(S;A)\u0003\nFigure 8.4: Dyna-Q Algorithm. Model (s;a) denotes the contents of the model\n(predicted next state and reward) for state{action pair s;a. Direct reinforce-\nment learning, model-learning, and planning are implemented by steps (d),\n(e), and (f), respectively. If (e) and (f) were omitted, the remaining algorithm\nwould be one-step tabular Q-learning.\nExample 8.1: Dyna Maze Consider the simple maze shown inset in\nFigure 8.5. In each of the 47 states there are four actions, up,down ,right , and\nleft , which take the agent deterministically to the corresponding neighboring\nstates, except when movement is blocked by an obstacle or the edge of the\nmaze, in which case the agent remains where it is. Reward is zero on all\ntransitions, except those into the goal state, on which it is +1. After reaching\nthe goal state ( G), the agent returns to the start state ( S) to begin a new\nepisode. This is a discounted, episodic task with \r= 0:95.\nThe main part of Figure 8.5 shows average learning curves from an ex-\nperiment in which Dyna-Q agents were applied to the maze task. The initial\naction values were zero, the step-size parameter was \u000b= 0:1, and the explo-\nration parameter was \u000f= 0:1. When selecting greedily among actions, ties\nwere broken randomly. The agents varied in the number of planning steps,\nn, they performed per real step. For each n, the curves show the number of\nsteps taken by the agent in each episode, averaged over 30 repetitions of the\nexperiment. In each repetition, the initial seed for the random number gen-\nerator was held constant across algorithms. Because of this, the \frst episode\nwas exactly the same (about 1700 steps) for all values of n, and its data are\nnot shown in the \fgure. After the \frst episode, performance improved for all\nvalues ofn, but much more rapidly for larger values. Recall that the n= 0\nagent is a nonplanning agent, utilizing only direct reinforcement learning (one-\nstep tabular Q-learning). This was by far the slowest agent on this problem,\ndespite the fact that the parameter values ( \u000band\") were optimized for it. The\n202CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\n2800\n600\n400\n200\n14\n20 10 30 40 500 planning steps\n(direct RL only)\nEpisodesSteps\nper\nepisode5 planning steps\n50 planning stepsSG\nactions\nFigure 8.5: A simple maze (inset) and the average learning curves for Dyna-Q\nagents varying in their number of planning steps ( n) per real step. The task\nis to travel from StoGas quickly as possible.\nnonplanning agent took about 25 episodes to reach ( \"-)optimal performance,\nwhereas the n= 5 agent took about \fve episodes, and the n= 50 agent took\nonly three episodes.\nFigure 8.6 shows why the planning agents found the solution so much faster\nthan the nonplanning agent. Shown are the policies found by the n= 0 and\nn= 50 agents halfway through the second episode. Without planning ( n= 0),\neach episode adds only one additional step to the policy, and so only one step\n(the last) has been learned so far. With planning, again only one step is learned\nduring the \frst episode, but here during the second episode an extensive policy\nhas been developed that by the episode's end will reach almost back to the\nstart state. This policy is built by the planning process while the agent is still\nwandering near the start state. By the end of the third episode a complete\noptimal policy will have been found and perfect performance attained.\nIn Dyna-Q, learning and planning are accomplished by exactly the same al-\ngorithm, operating on real experience for learning and on simulated experience\nfor planning. Because planning proceeds incrementally, it is trivial to intermix\nplanning and acting. Both proceed as fast as they can. The agent is always\nreactive and always deliberative, responding instantly to the latest sensory\ninformation and yet always planning in the background. Also ongoing in the\n8.3. WHEN THE MODEL IS WRONG 203\nSGSGWITHOUT PLANNING (N=0)WITH PLANNING (N=50)nn\nFigure 8.6: Policies found by planning and nonplanning Dyna-Q agents\nhalfway through the second episode. The arrows indicate the greedy action in\neach state; no arrow is shown for a state if all of its action values are equal.\nThe black square indicates the location of the agent.\nbackground is the model-learning process. As new information is gained, the\nmodel is updated to better match reality. As the model changes, the ongoing\nplanning process will gradually compute a di\u000berent way of behaving to match\nthe new model.\n8.3 When the Model Is Wrong\nIn the maze example presented in the previous section, the changes in the\nmodel were relatively modest. The model started out empty, and was then\n\flled only with exactly correct information. In general, we cannot expect to be\nso fortunate. Models may be incorrect because the environment is stochastic\nand only a limited number of samples have been observed, because the model\nwas learned using function approximation that has generalized imperfectly, or\nsimply because the environment has changed and its new behavior has not\nyet been observed. When the model is incorrect, the planning process will\ncompute a suboptimal policy.\nIn some cases, the suboptimal policy computed by planning quickly leads\nto the discovery and correction of the modeling error. This tends to happen\nwhen the model is optimistic in the sense of predicting greater reward or better\nstate transitions than are actually possible. The planned policy attempts to\nexploit these opportunities and in doing so discovers that they do not exist.\nExample 8.2: Blocking Maze A maze example illustrating this relatively\nminor kind of modeling error and recovery from it is shown in Figure 8.7.\nInitially, there is a short path from start to goal, to the right of the barrier,\nas shown in the upper left of the \fgure. After 1000 time steps, the short\npath is \\blocked,\" and a longer path is opened up along the left-hand side of\n204CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nCumulative\nreward\n0 1000 2000 3000\nTime steps150\n0Dyna-Q+\nDyna-Q\nDyna-ACSG G\nS\nFigure 8.7: Average performance of Dyna agents on a blocking task. The left\nenvironment was used for the \frst 1000 steps, the right environment for the\nrest. Dyna-Q+ is Dyna-Q with an exploration bonus that encourages explo-\nration. Dyna-AC is a Dyna agent that uses an actor{critic learning method\ninstead of Q-learning.\nthe barrier, as shown in upper right of the \fgure. The graph shows average\ncumulative reward for Dyna-Q and two other Dyna agents. The \frst part of the\ngraph shows that all three Dyna agents found the short path within 1000 steps.\nWhen the environment changed, the graphs become \rat, indicating a period\nduring which the agents obtained no reward because they were wandering\naround behind the barrier. After a while, however, they were able to \fnd the\nnew opening and the new optimal behavior.\nGreater di\u000eculties arise when the environment changes to become better\nthan it was before, and yet the formerly correct policy does not reveal the\nimprovement. In these cases the modeling error may not be detected for a\nlong time, if ever, as we see in the next example.\nExample 8.3: Shortcut Maze The problem caused by this kind of en-\nvironmental change is illustrated by the maze example shown in Figure 8.8.\nInitially, the optimal path is to go around the left side of the barrier (upper\nleft). After 3000 steps, however, a shorter path is opened up along the right\nside, without disturbing the longer path (upper right). The graph shows that\ntwo of the three Dyna agents never switched to the shortcut. In fact, they\nnever realized that it existed. Their models said that there was no short-\ncut, so the more they planned, the less likely they were to step to the right\n8.3. WHEN THE MODEL IS WRONG 205\nCumulative\nrewardSG G\nS\n0 3000 6000\nTime steps400\n0Dyna-Q+Dyna-Q\nDyna-AC\nFigure 8.8: Average performance of Dyna agents on a shortcut task. The left\nenvironment was used for the \frst 3000 steps, the right environment for the\nrest.\nand discover it. Even with an \"-greedy policy, it is very unlikely that an\nagent will take so many exploratory actions as to discover the shortcut.\nThe general problem here is another version of the con\rict between ex-\nploration and exploitation. In a planning context, exploration means trying\nactions that improve the model, whereas exploitation means behaving in the\noptimal way given the current model. We want the agent to explore to \fnd\nchanges in the environment, but not so much that performance is greatly de-\ngraded. As in the earlier exploration/exploitation con\rict, there probably is\nno solution that is both perfect and practical, but simple heuristics are often\ne\u000bective.\nThe Dyna-Q+ agent that did solve the shortcut maze uses one such heuris-\ntic. This agent keeps track for each state{action pair of how many time steps\nhave elapsed since the pair was last tried in a real interaction with the envi-\nronment. The more time that has elapsed, the greater (we might presume)\nthe chance that the dynamics of this pair has changed and that the model of\nit is incorrect. To encourage behavior that tests long-untried actions, a spe-\ncial \\bonus reward\" is given on simulated experiences involving these actions.\nIn particular, if the modeled reward for a transition is R, and the transition\nhas not been tried in \u001ctime steps, then planning backups are done as if that\ntransition produced a reward of R+\u0014p\u001c, for some small \u0014. This encour-\nages the agent to keep testing all accessible state transitions and even to plan\n206CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nlong sequences of actions in order to carry out such tests.1Of course all this\ntesting has its cost, but in many cases, as in the shortcut maze, this kind of\ncomputational curiosity is well worth the extra exploration.\n8.4 Prioritized Sweeping\nIn the Dyna agents presented in the preceding sections, simulated transitions\nare started in state{action pairs selected uniformly at random from all pre-\nviously experienced pairs. But a uniform selection is usually not the best;\nplanning can be much more e\u000ecient if simulated transitions and backups are\nfocused on particular state{action pairs. For example, consider what happens\nduring the second episode of the \frst maze task (Figure 8.6). At the beginning\nof the second episode, only the state{action pair leading directly into the goal\nhas a positive value; the values of all other pairs are still zero. This means\nthat it is pointless to back up along almost all transitions, because they take\nthe agent from one zero-valued state to another, and thus the backups would\nhave no e\u000bect. Only a backup along a transition into the state just prior to\nthe goal, or from it into the goal, will change any values. If simulated transi-\ntions are generated uniformly, then many wasteful backups will be made before\nstumbling onto one of the two useful ones. As planning progresses, the region\nof useful backups grows, but planning is still far less e\u000ecient than it would\nbe if focused where it would do the most good. In the much larger problems\nthat are our real objective, the number of states is so large that an unfocused\nsearch would be extremely ine\u000ecient.\nThis example suggests that search might be usefully focused by working\nbackward from goal states. Of course, we do not really want to use any methods\nspeci\fc to the idea of \\goal state.\" We want methods that work for general\nreward functions. Goal states are just a special case, convenient for stimulating\nintuition. In general, we want to work back not just from goal states but from\nany state whose value has changed. Assume that the values are initially correct\ngiven the model, as they were in the maze example prior to discovering the\ngoal. Suppose now that the agent discovers a change in the environment and\nchanges its estimated value of one state. Typically, this will imply that the\nvalues of many other states should also be changed, but the only useful one-\nstep backups are those of actions that lead directly into the one state whose\nvalue has already been changed. If the values of these actions are updated,\n1The Dyna-Q+ agent was changed in two other ways as well. First, actions that had\nnever before been tried before from a state were allowed to be considered in the planning\nstep (f) of Figure 8.4. Second, the initial model for such actions was that they would lead\nback to the same state with a reward of zero.\n8.4. PRIORITIZED SWEEPING 207\nthen the values of the predecessor states may change in turn. If so, then actions\nleading into them need to be backed up, and then their predecessor states may\nhave changed. In this way one can work backward from arbitrary states that\nhave changed in value, either performing useful backups or terminating the\npropagation.\nAs the frontier of useful backups propagates backward, it often grows\nrapidly, producing many state{action pairs that could usefully be backed up.\nBut not all of these will be equally useful. The values of some states may\nhave changed a lot, whereas others have changed little. The predecessor pairs\nof those that have changed a lot are more likely to also change a lot. In a\nstochastic environment, variations in estimated transition probabilities also\ncontribute to variations in the sizes of changes and in the urgency with which\npairs need to be backed up. It is natural to prioritize the backups according to\na measure of their urgency, and perform them in order of priority. This is the\nidea behind prioritized sweeping . A queue is maintained of every state{action\npair whose estimated value would change nontrivially if backed up, prioritized\nby the size of the change. When the top pair in the queue is backed up, the\ne\u000bect on each of its predecessor pairs is computed. If the e\u000bect is greater\nthan some small threshold, then the pair is inserted in the queue with the new\npriority (if there is a previous entry of the pair in the queue, then insertion re-\nsults in only the higher priority entry remaining in the queue). In this way the\ne\u000bects of changes are e\u000eciently propagated backward until quiescence. The\nfull algorithm for the case of deterministic environments is given in Figure 8.9.\nExample 8.4: Prioritized Sweeping on Mazes Prioritized sweeping\nhas been found to dramatically increase the speed at which optimal solutions\nare found in maze tasks, often by a factor of 5 to 10. A typical example is\nshown in Figure 8.10. These data are for a sequence of maze tasks of exactly\nthe same structure as the one shown in Figure 8.5, except that they vary\nin the grid resolution. Prioritized sweeping maintained a decisive advantage\nover unprioritized Dyna-Q. Both systems made at most n= 5 backups per\nenvironmental interaction.\nExample 8.5: Rod Maneuvering The objective in this task is to maneuver\na rod around some awkwardly placed obstacles to a goal position in the fewest\nnumber of steps (Figure 8.11). The rod can be translated along its long axis\nor perpendicular to that axis, or it can be rotated in either direction around\nits center. The distance of each movement is approximately 1/20 of the work\nspace, and the rotation increment is 10 degrees. Translations are deterministic\nand quantized to one of 20 \u000220 positions. The \fgure shows the obstacles and\nthe shortest solution from start to goal, found by prioritized sweeping. This\nproblem is still deterministic, but has four actions and 14,400 potential states\n(some of these are unreachable because of the obstacles). This problem is\n208CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nInitializeQ(s;a),Model (s;a), for alls;a, andPQueue to empty\nDo forever:\n(a)S current (nonterminal) state\n(b)A policy (S;Q)\n(c) Execute action A; observe resultant reward, R, and state, S0\n(d)Model (S;A) R;S0\n(e)P jR+\rmaxaQ(S0;a)\u0000Q(S;A)j.\n(f) ifP >\u0012 , then insert S;A intoPQueue with priority P\n(g) Repeat ntimes, while PQueue is not empty:\nS;A first (PQueue )\nR;S0 Model (S;A)\nQ(S;A) Q(S;A) +\u000b\u0002\nR+\rmaxaQ(S0;a)\u0000Q(S;A)\u0003\nRepeat, for all \u0016S;\u0016Apredicted to lead to S:\n\u0016R predicted reward for \u0016S;\u0016A;S\nP j\u0016R+\rmaxaQ(S;a)\u0000Q(\u0016S;\u0016A)j.\nifP >\u0012 then insert \u0016S;\u0016AintoPQueue with priority P\nFigure 8.9: The prioritized sweeping algorithm for a deterministic environ-\nment.\nBackups\nuntil\noptimal\nsolution\n10103104105106107\n102\n0 47 94186 376 752 1504 3008 6016\nGridworld size (#states)Dyna-Q\nprioritized\nsweeping\nFigure 8.10: Prioritized sweeping signi\fcantly shortens learning time on the\nDyna maze task for a wide range of grid resolutions. Reprinted from Peng and\nWilliams (1993).\n8.4. PRIORITIZED SWEEPING 209\nStartGoal\nFigure 8.11: A rod-maneuvering task and its solution by prioritized sweeping.\nReprinted from Moore and Atkeson (1993).\nprobably too large to be solved with unprioritized methods.\nPrioritized sweeping is clearly a powerful idea, but the algorithms that have\nbeen developed so far appear not to extend easily to more interesting cases.\nThe greatest problem is that the algorithms appear to rely on the assumption\nof discrete states. When a change occurs at one state, these methods perform\na computation on all the predecessor states that may have been a\u000bected. If\nfunction approximation is used to learn the model or the value function, then\na single backup could in\ruence a great many other states. It is not apparent\nhow these states could be identi\fed or processed e\u000eciently. On the other hand,\nthe general idea of focusing search on the states believed to have changed in\nvalue, and then on their predecessors, seems intuitively to be valid in general.\nAdditional research may produce more general versions of prioritized sweeping.\nExtensions of prioritized sweeping to stochastic environments are relatively\nstraightforward. The model is maintained by keeping counts of the number of\ntimes each state{action pair has been experienced and of what the next states\nwere. It is natural then to backup each pair not with a sample backup, as we\nhave been using so far, but with a full backup, taking into account all possible\nnext states and their probabilities of occurring.\n210CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\n8.5 Full vs. Sample Backups\nThe examples in the previous sections give some idea of the range of possi-\nbilities for combining methods of learning and planning. In the rest of this\nchapter, we analyze some of the component ideas involved, starting with the\nrelative advantages of full and sample backups.\nMuch of this book has been about di\u000berent kinds of backups, and we have\nconsidered a great many varieties. Focusing for the moment on one-step back-\nups, they vary primarily along three binary dimensions. The \frst two dimen-\nsions are whether they back up state values or action values and whether they\nestimate the value for the optimal policy or for an arbitrary given policy. These\ntwo dimensions give rise to four classes of backups for approximating the four\nvalue functions, q\u0003,v\u0003,q\u0019, andv\u0019. The other binary dimension is whether the\nbackups are fullbackups, considering all possible events that might happen,\norsample backups, considering a single sample of what might happen. These\nthree binary dimensions give rise to eight cases, seven of which correspond to\nspeci\fc algorithms, as shown in Figure 8.12. (The eighth case does not seem\nto correspond to any useful backup.) Any of these one-step backups can be\nused in planning methods. The Dyna-Q agents discussed earlier use q\u0003sample\nbackups, but they could just as well use q\u0003full backups, or either full or sam-\npleq\u0019backups. The Dyna-AC system uses v\u0019sample backups together with\na learning policy structure. For stochastic problems, prioritized sweeping is\nalways done using one of the full backups.\nWhen we introduced one-step sample backups in Chapter 6, we presented\nthem as substitutes for full backups. In the absence of a distribution model,\nfull backups are not possible, but sample backups can be done using sample\ntransitions from the environment or a sample model. Implicit in that point\nof view is that full backups, if possible, are preferable to sample backups.\nBut are they? Full backups certainly yield a better estimate because they are\nuncorrupted by sampling error, but they also require more computation, and\ncomputation is often the limiting resource in planning. To properly assess the\nrelative merits of full and sample backups for planning we must control for\ntheir di\u000berent computational requirements.\nFor concreteness, consider the full and sample backups for approximating\nq\u0003, and the special case of discrete states and actions, a table-lookup repre-\nsentation of the approximate value function, Q, and a model in the form of\nestimated dynamics, ^ p(s0;rjs;a). The full backup for a state{action pair, s;a,\nis:\nQ(s;a) X\ns0;r^p(s0;rjs;a)h\nr+\rmax\na0Q(s0;a0)i\n: (8.1)\n8.5. FULL VS. SAMPLE BACKUPS 211\nFull backups(DP)Sample backups(one-step TD)ValueestimatedV!(s)V*(s)Q!(a,s)Q*(a,s)sas'rpolicy evaluationsas'rmaxvalue iterationsars'TD(0)\ns,aa's'rQ-policy  evaluations,aa's'rmaxQ-value iterations,aa's'rSarsas,aa's'rQ-learningmaxS'RS'Rv\u03c0v*q\u03c0q*ARS'\nA'\nFigure 8.12: The one-step backups.\n212CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nThe corresponding sample backup for s;a, given a sample next state and re-\nward,S0andR(from the model), is the Q-learning-like update:\nQ(s;a) Q(s;a) +\u000bh\nR+\rmax\na0Q(S0;a0)\u0000Q(s;a)i\n; (8.2)\nwhere\u000bis the usual positive step-size parameter.\nThe di\u000berence between these full and sample backups is signi\fcant to the\nextent that the environment is stochastic, speci\fcally, to the extent that, given\na state and action, many possible next states may occur with various probabil-\nities. If only one next state is possible, then the full and sample backups given\nabove are identical (taking \u000b= 1). If there are many possible next states, then\nthere may be signi\fcant di\u000berences. In favor of the full backup is that it is an\nexact computation, resulting in a new Q(s;a) whose correctness is limited only\nby the correctness of the Q(s0;a0) at successor states. The sample backup is\nin addition a\u000bected by sampling error. On the other hand, the sample backup\nis cheaper computationally because it considers only one next state, not all\npossible next states. In practice, the computation required by backup oper-\nations is usually dominated by the number of state{action pairs at which Q\nis evaluated. For a particular starting pair, s;a, letbbe the branching factor\n(i.e., the number of possible next states, s0, for which ^ p(s0js;a)>0). Then\na full backup of this pair requires roughly btimes as much computation as a\nsample backup.\nIf there is enough time to complete a full backup, then the resulting esti-\nmate is generally better than that of bsample backups because of the absence\nof sampling error. But if there is insu\u000ecient time to complete a full backup,\nthen sample backups are always preferable because they at least make some\nimprovement in the value estimate with fewer than bbackups. In a large prob-\nlem with many state{action pairs, we are often in the latter situation. With\nso many state{action pairs, full backups of all of them would take a very long\ntime. Before that we may be much better o\u000b with a few sample backups at\nmany state{action pairs than with full backups at a few pairs. Given a unit\nof computational e\u000bort, is it better devoted to a few full backups or to btimes\nas many sample backups?\nFigure 8.13 shows the results of an analysis that suggests an answer to\nthis question. It shows the estimation error as a function of computation\ntime for full and sample backups for a variety of branching factors, b. The\ncase considered is that in which all bsuccessor states are equally likely and\nin which the error in the initial estimate is 1. The values at the next states\nare assumed correct, so the full backup reduces the error to zero upon its\ncompletion. In this case, sample backups reduce the error according toq\nb\u00001\nbt\nwheretis the number of sample backups that have been performed (assuming\n8.6. TRAJECTORY SAMPLING 213\nb = 2 (branching factor)b =10b =100b =1000b =10,000samplebackupsfullbackups1\n001b2bRMS errorin valueestimateNumber of                         computationsmaxa0Q(s0,a0)\nFigure 8.13: Comparison of e\u000eciency of full and sample backups.\nsample averages, i.e., \u000b= 1=t). The key observation is that for moderately\nlargebthe error falls dramatically with a tiny fraction of bbackups. For these\ncases, many state{action pairs could have their values improved dramatically,\nto within a few percent of the e\u000bect of a full backup, in the same time that\none state{action pair could be backed up fully.\nThe advantage of sample backups shown in Figure 8.13 is probably an\nunderestimate of the real e\u000bect. In a real problem, the values of the succes-\nsor states would themselves be estimates updated by backups. By causing\nestimates to be more accurate sooner, sample backups will have a second ad-\nvantage in that the values backed up from the successor states will be more\naccurate. These results suggest that sample backups are likely to be superior\nto full backups on problems with large stochastic branching factors and too\nmany states to be solved exactly.\n8.6 Trajectory Sampling\nIn this section we compare two ways of distributing backups. The classical\napproach, from dynamic programming, is to perform sweeps through the entire\nstate (or state{action) space, backing up each state (or state{action pair) once\nper sweep. This is problematic on large tasks because there may not be time\nto complete even one sweep. In many tasks the vast majority of the states are\nirrelevant because they are visited only under very poor policies or with very\nlow probability. Exhaustive sweeps implicitly devote equal time to all parts\n214CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nof the state space rather than focusing where it is needed. As we discussed in\nChapter 4, exhaustive sweeps and the equal treatment of all states that they\nimply are not necessary properties of dynamic programming. In principle,\nbackups can be distributed any way one likes (to assure convergence, all states\nor state{action pairs must be visited in the limit an in\fnite number of times),\nbut in practice exhaustive sweeps are often used.\nThe second approach is to sample from the state or state{action space ac-\ncording to some distribution. One could sample uniformly, as in the Dyna-Q\nagent, but this would su\u000ber from some of the same problems as exhaustive\nsweeps. More appealing is to distribute backups according to the on-policy\ndistribution, that is, according to the distribution observed when following the\ncurrent policy. One advantage of this distribution is that it is easily gener-\nated; one simply interacts with the model, following the current policy. In an\nepisodic task, one starts in the start state (or according to the starting-state\ndistribution) and simulates until the terminal state. In a continuing task, one\nstarts anywhere and just keeps simulating. In either case, sample state tran-\nsitions and rewards are given by the model, and sample actions are given by\nthe current policy. In other words, one simulates explicit individual trajecto-\nries and performs backups at the state or state{action pairs encountered along\nthe way. We call this way of generating experience and backups trajectory\nsampling .\nIt is hard to imagine any e\u000ecient way of distributing backups according\nto the on-policy distribution other than by trajectory sampling. If one had\nan explicit representation of the on-policy distribution, then one could sweep\nthrough all states, weighting the backup of each according to the on-policy dis-\ntribution, but this leaves us again with all the computational costs of exhaus-\ntive sweeps. Possibly one could sample and update individual state{action\npairs from the distribution, but even if this could be done e\u000eciently, what\nbene\ft would this provide over simulating trajectories? Even knowing the\non-policy distribution in an explicit form is unlikely. The distribution changes\nwhenever the policy changes, and computing the distribution requires com-\nputation comparable to a complete policy evaluation. Consideration of such\nother possibilities makes trajectory sampling seem both e\u000ecient and elegant.\nIs the on-policy distribution of backups a good one? Intuitively it seems\nlike a good choice, at least better than the uniform distribution. For example,\nif you are learning to play chess, you study positions that might arise in real\ngames, not random positions of chess pieces. The latter may be valid states,\nbut to be able to accurately value them is a di\u000berent skill from evaluating\npositions in real games. We will also see in Chapter 9 that the on-policy\ndistribution has signi\fcant advantages when function approximation is used.\nWhether or not function approximation is used, one might expect on-policy\n8.6. TRAJECTORY SAMPLING 215\nfocusing to signi\fcantly improve the speed of planning.\nFocusing on the on-policy distribution could be bene\fcial because it causes\nvast, uninteresting parts of the space to be ignored, or it could be detrimental\nbecause it causes the same old parts of the space to be backed up over and\nover. We conducted a small experiment to assess the e\u000bect empirically. To\nisolate the e\u000bect of the backup distribution, we used entirely one-step full\ntabular backups, as de\fned by (8.1). In the uniform case, we cycled through\nall state{action pairs, backing up each in place, and in the on-policy case we\nsimulated episodes, backing up each state{action pair that occurred under\nthe current \u000f-greedy policy ( \u000f= 0:1). The tasks were undiscounted episodic\ntasks, generated randomly as follows. From each of the jSjstates, two actions\nwere possible, each of which resulted in one of bnext states, all equally likely,\nwith a di\u000berent random selection of bstates for each state{action pair. The\nbranching factor, b, was the same for all state{action pairs. In addition, on\nall transitions there was a 0.1 probability of transition to the terminal state,\nending the episode. We used episodic tasks to get a clear measure of the quality\nof the current policy. At any point in the planning process one can stop and\nexhaustively compute v~\u0019(s0), the true value of the start state under the greedy\npolicy, ~\u0019, given the current action-value function Q, as an indication of how\nwell the agent would do on a new episode on which it acted greedily (all the\nwhile assuming the model is correct).\nThe upper part of Figure 8.14 shows results averaged over 200 sample\ntasks with 1000 states and branching factors of 1, 3, and 10. The quality\nof the policies found is plotted as a function of the number of full backups\ncompleted. In all cases, sampling according to the on-policy distribution re-\nsulted in faster planning initially and retarded planning in the long run. The\ne\u000bect was stronger, and the initial period of faster planning was longer, at\nsmaller branching factors. In other experiments, we found that these e\u000bects\nalso became stronger as the number of states increased. For example, the\nlower part of Figure 8.14 shows results for a branching factor of 1 for tasks\nwith 10,000 states. In this case the advantage of on-policy focusing is large\nand long-lasting.\nAll of these results make sense. In the short term, sampling according to the\non-policy distribution helps by focusing on states that are near descendants\nof the start state. If there are many states and a small branching factor,\nthis e\u000bect will be large and long-lasting. In the long run, focusing on the\non-policy distribution may hurt because the commonly occurring states all\nalready have their correct values. Sampling them is useless, whereas sampling\nother states may actually perform some useful work. This presumably is why\nthe exhaustive, unfocused approach does better in the long run, at least for\nsmall problems. These results are not conclusive because they are only for\n216CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nb=10b=3b=1on-policy\nuniform1000 STATES\n0123\nValue of\nstart state\nunder\ngreedy\npolicy\n0 5,000 10,000 15,000 20,000\nComputation time, in full backups\n0123\nValue of\nstart state\nunder\ngreedy\npolicy\n0 50,000 100,000 150,000 200,000\nComputation time, in full backupsb=1\n10,000 STATESuniform\non-policy\nuniform\non-policy\non-policy\nuniform\nFigure 8.14: Relative e\u000eciency of backups distributed uniformly across the\nstate space versus focused on simulated on-policy trajectories. Results are for\nrandomly generated tasks of two sizes and various branching factors, b.\n8.7. HEURISTIC SEARCH 217\nproblems generated in a particular, random way, but they do suggest that\nsampling according to the on-policy distribution can be a great advantage\nfor large problems, in particular for problems in which a small subset of the\nstate{action space is visited under the on-policy distribution.\n8.7 Heuristic Search\nThe predominant state-space planning methods in arti\fcial intelligence are\ncollectively known as heuristic search . Although super\fcially di\u000berent from\nthe planning methods we have discussed so far in this chapter, heuristic search\nand some of its component ideas can be combined with these methods in\nuseful ways. Unlike these methods, heuristic search is not concerned with\nchanging the approximate, or \\heuristic,\" value function, but only with making\nimproved action selections given the current value function. In other words,\nheuristic search is planning as part of a policy computation.\nIn heuristic search, for each state encountered, a large tree of possible\ncontinuations is considered. The approximate value function is applied to the\nleaf nodes and then backed up toward the current state at the root. The\nbacking up within the search tree is just the same as in the max-backups\n(those forv\u0003andq\u0003) discussed throughout this book. The backing up stops\nat the state{action nodes for the current state. Once the backed-up values of\nthese nodes are computed, the best of them is chosen as the current action,\nand then all backed-up values are discarded.\nIn conventional heuristic search no e\u000bort is made to save the backed-up\nvalues by changing the approximate value function. In fact, the value func-\ntion is generally designed by people and never changed as a result of search.\nHowever, it is natural to consider allowing the value function to be improved\nover time, using either the backed-up values computed during heuristic search\nor any of the other methods presented throughout this book. In a sense we\nhave taken this approach all along. Our greedy and \"-greedy action-selection\nmethods are not unlike heuristic search, albeit on a smaller scale. For exam-\nple, to compute the greedy action given a model and a state-value function, we\nmust look ahead from each possible action to each possible next state, backup\nthe rewards and estimated values, and then pick the best action. Just as in\nconventional heuristic search, this process computes backed-up values of the\npossible actions, but does not attempt to save them. Thus, heuristic search\ncan be viewed as an extension of the idea of a greedy policy beyond a single\nstep.\nThe point of searching deeper than one step is to obtain better action\n218CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nselections. If one has a perfect model and an imperfect action-value function,\nthen in fact deeper search will usually yield better policies.2Certainly, if the\nsearch is all the way to the end of the episode, then the e\u000bect of the imperfect\nvalue function is eliminated, and the action determined in this way must be\noptimal. If the search is of su\u000ecient depth ksuch that\rkis very small, then the\nactions will be correspondingly near optimal. On the other hand, the deeper\nthe search, the more computation is required, usually resulting in a slower\nresponse time. A good example is provided by Tesauro's grandmaster-level\nbackgammon player, TD-Gammon (Section 15.1). This system used TD( \u0015)\nto learn an afterstate value function through many games of self-play, using a\nform of heuristic search to make its moves. As a model, TD-Gammon used a\npriori knowledge of the probabilities of dice rolls and the assumption that the\nopponent always selected the actions that TD-Gammon rated as best for it.\nTesauro found that the deeper the heuristic search, the better the moves made\nby TD-Gammon, but the longer it took to make each move. Backgammon\nhas a large branching factor, yet moves must be made within a few seconds.\nIt was only feasible to search ahead selectively a few steps, but even so the\nsearch resulted in signi\fcantly better action selections.\nSo far we have emphasized heuristic search as an action-selection technique,\nbut this may not be its most important aspect. Heuristic search also suggests\nways of selectively distributing backups that may lead to better and faster\napproximation of the optimal value function. A great deal of research on\nheuristic search has been devoted to making the search as e\u000ecient as possible.\nThe search tree is grown selectively, deeper along some lines and shallower\nalong others. For example, the search tree is often deeper for the actions\nthat seem most likely to be best, and shallower for those that the agent will\nprobably not want to take anyway. Can we use a similar idea to improve the\ndistribution of backups? Perhaps it can be done by preferentially updating\nstate{action pairs whose values appear to be close to the maximum available\nfrom the state. To our knowledge, this and other possibilities for distributing\nbackups based on ideas borrowed from heuristic search have not yet been\nexplored.\nWe should not overlook the most obvious way in which heuristic search\nfocuses backups: on the current state. Much of the e\u000bectiveness of heuristic\nsearch is due to its search tree being tightly focused on the states and actions\nthat might immediately follow the current state. You may spend more of\nyour life playing chess than checkers, but when you play checkers, it pays to\nthink about checkers and about your particular checkers position, your likely\nnext moves, and successor positions. However you select actions, it is these\nstates and actions that are of highest priority for backups and where you\n2There are interesting exceptions to this. See, e.g., Pearl (1984).\n8.7. HEURISTIC SEARCH 219\n1 23\n4 56\n78 910\nFigure 8.15: The deep backups of heuristic search can be implemented as a\nsequence of one-step backups (shown here outlined). The ordering shown is\nfor a selective depth-\frst search.\nmost urgently want your approximate value function to be accurate. Not only\nshould your computation be preferentially devoted to imminent events, but so\nshould your limited memory resources. In chess, for example, there are far too\nmany possible positions to store distinct value estimates for each of them, but\nchess programs based on heuristic search can easily store distinct estimates for\nthe millions of positions they encounter looking ahead from a single position.\nThis great focusing of memory and computational resources on the current\ndecision is presumably the reason why heuristic search can be so e\u000bective.\nThe distribution of backups can be altered in similar ways to focus on the\ncurrent state and its likely successors. As a limiting case we might use exactly\nthe methods of heuristic search to construct a search tree, and then perform the\nindividual, one-step backups from bottom up, as suggested by Figure 8.15. If\nthe backups are ordered in this way and a table-lookup representation is used,\nthen exactly the same backup would be achieved as in heuristic search. Any\nstate-space search can be viewed in this way as the piecing together of a large\nnumber of individual one-step backups. Thus, the performance improvement\nobserved with deeper searches is not due to the use of multistep backups as\nsuch. Instead, it is due to the focus and concentration of backups on states\nand actions immediately downstream from the current state. By devoting a\nlarge amount of computation speci\fcally relevant to the candidate actions, a\nmuch better decision can be made than by relying on unfocused backups.\n220CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\n8.8 Monte Carlo Tree Search\n8.9 Summary\nWe have presented a perspective emphasizing the surprisingly close relation-\nships between planning optimal behavior and learning optimal behavior. Both\ninvolve estimating the same value functions, and in both cases it is natural\nto update the estimates incrementally, in a long series of small backup op-\nerations. This makes it straightforward to integrate learning and planning\nprocesses simply by allowing both to update the same estimated value func-\ntion. In addition, any of the learning methods can be converted into planning\nmethods simply by applying them to simulated (model-generated) experience\nrather than to real experience. In this case learning and planning become even\nmore similar; they are possibly identical algorithms operating on two di\u000berent\nsources of experience.\nIt is straightforward to integrate incremental planning methods with acting\nand model-learning. Planning, acting, and model-learning interact in a circular\nfashion (Figure 8.2), each producing what the other needs to improve; no other\ninteraction among them is either required or prohibited. The most natural\napproach is for all processes to proceed asynchronously and in parallel. If\nthe processes must share computational resources, then the division can be\nhandled almost arbitrarily|by whatever organization is most convenient and\ne\u000ecient for the task at hand.\nIn this chapter we have touched upon a number of dimensions of variation\namong state-space planning methods. One of the most important of these is\nthe distribution of backups, that is, of the focus of search. Prioritized sweep-\ning focuses on the predecessors of states whose values have recently changed.\nHeuristic search applied to reinforcement learning focuses, inter alia, on the\nsuccessors of the current state. Trajectory sampling is a convenient way of fo-\ncusing on the on-policy distribution. All of these approaches can signi\fcantly\nspeed planning and are current topics of research.\nAnother interesting dimension of variation is the size of backups. The\nsmaller the backups, the more incremental the planning methods can be.\nAmong the smallest backups are one-step sample backups. We presented one\nstudy suggesting that one-step sample backups may be preferable on very large\nproblems. A related issue is the depth of backups. In many cases deep backups\ncan be implemented as sequences of shallow backups.\n8.9. SUMMARY 221\nBibliographical and Historical Remarks\n8.1 The overall view of planning and learning presented here has devel-\noped gradually over a number of years, in part by the authors (Sutton,\n1990, 1991a, 1991b; Barto, Bradtke, and Singh, 1991, 1995; Sutton and\nPinette, 1985; Sutton and Barto, 1981b); it has been strongly in\ru-\nenced by Agre and Chapman (1990; Agre 1988), Bertsekas and Tsit-\nsiklis (1989), Singh (1993), and others. The authors were also strongly\nin\ruenced by psychological studies of latent learning (Tolman, 1932)\nand by psychological views of the nature of thought (e.g., Galanter and\nGerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978).\n8.2{3 The terms direct and indirect , which we use to describe di\u000berent kinds\nof reinforcement learning, are from the adaptive control literature (e.g.,\nGoodwin and Sin, 1984), where they are used to make the same kind of\ndistinction. The term system identi\fcation is used in adaptive control\nfor what we call model-learning (e.g., Goodwin and Sin, 1984; Ljung\nand S\u007f oderstrom, 1983; Young, 1984). The Dyna architecture is due to\nSutton (1990), and the results in these sections are based on results\nreported there.\n8.4 Prioritized sweeping was developed simultaneously and independently\nby Moore and Atkeson (1993) and Peng and Williams (1993). The\nresults in Figure 8.10 are due to Peng and Williams (1993). The results\nin Figure 8.11 are due to Moore and Atkeson.\n8.5 This section was strongly in\ruenced by the experiments of Singh (1993).\n8.7 For further reading on heuristic search, the reader is encouraged to\nconsult texts and surveys such as those by Russell and Norvig (2009)\nand Korf (1988). Peng and Williams (1993) explored a forward focusing\nof backups much as is suggested in this section.\nExercises\nExercise 8.1 There is no Exercise 8.1.\nExercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+,\nperform better in the \frst phase as well as in the second phase of the blocking\nand shortcut experiments?\n222CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS\nExercise 8.3 Careful inspection of Figure 8.8 reveals that the di\u000berence\nbetween Dyna-Q+ and Dyna-Q narrowed slightly over the \frst part of the\nexperiment. What is the reason for this?\nExercise 8.4 (programming) The exploration bonus described above ac-\ntually changes the estimated values of states and actions. Is this necessary?\nSuppose the bonus \u0014p\u001cwas used not in backups, but solely in action selection.\nThat is, suppose the action selected was always that for which Q(S;a)+\u0014p\u001cSa\nwas maximal. Carry out a gridworld experiment that tests and illustrates the\nstrengths and weaknesses of this alternate approach.\nExercise 8.5 The analysis above assumed that all of the bpossible next\nstates were equally likely to occur. Suppose instead that the distribution was\nhighly skewed, that some of the bstates were much more likely to occur than\nmost. Would this strengthen or weaken the case for sample backups over full\nbackups? Support your answer.\nExercise 8.6 Some of the graphs in Figure 8.14 seem to be scalloped in\ntheir early portions, particularly the upper graph for b= 1 and the uniform\ndistribution. Why do you think this is? What aspects of the data shown\nsupport your hypothesis?\nExercise 8.7 (programming) If you have access to a moderately large\ncomputer, try replicating the experiment whose results are shown in the lower\npart of Figure 8.14. Then try the same experiment but with b= 3. Discuss\nthe meaning of your results.\nPart II\nApproximate Solution Methods\n223\n\nChapter 9\nOn-policy Approximation of\nAction Values\nWe have so far assumed that our estimates of value functions are represented\nas a table with one entry for each state or for each state{action pair. This is\na particularly clear and instructive case, but of course it is limited to tasks\nwith small numbers of states and actions. The problem is not just the memory\nneeded for large tables, but the time and data needed to \fll them accurately.\nIn other words, the key issue is that of generalization . How can experience\nwith a limited subset of the state space be usefully generalized to produce a\ngood approximation over a much larger subset?\nThis is a severe problem. In many tasks to which we would like to apply\nreinforcement learning, most states encountered will never have been expe-\nrienced exactly before. This will almost always be the case when the state\nor action spaces include continuous variables or complex sensations, such as\na visual image. The only way to learn anything at all on these tasks is to\ngeneralize from previously experienced states to ones that have never been\nseen.\nFortunately, generalization from examples has already been extensively\nstudied, and we do not need to invent totally new methods for use in reinforce-\nment learning. To a large extent we need only combine reinforcement learning\nmethods with existing generalization methods. The kind of generalization we\nrequire is often called function approximation because it takes examples from a\ndesired function (e.g., a value function) and attempts to generalize from them\nto construct an approximation of the entire function. Function approximation\nis an instance of supervised learning , the primary topic studied in machine\nlearning, arti\fcial neural networks, pattern recognition, and statistical curve\n\ftting. In principle, any of the methods studied in these \felds can be used in\n225\n226CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nreinforcement learning as described in this chapter.\n9.1 Value Prediction with Function Approxi-\nmation\nAs usual, we begin with the prediction problem of estimating the state-value\nfunctionv\u0019from experience generated using policy \u0019. The novelty in this\nchapter is that the approximate value function is represented not as a table\nbut as a parameterized functional form with parameter vector w2Rn. We\nwill write ^v(s;w)\u0019v\u0019(s) for the approximated value of state sgiven weight\nvector w. For example, ^ vmight be the function computed by an arti\fcial\nneural network, with wthe vector of connection weights. By adjusting the\nweights, any of a wide range of di\u000berent functions ^ vcan be implemented by\nthe network. Or ^ vmight be the function computed by a decision tree, where\nwis all the parameters de\fning the split points and leaf values of the tree.\nTypically, the number of parameters n(the number of components of w) is\nmuch less than the number of states, and changing one parameter changes the\nestimated value of many states. Consequently, when a single state is backed\nup, the change generalizes from that state to a\u000bect the values of many other\nstates.\nAll of the prediction methods covered in this book have been described\nas backups, that is, as updates to an estimated value function that shift its\nvalue at particular states toward a \\backed-up value\" for that state. Let us\nrefer to an individual backup by the notation s7!v, wheresis the state\nbacked up and vis the backed-up value, or target, that s's estimated value\nis shifted toward. For example, the Monte Carlo backup for value predic-\ntion isSt7!Gt, the TD(0) backup is St7!Rt+1+\r^v(St+1;wt), and the\ngeneral TD( \u0015) backup is St7!G\u0015\nt. In the DP policy evaluation backup\ns7!E\u0019[Rt+1+\r^v(St+1;wt)jSt=s], an arbitrary state sis backed up, whereas\nin the the other cases the state, St, encountered in (possibly simulated) expe-\nrience is backed up.\nIt is natural to interpret each backup as specifying an example of the\ndesired input{output behavior of the estimated value function. In a sense, the\nbackups7!vmeans that the estimated value for state sshould be more like\nv. Up to now, the actual update implementing the backup has been trivial:\nthe table entry for s's estimated value has simply been shifted a fraction of the\nway toward v. Now we permit arbitrarily complex and sophisticated function\napproximation methods to implement the backup. The normal inputs to these\nmethods are examples of the desired input{output behavior of the function\n9.1. VALUE PREDICTION WITH FUNCTION APPROXIMATION 227\nthey are trying to approximate. We use these methods for value prediction\nsimply by passing to them the s7!vof each backup as a training example. We\nthen interpret the approximate function they produce as an estimated value\nfunction.\nViewing each backup as a conventional training example in this way enables\nus to use any of a wide range of existing function approximation methods for\nvalue prediction. In principle, we can use any method for supervised learning\nfrom examples, including arti\fcial neural networks, decision trees, and vari-\nous kinds of multivariate regression. However, not all function approximation\nmethods are equally well suited for use in reinforcement learning. The most so-\nphisticated neural network and statistical methods all assume a static training\nset over which multiple passes are made. In reinforcement learning, however,\nit is important that learning be able to occur on-line, while interacting with\nthe environment or with a model of the environment. To do this requires\nmethods that are able to learn e\u000eciently from incrementally acquired data.\nIn addition, reinforcement learning generally requires function approximation\nmethods able to handle nonstationary target functions (target functions that\nchange over time). For example, in GPI control methods we often seek to\nlearnq\u0019while\u0019changes. Even if the policy remains the same, the target\nvalues of training examples are nonstationary if they are generated by boot-\nstrapping methods (DP and TD). Methods that cannot easily handle such\nnonstationarity are less suitable for reinforcement learning.\nWhat performance measures are appropriate for evaluating function ap-\nproximation methods? Most supervised learning methods seek to minimize\nthe root-mean-squared error (RMSE) over some distribution over the inputs.\nIn our value prediction problem, the inputs are states and the target function is\nthe true value function v\u0019, so RMSE for an approximation ^ v, using parameter\nw, is\nRMSE( w) =sX\ns2Sd(s)h\nv\u0019(s)\u0000^v(s;w)i2\n; (9.1)\nwhered:S![0;1], such thatP\nsd(s) = 1, is a distribution over the states\nspecifying the relative importance of errors in di\u000berent states. This distri-\nbution is important because it is usually not possible to reduce the error to\nzero at all states. After all, there are generally far more states than there\nare components to w. The \rexibility of the function approximator is thus a\nscarce resource. Better approximation at some states can be gained, generally,\nonly at the expense of worse approximation at other states. The distribution\nspeci\fes how these trade-o\u000bs should be made.\nThe distribution dis also usually the distribution from which the states in\nthe training examples are drawn, and thus the distribution of states at which\n228CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nbackups are done. If we wish to minimize error over a certain distribution of\nstates, then it makes sense to train the function approximator with examples\nfrom that same distribution. For example, if you want a uniform level of error\nover the entire state set, then it makes sense to train with backups distributed\nuniformly over the entire state set, such as in the exhaustive sweeps of some\nDP methods. Henceforth, let us assume that the distribution of states at which\nbackups are done and the distribution that weights errors, d, are the same.\nA distribution of particular interest is the one describing the frequency with\nwhich states are encountered while the agent is interacting with the environ-\nment and selecting actions according to \u0019, the policy whose value function we\nare approximating. We call this the on-policy distribution , in part because it\nis the distribution of backups in on-policy control methods. Minimizing error\nover the on-policy distribution focuses function approximation resources on\nthe states that actually occur while following the policy, ignoring those that\nnever occur. The on-policy distribution is also the one for which it is easiest to\nget training examples using Monte Carlo or TD methods. These methods gen-\nerate backups from sample experience using the policy \u0019. Because a backup\nis generated for each state encountered in the experience, the training exam-\nples available are naturally distributed according to the on-policy distribution.\nStronger convergence results are available for the on-policy distribution than\nfor other distributions, as we discuss later.\nIt is not completely clear that we should care about minimizing the RMSE.\nOur goal in value prediction is potentially di\u000berent because our ultimate pur-\npose is to use the predictions to aid in \fnding a better policy. The best pre-\ndictions for that purpose are not necessarily the best for minimizing RMSE.\nHowever, it is not yet clear what a more useful alternative goal for value pre-\ndiction might be. For now, we continue to focus on RMSE.\nAn ideal goal in terms of RMSE would be to \fnd a global optimum , a\nparameter vector w\u0003for which RMSE( w\u0003)\u0014RMSE( w) for all possible w.\nReaching this goal is sometimes possible for simple function approximators\nsuch as linear ones, but is rarely possible for complex function approximators\nsuch as arti\fcial neural networks and decision trees. Short of this, complex\nfunction approximators may seek to converge instead to a local optimum , a\nparameter vector w\u0003for which RMSE( w\u0003)\u0014RMSE( w) for all win some\nneighborhood of w\u0003. Although this guarantee is only slightly reassuring, it is\ntypically the best that can be said for nonlinear function approximators. For\nmany cases of interest in reinforcement learning, convergence to an optimum,\nor even all bound of an optimum may still be achieved with some methods.\nOther methods may in fact diverge, with their RMSE approaching in\fnity in\nthe limit.\n9.2. GRADIENT-DESCENT METHODS 229\nIn this section we have outlined a framework for combining a wide range\nof reinforcement learning methods for value prediction with a wide range of\nfunction approximation methods, using the backups of the former to generate\ntraining examples for the latter. We have also outlined a range of RMSE per-\nformance measures to which these methods may aspire. The range of possible\nmethods is far too large to cover all, and anyway too little is known about\nmost of them to make a reliable evaluation or recommendation. Of necessity,\nwe consider only a few possibilities. In the rest of this chapter we focus on\nfunction approximation methods based on gradient principles, and on linear\ngradient-descent methods in particular. We focus on these methods in part\nbecause we consider them to be particularly promising and because they re-\nveal key theoretical issues, but also because they are simple and our space\nis limited. If we had another chapter devoted to function approximation, we\nwould also cover at least memory-based and decision-tree methods.\n9.2 Gradient-Descent Methods\nWe now develop in detail one class of learning methods for function approxi-\nmation in value prediction, those based on gradient descent. Gradient-descent\nmethods are among the most widely used of all function approximation meth-\nods and are particularly well suited to online reinforcement learning.\nIn gradient-descent methods, the parameter vector is a column vector with\na \fxed number of real valued components, w= (w1;w2;:::;wn)>(the>here\ndenotes transpose), and the approximate value function ^ v(s;w) is a smooth\ndi\u000berentiable function of wfor alls2S. We will be updating wat each of\na series of discrete time steps, t= 1;2;3;:::, so we will need a notation wt\nfor the weight vector at each step. For now, let us assume that, on each step,\nwe observe a new example St7!v\u0019(St) consisting of a (possibly randomly\nselected) state Stand its true value under the policy. These states might be\nsuccessive states from an interaction with the environment, but for now we do\nnot assume so. Even though we are given the exact, correct values, v\u0019(St) for\neachSt, there is still a di\u000ecult problem because our function approximator has\nlimited resources and thus limited resolution. In particular, there is generally\nnowthat gets all the states, or even all the examples, exactly correct. In\naddition, we must generalize to all the other states that have not appeared in\nexamples.\nWe assume that states appear in examples with the same distribution, d,\nover which we are trying to minimize the RMSE as given by (9.1). A good\nstrategy in this case is to try to minimize error on the observed examples.\nGradient-descent methods do this by adjusting the parameter vector after\n230CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\neach example by a small amount in the direction that would most reduce the\nerror on that example:\nwt+1=wt\u00001\n2\u000brh\nv\u0019(St)\u0000^v(St;wt)i2\n=wt+\u000bh\nv\u0019(St)\u0000^v(St;wt)i\nr^v(St;wt); (9.2)\nwhere\u000bis a positive step-size parameter, and rf(wt), for any expression\nf(wt), denotes the vector of partial derivatives with respect to the components\nof the weight vector:\n\u0012@f(wt)\n@wt;1;@f(wt)\n@wt;2;:::;@f(wt)\n@wt;n\u0013>\n:\nThis derivative vector is the gradient offwith respect to wt. This kind of\nmethod is called gradient descent because the overall step in wtis proportional\nto the negative gradient of the example's squared error. This is the direction\nin which the error falls most rapidly.\nIt may not be immediately apparent why only a small step is taken in the\ndirection of the gradient. Could we not move all the way in this direction\nand completely eliminate the error on the example? In many cases this could\nbe done, but usually it is not desirable. Remember that we do not seek or\nexpect to \fnd a value function that has zero error on all states, but only an\napproximation that balances the errors in di\u000berent states. If we completely\ncorrected each example in one step, then we would not \fnd such a balance. In\nfact, the convergence results for gradient methods assume that the step-size\nparameter decreases over time. If it decreases in such a way as to satisfy the\nstandard stochastic approximation conditions (2.7), then the gradient-descent\nmethod (9.2) is guaranteed to converge to a local optimum.\nWe turn now to the case in which the target output, Vt, of thetth training\nexample,St7!Vt, is not the true value, v\u0019(St), but some, possibly random,\napproximation of it. For example, Vtmight be a noise-corrupted version of\nv\u0019(St), or it might be one of the backed-up values using ^ vmentioned in the\nprevious section. In such cases we cannot perform the exact update (9.2)\nbecausev\u0019(St) is unknown, but we can approximate it by substituting Vtin\nplace ofv\u0019(St). This yields the general gradient-descent method for state-value\nprediction:\nwt+1=wt+\u000bh\nVt\u0000^v(St;wt)i\nr^v(St;wt): (9.3)\nIfVtis an unbiased estimate, that is, if E[Vt] =v\u0019(St), for eacht, then wtis\nguaranteed to converge to a local optimum under the usual stochastic approx-\nimation conditions (2.7) for decreasing the step-size parameter \u000b.\n9.2. GRADIENT-DESCENT METHODS 231\nFor example, suppose the states in the examples are the states generated\nby interaction (or simulated interaction) with the environment using policy\n\u0019. LetGtdenote the return following each state, St. Because the true value\nof a state is the expected value of the return following it, the Monte Carlo\ntargetVt=Gtis by de\fnition an unbiased estimate of v\u0019(St). With this\nchoice, the general gradient-descent method (9.3) converges to a locally op-\ntimal approximation to v\u0019(St). Thus, the gradient-descent version of Monte\nCarlo state-value prediction is guaranteed to \fnd a locally optimal solution.\nSimilarly, we can use n-step TD returns and their averages for Vt. For\nexample, the gradient-descent form of TD( \u0015) uses the\u0015-return,Vt=G\u0015\nt, as\nits approximation to v\u0019(St), yielding the forward-view update:\nwt+1=wt+\u000bh\nG\u0015\nt\u0000^v(St;wt)i\nr^v(St;wt): (9.4)\nUnfortunately, for \u0015 <1,G\u0015\ntis not an unbiased estimate of v\u0019(St), and thus\nthis method does not converge to a local optimum. The situation is the same\nwhen DP targets are used such as Vt=E\u0019[Rt+1+\r^v(St+1;wt)jSt]. Never-\ntheless, such bootstrapping methods can be quite e\u000bective, and other perfor-\nmance guarantees are available for important special cases, as we discuss later\nin this chapter. For now we emphasize the relationship of these methods to\nthe general gradient-descent form (9.3). Although increments as in (9.4) are\nnot themselves gradients, it is useful to view this method as a gradient-descent\nmethod (9.3) with a bootstrapping approximation in place of the desired out-\nput,v\u0019(St).\nAs (9.4) provides the forward view of gradient-descent TD( \u0015), so the back-\nward view is provided by\nwt+1=wt+\u000b\u000etet; (9.5)\nwhere\u000etis the usual TD error, now using ^ v,\n\u000et=Rt+1+\r^v(St+1;wt)\u0000^v(St;wt); (9.6)\nandet= (et;1;et;2;:::;et;n)>is a column vector of eligibility traces, one for\neach component of wt, updated by\net=\r\u0015et\u00001+r^v(St;wt); (9.7)\nwithe0=0. A complete algorithm for on-line gradient-descent TD( \u0015) is given\nin Figure 9.1.\nTwo methods for gradient-based function approximation have been used\nwidely in reinforcement learning. One is multilayer arti\fcial neural networks\nusing the error backpropagation algorithm. This maps immediately onto the\nequations and algorithms just given, where the backpropagation process is the\nway of computing the gradients. The second popular form is the linear form,\nwhich we discuss extensively in the next section.\n232CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nInitialize was appropriate for the problem, e.g., w=0\nRepeat (for each episode):\ne= 0\nS initial state of episode\nRepeat (for each step of episode):\nA action given by \u0019forS\nTake action A, observe reward, R, and next state, S0\n\u000e R+\r^v(S0;w)\u0000^v(S;w)\ne \r\u0015e+r^v(S;w)\nw w+\u000b\u000ee\nS S0\nuntilS0is terminal\nFigure 9.1: On-line gradient-descent TD( \u0015) for estimating v\u0019.\n9.3 Linear Methods\nOne of the most important special cases of gradient-descent function approxi-\nmation is that in which the approximate function, ^ v, is a linear function of the\nparameter vector, w. Corresponding to every state s, there is a vector of fea-\ntures x(s) = (x1(s);x2(s);:::;xn(s))>, with the same number of components\nasw. The features may be constructed from the states in many di\u000berent ways;\nwe cover a few possibilities below. However the features are constructed, the\napproximate state-value function is given by\n^v(s;w) =w>x(s) =nX\ni=1wixi(s): (9.8)\nIn this case the approximate value function is said to be linear in the param-\neters, or simply linear .\nIt is natural to use gradient-descent updates with linear function approxi-\nmation. The gradient of the approximate value function with respect to win\nthis case is\nr^v(s;w) =x(s):\nThus, the general gradient-descent update (9.3) reduces to a particularly sim-\nple form in the linear case. In addition, in the linear case there is only one\noptimum w\u0003(or, in degenerate cases, one set of equally good optima). Thus,\nany method guaranteed to converge to or near a local optimum is automat-\nically guaranteed to converge to or near the global optimum. Because it is\nsimple in these ways, the linear, gradient-descent case is one of the most fa-\nvorable for mathematical analysis. Almost all useful convergence results for\n9.3. LINEAR METHODS 233\nlearning systems of all kinds are for linear (or simpler) function approximation\nmethods.\nIn particular, the gradient-descent TD( \u0015) algorithm discussed in the pre-\nvious section (Figure 9.1) has been proved to converge in the linear case if\nthe step-size parameter is reduced over time according to the usual conditions\n(2.7). Convergence is not to the minimum-error parameter vector, w\u0003, but to\na nearby parameter vector, w1, whose error is bounded according to\nRMSE( w1)\u00141\u0000\r\u0015\n1\u0000\rRMSE( w\u0003): (9.9)\nThat is, the asymptotic error is no more than1\u0000\r\u0015\n1\u0000\rtimes the smallest possible\nerror. As\u0015approaches 1, the bound approaches the minimum error. An anal-\nogous bound applies to other on-policy bootstrapping methods. For example,\nlinear gradient-descent DP backups (9.3), with the on-policy distribution, will\nconverge to the same result as TD(0). Technically, this bound applies only to\ndiscounted continuing tasks, but a related result presumably holds for episodic\ntasks. There are also a few technical conditions on the rewards, features, and\ndecrease in the step-size parameter, which we are omitting here. The full\ndetails can be found in the original paper (Tsitsiklis and Van Roy, 1997).\nCritical to the above result is that states are backed up according to the\non-policy distribution. For other backup distributions, bootstrapping methods\nusing function approximation may actually diverge to in\fnity. Examples of\nthis and a discussion of possible solution methods are given in Chapter 10.\nBeyond these theoretical results, linear learning methods are also of inter-\nest because in practice they can be very e\u000ecient in terms of both data and\ncomputation. Whether or not this is so depends critically on how the states\nare represented in terms of the features. Choosing features appropriate to the\ntask is an important way of adding prior domain knowledge to reinforcement\nlearning systems. Intuitively, the features should correspond to the natural\nfeatures of the task, those along which generalization is most appropriate. If\nwe are valuing geometric objects, for example, we might want to have features\nfor each possible shape, color, size, or function. If we are valuing states of a\nmobile robot, then we might want to have features for locations, degrees of\nremaining battery power, recent sonar readings, and so on.\nIn general, we also need features for combinations of these natural qualities.\nThis is because the linear form prohibits the representation of interactions\nbetween features, such as the presence of feature ibeing good only in the\nabsence of feature j. For example, in the pole-balancing task (Example 3.4),\na high angular velocity may be either good or bad depending on the angular\nposition. If the angle is high, then high angular velocity means an imminent\ndanger of falling, a bad state, whereas if the angle is low, then high angular\n234CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nFigure 9.2: Coarse coding. Generalization from state Xto stateYdepends\non the number of their features whose receptive \felds (in this case, circles)\noverlap. These states have one feature in common, so there will be slight\ngeneralization between them.\nvelocity means the pole is righting itself, a good state. In cases with such\ninteractions one needs to introduce features for conjunctions of feature values\nwhen using linear function approximation methods. We next consider some\ngeneral ways of doing this.\nCoarse Coding\nConsider a task in which the state set is continuous and two-dimensional. A\nstate in this case is a point in 2-space, a vector with two real components. One\nkind of feature for this case is those corresponding to circles in state space,\nas shown in Figure 9.2. If the state is inside a circle, then the corresponding\nfeature has the value 1 and is said to be present ; otherwise the feature is 0 and\nis said to be absent . This kind of 1{0-valued feature is called a binary feature .\nGiven a state, which binary features are present indicate within which circles\nthe state lies, and thus coarsely code for its location. Representing a state\nwith features that overlap in this way (although they need not be circles or\nbinary) is known as coarse coding .\nAssuming linear gradient-descent function approximation, consider the ef-\nfect of the size and density of the circles. Corresponding to each circle is a\nsingle parameter (a component of w) that is a\u000bected by learning. If we train\nat one point (state) X, then the parameters of all circles intersecting X will be\na\u000bected. Thus, by (9.8), the approximate value function will be a\u000bected at all\npoints within the union of the circles, with a greater e\u000bect the more circles a\npoint has \\in common\" with X, as shown in Figure 9.2. If the circles are small,\nthen the generalization will be over a short distance, as in Figure 9.3a, whereas\n9.3. LINEAR METHODS 235\na) Narrow generalization b) Broad generalization c) Asymmetric generalization\nFigure 9.3: Generalization in linear function approximation methods is deter-\nmined by the sizes and shapes of the features' receptive \felds. All three of\nthese cases have roughly the same number and density of features.\nif they are large, it will be over a large distance, as in Figure 9.3b. Moreover,\nthe shape of the features will determine the nature of the generalization. For\nexample, if they are not strictly circular, but are elongated in one direction,\nthen generalization will be similarly a\u000bected, as in Figure 9.3c.\nFeatures with large receptive \felds give broad generalization, but might\nalso seem to limit the learned function to a coarse approximation, unable\nto make discriminations much \fner than the width of the receptive \felds.\nHappily, this is not the case. Initial generalization from one point to another\nis indeed controlled by the size and shape of the receptive \felds, but acuity,\nthe \fnest discrimination ultimately possible, is controlled more by the total\nnumber of features.\nExample 9.1: Coarseness of Coarse Coding This example illustrates\nthe e\u000bect on learning of the size of the receptive \felds in coarse coding. Linear\nfunction approximation based on coarse coding and (9.3) was used to learn a\none-dimensional square-wave function (shown at the top of Figure 9.4). The\nvalues of this function were used as the targets, Vt. With just one dimension,\nthe receptive \felds were intervals rather than circles. Learning was repeated\nwith three di\u000berent sizes of the intervals: narrow, medium, and broad, as\nshown at the bottom of the \fgure. All three cases had the same density of\nfeatures, about 50 over the extent of the function being learned. Training\nexamples were generated uniformly at random over this extent. The step-size\nparameter was \u000b=0:2\nm, wheremis the number of features that were present\nat one time. Figure 9.4 shows the functions learned in all three cases over the\ncourse of learning. Note that the width of the features had a strong e\u000bect early\nin learning. With broad features, the generalization tended to be broad; with\nnarrow features, only the close neighbors of each trained point were changed,\ncausing the function learned to be more bumpy. However, the \fnal function\n236CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\n10\n40\n160\n640\n2560\n10240\nNarrow\nfeaturesdesired\nfunction\nMedium\nfeaturesBroad\nfeatures#Examplesapprox-\nimation\nfeature\nwidth\nFigure 9.4: Example of feature width's strong e\u000bect on initial generalization\n(\frst row) and weak e\u000bect on asymptotic accuracy (last row).\nlearned was a\u000bected only slightly by the width of the features. Receptive\n\feld shape tends to have a strong e\u000bect on generalization but little e\u000bect on\nasymptotic solution quality.\nTile Coding\nTile coding is a form of coarse coding that is particularly well suited for use on\nsequential digital computers and for e\u000ecient on-line learning. In tile coding\nthe receptive \felds of the features are grouped into exhaustive partitions of\nthe input space. Each such partition is called a tiling , and each element of the\npartition is called a tile. Each tile is the receptive \feld for one binary feature.\nAn immediate advantage of tile coding is that the overall number of features\nthat are present at one time is strictly controlled and independent of the input\nstate. Exactly one feature is present in each tiling, so the total number of\nfeatures present is always the same as the number of tilings. This allows\nthe step-size parameter, \u000b, to be set in an easy, intuitive way. For example,\nchoosing\u000b=1\nm, wheremis the number of tilings, results in exact one-trial\nlearning. If the example s7!vis received, then whatever the prior value,\n^v(s;w), the new value will be ^ v(s;w) =v. Usually one wishes to change more\nslowly than this, to allow for generalization and stochastic variation in target\noutputs. For example, one might choose \u000b=1\n10m, in which case one would\nmove one-tenth of the way to the target in one update.\nBecause tile coding uses exclusively binary (0{1-valued) features, the weighted\n9.3. LINEAR METHODS 237\nsum making up the approximate value function (9.8) is almost trivial to com-\npute. Rather than performing nmultiplications and additions, one simply\ncomputes the indices of the m\u001cnpresent features and then adds up the\nmcorresponding components of the parameter vector. The eligibility trace\ncomputation (9.7) is also simpli\fed because the components of the gradient,\nr^v(s;w), are also usually 0, and otherwise 1.\nThe computation of the indices of the present features is particularly easy\nif gridlike tilings are used. The ideas and techniques here are best illustrated\nby examples. Suppose we address a task with two continuous state variables.\nThen the simplest way to tile the space is with a uniform two-dimensional\ngrid:\nGiven thexandycoordinates of a point in the space, it is computationally\neasy to determine the index of the tile it is in. When multiple tilings are used,\neach is o\u000bset by a di\u000berent amount, so that each cuts the space in a di\u000berent\nway. In the example shown in Figure 9.5, an extra row and an extra column\nof tiles have been added to the grid so that no points are left uncovered. The\ntwo tiles highlighted are those that are present in the state indicated by the\nX. The di\u000berent tilings may be o\u000bset by random amounts, or by cleverly de-\nsigned deterministic strategies (simply o\u000bsetting each dimension by the same\nincrement is known not to be a good idea). The e\u000bects on generalization and\nasymptotic accuracy illustrated in Figures 9.3 and 9.4 apply here as well. The\nwidth and shape of the tiles should be chosen to match the width of general-\nization that one expects to be appropriate. The number of tilings should be\nchosen to in\ruence the density of tiles. The denser the tiling, the \fner and\nmore accurately the desired function can be approximated, but the greater the\ncomputational costs.\nIt is important to note that the tilings can be arbitrary and need not be\nuniform grids. Not only can the tiles be strangely shaped, as in Figure 9.6a,\nbut they can be shaped and distributed to give particular kinds of generaliza-\ntion. For example, the stripe tiling in Figure 9.6b will promote generalization\nalong the vertical dimension and discrimination along the horizontal dimen-\nsion, particularly on the left. The diagonal stripe tiling in Figure 9.6c will\npromote generalization along one diagonal. In higher dimensions, axis-aligned\nstripes correspond to ignoring some of the dimensions in some of the tilings,\n238CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nthat is, to hyperplanar slices.\nAnother important trick for reducing memory requirements is hashing |a\nconsistent pseudo-random collapsing of a large tiling into a much smaller set\nof tiles. Hashing produces tiles consisting of noncontiguous, disjoint regions\nrandomly spread throughout the state space, but that still form an exhaustive\ntiling. For example, one tile might consist of the four subtiles shown below:\none\ntile\nThrough hashing, memory requirements are often reduced by large factors with\nlittle loss of performance. This is possible because high resolution is needed\nin only a small fraction of the state space. Hashing frees us from the curse of\ndimensionality in the sense that memory requirements need not be exponential\nin the number of dimensions, but need merely match the real demands of the\ntask. Good public-domain implementations of tile coding, including hashing,\nare widely available.\nRadial Basis Functions\nRadial basis functions (RBFs) are the natural generalization of coarse coding\nto continuous-valued features. Rather than each feature being either 0 or 1,\nit can be anything in the interval [0 ;1], re\recting various degrees to which\nthe feature is present. A typical RBF feature, i, has a Gaussian (bell-shaped)\nresponsexi(s) dependent only on the distance between the state, s, and the\ntiling #1\ntiling #2\nShape of tiles ! Generalization\n#Tilings ! Resolution of final approximation2D state\nspace\nFigure 9.5: Multiple, overlapping gridtilings.\n9.3. LINEAR METHODS 239\na) Irregular b) Log stripes c) Diagonal stripes\nFigure 9.6: Tilings.\nci!i\nci+1ci-1\nFigure 9.7: One-dimensional radial basis functions.\nfeature's prototypical or center state, ci, and relative to the feature's width,\n\u001bi:\nxi(s) = exp\u0012\n\u0000jjs\u0000cijj2\n2\u001b2\ni\u0013\n:\nThe norm or distance metric of course can be chosen in whatever way seems\nmost appropriate to the states and task at hand. Figure 9.7 shows a one-\ndimensional example with a Euclidean distance metric.\nAnRBF network is a linear function approximator using RBFs for its fea-\ntures. Learning is de\fned by equations (9.3) and (9.8), exactly as in other lin-\near function approximators. The primary advantage of RBFs over binary fea-\ntures is that they produce approximate functions that vary smoothly and are\ndi\u000berentiable. In addition, some learning methods for RBF networks change\nthe centers and widths of the features as well. Such nonlinear methods may\nbe able to \ft the target function much more precisely. The downside to RBF\nnetworks, and to nonlinear RBF networks especially, is greater computational\ncomplexity and, often, more manual tuning before learning is robust and e\u000e-\ncient.\n240CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nKanerva Coding\nOn tasks with very high dimensionality, say hundreds of dimensions, tile coding\nand RBF networks become impractical. If we take either method at face\nvalue, its computational complexity increases exponentially with the number\nof dimensions. There are a number of tricks that can reduce this growth (such\nas hashing), but even these become impractical after a few tens of dimensions.\nOn the other hand, some of the general ideas underlying these methods can\nbe practical for high-dimensional tasks. In particular, the idea of representing\nstates by a list of the features present and then mapping those features linearly\nto an approximation may scale well to large tasks. The key is to keep the\nnumber of features from scaling explosively. Is there any reason to think this\nmight be possible?\nFirst we need to establish some realistic expectations. Roughly speaking, a\nfunction approximator of a given complexity can only accurately approximate\ntarget functions of comparable complexity. But as dimensionality increases,\nthe size of the state space inherently increases exponentially. It is reasonable\nto assume that in the worst case the complexity of the target function scales\nlike the size of the state space. Thus, if we focus the worst case, then there\nis no solution, no way to get good approximations for high-dimensional tasks\nwithout using resources exponential in the dimension.\nA more useful way to think about the problem is to focus on the complexity\nof the target function as separate and distinct from the size and dimensionality\nof the state space. The size of the state space may give an upper bound on\ncomplexity, but short of that high bound, complexity and dimension can be\nunrelated. For example, one might have a 1000-dimensional task where only\none of the dimensions happens to matter. Given a certain level of complexity,\nwe then seek to be able to accurately approximate any target function of that\ncomplexity or less. As the target level of complexity increases, we would like\nto get by with a proportionate increase in computational resources.\nFrom this point of view, the real source of the problem is the complexity of\nthe target function, or of a reasonable approximation of it, not the dimension-\nality of the state space. Thus, adding dimensions, such as new sensors or new\nfeatures, to a task should be almost without consequence if the complexity of\nthe needed approximations remains the same. The new dimensions may even\nmake things easier if the target function can be simply expressed in terms of\nthem. Unfortunately, methods like tile coding and RBF coding do not work\nthis way. Their complexity increases exponentially with dimensionality even if\nthe complexity of the target function does not. For these methods, dimension-\nality itself is still a problem. We need methods whose complexity is una\u000bected\n9.4. CONTROL WITH FUNCTION APPROXIMATION 241\nby dimensionality per se, methods that are limited only by, and scale well\nwith, the complexity of what they approximate.\nOne simple approach that meets these criteria, which we call Kanerva\ncoding , is to choose binary features that correspond to particular prototype\nstates . For de\fniteness, let us say that the prototypes are randomly selected\nfrom the entire state space. The receptive \feld of such a feature is all states\nsu\u000eciently close to the prototype. Kanerva coding uses a di\u000berent kind of\ndistance metric than in is used in tile coding and RBFs. For de\fniteness,\nconsider a binary state space and the hamming distance , the number of bits\nat which two states di\u000ber. States are considered similar if they agree on enough\ndimensions, even if they are totally di\u000berent on others.\nThe strength of Kanerva coding is that the complexity of the functions\nthat can be learned depends entirely on the number of features, which bears\nno necessary relationship to the dimensionality of the task. The number of\nfeatures can be more or less than the number of dimensions. Only in the worst\ncase must it be exponential in the number of dimensions. Dimensionality itself\nis thus no longer a problem. Complex functions are still a problem, as they\nhave to be. To handle more complex tasks, a Kanerva coding approach simply\nneeds more features. There is not a great deal of experience with such systems,\nbut what there is suggests that their abilities increase in proportion to their\ncomputational resources. This is an area of current research, and signi\fcant\nimprovements in existing methods can still easily be found.\n9.4 Control with Function Approximation\nWe now extend value prediction methods using function approximation to\ncontrol methods, following the pattern of GPI. First we extend the state-\nvalue prediction methods to action-value prediction methods, then we combine\nthem with policy improvement and action selection techniques. As usual, the\nproblem of ensuring exploration is solved by pursuing either an on-policy or\nan o\u000b-policy approach.\nThe extension to action-value prediction is straightforward. In this case\nit is the approximate action-value function, ^ q\u0019q\u0019, that is represented as a\nparameterized functional form with parameter vector w. Whereas before we\nconsidered random training examples of the form St7!Vt, now we consider ex-\namples of the form St;At7!Qt. The target output, Qt, can be any approxima-\ntion ofq\u0019(St;At), including the usual backed-up values such as the full Monte\nCarlo return, Gt, or the one-step Sarsa-style return, Gt+1+\r^q(St+1;At+1;wt).\n242CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nThe general gradient-descent update for action-value prediction is\nwt+1=wt+\u000bh\nQt\u0000^q(St;At;wt)i\nr^q(St;At;wt):\nFor example, the backward view of the action-value method analogous to\nTD(\u0015) is\nwt+1=wt+\u000b\u000etet;\nwhere\n\u000et=Rt+1+\r^q(St+1;At+1;wt)\u0000^q(St;At;wt);\nand\net=\r\u0015et\u00001+r^q(St;At;wt);\nwith e0=0. We call this method gradient-descent Sarsa( \u0015), particularly\nwhen it is elaborated to form a full control method. For a constant policy,\nthis method converges in the same way that TD( \u0015) does, with the same kind\nof error bound (9.9).\nTo form control methods, we need to couple such action-value prediction\nmethods with techniques for policy improvement and action selection. Suitable\ntechniques applicable to continuous actions, or to actions from large discrete\nsets, are a topic of ongoing research with as yet no clear resolution. On the\nother hand, if the action set is discrete and not too large, then we can use the\ntechniques already developed in previous chapters. That is, for each possible\naction,a, available in the current state, St, we can compute ^ q(St;a;wt) and\nthen \fnd the greedy action a\u0003\nt= argmaxa^q(St;a;wt). Policy improvement\nis done by changing the estimation policy to the greedy policy (in o\u000b-policy\nmethods) or to a soft approximation of the greedy policy such as the \"-greedy\npolicy (in on-policy methods). Actions are selected according to this same\npolicy in on-policy methods, or by an arbitrary policy in o\u000b-policy methods.\nFigures 9.8 and 9.9 show examples of on-policy (Sarsa( \u0015)) and o\u000b-policy\n(Watkins's Q( \u0015)) control methods using function approximation. Both meth-\nods use linear, gradient-descent function approximation with binary features,\nsuch as in tile coding and Kanerva coding. Both methods use an \"-greedy\npolicy for action selection, and the Sarsa method uses it for GPI as well. Both\ncompute the sets of present features, Fa, corresponding to the current state\nand all possible actions, a. If the value function for each action is a sepa-\nrate linear function of the same features (a common case), then the indices of\ntheFafor each action are essentially the same, simplifying the computation\nsigni\fcantly.\n9.4. CONTROL WITH FUNCTION APPROXIMATION 243\nLetwandebe vectors with one component for each possible feature\nLetFa, for every possible action a, be a set of feature indices, initially empty\nInitialize was appropriate for the problem, e.g., w=0\nRepeat (for each episode):\ne=0\nS;A initial state and action of episode (e.g., \"-greedy)\nFA set of features present in S;A\nRepeat (for each step of episode):\nFor alli2FA:\nei ei+ 1 (accumulating traces)\norei 1 (replacing traces)\nTake action A, observe reward, R, and next state, S0\n\u000e R\u0000P\ni2FAwi\nIfS0is terminal, then w w+\u000b\u000ee; go to next episode\nFor alla2A(S0):\nFa set of features present in S0;a\nQa P\ni2Fawi\nA0 new action in S0(e.g.,\"-greedy)\n\u000e \u000e+\rQA0\nw w+\u000b\u000ee\ne \r\u0015e\nS S0\nA A0\nFigure 9.8: Linear, gradient-descent Sarsa( \u0015) with binary features and \"-\ngreedy policy. Updates for both accumulating and replacing traces are speci-\n\fed.\n244CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nLetwandebe vectors with one component for each possible feature\nLetFa, for every possible action a, be a set of feature indices, initially empty\nInitialize was appropriate for the problem, e.g., w=0\nRepeat (for each episode):\ne=0\nS initial state of episode\nRepeat (for each step of episode):\nFor alla2A(S):\nFa set of features present in S;a\nQa P\ni2Fawi\nA\u0003 argmaxaQa\nA A\u0003with prob. 1\u0000\", else a random action 2A(S)\nIfA6=A\u0003, then e=0\nTake action A, observe reward, R, and next state, S0\n\u000e R\u0000QA\nFor alli2FA:\nei ei+ 1 (accumulating traces)\norei 1 (replacing traces)\nIfS0is terminal, then w w+\u000b\u000ee; go to next episode\nFor alla2A(S0):\nFa set of features present in S0;a\nQa P\ni2Fawi\n\u000e \u000e+\rmaxa2A(S0)Qa\nw w+\u000b\u000ee\ne \r\u0015e\nS S0\nFigure 9.9: A linear, gradient-descent version of Watkins's Q( \u0015) with binary\nfeatures and \"-greedy policy. Updates for both accumulating and replacing\ntraces are speci\fed.\n9.4. CONTROL WITH FUNCTION APPROXIMATION 245\nAll the methods we have discussed above have used accumulating eligibility\ntraces. Although replacing traces (Section 7.8) are known to have advantages\nin tabular methods, replacing traces do not directly extend to the use of func-\ntion approximation. Recall that the idea of replacing traces is to reset a state's\ntrace to 1 each time it is visited instead of incrementing it by 1. But with func-\ntion approximation there is no single trace corresponding to a state, just a trace\nfor each component of w, which corresponds to many states. One approach\nthat seems to work well for linear, gradient-descent function approximation\nmethods with binary features is to treat the features as if they were states for\nthe purposes of replacing traces. That is, each time a state is encountered that\nhas feature i, the trace for feature iis set to 1 rather than being incremented\nby 1, as it would be with accumulating traces.\nWhen working with state{action traces, it may also be useful to clear (set\nto zero) the traces of all nonselected actions in the states encountered (see\nSection 7.8). This idea can also be extended to the case of linear function\napproximation with binary features. For each state encountered, we \frst clear\nthe traces of all features for the state and the actions not selected, then we set\nto 1 the traces of the features for the state and the action that was selected.\nAs we noted for the tabular case, this may or may not be the best way to\nproceed when using replacing traces. A procedural speci\fcation of both kinds\nof traces, including the optional clearing for nonselected actions, is given for\nthe Sarsa algorithm in Figure 9.8.\nExample 9.2: Mountain{Car Task Consider the task of driving an un-\nderpowered car up a steep mountain road, as suggested by the diagram in the\nupper left of Figure 9.10. The di\u000eculty is that gravity is stronger than the\ncar's engine, and even at full throttle the car cannot accelerate up the steep\nslope. The only solution is to \frst move away from the goal and up the op-\nposite slope on the left. Then, by applying full throttle the car can build up\nenough inertia to carry it up the steep slope even though it is slowing down\nthe whole way. This is a simple example of a continuous control task where\nthings have to get worse in a sense (farther from the goal) before they can get\nbetter. Many control methodologies have great di\u000eculties with tasks of this\nkind unless explicitly aided by a human designer.\nThe reward in this problem is \u00001 on all time steps until the car moves past\nits goal position at the top of the mountain, which ends the episode. There are\nthree possible actions: full throttle forward (+1), full throttle reverse ( \u00001),\nand zero throttle (0). The car moves according to a simpli\fed physics. Its\nposition,pt, and velocity, _ pt, are updated by\npt+1=bound\u0002\npt+ _pt+1\u0003\n_pt+1=bound\u0002\n_pt+ 0:001At\u00000:0025 cos(3pt)\u0003\n;\n246CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\n!1.2\nPosition\n0.6Step 428Goal\nPosition4\n0\n!.07.07\nVelocity\nVelocity\nVelocity\nVelocity\nVelocity\nVelocity\nPositionPosition\nPosition027\n0120\n0104\n046Episode 12\nEpisode 104 Episode 1000 Episode 9000MOUNTAIN   CAR\nFigure 9.10: The mountain{car task (upper left panel) and the cost-to-go\nfunction (\u0000maxa^q(s;a;w)) learned during one run.\nwhere thebound operation enforces \u00001:2\u0014pt+1\u00140:5 and\u00000:07\u0014_pt+1\u0014\n0:07. When pt+1reached the left bound, _ pt+1was reset to zero. When it\nreached the right bound, the goal was reached and the episode was terminated.\nEach episode started from a random position and velocity uniformly chosen\nfrom these ranges. To convert the two continuous state variables to binary\nfeatures, we used gridtilings as in Figure 9.5. We used ten 9 \u00029 tilings, each\no\u000bset by a random fraction of a tile width.\nThe Sarsa algorithm in Figure 9.8 (using replace traces and the optional\nclearing) readily solved this task, learning a near optimal policy within 100\nepisodes. Figure 9.10 shows the negative of the value function (the cost-to-\ngofunction) learned on one run, using the parameters \u0015= 0:9,\"= 0, and\n\u000b= 0:05 (e.g.,0:5\nm). The initial action values were all zero, which was optimistic\n(all true values are negative in this task), causing extensive exploration to\noccur even though the exploration parameter, \", was 0. This can be seen in\nthe middle-top panel of the \fgure, labeled \\Step 428.\" At this time not even\none episode had been completed, but the car has oscillated back and forth in\nthe valley, following circular trajectories in state space. All the states visited\nfrequently are valued worse than unexplored states, because the actual rewards\nhave been worse than what was (unrealistically) expected. This continually\ndrives the agent away from wherever it has been, to explore new states, until\na solution is found. Figure 9.11 shows the results of a detailed study of the\n9.5. SHOULD WE BOOTSTRAP? 247\n!=.9400500600700800\n00.20.40.60.811.200.20.40.60.811.2\" \u00d7 5REPLACE  TRACESACCUMULATE  TRACES!=1!=.99!=.95\n!=0!=.4!=.7!=.8!=.5!=0!=.3!=.6\" \u00d7 5Steps per episodeaveraged overfirst 20 trialsand 30 runs\ufb01rst 20 episodes\nFigure 9.11: The e\u000bect of \u000b,\u0015, and the kind of traces on early performance\non the mountain{car task. This study used \fve 9 \u00029 tilings.\ne\u000bect of the parameters \u000band\u0015, and of the kind of traces, on the rate of\nlearning on this task.\n9.5 Should We Bootstrap?\nAt this point you may be wondering why we bother with bootstrapping meth-\nods at all. Nonbootstrapping methods can be used with function approxima-\ntion more reliably and over a broader range of conditions than bootstrapping\nmethods. Nonbootstrapping methods achieve a lower asymptotic error than\nbootstrapping methods, even when backups are done according to the on-\npolicy distribution. By using eligibility traces and \u0015= 1, it is even possible to\nimplement nonbootstrapping methods on-line, in a step-by-step incremental\nmanner. Despite all this, in practice bootstrapping methods are usually the\nmethods of choice.\nIn empirical comparisons, bootstrapping methods usually perform much\nbetter than nonbootstrapping methods. A convenient way to make such com-\nparisons is to use a TD method with eligibility traces and vary \u0015from 0 (pure\nbootstrapping) to 1 (pure nonbootstrapping). Figure 9.12 summarizes a col-\nlection of such results. In all cases, performance became much worse as \u0015\napproached 1, the nonbootstrapping case. The example in the upper right of\nthe \fgure is particularly signi\fcant in this regard. This is a policy evaluation\n(prediction) task and the performance measure used is RMSE (at the end of\neach episode, averaged over the \frst 20 episodes). Asymptotically, the \u0015= 1\ncase must be best according to this measure, but here, short of the asymptote,\n248CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\naccumulating\ntraces  \n0.20.30.40.5\n0 0.2 0.4 0.6 0.8 1\n!RANDOM WALK\n50100150200250300\nFailures per\n100,000 steps\n0 0.2 0.4 0.6 0.8 1\n!CART AND POLE400450500550600650700\n Steps per\nepisode\n0 0.2 0.4 0.6 0.8 1\n!MOUNTAIN  CAR\nreplacing\ntraces\n150160170180190200210220230240\nCost per\nepisode\n0 0.2 0.4 0.6 0.8 1\n!PUDDLE WORLD\nreplacing\ntracesaccumulating\ntraces \nreplacing\ntraces\naccumulating\ntracesRMS error\nFigure 9.12: The e\u000bect of \u0015on reinforcement learning performance. In all\ncases, the better the performance, the lower the curve. The two left panels\nare applications to simple continuous-state control tasks using the Sarsa( \u0015) al-\ngorithm and tile coding, with either replacing or accumulating traces (Sutton,\n1996). The upper-right panel is for policy evaluation on a random walk task\nusing TD(\u0015) (Singh and Sutton, 1996). The lower right panel is unpublished\ndata for the pole-balancing task (Example 3.4) from an earlier study (Sutton,\n1984).\n9.6. SUMMARY 249\nwe see it performing much worse.\nAt this time it is unclear why methods that involve some bootstrapping\nperform so much better than pure nonbootstrapping methods. It could be\nthat bootstrapping methods learn faster, or it could be that they actually\nlearn something better than nonbootstrapping methods. The available results\nindicate that nonbootstrapping methods are better than bootstrapping meth-\nods at reducing RMSE from the true value function, but reducing RMSE is\nnot necessarily the most important goal. For example, if you add 1000 to the\ntrue action-value function at all state{action pairs, then it will have very poor\nRMSE, but you will still get the optimal policy. Nothing quite that simple is\ngoing on with bootstrapping methods, but they do seem to do something right.\nWe expect the understanding of these issues to improve as research continues.\n9.6 Summary\nReinforcement learning systems must be capable of generalization if they are to\nbe applicable to arti\fcial intelligence or to large engineering applications. To\nachieve this, any of a broad range of existing methods for supervised-learning\nfunction approximation can be used simply by treating each backup as a train-\ning example. Gradient-descent methods , in particular, allow a natural exten-\nsion to function approximation of all the techniques developed in previous\nchapters, including eligibility traces. Linear gradient-descent methods are\nparticularly appealing theoretically and work well in practice when provided\nwith appropriate features. Choosing the features is one of the most important\nways of adding prior domain knowledge to reinforcement learning systems.\nLinear methods include radial basis functions, tile coding, and Kanerva cod-\ning. Backpropagation methods for multilayer neural networks are methods for\nnonlinear gradient-descent function approximation.\nFor the most part, the extension of reinforcement learning prediction and\ncontrol methods to gradient-descent forms is straightforward for the on-policy\ncase. On-policy bootstrapping methods converge reliably with linear gradient-\ndescent function approximation to a solution with mean-squared error bounded\nby1\u0000\r\u0015\n1\u0000\rtimes the minimum possible error. Bootstrapping methods are of\npersistent interest in reinforcement learning, despite their limited theoretical\nguarantees, because in practice they usually work signi\fcantly better than\nnonbootstrapping methods. The o\u000b-policy case involves considerably greater\nsubtlety and is postponed to a later (future) chapter.\n250CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nBibliographical and Historical Remarks\nDespite our treatment of generalization and function approximation late in\nthe book, they have always been an integral part of reinforcement learning. It\nis only in the last decade or less that the \feld has focused on the tabular case,\nas we have here for the \frst seven chapters. Bertsekas and Tsitsiklis (1996)\npresent the state of the art in function approximation in reinforcement learn-\ning, and the collection of papers by Boyan, Moore, and Sutton (1995) is also\nuseful. Some of the early work with function approximation in reinforcement\nlearning is discussed at the end of this section.\n9.2 Gradient-descent methods for the minimizing mean-squared error in su-\npervised learning are well known. Widrow and Ho\u000b (1960) introduced\nthe least-mean-square (LMS) algorithm, which is the prototypical in-\ncremental gradient-descent algorithm. Details of this and related al-\ngorithms are provided in many texts (e.g., Widrow and Stearns, 1985;\nBishop, 1995; Duda and Hart, 1973).\nGradient-descent analyses of TD learning date back at least to Sutton\n(1988). Methods more sophisticated than the simple gradient-descent\nmethods covered in this section have also been studied in the context of\nreinforcement learning, such as quasi-Newton methods (Werbos, 1990)\nand recursive-least-squares methods (Bradtke, 1993, 1994; Bradtke and\nBarto, 1996; Bradtke, Ydstie, and Barto, 1994). Bertsekas and Tsit-\nsiklis (1996) provide a good discussion of these methods.\nThe earliest use of state aggregation in reinforcement learning may\nhave been Michie and Chambers's BOXES system (1968). The theory\nof state aggregation in reinforcement learning has been developed by\nSingh, Jaakkola, and Jordan (1995) and Tsitsiklis and Van Roy (1996).\n9.3 TD(\u0015) with linear gradient-descent function approximation was \frst ex-\nplored by Sutton (1984, 1988), who proved convergence of TD(0) in the\nmean to the minimal RMSE solution for the case in which the feature\nvectors,fx(s) :s2Sg, are linearly independent. Convergence with\nprobability 1 for general \u0015was proved by several researchers at about\nthe same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994;\nGurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and\nSingh (1994) proved convergence under on-line updating. All of these\nresults assumed linearly independent feature vectors, which implies at\nleast as many components to wtas there are states. Convergence of lin-\near TD(\u0015) for the more interesting case of general (dependent) feature\nvectors was \frst shown by Dayan (1992). A signi\fcant generalization\n9.6. SUMMARY 251\nand strengthening of Dayan's result was proved by Tsitsiklis and Van\nRoy (1997). They proved the main result presented in Section 9.2,\nthe bound on the asymptotic error of TD( \u0015) and other bootstrapping\nmethods. Recently they extended their analysis to the undiscounted\ncontinuing case (Tsitsiklis and Van Roy, 1999).\nOur presentation of the range of possibilities for linear function approx-\nimation is based on that by Barto (1990). The term coarse coding is\ndue to Hinton (1984), and our Figure 9.2 is based on one of his \fgures.\nWaltz and Fu (1965) provide an early example of this type of function\napproximation in a reinforcement learning system.\nTile coding, including hashing, was introduced by Albus (1971, 1981).\nHe described it in terms of his \\cerebellar model articulator controller,\"\nor CMAC, as tile coding is known in the literature. The term \\tile\ncoding\" is new to this book, though the idea of describing CMAC in\nthese terms is taken from Watkins (1989). Tile coding has been used in\nmany reinforcement learning systems (e.g., Shewchuk and Dean, 1990;\nLin and Kim, 1991; Miller, Scalera, and Kim, 1994; Sofge and White,\n1992; Tham, 1994; Sutton, 1996; Watkins, 1989) as well as in other\ntypes of learning control systems (e.g., Kraft and Campagna, 1990;\nKraft, Miller, and Dietz, 1992).\nFunction approximation using radial basis functions (RBFs) has re-\nceived wide attention ever since being related to neural networks by\nBroomhead and Lowe (1988). Powell (1987) reviewed earlier uses of\nRBFs, and Poggio and Girosi (1989, 1990) extensively developed and\napplied this approach.\nWhat we call \\Kanerva coding\" was introduced by Kanerva (1988) as\npart of his more general idea of sparse distributed memory . A good re-\nview of this and related memory models is provided by Kanerva (1993).\nThis approach has been pursued by Gallant (1993) and by Sutton and\nWhitehead (1993), among others.\n9.4 Q(\u0015) with function approximation was \frst explored by Watkins (1989).\nSarsa(\u0015) with function approximation was \frst explored by Rummery\nand Niranjan (1994). The mountain{car example is based on a similar\ntask studied by Moore (1990). The results on it presented here are from\nSutton (1996) and Singh and Sutton (1996).\nConvergence of the Sarsa control method presented in this section has\nnot been proved. The Q-learning control method is now known not\nto be sound and will diverge for some problems. Convergence results\nfor control methods with state aggregation and other special kinds of\n252CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\nfunction approximation are proved by Tsitsiklis and Van Roy (1996),\nSingh, Jaakkola, and Jordan (1995), and Gordon (1995).\nThe use of function approximation in reinforcement learning goes back to\nthe early neural networks of Farley and Clark (1954; Clark and Farley, 1955),\nwho used reinforcement learning to adjust the parameters of linear threshold\nfunctions representing policies. The earliest example we know of in which\nfunction approximation methods were used for learning value functions was\nSamuel's checkers player (1959, 1967). Samuel followed Shannon's (1950)\nsuggestion that a value function did not have to be exact to be a useful guide\nto selecting moves in a game and that it might be approximated by linear\ncombination of features. In addition to linear function approximation, Samuel\nexperimented with lookup tables and hierarchical lookup tables called signa-\nture tables (Gri\u000eth, 1966, 1974; Page, 1977; Biermann, Fair\feld, and Beres,\n1982).\nAt about the same time as Samuel's work, Bellman and Dreyfus (1959)\nproposed using function approximation methods with DP. (It is tempting to\nthink that Bellman and Samuel had some in\ruence on one another, but we\nknow of no reference to the other in the work of either.) There is now a\nfairly extensive literature on function approximation methods and DP, such\nas multigrid methods and methods using splines and orthogonal polynomials\n(e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel,\n1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and\nTsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996).\nHolland's (1986) classi\fer system used a selective feature-match technique\nto generalize evaluation information across state{action pairs. Each classi\fer\nmatched a subset of states having speci\fed values for a subset of features, with\nthe remaining features having arbitrary values (\\wild cards\"). These subsets\nwere then used in a conventional state-aggregation approach to function ap-\nproximation. Holland's idea was to use a genetic algorithm to evolve a set\nof classi\fers that collectively would implement a useful action-value function.\nHolland's ideas in\ruenced the early research of the authors on reinforcement\nlearning, but we focused on di\u000berent approaches to function approximation.\nAs function approximators, classi\fers are limited in several ways. First, they\nare state-aggregation methods, with concomitant limitations in scaling and in\nrepresenting smooth functions e\u000eciently. In addition, the matching rules of\nclassi\fers can implement only aggregation boundaries that are parallel to the\nfeature axes. Perhaps the most important limitation of conventional classi-\n\fer systems is that the classi\fers are learned via the genetic algorithm, an\nevolutionary method. As we discussed in Chapter 1, there is available dur-\ning learning much more detailed information about how to learn than can be\n9.6. SUMMARY 253\nused by evolutionary methods. This perspective led us to instead adapt super-\nvised learning methods for use in reinforcement learning, speci\fcally gradient-\ndescent and neural network methods. These di\u000berences between Holland's\napproach and ours are not surprising because Holland's ideas were developed\nduring a period when neural networks were generally regarded as being too\nweak in computational power to be useful, whereas our work was at the be-\nginning of the period that saw widespread questioning of that conventional\nwisdom. There remain many opportunities for combining aspects of these\ndi\u000berent approaches.\nA number of reinforcement learning studies using function approximation\nmethods that we have not covered previously should be mentioned. Barto,\nSutton, and Brouwer (1981) and Barto and Sutton (1981b) extended the idea\nof an associative memory network (e.g., Kohonen, 1977; Anderson, Silverstein,\nRitz, and Jones, 1977) to reinforcement learning. Hampson (1983, 1989) was\nan early proponent of multilayer neural networks for learning value functions.\nAnderson (1986, 1987) coupled a TD algorithm with the error backpropagation\nalgorithm to learn a value function. Barto and Anandan (1985) introduced a\nstochastic version of Widrow, Gupta, and Maitra's (1973) selective bootstrap\nalgorithm , which they called the associative reward-penalty (AR\u0000P)algorithm .\nWilliams (1986, 1987, 1988, 1992) extended this type of algorithm to a general\nclass of REINFORCE algorithms, showing that they perform stochastic gra-\ndient ascent on the expected reinforcement. Gullapalli (1990) and Williams\ndevised algorithms for learning generalizing policies for the case of continuous\nactions. Phansalkar and Thathachar (1995) proved both local and global con-\nvergence theorems for modi\fed versions of REINFORCE algorithms. Chris-\ntensen and Korf (1986) experimented with regression methods for modifying\ncoe\u000ecients of linear value function approximations in the game of chess. Chap-\nman and Kaelbling (1991) and Tan (1991) adapted decision-tree methods for\nlearning value functions. Explanation-based learning methods have also been\nadapted for learning value functions, yielding compact representations (Yee,\nSaxena, Utgo\u000b, and Barto, 1990; Dietterich and Flann, 1995).\nExercises\nExercise 9.1 Show that table-lookup TD( \u0015) is a special case of general TD( \u0015)\nas given by equations (9.5{9.7).\nExercise 9.2 State aggregation is a simple form of generalizing function ap-\nproximation in which states are grouped together, with one table entry (value\nestimate) used for each group. Whenever a state in a group is encountered, the\ngroup's entry is used to determine the state's value, and when the state is up-\n254CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES\ndated, the group's entry is updated. Show that this kind of state aggregation\nis a special case of a gradient method such as (9.4).\nExercise 9.3 The equations given in this section are for the on-line version\nof gradient-descent TD( \u0015). What are the equations for the o\u000b-line version?\nGive a complete description specifying the new weight vector at the end of an\nepisode, w0, in terms of the weight vector used during the episode, w. Start\nby modifying a forward-view equation for TD( \u0015), such as (9.4).\nExercise 9.4 For o\u000b-line updating, show that equations (9.5{9.7) produce\nupdates identical to (9.4).\nExercise 9.5 How could we reproduce the tabular case within the linear\nframework?\nExercise 9.6 How could we reproduce the state aggregation case (see Exer-\ncise 8.4) within the linear framework?\nExercise 9.7 Suppose we believe that one of two state dimensions is more\nlikely to have an e\u000bect on the value function than is the other, that general-\nization should be primarily across this dimension rather than along it. What\nkind of tilings could be used to take advantage of this prior knowledge?\nChapter 10\nO\u000b-policy Approximation of\nAction Values\n255\n256CHAPTER 10. OFF-POLICY APPROXIMATION OF ACTION VALUES\nChapter 11\nPolicy Approximation\nAll of the methods we have considered so far in this book have learned the\nvalues of states or state{action pairs. To use them for control, we learned\nthe values of state{action pairs, and then used those action values directly to\nimplement the policy (e.g., \"-greedy) and select actions. All methods of this\nform can be called action-value methods .\nIn this chapter we explore methods that are not action-value methods.\nThey may still compute action (or state) values, but they do not use them\ndirectly to select actions. Instead, the policy is represented directly, with its\nown weights independent of any value function.\n11.1 Actor{Critic Methods\nActor{critic methods are TD methods that have a separate memory structure\nto explicitly represent the policy independent of the value function. The policy\nstructure is known as the actor , because it is used to select actions, and the\nestimated value function is known as the critic , because it criticizes the actions\nmade by the actor. Learning is always on-policy: the critic must learn about\nand critique whatever policy is currently being followed by the actor. The\ncritique takes the form of a TD error. This scalar signal is the sole output\nof the critic and drives all learning in both actor and critic, as suggested by\nFigure 11.1.\nActor{critic methods are the natural extension of the idea of gradient-\nbandit methods (Section 2.7) to TD learning and to the full reinforcement\nlearning problem. Typically, the critic is a state-value function. After each\naction selection, the critic evaluates the new state to determine whether things\nhave gone better or worse than expected. That evaluation is the TD error:\n257\n258 CHAPTER 11. POLICY APPROXIMATION\nPolicy\nTD\nerrorEnvironmentValue\nFunction\nrewardstate actionActor\nCritic\nFigure 11.1: The actor{critic architecture.\n\u000et=Rt+1+\rVt(St+1)\u0000V(St);\nwhereVtis the value function implemented by the critic at time t. This TD\nerror can be used to evaluate the action just selected, the action Attaken in\nstateSt. If the TD error is positive, it suggests that the tendency to select At\nshould be strengthened for the future, whereas if the TD error is negative, it\nsuggests the tendency should be weakened. Suppose actions are generated by\nthe Gibbs softmax method:\n\u0019t(ajs) = PrfAt=ajSt=sg=eHt(s;a)\nP\nbeHt(s;b);\nwhere theHt(s;a) are the values at time tof the modi\fable policy parameters\nof the actor, indicating the tendency to select ( preference for) each action a\nwhen in each state sat timet. Then the strengthening or weakening described\nabove can be implemented by increasing or decreasing Ht(St;At), for instance,\nby\nHt+1(St;At) =Ht(St;At) +\f\u000et;\nwhere\fis another positive step-size parameter.\nThis is just one example of an actor{critic method. Other variations select\nthe actions in di\u000berent ways, or use eligibility traces like those described in the\n11.2. ELIGIBILITY TRACES FOR ACTOR{CRITIC METHODS 259\nnext chapter. Another common dimension of variation, as in reinforcement\ncomparison methods, is to include additional factors varying the amount of\ncredit assigned to the action taken, At. For example, one of the most common\nsuch factors is inversely related to the probability of selecting At, resulting in\nthe update rule:\nHt(St;At) =Ht(St;At) +\f\u000eth\n1\u0000\u0019t(AtjSt)i\n:\nThese issues were explored early on, primarily for the immediate reward case\n(Sutton, 1984; Williams, 1992) and have not been brought fully up to date.\nMany of the earliest reinforcement learning systems that used TD methods\nwere actor{critic methods (Witten, 1977; Barto, Sutton, and Anderson, 1983).\nSince then, more attention has been devoted to methods that learn action-value\nfunctions and determine a policy exclusively from the estimated values (such\nas Sarsa and Q-learning). This divergence may be just historical accident.\nFor example, one could imagine intermediate architectures in which both an\naction-value function and an independent policy would be learned. In any\nevent, actor{critic methods are likely to remain of current interest because of\ntwo signi\fcant apparent advantages:\n\u000fThey require minimal computation in order to select actions. Consider a\ncase where there are an in\fnite number of possible actions|for example,\na continuous-valued action. Any method learning just action values must\nsearch through this in\fnite set in order to pick an action. If the policy\nis explicitly stored, then this extensive computation may not be needed\nfor each action selection.\n\u000fThey can learn an explicitly stochastic policy; that is, they can learn\nthe optimal probabilities of selecting various actions. This ability turns\nout to be useful in competitive and non-Markov cases (e.g., see Singh,\nJaakkola, and Jordan, 1994).\nIn addition, the separate actor in actor{critic methods makes them more ap-\npealing in some respects as psychological and biological models. In some cases\nit may also make it easier to impose domain-speci\fc constraints on the set of\nallowed policies.\n11.2 Eligibility Traces for Actor{Critic Meth-\nods\nIn this section we describe how to extend the actor{critic methods introduced\nin Section 11.1 to use eligibility traces. This is fairly straightforward. The\n260 CHAPTER 11. POLICY APPROXIMATION\ncritic part of an actor{critic method is simply on-policy learning of v\u0019. The\nTD(\u0015) algorithm can be used for that, with one eligibility trace for each state.\nThe actor part needs to use an eligibility trace for each state{action pair.\nThus, an actor{critic method needs two sets of traces, one for each state and\none for each state{action pair.\nRecall that the one-step actor{critic method updates the actor by\nHt+1(s;a) =\u001aHt(s;a) +\u000b\u000etifa=Atands=St\nHt(s;a) otherwise,\nwhere\u000etis the TD(\u0015) error (7.10), and Ht(s;a) is the preference for taking\nactionaat timetif in states. The preferences determine the policy via, for\nexample, a softmax method (Section 2.3). We generalize the above equation\nto use eligibility traces as follows:\nHt+1(s;a) =Ht(s;a) +\u000b\u000etEt(s;a); (11.1)\nwhereEt(s;a) denotes the trace at time tfor state{action pair s;a. For the\nsimplest case mentioned above, the trace can be updated as in Sarsa( \u0015).\nIn Section 11.1 we also discussed a more sophisticated actor{critic method\nthat uses the update\nHt+1(s;a) =\u001aHt(s;a) +\u000b\u000et[1\u0000\u0019t(ajs)] ifa=Atands=St\nHt(s;a) otherwise.\nTo generalize this equation to eligibility traces we can use the same update\n(11.1) with a slightly di\u000berent trace. Rather than incrementing the trace by 1\neach time a state{action pair occurs, it is updated by 1 \u0000\u0019t(St;At):\nEt(s;a) =\u001a\r\u0015Et\u00001(s;a) + 1\u0000\u0019t(St;At) ifs=Standa=At;\n\r\u0015Et\u00001(s;a) otherwise,(11.2)\nfor alls;a.\n11.3 R-Learning and the Average-Reward Set-\nting\nWhen the policy is approximated, we generally have to abandon the discounted-\nreward setting that we have relied on up to now. We replace it with the\naverage-reward setting , which we discuss in this section.\nR-learning is an o\u000b-policy control method for the advanced version of the\nreinforcement learning problem in which one neither discounts nor divides\n11.3. R-LEARNING AND THE AVERAGE-REWARD SETTING 261\nexperience into distinct episodes with \fnite returns. In this average-reward\nsetting , one seeks to maximize the average reward per time step. The value\nfunctions for a policy, \u0019, are de\fned relative to the average expected reward\nper step under the policy, \u0016 r(\u0019):\n\u0016r(\u0019) = lim\nn!11\nnnX\nt=1E\u0019[Rt]:\nThis average reward is well de\fned if we assume that the process is ergodic\n(nonzero probability of reaching any state from any other under any policy),\nand thus that \u0016 r(\u0019) does not depend on the starting state. From any state, in\nthe long run the average reward is the same, but there is a transient. From\nsome states better-than-average rewards are received for a while, and from\nothers worse-than-average rewards are received. It is this transient that de\fnes\nthe value of a state:\nv\u0019(s) =1X\nk=1E\u0019[Rt+k\u0000\u0016r(\u0019)jSt=s];\nand the value of a state{action pair is similarly the transient di\u000berence in\nreward when starting in that state and taking that action:\nq\u0019(s;a) =1X\nk=1E\u0019[Rt+k\u0000\u0016r(\u0019)jSt=s;At=a]:\nWe call these relative values because they are relative to the average reward\nunder the current policy.\nThere are subtle distinctions that need to be drawn between di\u000berent kinds\nof optimality in the undiscounted continuing case. Nevertheless, for most\npractical purposes it may be adequate simply to order policies according to\ntheir average reward per time step, in other words, according to their \u0016 r(\u0019).\nFor now let us consider all policies that attain the maximal value of \u0016 r(\u0019) to\nbe optimal.\nOther than its use of relative values, R-learning is a standard TD control\nmethod based on o\u000b-policy GPI, much like Q-learning. It maintains two poli-\ncies, a behavior policy and an estimation policy, plus an action-value function\nand an estimated average reward. The behavior policy is used to generate\nexperience; it might be the \"-greedy policy with respect to the action-value\nfunction. The estimation policy is the one involved in GPI. It is typically the\ngreedy policy with respect to the action-value function. If \u0019is the estimation\npolicy, then the action-value function, Q, is an approximation of q\u0019and the\naverage reward, \u0016R, is an approximation of \u0016 r(\u0019). The complete algorithm is\ngiven in Figure 11.2.\n262 CHAPTER 11. POLICY APPROXIMATION\nInitialize \u0016RandQ(s;a), for alls;a, arbitrarily\nRepeat forever:\nS current state\nChoose action AinSusing behavior policy (e.g., \u000f-greedy)\nTake action A, observeR,S0\n\u000e R\u0000\u0016R+ maxaQ(S0;a)\u0000Q(S;A)\nQ(S;A) Q(S;A) +\u000b\u000e\nIfQ(S;A) = maxaQ(S;a), then:\n\u0016R \u0016R+\f\u000e\nFigure 11.2: R-learning: An o\u000b-policy TD control algorithm for undiscounted,\ncontinuing tasks. The scalars \u000band\fare step-size parameters.\nExample 11.1: An Access-Control Queuing Task This is a decision\ntask involving access control to a set of nservers. Customers of four di\u000berent\npriorities arrive at a single queue. If given access to a server, the customers\npay a reward of 1, 2, 4, or 8, depending on their priority, with higher priority\ncustomers paying more. In each time step, the customer at the head of the\nqueue is either accepted (assigned to one of the servers) or rejected (removed\nfrom the queue). In either case, on the next time step the next customer\nin the queue is considered. The queue never empties, and the proportion of\n(randomly distributed) high priority customers in the queue is h. Of course a\ncustomer can be served only if there is a free server. Each busy server becomes\nfree with probability pon each time step. Although we have just described\nthem for de\fniteness, let us assume the statistics of arrivals and departures are\nunknown. The task is to decide on each step whether to accept or reject the\nnext customer, on the basis of his priority and the number of free servers, so\nas to maximize long-term reward without discounting. Figure 11.3 shows the\nsolution found by R-learning for this task with n= 10,h= 0:5, andp= 0:06.\nThe R-learning parameters were \u000b= 0:01,\f= 0:01, and\u000f= 0:1. The initial\naction values and \u0016Rwere zero.\nExercises\n\u0003Exercise 11.1 Design an on-policy method for undiscounted, continuing\ntasks.\n11.3. R-LEARNING AND THE AVERAGE-REWARD SETTING 263\n1 2 3 4 5 6 7 8 9 10 0!15!10!5057\npriority 8\npriority 4\npriority 2\npriority 1\nNumber of free servers42\n8ACCEPTREJECT\n1 23456 78 910\nNumber of free serversPriority1 POLICY\nValue of\nbest actionVALUE\nFUNCTION\nFigure 11.3: The policy and value function found by R-learning on the access-\ncontrol queuing task after 2 million steps. The drop on the right of the graph is\nprobably due to insu\u000ecient data; many of these states were never experienced.\nThe value learned for \u0016Rwas about 2 :73.\n264 CHAPTER 11. POLICY APPROXIMATION\nPart III\nFrontiers\n265\n\n267\nIn this last part of the book we discuss some of the frontiers of reinforcement\nlearning research, including its relationship to neuroscience and animal learn-\ning behavior, a sampling of reinforcement learning applications, and prospects\nfor the future of reinforcement learning.\n268\nChapter 12\nPsychology\n269\n270 CHAPTER 12. PSYCHOLOGY\nChapter 13\nNeuroscience\n271\n272 CHAPTER 13. NEUROSCIENCE\nChapter 14\nApplications and Case Studies\nIn this \fnal chapter we present a few case studies of reinforcement learning.\nSeveral of these are substantial applications of potential economic signi\fcance.\nOne, Samuel's checkers player, is primarily of historical interest. Our presen-\ntations are intended to illustrate some of the trade-o\u000bs and issues that arise in\nreal applications. For example, we emphasize how domain knowledge is incor-\nporated into the formulation and solution of the problem. We also highlight\nthe representation issues that are so often critical to successful applications.\nThe algorithms used in some of these case studies are substantially more com-\nplex than those we have presented in the rest of the book. Applications of\nreinforcement learning are still far from routine and typically require as much\nart as science. Making applications easier and more straightforward is one of\nthe goals of current research in reinforcement learning.\n14.1 TD-Gammon\nOne of the most impressive applications of reinforcement learning to date is\nthat by Gerry Tesauro to the game of backgammon (Tesauro, 1992, 1994,\n1995). Tesauro's program, TD-Gammon , required little backgammon knowl-\nedge, yet learned to play extremely well, near the level of the world's strongest\ngrandmasters. The learning algorithm in TD-Gammon was a straightforward\ncombination of the TD( \u0015) algorithm and nonlinear function approximation\nusing a multilayer neural network trained by backpropagating TD errors.\nBackgammon is a major game in the sense that it is played throughout the\nworld, with numerous tournaments and regular world championship matches.\nIt is in part a game of chance, and it is a popular vehicle for waging signi\fcant\nsums of money. There are probably more professional backgammon players\n273\n274 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nwhite pieces move \n   counterclockwise\n123456 789101112181716151413 19 20 21 22 23 24\n    black pieces \nmove clockwise\nFigure 14.1: A backgammon position\nthan there are professional chess players. The game is played with 15 white\nand 15 black pieces on a board of 24 locations, called points . Figure 14.1 shows\na typical position early in the game, seen from the perspective of the white\nplayer.\nIn this \fgure, white has just rolled the dice and obtained a 5 and a 2. This\nmeans that he can move one of his pieces 5 steps and one (possibly the same\npiece) 2 steps. For example, he could move two pieces from the 12 point, one\nto the 17 point, and one to the 14 point. White's objective is to advance all of\nhis pieces into the last quadrant (points 19{24) and then o\u000b the board. The\n\frst player to remove all his pieces wins. One complication is that the pieces\ninteract as they pass each other going in di\u000berent directions. For example, if\nit were black's move in Figure 14.1, he could use the dice roll of 2 to move a\npiece from the 24 point to the 22 point, \\hitting\" the white piece there. Pieces\nthat have been hit are placed on the \\bar\" in the middle of the board (where\nwe already see one previously hit black piece), from whence they reenter the\nrace from the start. However, if there are two pieces on a point, then the\nopponent cannot move to that point; the pieces are protected from being hit.\nThus, white cannot use his 5{2 dice roll to move either of his pieces on the 1\npoint, because their possible resulting points are occupied by groups of black\npieces. Forming contiguous blocks of occupied points to block the opponent is\none of the elementary strategies of the game.\nBackgammon involves several further complications, but the above descrip-\ntion gives the basic idea. With 30 pieces and 24 possible locations (26, count-\ning the bar and o\u000b-the-board) it should be clear that the number of possible\nbackgammon positions is enormous, far more than the number of memory el-\nements one could have in any physically realizable computer. The number of\nmoves possible from each position is also large. For a typical dice roll there\nmight be 20 di\u000berent ways of playing. In considering future moves, such as\n14.1. TD-GAMMON 275\nVt+1! Vthidden units (40-80)backgammon position (198 input units)predicted probabilityof winning, VtTD error,. . .. . .. . .. . .. . .. . .15.1. TD-GAMMON263gation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that thegeneral update rule for this case iswt+1=wt+\u21b5hRt+1+\u0000\u02c6v(St+1,wt)\u0000\u02c6v(St,wt)iet,(15.1)wherewtis the vector of all modi\ufb01able parameters (in this case, the weightsof the network) andetis a vector of eligibility traces, one for each componentofwt, updated byet=\u0000\u0000et\u00001+rwt\u02c6v(St,wt),withe0=0. The gradient in this equation can be computed e\u0000ciently by thebackpropagation procedure. For the backgammon application, in which\u0000=1and the reward is always zero except upon winning, the TD error portion of thelearning rule is usually just \u02c6v(St+1,w)\u0000\u02c6v(St,w), as suggested in Figure 15.2.To apply the learning rule we need a source of backgammon games. Tesauroobtained an unending sequence of games by playing his learning backgammonplayer against itself. To choose its moves, TD-Gammon considered each of the20 or so ways it could play its dice roll and the corresponding positions thatwould result. The resulting positions areafterstatesas discussed in Section 6.8.The network was consulted to estimate each of their values. The move wasthen selected that would lead to the position with the highest estimated value.Continuing in this way, with TD-Gammon making the moves for both sides,it was possible to easily generate large numbers of backgammon games. Eachgame was treated as an episode, with the sequence of positions acting asthe states,S0,S1,S2,.... Tesauro applied the nonlinear TD rule (15.1) fullyincrementally, that is, after each individual move.The weights of the network were set initially to small random values. Theinitial evaluations were thus entirely arbitrary. Since the moves were selectedon the basis of these evaluations, the initial moves were inevitably poor, andthe initial games often lasted hundreds or thousands of moves before one sideor the other won, almost by accident. After a few dozen games however,performance improved rapidly.After playing about 300,000 games against itself, TD-Gammon 0.0 as de-scribed above learned to play approximately as well as the best previousbackgammon computer programs. This was a striking result because all theprevious high-performance computer programs had used extensive backgam-mon knowledge. For example, the reigning champion program at the timewas, arguably,Neurogammon, another program written by Tesauro that useda neural network but not TD learning. Neurogammon\u2019s network was trainedon a large training corpus of exemplary moves provided by backgammon ex-perts, and, in addition, started with a set of features specially crafted forTD error15.1. TD-GAMMON263gation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that thegeneral update rule for this case iswt+1=wt+\u21b5hRt+1+\u0000\u02c6v(St+1,wt)\u0000\u02c6v(St,wt)iet,(15.1)wherewtis the vector of all modi\ufb01able parameters (in this case, the weightsof the network) andetis a vector of eligibility traces, one for each componentofwt, updated byet=\u0000\u0000et\u00001+rwt\u02c6v(St,wt),withe0=0. The gradient in this equation can be computed e\u0000ciently by thebackpropagation procedure. For the backgammon application, in which\u0000=1and the reward is always zero except upon winning, the TD error portion of thelearning rule is usually just \u02c6v(St+1,w)\u0000\u02c6v(St,w), as suggested in Figure 15.2.To apply the learning rule we need a source of backgammon games. Tesauroobtained an unending sequence of games by playing his learning backgammonplayer against itself. To choose its moves, TD-Gammon considered each of the20 or so ways it could play its dice roll and the corresponding positions thatwould result. The resulting positions areafterstatesas discussed in Section 6.8.The network was consulted to estimate each of their values. The move wasthen selected that would lead to the position with the highest estimated value.Continuing in this way, with TD-Gammon making the moves for both sides,it was possible to easily generate large numbers of backgammon games. Eachgame was treated as an episode, with the sequence of positions acting asthe states,S0,S1,S2,.... Tesauro applied the nonlinear TD rule (15.1) fullyincrementally, that is, after each individual move.The weights of the network were set initially to small random values. Theinitial evaluations were thus entirely arbitrary. Since the moves were selectedon the basis of these evaluations, the initial moves were inevitably poor, andthe initial games often lasted hundreds or thousands of moves before one sideor the other won, almost by accident. After a few dozen games however,performance improved rapidly.After playing about 300,000 games against itself, TD-Gammon 0.0 as de-scribed above learned to play approximately as well as the best previousbackgammon computer programs. This was a striking result because all theprevious high-performance computer programs had used extensive backgam-mon knowledge. For example, the reigning champion program at the timewas, arguably,Neurogammon, another program written by Tesauro that useda neural network but not TD learning. Neurogammon\u2019s network was trainedon a large training corpus of exemplary moves provided by backgammon ex-perts, and, in addition, started with a set of features specially crafted for\nFigure 14.2: The neural network used in TD-Gammon\nthe response of the opponent, one must consider the possible dice rolls as well.\nThe result is that the game tree has an e\u000bective branching factor of about 400.\nThis is far too large to permit e\u000bective use of the conventional heuristic search\nmethods that have proved so e\u000bective in games like chess and checkers.\nOn the other hand, the game is a good match to the capabilities of TD\nlearning methods. Although the game is highly stochastic, a complete de-\nscription of the game's state is available at all times. The game evolves over\na sequence of moves and positions until \fnally ending in a win for one player\nor the other, ending the game. The outcome can be interpreted as a \fnal\nreward to be predicted. On the other hand, the theoretical results we have\ndescribed so far cannot be usefully applied to this task. The number of states\nis so large that a lookup table cannot be used, and the opponent is a source\nof uncertainty and time variation.\nTD-Gammon used a nonlinear form of TD( \u0015). The estimated value, ^ v(s),of\nany state (board position) swas meant to estimate the probability of winning\nstarting from state s. To achieve this, rewards were de\fned as zero for all\ntime steps except those on which the game is won. To implement the value\nfunction, TD-Gammon used a standard multilayer neural network, much as\nshown in Figure 14.2. (The real network had two additional units in its \fnal\nlayer to estimate the probability of each player's winning in a special way\ncalled a \\gammon\" or \\backgammon.\") The network consisted of a layer of\ninput units, a layer of hidden units, and a \fnal output unit. The input to the\nnetwork was a representation of a backgammon position, and the output was\nan estimate of the value of that position.\nIn the \frst version of TD-Gammon, TD-Gammon 0.0, backgammon posi-\ntions were represented to the network in a relatively direct way that involved\nlittle backgammon knowledge. It did, however, involve substantial knowledge\n276 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nof how neural networks work and how information is best presented to them.\nIt is instructive to note the exact representation Tesauro chose. There were a\ntotal of 198 input units to the network. For each point on the backgammon\nboard, four units indicated the number of white pieces on the point. If there\nwere no white pieces, then all four units took on the value zero. If there was\none piece, then the \frst unit took on the value 1. If there were two pieces,\nthen both the \frst and the second unit were 1. If there were three or more\npieces on the point, then all of the \frst three units were 1. If there were more\nthan three pieces, the fourth unit also came on, to a degree indicating the\nnumber of additional pieces beyond three. Letting ndenote the total num-\nber of pieces on the point, if n > 3, then the fourth unit took on the value\n(n\u00003)=2. With four units for white and four for black at each of the 24 points,\nthat made a total of 192 units. Two additional units encoded the number of\nwhite and black pieces on the bar (each took the value n=2, wherenis the\nnumber of pieces on the bar), and two more encoded the number of black and\nwhite pieces already successfully removed from the board (these took the value\nn=15, wherenis the number of pieces already borne o\u000b). Finally, two units\nindicated in a binary fashion whether it was white's or black's turn to move.\nThe general logic behind these choices should be clear. Basically, Tesauro\ntried to represent the position in a straightforward way, making little attempt\nto minimize the number of units. He provided one unit for each conceptually\ndistinct possibility that seemed likely to be relevant, and he scaled them to\nroughly the same range, in this case between 0 and 1.\nGiven a representation of a backgammon position, the network computed\nits estimated value in the standard way. Corresponding to each connection\nfrom an input unit to a hidden unit was a real-valued weight. Signals from\neach input unit were multiplied by their corresponding weights and summed\nat the hidden unit. The output, h(j), of hidden unit jwas a nonlinear sigmoid\nfunction of the weighted sum:\nh(j) =\u001b X\niwijxi!\n=1\n1 +e\u0000P\niwijxi;\nwherexiis the value of the ith input unit and wijis the weight of its connection\nto thejth hidden unit. The output of the sigmoid is always between 0 and\n1, and has a natural interpretation as a probability based on a summation of\nevidence. The computation from hidden units to the output unit was entirely\nanalogous. Each connection from a hidden unit to the output unit had a\nseparate weight. The output unit formed the weighted sum and then passed\nit through the same sigmoid nonlinearity.\nTD-Gammon used the gradient-descent form of the TD( \u0015) algorithm de-\nscribed in Section 9.2, with the gradients computed by the error backpropa-\n14.1. TD-GAMMON 277\ngation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that the\ngeneral update rule for this case is\nwt+1=wt+\u000bh\nRt+1+\r^v(St+1;wt)\u0000^v(St;wt)i\net; (14.1)\nwhere wtis the vector of all modi\fable parameters (in this case, the weights\nof the network) and etis a vector of eligibility traces, one for each component\nofwt, updated by\net=\r\u0015et\u00001+r^v(St;wt);\nwithe0=0. The gradient in this equation can be computed e\u000eciently by the\nbackpropagation procedure. For the backgammon application, in which \r= 1\nand the reward is always zero except upon winning, the TD error portion of the\nlearning rule is usually just ^ v(St+1;w)\u0000^v(St;w), as suggested in Figure 14.2.\nTo apply the learning rule we need a source of backgammon games. Tesauro\nobtained an unending sequence of games by playing his learning backgammon\nplayer against itself. To choose its moves, TD-Gammon considered each of the\n20 or so ways it could play its dice roll and the corresponding positions that\nwould result. The resulting positions are afterstates as discussed in Section 6.6.\nThe network was consulted to estimate each of their values. The move was\nthen selected that would lead to the position with the highest estimated value.\nContinuing in this way, with TD-Gammon making the moves for both sides,\nit was possible to easily generate large numbers of backgammon games. Each\ngame was treated as an episode, with the sequence of positions acting as\nthe states, S0;S1;S2;:::. Tesauro applied the nonlinear TD rule (14.1) fully\nincrementally, that is, after each individual move.\nThe weights of the network were set initially to small random values. The\ninitial evaluations were thus entirely arbitrary. Since the moves were selected\non the basis of these evaluations, the initial moves were inevitably poor, and\nthe initial games often lasted hundreds or thousands of moves before one side\nor the other won, almost by accident. After a few dozen games however,\nperformance improved rapidly.\nAfter playing about 300,000 games against itself, TD-Gammon 0.0 as de-\nscribed above learned to play approximately as well as the best previous\nbackgammon computer programs. This was a striking result because all the\nprevious high-performance computer programs had used extensive backgam-\nmon knowledge. For example, the reigning champion program at the time\nwas, arguably, Neurogammon , another program written by Tesauro that used\na neural network but not TD learning. Neurogammon's network was trained\non a large training corpus of exemplary moves provided by backgammon ex-\nperts, and, in addition, started with a set of features specially crafted for\n278 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nProgram Hidden Training Opponents Results\nUnits Games\nTD-Gam 0.0 40 300,000 other programs tied for best\nTD-Gam 1.0 80 300,000 Robertie, Magriel, ... \u000013 pts / 51 games\nTD-Gam 2.0 40 800,000 various Grandmasters \u00007 pts / 38 games\nTD-Gam 2.1 80 1,500,000 Robertie \u00001 pt / 40 games\nTD-Gam 3.0 80 1,500,000 Kazaros +6 pts / 20 games\nTable 14.1: Summary of TD-Gammon Results\nbackgammon. Neurogammon was a highly tuned, highly e\u000bective backgam-\nmon program that decisively won the World Backgammon Olympiad in 1989.\nTD-Gammon 0.0, on the other hand, was constructed with essentially zero\nbackgammon knowledge. That it was able to do as well as Neurogammon and\nall other approaches is striking testimony to the potential of self-play learning\nmethods.\nThe tournament success of TD-Gammon 0.0 with zero backgammon knowl-\nedge suggested an obvious modi\fcation: add the specialized backgammon\nfeatures but keep the self-play TD learning method. This produced TD-\nGammon 1.0. TD-Gammon 1.0 was clearly substantially better than all previ-\nous backgammon programs and found serious competition only among human\nexperts. Later versions of the program, TD-Gammon 2.0 (40 hidden units)\nand TD-Gammon 2.1 (80 hidden units), were augmented with a selective two-\nply search procedure. To select moves, these programs looked ahead not just\nto the positions that would immediately result, but also to the opponent's pos-\nsible dice rolls and moves. Assuming the opponent always took the move that\nappeared immediately best for him, the expected value of each candidate move\nwas computed and the best was selected. To save computer time, the second\nply of search was conducted only for candidate moves that were ranked highly\nafter the \frst ply, about four or \fve moves on average. Two-ply search a\u000bected\nonly the moves selected; the learning process proceeded exactly as before. The\nmost recent version of the program, TD-Gammon 3.0, uses 160 hidden units\nand a selective three-ply search. TD-Gammon illustrates the combination of\nlearned value functions and decide-time search as in heuristic search meth-\nods. In more recent work, Tesauro and Galperin (1997) have begun exploring\ntrajectory sampling methods as an alternative to search.\nTesauro was able to play his programs in a signi\fcant number of games\nagainst world-class human players. A summary of the results is given in Ta-\nble 14.1. Based on these results and analyses by backgammon grandmasters\n(Robertie, 1992; see Tesauro, 1995), TD-Gammon 3.0 appears to be at, or\nvery near, the playing strength of the best human players in the world. It\nmay already be the world champion. These programs have already changed\n14.2. SAMUEL'S CHECKERS PLAYER 279\nthe way the best human players play the game. For example, TD-Gammon\nlearned to play certain opening positions di\u000berently than was the convention\namong the best human players. Based on TD-Gammon's success and further\nanalysis, the best human players now play these positions as TD-Gammon\ndoes (Tesauro, 1995).\n14.2 Samuel's Checkers Player\nAn important precursor to Tesauro's TD-Gammon was the seminal work of\nArthur Samuel (1959, 1967) in constructing programs for learning to play\ncheckers. Samuel was one of the \frst to make e\u000bective use of heuristic search\nmethods and of what we would now call temporal-di\u000berence learning. His\ncheckers players are instructive case studies in addition to being of historical\ninterest. We emphasize the relationship of Samuel's methods to modern re-\ninforcement learning methods and try to convey some of Samuel's motivation\nfor using them.\nSamuel \frst wrote a checkers-playing program for the IBM 701 in 1952.\nHis \frst learning program was completed in 1955 and was demonstrated on\ntelevision in 1956. Later versions of the program achieved good, though not\nexpert, playing skill. Samuel was attracted to game-playing as a domain for\nstudying machine learning because games are less complicated than problems\n\\taken from life\" while still allowing fruitful study of how heuristic procedures\nand learning can be used together. He chose to study checkers instead of\nchess because its relative simplicity made it possible to focus more strongly on\nlearning.\nSamuel's programs played by performing a lookahead search from each\ncurrent position. They used what we now call heuristic search methods to\ndetermine how to expand the search tree and when to stop searching. The\nterminal board positions of each search were evaluated, or \\scored,\" by a\nvalue function, or \\scoring polynomial,\" using linear function approximation.\nIn this and other respects Samuel's work seems to have been inspired by the\nsuggestions of Shannon (1950). In particular, Samuel's program was based on\nShannon's minimax procedure to \fnd the best move from the current position.\nWorking backward through the search tree from the scored terminal positions,\neach position was given the score of the position that would result from the\nbest move, assuming that the machine would always try to maximize the score,\nwhile the opponent would always try to minimize it. Samuel called this the\nbacked-up score of the position. When the minimax procedure reached the\nsearch tree's root|the current position|it yielded the best move under the\nassumption that the opponent would be using the same evaluation criterion,\n280 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nshifted to its point of view. Some versions of Samuel's programs used sophis-\nticated search control methods analogous to what are known as \\alpha-beta\"\ncuto\u000bs (e.g., see Pearl, 1984).\nSamuel used two main learning methods, the simplest of which he called\nrote learning . It consisted simply of saving a description of each board position\nencountered during play together with its backed-up value determined by the\nminimax procedure. The result was that if a position that had already been\nencountered were to occur again as a terminal position of a search tree, the\ndepth of the search was e\u000bectively ampli\fed since this position's stored value\ncached the results of one or more searches conducted earlier. One initial prob-\nlem was that the program was not encouraged to move along the most direct\npath to a win. Samuel gave it a \\a sense of direction\" by decreasing a position's\nvalue a small amount each time it was backed up a level (called a ply) during\nthe minimax analysis. \\If the program is now faced with a choice of board\npositions whose scores di\u000ber only by the ply number, it will automatically\nmake the most advantageous choice, choosing a low-ply alternative if winning\nand a high-ply alternative if losing\" (Samuel, 1959, p. 80). Samuel found this\ndiscounting-like technique essential to successful learning. Rote learning pro-\nduced slow but continuous improvement that was most e\u000bective for opening\nand endgame play. His program became a \\better-than-average novice\" after\nlearning from many games against itself, a variety of human opponents, and\nfrom book games in a supervised learning mode.\nRote learning and other aspects of Samuel's work strongly suggest the\nessential idea of temporal-di\u000berence learning|that the value of a state should\nequal the value of likely following states. Samuel came closest to this idea\nin his second learning method, his \\learning by generalization\" procedure for\nmodifying the parameters of the value function. Samuel's method was the same\nin concept as that used much later by Tesauro in TD-Gammon. He played\nhis program many games against another version of itself and performed a\nbackup operation after each move. The idea of Samuel's backup is suggested\nby the diagram in Figure 14.3. Each open circle represents a position where\nthe program moves next, an on-move position, and each solid circle represents\na position where the opponent moves next. A backup was made to the value of\neach on-move position after a move by each side, resulting in a second on-move\nposition. The backup was toward the minimax value of a search launched from\nthe second on-move position. Thus, the overall e\u000bect was that of a backup\nconsisting of one full move of real events and then a search over possible events,\nas suggested by Figure 14.3. Samuel's actual algorithm was signi\fcantly more\ncomplex than this for computational reasons, but this was the basic idea.\nSamuel did not include explicit rewards. Instead, he \fxed the weight of the\nmost important feature, the piece advantage feature, which measured the num-\n14.2. SAMUEL'S CHECKERS PLAYER 281\nhypothetical eventsactual events\nbackup\nFigure 14.3: The backup diagram for Samuel's checkers player.\nber of pieces the program had relative to how many its opponent had, giving\nhigher weight to kings, and including re\fnements so that it was better to trade\npieces when winning than when losing. Thus, the goal of Samuel's program\nwas to improve its piece advantage, which in checkers is highly correlated with\nwinning.\nHowever, Samuel's learning method may have been missing an essential\npart of a sound temporal-di\u000berence algorithm. Temporal-di\u000berence learning\ncan be viewed as a way of making a value function consistent with itself, and\nthis we can clearly see in Samuel's method. But also needed is a way of tying\nthe value function to the true value of the states. We have enforced this via\nrewards and by discounting or giving a \fxed value to the terminal state. But\nSamuel's method included no rewards and no special treatment of the terminal\npositions of games. As Samuel himself pointed out, his value function could\nhave become consistent merely by giving a constant value to all positions.\nHe hoped to discourage such solutions by giving his piece-advantage term a\nlarge, nonmodi\fable weight. But although this may decrease the likelihood of\n\fnding useless evaluation functions, it does not prohibit them. For example,\na constant function could still be attained by setting the modi\fable weights\nso as to cancel the e\u000bect of the nonmodi\fable one.\nSince Samuel's learning procedure was not constrained to \fnd useful eval-\nuation functions, it should have been possible for it to become worse with\nexperience. In fact, Samuel reported observing this during extensive self-play\ntraining sessions. To get the program improving again, Samuel had to in-\ntervene and set the weight with the largest absolute value back to zero. His\ninterpretation was that this drastic intervention jarred the program out of local\noptima, but another possibility is that it jarred the program out of evaluation\n282 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nfunctions that were consistent but had little to do with winning or losing the\ngame.\nDespite these potential problems, Samuel's checkers player using the gener-\nalization learning method approached \\better-than-average\" play. Fairly good\namateur opponents characterized it as \\tricky but beatable\" (Samuel, 1959).\nIn contrast to the rote-learning version, this version was able to develop a\ngood middle game but remained weak in opening and endgame play. This\nprogram also included an ability to search through sets of features to \fnd\nthose that were most useful in forming the value function. A later version\n(Samuel, 1967) included re\fnements in its search procedure, such as alpha-\nbeta pruning, extensive use of a supervised learning mode called \\book learn-\ning,\" and hierarchical lookup tables called signature tables (Gri\u000eth, 1966) to\nrepresent the value function instead of linear function approximation. This\nversion learned to play much better than the 1959 program, though still not\nat a master level. Samuel's checkers-playing program was widely recognized\nas a signi\fcant achievement in arti\fcial intelligence and machine learning.\n14.3 The Acrobot\nReinforcement learning has been applied to a wide variety of physical control\ntasks (e.g., for a collection of robotics applications, see Connell and Mahade-\nvan, 1993). One such task is the acrobot , a two-link, underactuated robot\nroughly analogous to a gymnast swinging on a high bar (Figure 14.4). The \frst\njoint (corresponding to the gymnast's hands on the bar) cannot exert torque,\nbut the second joint (corresponding to the gymnast bending at the waist)\ncan. The system has four continuous state variables: two joint positions and\ntwo joint velocities. The equations of motion are given in Figure 14.5. This\nsystem has been widely studied by control engineers (e.g., Spong, 1994) and\nmachine-learning researchers (e.g., Dejong and Spong, 1994; Boone, 1997).\nOne objective for controlling the acrobot is to swing the tip (the \\feet\")\nabove the \frst joint by an amount equal to one of the links in minimum time.\nIn this task, the torque applied at the second joint is limited to three choices:\npositive torque of a \fxed magnitude, negative torque of the same magnitude,\nor no torque. A reward of \u00001 is given on all time steps until the goal is reached,\nwhich ends the episode. No discounting is used ( \r= 1). Thus, the optimal\nvalue,v\u0003(s), of any state, s, is the minimum time to reach the goal (an integer\nnumber of steps) starting from s.\nSutton (1996) addressed the acrobot swing-up task in an on-line, modelfree\ncontext. Although the acrobot was simulated, the simulator was not available\n14.3. THE ACROBOT 283\n!1\n!2Goal: Raise tip above line\nTorque\napplied\nhere\ntip\nFigure 14.4: The acrobot.\n\u007f\u00121=\u0000d\u00001\n1(d2\u007f\u00122+\u001e1)\n\u007f\u00122=\u0012\nm2l2\nc2+I2\u0000d2\n2\nd1\u0013\u00001\u0012\n\u001c+d2\nd1\u001e1\u0000m2l1lc2_\u00122\n1sin\u00122\u0000\u001e2\u0013\nd1=m1l2\nc1+m2(l2\n1+l2\nc2+ 2l1lc2cos\u00122) +I1+I2\nd2=m2(l2\nc2+l1lc2cos\u00122) +I2\n\u001e1=\u0000m2l1lc2_\u00122\n2sin\u00122\u00002m2l1lc2_\u00122_\u00121sin\u00122\n+ (m1lc1+m2l1)gcos(\u00121\u0000\u0019=2) +\u001e2\n\u001e2=m2lc2gcos(\u00121+\u00122\u0000\u0019=2)\nFigure 14.5: The equations of motions of the simulated acrobot. A time\nstep of 0.05 seconds was used in the simulation, with actions chosen after\nevery four time steps. The torque applied at the second joint is denoted by\n\u001c2f+1;\u00001;0g. There were no constraints on the joint positions, but the\nangular velocities were limited to _\u001212[\u00004\u0019;4\u0019] and _\u001222[\u00009\u0019;9\u0019]. The\nconstants were m1=m2= 1 (masses of the links), l1=l2= 1 (lengths of\nlinks),lc1=lc2= 0:5 (lengths to center of mass of links), I1=I2= 1 (moments\nof inertia of links), and g= 9:8 (gravity).\n284 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nfor use by the agent/controller in any way. The training and interaction were\njust as if a real, physical acrobot had been used. Each episode began with\nboth links of the acrobot hanging straight down and at rest. Torques were\napplied by the reinforcement learning agent until the goal was reached, which\nalways happened eventually. Then the acrobot was restored to its initial rest\nposition and a new episode was begun.\nThe learning algorithm used was Sarsa( \u0015) with linear function approxima-\ntion, tile coding, and replacing traces as in Figure 9.8. With a small, discrete\naction set, it is natural to use a separate set of tilings for each action. The\nnext choice is of the continuous variables with which to represent the state.\nA clever designer would probably represent the state in terms of the angular\nposition and velocity of the center of mass and of the second link, which might\nmake the solution simpler and consistent with broad generalization. But since\nthis was just a test problem, a more naive, direct representation was used in\nterms of the positions and velocities of the links: \u00121;_\u00121;\u00122, and _\u00122. The two\nangles are restricted to a limited range by the physics of the acrobot (see Fig-\nure 14.5) and the two angles are naturally restricted to [0 ;2\u0019]. Thus, the state\nspace in this task is a bounded rectangular region in four dimensions.\nThis leaves the question of what tilings to use. There are many possi-\nbilities, as discussed in Chapter 9. One is to use a complete grid, slicing\nthe four-dimensional space along all dimensions, and thus into many small\nfour-dimensional tiles. Alternatively, one could slice along only one of the\ndimensions, making hyperplanar stripes. In this case one has to pick which\ndimension to slice along. And of course in all cases one has to pick the width of\nthe slices, the number of tilings of each kind, and, if there are multiple tilings,\nhow to o\u000bset them. One could also slice along pairs or triplets of dimensions\nto get other tilings. For example, if one expected the velocities of the two links\nto interact strongly in their e\u000bect on value, then one might make many tilings\nthat sliced along both of these dimensions. If one thought the region around\nzero velocity was particularly critical, then the slices could be more closely\nspaced there.\nSutton used tilings that sliced in a variety of simple ways. Each of the four\ndimensions was divided into six equal intervals. A seventh interval was added\nto the angular velocities so that tilings could be o\u000bset by a random fraction\nof an interval in all dimensions (see Chapter 9, subsection \\Tile Coding\"). Of\nthe total of 48 tilings, 12 sliced along all four dimensions as discussed above,\ndividing the space into 6 \u00027\u00026\u00027 = 1764 tiles each. Another 12 tilings sliced\nalong three dimensions (3 randomly o\u000bset tilings each for each of the 4 sets of\nthree dimensions), and another 12 sliced along two dimensions (2 tilings for\neach of the 6 sets of two dimensions. Finally, a set of 12 tilings depended each\non only one dimension (3 tilings for each of the 4 dimensions). This resulted\n14.3. THE ACROBOT 285\n1001000\n1 100 200 300 400Steps per episode\n(log scale)\n500\nEpisodestypical\nsingle runmedian of\n10 runs\nsmoothed\naverage of\n10 runs\nFigure 14.6: Learning curves for Sarsa( \u0015) on the acrobot task.\nin a total of approximately 25 ;000 tiles for each action. This number is small\nenough that hashing was not necessary. All tilings were o\u000bset by a random\nfraction of an interval in all relevant dimensions.\nThe remaining parameters of the learning algorithm were \u000b= 0:2=48,\n\u0015= 0:9,\u000f= 0, and w0= 0. The use of a greedy policy ( \"= 0) seemed\npreferable on this task because long sequences of correct actions are needed to\ndo well. One exploratory action could spoil a whole sequence of good actions.\nExploration was ensured instead by starting the action values optimistically, at\nthe low value of 0. As discussed in Section 2.7 and Example 9.2, this makes the\nagent continually disappointed with whatever rewards it initially experiences,\ndriving it to keep trying new things.\nFigure 14.6 shows learning curves for the acrobot task and the learning al-\ngorithm described above. Note from the single-run curve that single episodes\nwere sometimes extremely long. On these episodes, the acrobot was usu-\nally spinning repeatedly at the second joint while the \frst joint changed only\nslightly from vertical down. Although this often happened for many time steps,\nit always eventually ended as the action values were driven lower. All runs\nended with an e\u000ecient policy for solving the problem, usually lasting about\n75 steps. A typical \fnal solution is shown in Figure 14.7. First the acrobot\npumps back and forth several times symmetrically, with the second link always\ndown. Then, once enough energy has been added to the system, the second\nlink is swung upright and stabbed to the goal height.\n286 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nFigure 14.7: A typical learned behavior of the acrobot. Each group is a series\nof consecutive positions, the thicker line being the \frst. The arrow indicates\nthe torque applied at the second joint.\n14.4 Elevator Dispatching\nWaiting for an elevator is a situation with which we are all familiar. We press a\nbutton and then wait for an elevator to arrive traveling in the right direction.\nWe may have to wait a long time if there are too many passengers or not\nenough elevators. Just how long we wait depends on the dispatching strategy\nthe elevators use to decide where to go. For example, if passengers on several\n\roors have requested pickups, which should be served \frst? If there are no\npickup requests, how should the elevators distribute themselves to await the\nnext request? Elevator dispatching is a good example of a stochastic optimal\ncontrol problem of economic importance that is too large to solve by classical\ntechniques such as dynamic programming.\nCrites and Barto (1996; Crites, 1996) studied the application of reinforce-\nment learning techniques to the four-elevator, ten-\roor system shown in Fig-\nure 14.8. Along the right-hand side are pickup requests and an indication\nof how long each has been waiting. Each elevator has a position, direction,\nand speed, plus a set of buttons to indicate where passengers want to get\no\u000b. Roughly quantizing the continuous variables, Crites and Barto estimated\nthat the system has over 1022states. This large state set rules out classi-\ncal dynamic programming methods such as value iteration. Even if one state\ncould be backed up every microsecond it would still require over 1000 years to\ncomplete just one sweep through the state space.\n14.4. ELEVATOR DISPATCHING 287\nelevator\ngoing updropoff\nrequestpickup\nrequest\n(down)\nage of\nrequestD\nU\nD\nU\nD\nU\nD\nU\nD\nU\nD\nU\nD\nU\nD\nU\nD\nUhall\nbuttons\nFigure 14.8: Four elevators in a ten-story building.\nIn practice, modern elevator dispatchers are designed heuristically and eval-\nuated on simulated buildings. The simulators are quite sophisticated and de-\ntailed. The physics of each elevator car is modeled in continuous time with\ncontinuous state variables. Passenger arrivals are modeled as discrete, stochas-\ntic events, with arrival rates varying frequently over the course of a simulated\nday. Not surprisingly, the times of greatest tra\u000ec and greatest challenge to the\ndispatching algorithm are the morning and evening rush hours. Dispatchers\nare generally designed primarily for these di\u000ecult periods.\nThe performance of elevator dispatchers is measured in several di\u000berent\nways, all with respect to an average passenger entering the system. The aver-\nagewaiting time is how long the passenger waits before getting on an elevator,\nand the average system time is how long the passenger waits before being\ndropped o\u000b at the destination \roor. Another frequently encountered statistic\nis the percentage of passengers whose waiting time exceeds 60 seconds. The\nobjective that Crites and Barto focused on is the average squared waiting time .\nThis objective is commonly used because it tends to keep the waiting times\nlow while also encouraging fairness in serving all the passengers.\nCrites and Barto applied a version of one-step Q-learning augmented in\nseveral ways to take advantage of special features of the problem. The most\nimportant of these concerned the formulation of the actions. First, each ele-\nvator made its own decisions independently of the others. Second, a number\nof constraints were placed on the decisions. An elevator carrying passengers\ncould not pass by a \roor if any of its passengers wanted to get o\u000b there, nor\n288 CHAPTER 14. APPLICATIONS AND CASE STUDIES\ncould it reverse direction until all of its passengers wanting to go in its current\ndirection had reached their \roors. In addition, a car was not allowed to stop\nat a \roor unless someone wanted to get on or o\u000b there, and it could not stop to\npick up passengers at a \roor if another elevator was already stopped there. Fi-\nnally, given a choice between moving up or down, the elevator was constrained\nalways to move up (otherwise evening rush hour tra\u000ec would tend to push all\nthe elevators down to the lobby). These last three constraints were explicitly\nincluded to provide some prior knowledge and make the problem easier. The\nnet result of all these constraints was that each elevator had to make few and\nsimple decisions. The only decision that had to be made was whether or not\nto stop at a \roor that was being approached and that had passengers waiting\nto be picked up. At all other times, no choices needed to be made.\nThat each elevator made choices only infrequently permitted a second sim-\npli\fcation of the problem. As far as the learning agent was concerned, the\nsystem made discrete jumps from one time at which it had to make a decision\nto the next. When a continuous-time decision problem is treated as a discrete-\ntime system in this way it is known as a semi-Markov decision process. To a\nlarge extent, such processes can be treated just like any other Markov decision\nprocess by taking the reward on each discrete transition as the integral of the\nreward over the corresponding continuous-time interval. The notion of return\ngeneralizes naturally from a discounted sum of future rewards to a discounted\nintegral of future rewards:\nGt=1X\nk=0\rkRt+k+1 becomes Gt=Z1\n0e\u0000\f\u001cRt+\u001cd\u001c;\nwhereRton the left is the usual immediate reward in discrete time and Rt+\u001c\non the right is the instantaneous reward at continuous time t+\u001c. In the\nelevator problem the continuous-time reward is the negative of the sum of the\nsquared waiting times of all waiting passengers. The parameter \f >0 plays a\nrole similar to that of the discount-rate parameter \r2[0;1).\nThe basic idea of the extension of Q-learning to semi-Markov decision prob-\nlems can now be explained. Suppose the system is in state Sand takes action\nAat timet1, and then the next decision is required at time t2in stateS0.\nAfter this discrete-event transition, the semi-Markov Q-learning backup for a\ntabular action-value function, Q, would be:\nQ(S;A) Q(S;A)+\u000b\u0014Zt2\nt1e\u0000\f(\u001c\u0000t1)R\u001cd\u001c+e\u0000\f(t2\u0000t1)min\naQ(S0;a)\u0000Q(S;A)\u0015\n:\nNote howe\u0000\f(t2\u0000t1)acts as a variable discount factor that depends on the\namount of time between events. This method is due to Bradtke and Du\u000b\n(1995).\n14.4. ELEVATOR DISPATCHING 289\nOne complication is that the reward as de\fned|the negative sum of the\nsquared waiting times|is not something that would normally be known while\nan actual elevator was running. This is because in a real elevator system one\ndoes not know how many people are waiting at a \roor, only how long it has\nbeen since the button requesting a pickup on that \roor was pressed. Of course\nthis information is known in a simulator, and Crites and Barto used it to\nobtain their best results. They also experimented with another technique that\nused only information that would be known in an on-line learning situation\nwith a real set of elevators. In this case one can use how long since each button\nhas been pushed together with an estimate of the arrival rate to compute an\nexpected summed squared waiting time for each \roor. Using this in the reward\nmeasure proved nearly as e\u000bective as using the actual summed squared waiting\ntime.\nFor function approximation, a nonlinear neural network trained by back-\npropagation was used to represent the action-value function. Crites and Barto\nexperimented with a wide variety of ways of representing states to the network.\nAfter much exploration, their best results were obtained using networks with\n47 input units, 20 hidden units, and two output units, one for each action.\nThe way the state was encoded by the input units was found to be critical to\nthe e\u000bectiveness of the learning. The 47 input units were as follows:\n\u000f18 units: Two units encoded information about each of the nine hall\nbuttons for down pickup requests. A real-valued unit encoded the elapsed\ntime if the button had been pushed, and a binary unit was on if the\nbutton had not been pushed.\n\u000f16 units: A unit for each possible location and direction for the car whose\ndecision was required. Exactly one of these units was on at any given\ntime.\n\u000f10 units: The location of the other elevators superimposed over the 10\n\roors. Each elevator had a \\footprint\" that depended on its direction\nand speed. For example, a stopped elevator caused activation only on\nthe unit corresponding to its current \roor, but a moving elevator caused\nactivation on several units corresponding to the \roors it was approaching,\nwith the highest activations on the closest \roors. No information was\nprovided about which one of the other cars was at a particular location.\n\u000f1 unit: This unit was on if the elevator whose decision was required was\nat the highest \roor with a passenger waiting.\n\u000f1 unit: This unit was on if the elevator whose decision was required was\nat the \roor with the passenger who had been waiting for the longest\namount of time.\n290 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nSECTOR\nDLB\nHUFF1\nLQF\nHUFF2\nFIM\nESA/nq\nESA\nRL1\nRL2HUFF1\nLQF\nHUFF2\nFIM\nESA/nq\nESA\nRL1\nRL2020406080\nAverage\nwaiting\nand\nsystem\ntimes\nSECTOR\nDLB\nHUFF1\nLQF\nHUFF2\nFIM\nESA/nq\nESA\nRL1\nRL2\nDispatcher012\n% Waiting \n>1 minute\nSECTOR\nDLB\nDispatcher0200400600800\nDispatcherAverage\nsquared\nwaiting\ntime\nFigure 14.9: Comparison of elevator dispatchers. The SECTOR dispatcher is\nsimilar to what is used in many actual elevator systems. The RL1 and RL2\ndispatchers were constructed through reinforcement learning.\n\u000f1 unit: Bias unit was always on.\nTwo architectures were used. In RL1, each elevator was given its own\naction-value function and its own neural network. In RL2, there was only\none network and one action-value function, with the experiences of all four\nelevators contributing to learning in the one network. In both cases, each\nelevator made its decisions independently of the other elevators, but shared\na single reward signal with them. This introduced additional stochasticity as\nfar as each elevator was concerned because its reward depended in part on the\nactions of the other elevators, which it could not control. In the architecture\nin which each elevator had its own action-value function, it was possible for\ndi\u000berent elevators to learn di\u000berent specialized strategies (although in fact\nthey tended to learn the same strategy). On the other hand, the architecture\nwith a common action-value function could learn faster because it learned\nsimultaneously from the experiences of all elevators. Training time was an issue\nhere, even though the system was trained in simulation. The reinforcement\nlearning methods were trained for about four days of computer time on a 100\nmips processor (corresponding to about 60,000 hours of simulated time). While\nthis is a considerable amount of computation, it is negligible compared with\nwhat would be required by any conventional dynamic programming algorithm.\nThe networks were trained by simulating a great many evening rush hours\nwhile making dispatching decisions using the developing, learned action-value\nfunctions. Crites and Barto used the Gibbs softmax procedure to select ac-\ntions as described in Section 2.3, reducing the \\temperature\" gradually over\ntraining. A temperature of zero was used during test runs on which the per-\nformance of the learned dispatchers was assessed.\nFigure 14.9 shows the performance of several dispatchers during a simulated\nevening rush hour, what researchers call down-peak tra\u000ec. The dispatchers\n14.5. DYNAMIC CHANNEL ALLOCATION 291\ninclude methods similar to those commonly used in the industry, a variety\nof heuristic methods, sophisticated research algorithms that repeatedly run\ncomplex optimization algorithms on-line (Bao et al., 1994), and dispatchers\nlearned by using the two reinforcement learning architectures. By all of the\nperformance measures, the reinforcement learning dispatchers compare favor-\nably with the others. Although the optimal policy for this problem is unknown,\nand the state of the art is di\u000ecult to pin down because details of commercial\ndispatching strategies are proprietary, these learned dispatchers appeared to\nperform very well.\n14.5 Dynamic Channel Allocation\nAn important problem in the operation of a cellular telephone system is how\nto e\u000eciently use the available bandwidth to provide good service to as many\ncustomers as possible. This problem is becoming critical with the rapid growth\nin the use of cellular telephones. Here we describe a study due to Singh and\nBertsekas (1997) in which they applied reinforcement learning to this problem.\nMobile telephone systems take advantage of the fact that a communication\nchannel|a band of frequencies|can be used simultaneously by many callers\nif these callers are spaced physically far enough apart that their calls do not\ninterfere with each another. The minimum distance at which there is no inter-\nference is called the channel reuse constraint . In a cellular telephone system,\nthe service area is divided into a number of regions called cells. In each cell\nis a base station that handles all the calls made within the cell. The total\navailable bandwidth is divided permanently into a number of channels. Chan-\nnels must then be allocated to cells and to calls made within cells without\nviolating the channel reuse constraint. There are a great many ways to do\nthis, some of which are better than others in terms of how reliably they make\nchannels available to new calls, or to calls that are \\handed o\u000b\" from one cell\nto another as the caller crosses a cell boundary. If no channel is available for\na new or a handed-o\u000b call, the call is lost, or blocked . Singh and Bertsekas\nconsidered the problem of allocating channels so that the number of blocked\ncalls is minimized.\nA simple example provides some intuition about the nature of the problem.\nImagine a situation with three cells sharing two channels. The three cells are\narranged in a line where no two adjacent cells can use the same channel without\nviolating the channel reuse constraint. If the left cell is serving a call on channel\n1 while the right cell is serving another call on channel 2, as in the left diagram\nbelow, then any new call arriving in the middle cell must be blocked.\n292 CHAPTER 14. APPLICATIONS AND CASE STUDIES\n1 2 1 1 2\nObviously, it would be better for both the left and the right cells to use channel\n1 for their calls. Then a new call in the middle cell could be assigned channel 2,\nas in the right diagram, without violating the channel reuse constraint. Such\ninteractions and possible optimizations are typical of the channel assignment\nproblem. In larger and more realistic cases with many cells, channels, and\ncalls, and uncertainty about when and where new calls will arrive or existing\ncalls will have to be handed o\u000b, the problem of allocating channels to minimize\nblocking can become extremely complex.\nThe simplest approach is to permanently assign channels to cells in such a\nway that the channel reuse constraint can never be violated even if all channels\nof all cells are used simultaneously. This is called a \fxed assignment method.\nIn a dynamic assignment method, in contrast, all channels are potentially\navailable to all cells and are assigned to cells dynamically as calls arrive. If\nthis is done right, it can take advantage of temporary changes in the spatial\nand temporal distribution of calls in order to serve more users. For example,\nwhen calls are concentrated in a few cells, these cells can be assigned more\nchannels without increasing the blocking rate in the lightly used cells.\nThe channel assignment problem can be formulated as a semi-Markov de-\ncision process much as the elevator dispatching problem was in the previous\nsection. A state in the semi-MDP formulation has two components. The \frst\nis the con\fguration of the entire cellular system that gives for each cell the\nusage state (occupied or unoccupied) of each channel for that cell. A typical\ncellular system with 49 cells and 70 channels has a staggering 7049con\fgura-\ntions, ruling out the use of conventional dynamic programming methods. The\nother state component is an indicator of what kind of event caused a state\ntransition: arrival, departure, or hando\u000b. This state component determines\nwhat kinds of actions are possible. When a call arrives, the possible actions\nare to assign it a free channel or to block it if no channels are available. When\na call departs, that is, when a caller hangs up, the system is allowed to reassign\nthe channels in use in that cell in an attempt to create a better con\fguration.\nAt timetthe immediate reward, Rt, is the number of calls taking place at\nthat time, and the return is\nGt=Z1\n0e\u0000\f\u001cRt+\u001cd\u001c;\nwhere\f >0 plays a role similar to that of the discount-rate parameter \r. Max-\nimizing the expectation of this return is the same as minimizing the expected\n(discounted) number of calls blocked over an in\fnite horizon.\n14.5. DYNAMIC CHANNEL ALLOCATION 293\nThis is another problem greatly simpli\fed if treated in terms of afterstates\n(Section 6.6). For each state and action, the immediate result is a new con-\n\fguration, an afterstate. A value function is learned over just these con\fgu-\nrations. To select among the possible actions, the resulting con\fguration was\ndetermined and evaluated. The action was then selected that would lead to\nthe con\fguration of highest estimated value. For example, when a new call\narrived at a cell, it could be assigned to any of the free channels, if there were\nany; otherwise, it had to be blocked. The new con\fguration that would result\nfrom each assignment was easy to compute because it was always a simple de-\nterministic consequence of the assignment. When a call terminated, the newly\nreleased channel became available for reassigning to any of the ongoing calls.\nIn this case, the actions of reassigning each ongoing call in the cell to the newly\nreleased channel were considered. An action was then selected leading to the\ncon\fguration with the highest estimated value.\nLinear function approximation was used for the value function: the esti-\nmated value of a con\fguration was a weighted sum of features. Con\fgurations\nwere represented by two sets of features: an availability feature for each cell\nand a packing feature for each cell{channel pair. For any con\fguration, the\navailability feature for a cell gave the number of additional calls it could accept\nwithout con\rict if the rest of the cells were frozen in the current con\fguration.\nFor any given con\fguration, the packing feature for a cell{channel pair gave\nthe number of times that channel was being used in that con\fguration within\na four-cell radius of that cell. All of these features were normalized to lie be-\ntween\u00001 and 1. A semi-Markov version of linear TD(0) was used to update\nthe weights.\nSingh and Bertsekas compared three channel allocation methods using a\nsimulation of a 7\u00027 cellular array with 70 channels. The channel reuse con-\nstraint was that calls had to be 3 cells apart to be allowed to use the same chan-\nnel. Calls arrived at cells randomly according to Poisson distributions possibly\nhaving di\u000berent means for di\u000berent cells, and call durations were determined\nrandomly by an exponential distribution with a mean of three minutes. The\nmethods compared were a \fxed assignment method (FA), a dynamic alloca-\ntion method called \\borrowing with directional channel locking\" (BDCL), and\nthe reinforcement learning method (RL). BDCL (Zhang and Yum, 1989) was\nthe best dynamic channel allocation method they found in the literature. It is\na heuristic method that assigns channels to cells as in FA, but channels can be\nborrowed from neighboring cells when needed. It orders the channels in each\ncell and uses this ordering to determine which channels to borrow and how\ncalls are dynamically reassigned channels within a cell.\nFigure 14.10 shows the blocking probabilities of these methods for mean\narrival rates of 150, 200, and 350 calls/hour as well as for a case in which\n294 CHAPTER 14. APPLICATIONS AND CASE STUDIES\n0 10 20 30 40 50Nonuniform350 calls/hour200 calls/hour150 calls/hour\nRLBDCLFA\n% BlockingTraffic\nFigure 14.10: Performance of FA, BDCL, and RL channel allocation methods\nfor di\u000berent mean call arrival rates.\ndi\u000berent cells had di\u000berent mean arrival rates. The reinforcement learning\nmethod learned on-line. The data shown are for its asymptotic performance,\nbut in fact learning was rapid. The RL method blocked calls less frequently\nthan did the other methods for all arrival rates and soon after starting to learn.\nNote that the di\u000berences between the methods decreased as the call arrival rate\nincreased. This is to be expected because as the system gets saturated with\ncalls there are fewer opportunities for a dynamic allocation method to set up\nfavorable usage patterns. In practice, however, it is the performance of the\nunsaturated system that is most important. For marketing reasons, cellular\ntelephone systems are built with enough capacity that more than 10% blocking\nis rare.\nNie and Haykin (1996) also studied the application of reinforcement learn-\ning to dynamic channel allocation. They formulated the problem somewhat\ndi\u000berently than Singh and Bertsekas did. Instead of trying to minimize the\nprobability of blocking a call directly, their system tried to minimize a more\nindirect measure of system performance. Cost was assigned to patterns of\nchannel use depending on the distances between calls using the same chan-\nnels. Patterns in which channels were being used by multiple calls that were\nclose to each other were favored over patterns in which channel-sharing calls\nwere far apart. Nie and Haykin compared their system with a method called\nMAXAVAIL (Sivarajan, McEliece, and Ketchum, 1990), considered to be one\nof the best dynamic channel allocation methods. For each new call, it selects\nthe channel that maximizes the total number of channels available in the en-\ntire system. Nie and Haykin showed that the blocking probability achieved by\ntheir reinforcement learning system was closely comparable to that of MAX-\nAVAIL under a variety of conditions in a 49-cell, 70-channel simulation. A\n14.6. JOB-SHOP SCHEDULING 295\nkey point, however, is that the allocation policy produced by reinforcement\nlearning can be implemented on-line much more e\u000eciently than MAXAVAIL,\nwhich requires so much on-line computation that it is not feasible for large\nsystems.\nThe studies we described in this section are so recent that the many ques-\ntions they raise have not yet been answered. We can see, though, that there\ncan be di\u000berent ways to apply reinforcement learning to the same real-world\nproblem. In the near future, we expect to see many re\fnements of these\napplications, as well as many new applications of reinforcement learning to\nproblems arising in communication systems.\n14.6 Job-Shop Scheduling\nMany jobs in industry and elsewhere require completing a collection of tasks\nwhile satisfying temporal and resource constraints. Temporal constraints say\nthat some tasks have to be \fnished before others can be started; resource\nconstraints say that two tasks requiring the same resource cannot be done\nsimultaneously (e.g., the same machine cannot do two tasks at once). The\nobjective is to create a schedule specifying when each task is to begin and\nwhat resources it will use that satis\fes all the constraints while taking as\nlittle overall time as possible. This is the job-shop scheduling problem. In its\ngeneral form, it is NP-complete, meaning that there is probably no e\u000ecient\nprocedure for exactly \fnding shortest schedules for arbitrary instances of the\nproblem. Job-shop scheduling is usually done using heuristic algorithms that\ntake advantage of special properties of each speci\fc instance.\nZhang and Dietterich (1995, 1996; Zhang, 1996) were motivated to apply\nreinforcement learning to job-shop scheduling because the design of domain-\nspeci\fc, heuristic algorithms can be expensive and time-consuming. Their\ngoal was to show how reinforcement learning can be used to learn how to\nquickly \fnd constraint-satisfying schedules of short duration in speci\fc do-\nmains, thereby reducing the amount of hand engineering required. They ad-\ndressed the NASA space shuttle payload processing problem (SSPP), which\nrequires scheduling the tasks required for installation and testing of shuttle\ncargo bay payloads. An SSPP typically requires scheduling for two to six\nshuttle missions, each requiring between 34 and 164 tasks. An example of\na task is MISSION-SEQUENCE-TEST, which has a duration of 7200 time\nunits and requires the following resources: two quality control o\u000ecers, two\ntechnicians, one ATE, one SPCDS, and one HITS. Some resources are divided\ninto pools, and if a task needs more than one resource of a speci\fc type, the\nresources must belong to the same pool, and the pool has to be the right one.\n296 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nFor example, if a task needs two quality control o\u000ecers, they both have to be\nin the pool of quality control o\u000ecers working on the same shift at the right\nsite. It is not too hard to \fnd a con\rict-free schedule for a job, one that\nmeets all the temporal and resource constraints, but the objective is to \fnd a\ncon\rict-free schedule with the shortest possible total duration, which is much\nmore di\u000ecult.\nHow can you do this using reinforcement learning? Job-shop scheduling\nis usually formulated as a search in the space of schedules, what is called a\ndiscrete, or combinatorial, optimization problem. A typical solution method\nwould sequentially generate schedules, attempting to improve each over its\npredecessor in terms of constraint violations and duration (a hill-climbing, or\nlocal search, method). You could think of this as a nonassociative reinforce-\nment learning problem of the type we discussed in Chapter 2 with a very large\nnumber of possible actions: all the possible schedules! But aside from the\nproblem of having so many actions, any solution obtained this way would just\nbe a single schedule for a single job instance. In contrast, what Zhang and\nDietterich wanted their learning system to end up with was a policy that could\nquickly \fnd good schedules for anySSPP. They wanted it to learn a skill for\njob-shop scheduling in this speci\fc domain.\nFor clues about how to do this, they looked to an existing optimization\napproach to SSPP, in fact, the one actually in use by NASA at the time of\ntheir research: the iterative repair method developed by Zweben and Daun\n(1994). The starting point for the search is a critical path schedule , a schedule\nthat meets the temporal constraints but ignores the resource constraints. This\nschedule can be constructed e\u000eciently by scheduling each task prior to launch\nas late as the temporal constraints permit, and each task after landing as early\nas these constraints permit. Resource pools are assigned randomly. Two types\nof operators are used to modify schedules. They can be applied to any task\nthat violates a resource constraint. A Reassign-Pool operator changes the\npool assigned to one of the task's resources. This type of operator applies only\nif it can reassign a pool so that the resource requirement is satis\fed. A Move\noperator moves a task to the \frst earlier or later time at which its resource\nneeds can be satis\fed and uses the critical path method to reschedule all of\nthe task's temporal dependents.\nAt each step of the iterative repair search, one operator is applied to the\ncurrent schedule, selected according to the following rules. The earliest task\nwith a resource constraint violation is found, and a Reassign-Pool oper-\nator is applied to this task if possible. If more than one applies, that is, if\nseveral di\u000berent pool reassignments are possible, one is selected at random.\nIf noReassign-Pool operator applies, then a Move operator is selected at\nrandom based on a heuristic that prefers short-distance moves of tasks having\n14.6. JOB-SHOP SCHEDULING 297\nfew temporal dependents and whose resource requirements are close to the\ntask's overallocation. After an operator is applied, the number of constraint\nviolations of the resulting schedule is determined. A simulated annealing pro-\ncedure is used decide whether to accept or reject this new schedule. If \u0001 V\ndenotes the number of constraint violations removed by the repair, then the\nnew schedule is accepted with probability exp( \u0000\u0001V=T), whereTis the cur-\nrent computational temperature that is gradually decreased throughout the\nsearch. If accepted, the new schedule becomes the current schedule for the\nnext iteration; otherwise, the algorithm attempts to repair the old schedule\nagain, which will usually produce di\u000berent results due to the random decisions\ninvolved. Search stops when all constraints are satis\fed. Short schedules are\nobtained by running the algorithm several times and selecting the shortest of\nthe resulting con\rict-free schedules.\nZhang and Dietterich treated entire schedules as states in the sense of\nreinforcement learning. The actions were the applicable Reassign-Pool and\nMove operators, typically numbering about 20. The problem was treated\nas episodic, each episode starting with the same critical path schedule that\nthe iterative repair algorithm would start with and ending when a schedule\nwas found that did not violate any constraint. The initial state|a critical\npath schedule|is denoted S0. The rewards were designed to promote the\nquick construction of con\rict-free schedules of short duration. The system\nreceived a small negative reward ( \u00000:001) on each step that resulted in a\nschedule that still violated a constraint. This encouraged the agent to \fnd\ncon\rict-free schedules quickly, that is, with a small number of repairs to S0.\nEncouraging the system to \fnd short schedules is more di\u000ecult because what\nit means for a schedule to be short depends on the speci\fc SSPP instance. The\nshortest schedule for a di\u000ecult instance, one with a lot of tasks and constraints,\nwill be longer than the shortest schedule for a simpler instance. Zhang and\nDietterich devised a formula for a resource dilation factor (RDF), intended to\nbe an instance-independent measure of a schedule's duration. To account for\nan instance's intrinsic di\u000eculty, the formula makes use of a measure of the\nresource overallocation of S0. Since longer schedules tend to produce larger\nRDFs, the negative of the RDF of the \fnal con\rict-free schedule was used as\na reward at the end of each episode. With this reward function, if it takes N\nrepairs starting from a schedule sto obtain a \fnal con\rict-free schedule, Sf,\nthe return from sis\u0000RDF (Sf)\u00000:001(N\u00001).\nThis reward function was designed to try to make a system learn to satisfy\nthe two goals of \fnding con\rict-free schedules of short duration and \fnding\ncon\rict-free schedules quickly. But the reinforcement learning system really\nhas only one goal|maximizing expected return|so the particular reward val-\nues determine how a learning system will tend to trade o\u000b these two goals.\n298 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nSetting the immediate reward to the small value of \u00000:001 means that the\nlearning system will regard one repair, one step in the scheduling process, as\nbeing worth 0 :001 units of RDF. So, for example, if from some schedule it is\npossible to produce a con\rict-free schedule with one repair or with two, an\noptimal policy will take extra repair only if it promises a reduction in \fnal\nRDF of more than 0 :001.\nZhang and Dietterich used TD( \u0015) to learn the value function. Function\napproximation was by a multilayer neural network trained by backpropagating\nTD errors. Actions were selected by an \"-greedy policy, with \"decreasing\nduring learning. One-step lookahead search was used to \fnd the greedy action.\nTheir knowledge of the problem made it easy to predict the schedules that\nwould result from each repair operation. They experimented with a number of\nmodi\fcations to this basic procedure to improve its performance. One was to\nuse the TD( \u0015) algorithm backward after each episode, with the eligibility trace\nextending to future rather than to past states. Their results suggested that this\nwas more accurate and e\u000ecient than forward learning. In updating the weights\nof the network, they also sometimes performed multiple weight updates when\nthe TD error was large. This is apparently equivalent to dynamically varying\nthe step-size parameter in an error-dependent way during learning.\nThey also tried an experience replay technique due to Lin (1992). At any\npoint in learning, the agent remembered the best episode up to that point.\nAfter every four episodes, it replayed this remembered episode, learning from\nit as if it were a new episode. At the start of training, they similarly allowed\nthe system to learn from episodes generated by a good scheduler, and these\ncould also be replayed later in learning. To make the lookahead search faster\nfor large-scale problems, which typically had a branching factor of about 20,\nthey used a variant they called random sample greedy search that estimated\nthe greedy action by considering only random samples of actions, increasing\nthe sample size until a preset con\fdence was reached that the greedy action of\nthe sample was the true greedy action. Finally, having discovered that learning\ncould be slowed considerably by excessive looping in the scheduling process,\nthey made their system explicitly check for loops and alter action selections\nwhen a loop was detected. Although all of these techniques could improve the\ne\u000eciency of learning, it is not clear how crucial all of them were for the success\nof the system.\nZhang and Dietterich experimented with two di\u000berent network architec-\ntures. In the \frst version of their system, each schedule was represented using\na set of 20 handcrafted features. To de\fne these features, they studied small\nscheduling problems to \fnd features that had some ability to predict RDF. For\nexample, experience with small problems showed that only four of the resource\npools tended to cause allocation problems. The mean and standard deviation\n14.6. JOB-SHOP SCHEDULING 299\nof each of these pools' unused portions over the entire schedule were computed,\nresulting in 10 real-valued features. Two other features were the RDF of the\ncurrent schedule and the percentage of its duration during which it violated\nresource constraints. The network had 20 input units, one for each feature, a\nhidden layer of 40 sigmoidal units, and an output layer of 8 sigmoidal units.\nThe output units coded the value of a schedule using a code in which, roughly,\nthe location of the activity peak over the 8 units represented the value. Us-\ning the appropriate TD error, the network weights were updated using error\nbackpropagation, with the multiple weight-update technique mentioned above.\nThe second version of the system (Zhang and Dietterich, 1996) used a\nmore complicated time-delay neural network (TDNN) borrowed from the \feld\nof speech recognition (Lang, Waibel, and Hinton, 1990). This version divided\neach schedule into a sequence of blocks (maximal time intervals during which\ntasks and resource assignments did not change) and represented each block by\na set of features similar to those used in the \frst program. It then scanned\na set of \\kernel\" networks across the blocks to create a set of more abstract\nfeatures. Since di\u000berent schedules had di\u000berent numbers of blocks, another\nlayer averaged these abstract features over each third of the blocks. Then a\n\fnal layer of 8 sigmoidal output units represented the schedule's value using\nthe same code as in the \frst version of the system. In all, this network had\n1123 adjustable weights.\nA set of 100 arti\fcial scheduling problems was constructed and divided into\nsubsets used for training, determining when to stop training (a validation set),\nand \fnal testing. During training they tested the system on the validation\nset after every 100 episodes and stopped training when performance on the\nvalidation set stopped changing, which generally took about 10,000 episodes.\nThey trained networks with di\u000berent values of \u0015(0.2 and 0.7), with three\ndi\u000berent training sets, and they saved both the \fnal set of weights and the\nset of weights producing the best performance on the validation set. Counting\neach set of weights as a di\u000berent network, this produced 12 networks, each of\nwhich corresponded to a di\u000berent scheduling algorithm.\nFigure 14.11 shows how the mean performance of the 12 TDNN networks\n(labeled G12TDN) compared with the performances of two versions of Zweben\nand Daun's iterative repair algorithm, one using the number of constraint\nviolations as the function to be minimized by simulated annealing (IR-V)\nand the other using the RDF measure (IR-RDF). The \fgure also shows the\nperformance of the \frst version of their system that did not use a TDNN\n(G12N). The mean RDF of the best schedule found by repeatedly running\nan algorithm is plotted against the total number of schedule repairs (using a\nlog scale). These results show that the learning system produced scheduling\nalgorithms that needed many fewer repairs to \fnd con\rict-free schedules of the\n300 CHAPTER 14. APPLICATIONS AND CASE STUDIES\n1.151.21.251.31.351.41.451.5\n1024 2048 4096 8192 16384 32768 65536 131072 262144RDF\nAcce pted Re pairsG12TDN\nG12N\nIR-RDF\nIR-V\nFigure 14.11: Comparison of accepted schedule repairs. Reprinted with per-\nmission from Zhang and Dietterich, 1996.\nsame quality as those found by the iterative repair algorithms. Figure 14.12\ncompares the computer time required by each scheduling algorithm to \fnd\nschedules of various RDFs. According to this measure of performance, the\nbest trade-o\u000b between computer time and schedule quality is produced by\nthe non-TDNN algorithm (G12N). The TDNN algorithm (G12TDN) su\u000bered\ndue to the time it took to apply the kernel-scanning process, but Zhang and\nDietterich point out that there are many ways to make it run faster.\nThese results do not unequivocally establish the utility of reinforcement\nlearning for job-shop scheduling or for other di\u000ecult search problems. But\nthey do suggest that it is possible to use reinforcement learning methods to\nlearn how to improve the e\u000eciency of search. Zhang and Dietterich's job-\nshop scheduling system is the \frst successful instance of which we are aware\nin which reinforcement learning was applied in plan-space , that is, in which\nstates are complete plans (job-shop schedules in this case), and actions are plan\nmodi\fcations. This is a more abstract application of reinforcement learning\nthan we are used to thinking about. Note that in this application the system\nlearned not just to e\u000eciently create onegood schedule, a skill that would\nnot be particularly useful; it learned how to quickly \fnd good schedules for\na class of related scheduling problems. It is clear that Zhang and Dietterich\nwent through a lot of trial-and-error learning of their own in developing this\nexample. But remember that this was a groundbreaking exploration of a new\naspect of reinforcement learning. We expect that future applications of this\nkind and complexity will become more routine as experience accumulates.\n14.6. JOB-SHOP SCHEDULING 301\n1.151.21.251.31.351.41.451.5\n16 32 64 128 256 512 1024 2048RDF\nCPU Time (Seconds )G12TDN\nG12N\nIR-RDF\nIR-V\nFigure 14.12: Comparison of CPU time. Reprinted with permission from\nZhang and Dietterich, 1996.\n302 CHAPTER 14. APPLICATIONS AND CASE STUDIES\nChapter 15\nProspects\nIn this book we have tried to present reinforcement learning not as a collection\nof individual methods, but as a coherent set of ideas cutting across methods.\nEach idea can be viewed as a dimension along which methods vary. The set\nof such dimensions spans a large space of possible methods. By exploring\nthis space at the level of dimensions we hope to obtain the broadest and\nmost lasting understanding. In this chapter we use the concept of dimensions\nin method space to recapitulate the view of reinforcement learning we have\ndeveloped in this book and to identify some of the more important gaps in our\ncoverage of the \feld.\n15.1 The Uni\fed View\nAll of the reinforcement learning methods we have explored in this book have\nthree key ideas in common. First, the objective of all of them is the estimation\nof value functions. Second, all operate by backing up values along actual or\npossible state trajectories. Third, all follow the general strategy of general-\nized policy iteration (GPI), meaning that they maintain an approximate value\nfunction and an approximate policy, and they continually try to improve each\non the basis of the other. These three ideas that the methods have in common\ncircumscribe the subject covered in this book. We suggest that value func-\ntions, backups, and GPI are powerful organizing principles potentially relevant\nto any model of intelligence.\nTwo of the most important dimensions along which the methods vary are\nshown in Figure 15.1. These dimensions have to do with the kind of backup\nused to improve the value function. The horizontal dimension is whether they\nare sample backups (based on a sample trajectory) or full backups (based\n303\n304 CHAPTER 15. PROSPECTS\nwidthof backup\nheight(depth)of backup\nTemporal-differencelearning\nDynamicprogramming\nMonteCarlo...\nExhaustivesearch\nFigure 15.1: A slice of the space of reinforcement learning methods.\non a distribution of possible trajectories). Full backups of course require a\nmodel, whereas sample backups can be done either with or without a model\n(another dimension of variation). The vertical dimension corresponds to the\ndepth of backups, that is, to the degree of bootstrapping. At three of the\nfour corners of the space are the three primary methods for estimating values:\nDP, TD, and Monte Carlo. Along the left edge of the space are the sample-\nbackup methods, ranging from one-step TD backups to full-return Monte Carlo\nbackups. Between these is a spectrum including methods based on n-step\nbackups and mixtures of n-step backups such as the \u0015-backups implemented\nby eligibility traces.\nDP methods are shown in the extreme upper-right corner of the space be-\ncause they involve one-step full backups. The lower-right corner is the extreme\ncase of full backups so deep that they run all the way to terminal states (or, in\na continuing task, until discounting has reduced the contribution of any further\nrewards to a negligible level). This is the case of exhaustive search. Intermedi-\n15.1. THE UNIFIED VIEW 305\nate methods along this dimension include heuristic search and related methods\nthat search and backup up to a limited depth, perhaps selectively. There are\nalso methods that are intermediate along the horizontal dimension. These\ninclude methods that mix full and sample backups, as well as the possibility\nof methods that mix samples and distributions within a single backup. The\ninterior of the square is \flled in to represent the space of all such intermediate\nmethods.\nA third important dimension is that of function approximation. Function\napproximation can be viewed as an orthogonal spectrum of possibilities ranging\nfrom tabular methods at one extreme through state aggregation, a variety\nof linear methods, and then a diverse set of nonlinear methods. This third\ndimension might be visualized as perpendicular to the plane of the page in\nFigure 15.1.\nAnother dimension that we heavily emphasized in this book is the binary\ndistinction between on-policy and o\u000b-policy methods. In the former case, the\nagent learns the value function for the policy it is currently following, whereas\nin the latter case it learns the value function for the policy that it currently\nthinks is best. These two policies are often di\u000berent because of the need\nto explore. The interaction between this dimension and the bootstrapping\nand function approximation dimension discussed in Chapter 9 illustrates the\nadvantages of analyzing the space of methods in terms of dimensions. Even\nthough this did involve an interaction between three dimensions, many other\ndimensions were found to be irrelevant, greatly simplifying the analysis and\nincreasing its signi\fcance.\nIn addition to the four dimensions just discussed, we have identi\fed a\nnumber of others throughout the book:\nDe\fnition of return Is the task episodic or continuing, discounted or undis-\ncounted?\nAction values vs. state values vs. afterstate values What kind of val-\nues should be estimated? If only state values are estimated, then either\na model or a separate policy (as in actor{critic methods) is required for\naction selection.\nAction selection/exploration How are actions selected to ensure a suitable\ntrade-o\u000b between exploration and exploitation? We have considered only\nthe simplest ways to do this: \"-greedy and softmax action selection, and\noptimistic initialization of values.\nSynchronous vs. asynchronous Are the backups for all states performed\nsimultaneously or one by one in some order?\n306 CHAPTER 15. PROSPECTS\nReplacing vs. accumulating traces If eligibility traces are used, which kind\nis most appropriate?\nReal vs. simulated Should one backup real experience or simulated experi-\nence? If both, how much of each?\nLocation of backups What states or state{action pairs should be backed\nup? Modelfree methods can choose only among the states and state{\naction pairs actually encountered, but model-based methods can choose\narbitrarily. There are many potent possibilities here.\nTiming of backups Should backups be done as part of selecting actions, or\nonly afterward?\nMemory for backups How long should backed-up values be retained? Should\nthey be retained permanently, or only while computing an action selec-\ntion, as in heuristic search?\nOf course, these dimensions are neither exhaustive nor mutually exclusive.\nIndividual algorithms di\u000ber in many other ways as well, and many algorithms\nlie in several places along several dimensions. For example, Dyna methods use\nboth real and simulated experience to a\u000bect the same value function. It is also\nperfectly sensible to maintain multiple value functions computed in di\u000berent\nways or over di\u000berent state and action representations. These dimensions do,\nhowever, constitute a coherent set of ideas for describing and exploring a wide\nspace of possible methods.\n15.2 State Estimation\n15.3 Temporal Abstraction\n15.4 Predictive Representations\n15.5 Other Frontier Dimensions\nMuch research remains to be done within this space of reinforcement learning\nmethods. For example, even for the tabular case no control method using\nmultistep backups has been proved to converge to an optimal policy. Among\nplanning methods, basic ideas such as trajectory sampling and focusing sample\nbackups are almost completely unexplored. On closer inspection, parts of the\n15.5. OTHER FRONTIER DIMENSIONS 307\nspace will undoubtedly turn out to have far greater complexity and greater\ninternal structure than is now apparent. There are also other dimensions along\nwhich reinforcement learning can be extended, we have not yet mentioned,\nthat lead to a much larger space of methods. Here we identify some of these\ndimensions and note some of the open questions and frontiers that have been\nleft out of the preceding chapters.\nOne of the most important extensions of reinforcement learning beyond\nwhat we have treated in this book is to eliminate the requirement that the\nstate representation have the Markov property. There are a number of inter-\nesting approaches to the non-Markov case. Most strive to construct from the\ngiven state signal and its past values a new signal that is Markov, or more\nnearly Markov. For example, one approach is based on the theory of partially\nobservable MDPs (POMDPs). POMDPs are \fnite MDPs in which the state\nis not observable, but another \\sensation\" signal stochastically related to the\nstate is observable. The theory of POMDPs has been extensively studied for\nthe case of complete knowledge of the dynamics of the POMDP. In this case,\nBayesian methods can be used to compute at each time step the probability of\nthe environment's being in each state of the underlying MDP. This probability\ndistribution can then be used as a new state signal for the original problem.\nThe downside for the Bayesian POMDP approach is its computational ex-\npense and its strong reliance on complete environment models. Some of the\nrecent work pursuing this approach is by Littman, Cassandra, and Kaelbling\n(1995), Parr and Russell (1995), and Chrisman (1992). If we are not willing\nto assume a complete model of a POMDP's dynamics, then existing theory\nseems to o\u000ber little guidance. Nevertheless, one can still attempt to construct\na Markov state signal from the sequence of sensations. Various statistical and\nad hoc methods along these lines have been explored (e.g., Chrisman, 1992;\nMcCallum, 1993, 1995; Lin and Mitchell, 1992; Chapman and Kaelbling, 1991;\nMoore, 1994; Rivest and Schapire, 1987; Colombetti and Dorigo, 1994; White-\nhead and Ballard, 1991; Hochreiter and Schmidhuber, 1997).\nAll of the above methods involve constructing an improved state repre-\nsentation from the non-Markov one provided by the environment. Another\napproach is to leave the state representation unchanged and use methods that\nare not too adversely a\u000bected by its being non-Markov (e.g., Singh, Jaakkola,\nand Jordan, 1994, 1995; Jaakkola, Singh and Jordan, 1995). In fact, most\nfunction approximation methods can be viewed in this way. For example, state\naggregation methods for function approximation are in e\u000bect equivalent to a\nnon-Markov representation in which all members of a set of states are mapped\ninto a common sensation. There are other parallels between the issues of func-\ntion approximation and non-Markov representations. In both cases the overall\nproblem divides into two parts: constructing an improved representation, and\n308 CHAPTER 15. PROSPECTS\nmaking do with the current representation. In both cases the \\making do\"\npart is relatively well understood, whereas the constructive part is unclear\nand wide open. At this point we can only guess as to whether or not these\nparallels point to any common solution methods for the two problems.\nAnother important direction for extending reinforcement learning beyond\nwhat we have covered in this book is to incorporate ideas of modularity and\nhierarchy. Introductory reinforcement learning is about learning value func-\ntions and one-step models of the dynamics of the environment. But much of\nwhat people learn does not seem to fall exactly into either of these categories.\nFor example, consider what we know about tying our shoes, making a phone\ncall, or traveling to London. Having learned how to do such things, we are\nthen able to choose among them and plan as if they were primitive actions.\nWhat we have learned in order to do this are not conventional value functions\nor one-step models. We are able to plan and learn at a variety of levels and\n\rexibly interrelate them. Much of our learning appears not to be about learn-\ning values directly, but about preparing us to quickly estimate values later in\nresponse to new situations or new information. Considerable reinforcement\nlearning research has been directed at capturing such abilities (e.g., Watkins,\n1989; Dayan and Hinton, 1993; Singh, 1992a, 1992b; Ring, 1994, Kaelbling,\n1993b; Sutton, 1995).\nResearchers have also explored ways of using the structure of particular\ntasks to advantage. For example, many problems have state representations\nthat are naturally lists of variables, like the readings of multiple sensors or ac-\ntions that are lists of component actions. The independence or near indepen-\ndence of some variables from others can sometimes be exploited to obtain more\ne\u000ecient special forms of reinforcement learning algorithms. It is sometimes\neven possible to decompose a problem into several independent subproblems\nthat can be solved by separate learning agents. A reinforcement learning prob-\nlem can usually be structured in many di\u000berent ways, some re\recting natural\naspects of the problem, such as the existence of physical sensors, and others\nbeing the result of explicit attempts to decompose the problem into simpler\nsubproblems. Possibilities for exploiting structure in reinforcement learning\nand related planning problems have been studied by many researchers (e.g.,\nBoutilier, Dearden, and Goldszmidt, 1995; Dean and Lin, 1995). There are\nalso related studies of multiagent or distributed reinforcement learning (e.g.,\nLittman, 1994; Markey, 1994; Crites and Barto, 1996; Tan, 1993).\nFinally, we want to emphasize that reinforcement learning is meant to be a\ngeneral approach to learning from interaction. It is general enough not to re-\nquire special-purpose teachers and domain knowledge, but also general enough\nto utilize such things if they are available. For example, it is often possible\nto accelerate reinforcement learning by giving advice or hints to the agent\n15.5. OTHER FRONTIER DIMENSIONS 309\n(Clouse and Utgo\u000b, 1992; Maclin and Shavlik, 1994) or by demonstrating in-\nstructive behavioral trajectories (Lin, 1992). Another way to make learning\neasier, related to \\shaping\" in psychology, is to give the learning agent a series\nof relatively easy problems building up to the harder problem of ultimate inter-\nest (e.g., Selfridge, Sutton, and Barto, 1985). These methods, and others not\nyet developed, have the potential to give the machine-learning terms training\nand teaching new meanings that are closer to their meanings for animal and\nhuman learning.\n310 CHAPTER 15. PROSPECTS\nReferences\nAgrawal, R. (1995). Sample mean based index policies with O(logn) regret\nfor the multi-armed bandit problem. Advances in Applied Probability ,\n27:1054{1078.\nAgre, P. E. (1988). The Dynamic Structure of Everyday Life . Ph.D. the-\nsis, Massachusetts Institute of Technology. AI-TR 1085, MIT Arti\fcial\nIntelligence Laboratory.\nAgre, P. E., Chapman, D. (1990). What are plans for? Robotics and Au-\ntonomous Systems , 6:17{34.\nAlbus, J. S. (1971). A theory of cerebellar function. Mathematical Biosciences ,\n10:25{61.\nAlbus, J. S. (1981). Brain, Behavior, and Robotics . Byte Books, Peterbor-\nough, NH.\nAnderson, C. W. (1986). Learning and Problem Solving with Multilayer Con-\nnectionist Systems . Ph.D. thesis, University of Massachusetts, Amherst.\nAnderson, C. W. (1987). Strategy learning with multilayer connectionist\nrepresentations. Proceedings of the Fourth International Workshop on\nMachine Learning , pp. 103{114. Morgan Kaufmann, San Mateo, CA.\nAnderson, J. A., Silverstein, J. W., Ritz, S. A., Jones, R. S. (1977). Dis-\ntinctive features, categorical perception, and probability learning: Some\napplications of a neural model. Psychological Review , 84:413{451.\nAndreae, J. H. (1963). STELLA: A scheme for a learning machine. In\nProceedings of the 2nd IFAC Congress, Basle , pp. 497{502. Butterworths,\nLondon.\nAndreae, J. H. (1969a). A learning machine with monologue. International\nJournal of Man{Machine Studies , 1:1{20.\nAndreae, J. H. (1969b). Learning machines|a uni\fed view. In A. R. Meetham\nand R. A. Hudson (eds.), Encyclopedia of Information, Linguistics, and\nControl , pp. 261{270. Pergamon, Oxford.\n311\n312 CHAPTER 15. PROSPECTS\nAndreae, J. H. (1977). Thinking with the Teachable Machine . Academic\nPress, London.\nArthur, W. B. (1991). Designing economic agents that act like human agents:\nA behavioral approach to bounded rationality. The American Economic\nReview 81 (2):353-359.\nAuer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the\nmultiarmed bandit problem. Machine learning , 47(2-3):235{256.\nBaird, L. C. (1995). Residual algorithms: Reinforcement learning with func-\ntion approximation. In Proceedings of the Twelfth International Conference\non Machine Learning , pp. 30{37. Morgan Kaufmann, San Francisco.\nBao, G., Cassandras, C. G., Djaferis, T. E., Gandhi, A. D., Looze, D. P.\n(1994). Elevator dispatchers for down peak tra\u000ec. Technical report. ECE\nDepartment, University of Massachusetts, Amherst.\nBarnard, E. (1993). Temporal-di\u000berence methods and Markov models. IEEE\nTransactions on Systems, Man, and Cybernetics , 23:357{365.\nBarto, A. G. (1985). Learning by statistical cooperation of self-interested\nneuron-like computing elements. Human Neurobiology , 4:229{256.\nBarto, A. G. (1986). Game-theoretic cooperativity in networks of self-interested\nunits. In J. S. Denker (ed.), Neural Networks for Computing , pp. 41{46.\nAmerican Institute of Physics, New York.\nBarto, A. G. (1990). Connectionist learning for control: An overview. In\nT. Miller, R. S. Sutton, and P. J. Werbos (eds.), Neural Networks for\nControl , pp. 5{58. MIT Press, Cambridge, MA.\nBarto, A. G. (1991). Some learning tasks from a control perspective. In\nL. Nadel and D. L. Stein (eds.), 1990 Lectures in Complex Systems , pp. 195{\n223. Addison-Wesley, Redwood City, CA.\nBarto, A. G. (1992). Reinforcement learning and adaptive critic methods. In\nD. A. White and D. A. Sofge (eds.), Handbook of Intelligent Control: Neu-\nral, Fuzzy, and Adaptive Approaches , pp. 469{491. Van Nostrand Reinhold,\nNew York.\nBarto, A. G. (1995a). Adaptive critics and the basal ganglia. In J. C. Houk,\nJ. L. Davis, and D. G. Beiser (eds.), Models of Information Processing in\nthe Basal Ganglia , pp. 215{232. MIT Press, Cambridge, MA.\nBarto, A. G. (1995b). Reinforcement learning. In M. A. Arbib (ed.), Hand-\nbook of Brain Theory and Neural Networks , pp. 804{809. MIT Press, Cam-\nbridge, MA.\nBarto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning\n15.5. OTHER FRONTIER DIMENSIONS 313\nautomata. IEEE Transactions on Systems, Man, and Cybernetics , 15:360{\n375.\nBarto, A. G., Anderson, C. W. (1985). Structural learning in connectionist\nsystems. In Program of the Seventh Annual Conference of the Cognitive\nScience Society , pp. 43{54.\nBarto, A. G., Anderson, C. W., Sutton, R. S. (1982). Synthesis of nonlin-\near control surfaces by a layered associative search network. Biological\nCybernetics , 43:175{185.\nBarto, A. G., Bradtke, S. J., Singh, S. P. (1991). Real-time learning and\ncontrol using asynchronous dynamic programming. Technical Report 91-\n57. Department of Computer and Information Science, University of Mas-\nsachusetts, Amherst.\nBarto, A. G., Bradtke, S. J., Singh, S. P. (1995). Learning to act using\nreal-time dynamic programming. Arti\fcial Intelligence , 72:81{138.\nBarto, A. G., Du\u000b, M. (1994). Monte Carlo matrix inversion and reinforce-\nment learning. In J. D. Cohen, G. Tesauro, and J. Alspector (eds.), Ad-\nvances in Neural Information Processing Systems: Proceedings of the 1993\nConference , pp. 687{694. Morgan Kaufmann, San Francisco.\nBarto, A. G., Jordan, M. I. (1987). Gradient following without back-propagation\nin layered networks. In M. Caudill and C. Butler (eds.), Proceedings of the\nIEEE First Annual Conference on Neural Networks , pp. II629{II636. SOS\nPrinting, San Diego, CA.\nBarto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive\nintelligence: An initial assessment. Technical Report AFWAL-TR-81-\n1070. Air Force Wright Aeronautical Laboratories/Avionics Laboratory,\nWright-Patterson AFB, OH.\nBarto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of\nassociative search. Biological Cybernetics , 42:1{8.\nBarto, A. G., Sutton, R. S. (1982). Simulation of anticipatory responses\nin classical conditioning by a neuron-like adaptive element. Behavioural\nBrain Research , 4:221{235.\nBarto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements\nthat can solve di\u000ecult learning control problems. IEEE Transactions on\nSystems, Man, and Cybernetics , 13:835{846. Reprinted in J. A. Ander-\nson and E. Rosenfeld (eds.), Neurocomputing: Foundations of Research ,\npp. 535{549. MIT Press, Cambridge, MA, 1988.\nBarto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search net-\n314 CHAPTER 15. PROSPECTS\nwork: A reinforcement learning associative memory. Biological Cybernet-\nics, 40:201{211.\nBellman, R. E. (1956). A problem in the sequential design of experiments.\nSankhya , 16:221{229.\nBellman, R. E. (1957a). Dynamic Programming . Princeton University Press,\nPrinceton.\nBellman, R. E. (1957b). A Markov decision process. Journal of Mathematical\nMechanics , 6:679{684.\nBellman, R. E., Dreyfus, S. E. (1959). Functional approximations and dy-\nnamic programming. Mathematical Tables and Other Aids to Computa-\ntion, 13:247{251.\nBellman, R. E., Kalaba, R., Kotkin, B. (1973). Polynomial approximation|A\nnew computational technique in dynamic programming: Allocation pro-\ncesses. Mathematical Computation , 17:155{161.\nBerry, D. A., Fristedt, B. (1985). Bandit Problems . Chapman and Hall,\nLondon.\nBertsekas, D. P. (1982). Distributed dynamic programming. IEEE Transac-\ntions on Automatic Control , 27:610{616.\nBertsekas, D. P. (1983). Distributed asynchronous computation of \fxed points.\nMathematical Programming , 27:107{120.\nBertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic\nModels . Prentice-Hall, Englewood Cli\u000bs, NJ.\nBertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena\nScienti\fc, Belmont, MA.\nBertsekas, D. P., Tsitsiklis, J. N. (1989). Parallel and Distributed Computa-\ntion: Numerical Methods . Prentice-Hall, Englewood Cli\u000bs, NJ.\nBertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming .\nAthena Scienti\fc, Belmont, MA.\nBiermann, A. W., Fair\feld, J. R. C., Beres, T. R. (1982). Signature table sys-\ntems and learning. IEEE Transactions on Systems, Man, and Cybernetics ,\n12:635{648.\nBishop, C. M. (1995). Neural Networks for Pattern Recognition . Clarendon,\nOxford.\nBooker, L. B. (1982). Intelligent Behavior as an Adaptation to the Task\nEnvironment . Ph.D. thesis, University of Michigan, Ann Arbor.\n15.5. OTHER FRONTIER DIMENSIONS 315\nBoone, G. (1997). Minimum-time control of the acrobot. In 1997 Inter-\nnational Conference on Robotics and Automation , pp. 3281{3287. IEEE\nRobotics and Automation Society.\nBoutilier, C., Dearden, R., Goldszmidt, M. (1995). Exploiting structure in\npolicy construction. In Proceedings of the Fourteenth International Joint\nConference on Arti\fcial Intelligence , pp. 1104{1111. Morgan Kaufmann.\nBoyan, J. A., Moore, A. W. (1995). Generalization in reinforcement learning:\nSafely approximating the value function. In G. Tesauro, D. S. Touretzky,\nand T. Leen (eds.), Advances in Neural Information Processing Systems:\nProceedings of the 1994 Conference , pp. 369{376. MIT Press, Cambridge,\nMA.\nBoyan, J. A., Moore, A. W., Sutton, R. S. (eds.). (1995). Proceedings of\nthe Workshop on Value Function Approximation. Machine Learning Con-\nference 1995 . Technical Report CMU-CS-95-206. School of Computer\nScience, Carnegie Mellon University, Pittsburgh, PA.\nBradtke, S. J. (1993). Reinforcement learning applied to linear quadratic\nregulation. In S. J. Hanson, J. D. Cowan, and C. L. Giles (eds.), Ad-\nvances in Neural Information Processing Systems: Proceedings of the 1992\nConference , pp. 295{302. Morgan Kaufmann, San Mateo, CA.\nBradtke, S. J. (1994). Incremental Dynamic Programming for On-Line Adap-\ntive Optimal Control . Ph.D. thesis, University of Massachusetts, Amherst.\nAppeared as CMPSCI Technical Report 94-62.\nBradtke, S. J., Barto, A. G. (1996). Linear least{squares algorithms for\ntemporal di\u000berence learning. Machine Learning , 22:33{57.\nBradtke, S. J., Ydstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic\ncontrol using policy iteration. In Proceedings of the American Control Con-\nference , pp. 3475{3479. American Automatic Control Council, Evanston,\nIL.\nBradtke, S. J., Du\u000b, M. O. (1995). Reinforcement learning methods for\ncontinuous-time Markov decision problems. In G. Tesauro, D. Touretzky,\nand T. Leen (eds.), Advances in Neural Information Processing Systems:\nProceedings of the 1994 Conference , pp. 393{400. MIT Press, Cambridge,\nMA.\nBridle, J. S. (1990). Training stochastic model recognition algorithms as\nnetworks can lead to maximum mutual information estimates of parame-\nters. In D. S. Touretzky (ed.), Advances in Neural Information Processing\nSystems: Proceedings of the 1989 Conference , pp. 211{217. Morgan Kauf-\nmann, San Mateo, CA.\n316 CHAPTER 15. PROSPECTS\nBroomhead, D. S., Lowe, D. (1988). Multivariable functional interpolation\nand adaptive networks. Complex Systems , 2:321{355.\nBryson, A. E., Jr. (1996). Optimal control|1950 to 1985. IEEE Control\nSystems , 13(3):26{33.\nBush, R. R., Mosteller, F. (1955). Stochastic Models for Learning . Wiley,\nNew York.\nByrne, J. H., Gingrich, K. J., Baxter, D. A. (1990). Computational capa-\nbilities of single neurons: Relationship to simple forms of associative and\nnonassociative learning in aplysia . In R. D. Hawkins and G. H. Bower\n(eds.), Computational Models of Learning , pp. 31{63. Academic Press,\nNew York.\nCamerer, C. (2003). Behavioral game theory: Experiments in strategic inter-\naction . Princeton University Press.\nCampbell, D. T. (1960). Blind variation and selective survival as a general\nstrategy in knowledge-processes. In M. C. Yovits and S. Cameron (eds.),\nSelf-Organizing Systems , pp. 205{231. Pergamon, New York.\nCarlstr\u007f om, J., Nordstr\u007f om, E. (1997). Control of self-similar ATM call tra\u000ec\nby reinforcement learning. In Proceedings of the International Workshop\non Applications of Neural Networks to Telecommunications 3 , pp. 54{62.\nErlbaum, Hillsdale, NJ.\nChapman, D., Kaelbling, L. P. (1991). Input generalization in delayed rein-\nforcement learning: An algorithm and performance comparisons. In Pro-\nceedings of the Twelfth International Conference on Arti\fcial Intelligence ,\npp. 726{731. Morgan Kaufmann, San Mateo, CA.\nChow, C.-S., Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithm\nfor discrete-time stochastic control. IEEE Transactions on Automatic\nControl , 36:898{914.\nChrisman, L. (1992). Reinforcement learning with perceptual aliasing: The\nperceptual distinctions approach. In Proceedings of the Tenth National\nConference on Arti\fcial Intelligence , pp. 183{188. AAAI/MIT Press, Menlo\nPark, CA.\nChristensen, J., Korf, R. E. (1986). A uni\fed theory of heuristic evalua-\ntion functions and its application to learning. In Proceedings of the Fifth\nNational Conference on Arti\fcial Intelligence , pp. 148{152. Morgan Kauf-\nmann, San Mateo, CA.\nCichosz, P. (1995). Truncating temporal di\u000berences: On the e\u000ecient im-\nplementation of TD( \u0015) for reinforcement learning. Journal of Arti\fcial\n15.5. OTHER FRONTIER DIMENSIONS 317\nIntelligence Research , 2:287{318.\nClark, W. A., Farley, B. G. (1955). Generalization of pattern recognition in a\nself-organizing system. In Proceedings of the 1955 Western Joint Computer\nConference , pp. 86{91.\nClouse, J. (1996). On Integrating Apprentice Learning and Reinforcement\nLearning TITLE2 . Ph.D. thesis, University of Massachusetts, Amherst.\nAppeared as CMPSCI Technical Report 96-026.\nClouse, J., Utgo\u000b, P. (1992). A teaching method for reinforcement learning\nsystems. In Proceedings of the Ninth International Machine Learning\nConference , pp. 92{101. Morgan Kaufmann, San Mateo, CA.\nColombetti, M., Dorigo, M. (1994). Training agent to perform sequential\nbehavior. Adaptive Behavior , 2(3):247{275.\nConnell, J. (1989). A colony architecture for an arti\fcial creature. Technical\nReport AI-TR-1151. MIT Arti\fcial Intelligence Laboratory, Cambridge,\nMA.\nConnell, J., Mahadevan, S. (1993). Robot Learning . Kluwer Academic,\nBoston.\nCraik, K. J. W. (1943). The Nature of Explanation . Cambridge University\nPress, Cambridge.\nCrites, R. H. (1996). Large-Scale Dynamic Optimization Using Teams of\nReinforcement Learning Agents . Ph.D. thesis, University of Massachusetts,\nAmherst.\nCrites, R. H., Barto, A. G. (1996). Improving elevator performance using rein-\nforcement learning. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo\n(eds.), Advances in Neural Information Processing Systems: Proceedings\nof the 1995 Conference , pp. 1017{1023. MIT Press, Cambridge, MA.\nCross, J. G. (1973). A stochastic learning model of economic behavior. The\nQuarterly Journal of Economics 87 (2):239-266.\nCurtiss, J. H. (1954). A theoretical comparison of the e\u000eciencies of two\nclassical methods and a Monte Carlo method for computing one component\nof the solution of a set of linear algebraic equations. In H. A. Meyer (ed.),\nSymposium on Monte Carlo Methods , pp. 191{233. Wiley, New York.\nCziko, G. (1995). Without Miracles: Universal Selection Theory and the\nSecond Darvinian Revolution . MIT Press, Cambridge, MA.\nDaniel, J. W. (1976). Splines and e\u000eciency in dynamic programming. Journal\nof Mathematical Analysis and Applications , 54:402{407.\n318 CHAPTER 15. PROSPECTS\nDayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. El-\nman, T. J. Sejnowski, and G. E. Hinton (eds.), Connectionist Models: Pro-\nceedings of the 1990 Summer School , pp. 45{51. Morgan Kaufmann, San\nMateo, CA.\nDayan, P. (1992). The convergence of TD( \u0015) for general \u0015.Machine Learning ,\n8:341{362.\nDayan, P., Hinton, G. E. (1993). Feudal reinforcement learning. In S. J. Han-\nson, J. D. Cohen, and C. L. Giles (eds.), Advances in Neural Informa-\ntion Processing Systems: Proceedings of the 1992 Conference , pp. 271{278.\nMorgan Kaufmann, San Mateo, CA.\nDayan, P., Sejnowski, T. (1994). TD( \u0015) converges with probability 1. Machine\nLearning , 14:295{301.\nDean, T., Lin, S.-H. (1995). Decomposition techniques for planning in stochas-\ntic domains. In Proceedings of the Fourteenth International Joint Confer-\nence on Arti\fcial Intelligence , pp. 1121{1127. Morgan Kaufmann. See\nalso Technical Report CS-95-10, Brown University, Department of Com-\nputer Science, 1995.\nDeJong, G., Spong, M. W. (1994). Swinging up the acrobot: An example\nof intelligent control. In Proceedings of the American Control Conference ,\npp. 2158{2162. American Automatic Control Council, Evanston, IL.\nDenardo, E. V. (1967). Contraction mappings in the theory underlying dy-\nnamic programming. SIAM Review , 9:165{177.\nDennett, D. C. (1978). Brainstorms , pp. 71{89. Bradford/MIT Press, Cam-\nbridge, MA.\nDick, T. (2015). A Regret-full Perspective on Policy Gradient Methods for\nReinforcement Learning . MSc Thesis, University of Alberta.\nDietterich, T. G., Flann, N. S. (1995). Explanation-based learning and rein-\nforcement learning: A uni\fed view. In A. Prieditis and S. Russell (eds.),\nProceedings of the Twelfth International Conference on Machine Learning ,\npp. 176{184. Morgan Kaufmann, San Francisco.\nDoya, K. (1996). Temporal di\u000berence learning in continuous time and space.\nIn D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (eds.), Advances in\nNeural Information Processing Systems: Proceedings of the 1995 Confer-\nence, pp. 1073{1079. MIT Press, Cambridge, MA.\nDoyle, P. G., Snell, J. L. (1984). Random Walks and Electric Networks . The\nMathematical Association of America. Carus Mathematical Monograph\n22.\n15.5. OTHER FRONTIER DIMENSIONS 319\nDreyfus, S. E., Law, A. M. (1977). The Art and Theory of Dynamic Program-\nming . Academic Press, New York.\nDuda, R. O., Hart, P. E. (1973). Pattern Classi\fcation and Scene Analysis .\nWiley, New York.\nDu\u000b, M. O. (1995). Q-learning for bandit problems. In A. Prieditis and\nS. Russell (eds.), Proceedings of the Twelfth International Conference on\nMachine Learning , pp. 209{217. Morgan Kaufmann, San Francisco.\nEstes, W. K. (1950). Toward a statistical theory of learning. Psychololgical\nReview , 57:94{107.\nFarley, B. G., Clark, W. A. (1954). Simulation of self-organizing systems by\ndigital computer. IRE Transactions on Information Theory , 4:76{84.\nFeldbaum, A. A. (1965). Optimal Control Systems . Academic Press, New\nYork.\nFogel, L. J., Owens, A. J., Walsh, M. J. (1966). Arti\fcial intelligence through\nsimulated evolution . John Wiley and Sons.\nFriston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. (1994).\nValue-dependent selection in the brain: Simulation in a synthetic neural\nmodel. Neuroscience , 59:229{243.\nFu, K. S. (1970). Learning control systems|Review and outlook. IEEE\nTransactions on Automatic Control , 15:210{221.\nGalanter, E., Gerstenhaber, M. (1956). On thought: The extrinsic theory.\nPsychological Review , 63:218{227.\nGallant, S. I. (1993). Neural Network Learning and Expert Systems . MIT\nPress, Cambridge, MA.\nGallistel, C. R. (2005). Deconstructing the law of e\u000bect. Games and Economic\nBehavior 52 (2), 410-423.\nG\u007f allmo, O., Asplund, L. (1995). Reinforcement learning by construction\nof hypothetical targets. In J. Alspector, R. Goodman, and T. X. Brown\n(eds.), Proceedings of the International Workshop on Applications of Neural\nNetworks to Telecommunications 2 , pp. 300{307. Erlbaum, Hillsdale, NJ.\nGardner, M. (1973). Mathematical games. Scienti\fc American , 228(1):108{\n115.\nGelperin, A., Hop\feld, J. J., Tank, D. W. (1985). The logic of limax learning.\nIn A. Selverston (ed.), Model Neural Networks and Behavior , pp. 247{261.\nPlenum Press, New York.\nGittins, J. C., Jones, D. M. (1974). A dynamic allocation index for the\n320 CHAPTER 15. PROSPECTS\nsequential design of experiments. In J. Gani, K. Sarkadi, and I. Vincze\n(eds.), Progress in Statistics , pp. 241{266. North-Holland, Amsterdam{\nLondon.\nGoldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and\nMachine Learning . Addison-Wesley, Reading, MA.\nGoldstein, H. (1957). Classical Mechanics . Addison-Wesley, Reading, MA.\nGoodwin, G. C., Sin, K. S. (1984). Adaptive Filtering Prediction and Control .\nPrentice-Hall, Englewood Cli\u000bs, NJ.\nGordon, G. J. (1995). Stable function approximation in dynamic program-\nming. In A. Prieditis and S. Russell (eds.), Proceedings of the Twelfth In-\nternational Conference on Machine Learning , pp. 261{268. Morgan Kauf-\nmann, San Francisco. An expanded version was published as Technical Re-\nport CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995.\nGordon, G. J. (1996). Chattering in SARSA( \u0015). CMU learning lab internal\nreport.\nGordon, G. J. (1996). Stable \ftted reinforcement learning. In D. S. Touret-\nzky, M. C. Mozer, M. E. Hasselmo (eds.), Advances in Neural Information\nProcessing Systems: Proceedings of the 1995 Conference , pp. 1052{1058.\nMIT Press, Cambridge, MA.\nGordon, G. J. (2001). Reinforcement learning with function approximation\nconverges to a region. Advances in neural information processing systems .\nGreensmith, E., Bartlett, P. L., Baxter, J. (2001). Variance reduction tech-\nniques for gradient estimates in reinforcement learning. In Advances in\nNeural Information Processing Systems: Proceedings of the 2000 Confer-\nence, pp. 1507{1514.\nGreensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction tech-\nniques for gradient estimates in reinforcement learning. Journal of Machine\nLearning Research 5 , 1471-1530.\nGri\u000eth, A. K. (1966). A new machine learning technique applied to the game\nof checkers. Technical Report Project MAC, Arti\fcial Intelligence Memo\n94. Massachusetts Institute of Technology, Cambridge, MA.\nGri\u000eth, A. K. (1974). A comparison and evaluation of three machine learning\nprocedures as applied to the game of checkers. Arti\fcial Intelligence ,\n5:137{148.\nGullapalli, V. (1990). A stochastic reinforcement algorithm for learning real-\nvalued functions. Neural Networks , 3:671{692.\nGurvits, L., Lin, L.-J., Hanson, S. J. (1994). Incremental learning of evalua-\n15.5. OTHER FRONTIER DIMENSIONS 321\ntion functions for absorbing Markov chains: New methods and theorems.\nPreprint.\nHampson, S. E. (1983). A Neural Model of Adaptive Behavior . Ph.D. thesis,\nUniversity of California, Irvine.\nHampson, S. E. (1989). Connectionist Problem Solving: Computational As-\npects of Biological Learning . Birkhauser, Boston.\nHawkins, R. D., Kandel, E. R. (1984). Is there a cell-biological alphabet for\nsimple forms of learning? Psychological Review , 91:375{391.\nHerrnstein, R. J. (1970). On the Law of E\u000bect. Journal of the Experimental\nAnalysis of Behavior 13 (2), 243-266.\nHersh, R., Griego, R. J. (1969). Brownian motion and potential theory.\nScienti\fc American , 220:66{74.\nHesterberg, T. C. (1988), Advances in importance sampling , Ph.D. Disserta-\ntion, Statistics Department, Stanford University.\nHilgard, E. R., Bower, G. H. (1975). Theories of Learning . Prentice-Hall,\nEnglewood Cli\u000bs, NJ.\nHinton, G. E. (1984). Distributed representations. Technical Report CMU-\nCS-84-157. Department of Computer Science, Carnegie-Mellon University,\nPittsburgh, PA.\nHochreiter, S., Schmidhuber, J. (1997). LTSM can solve hard time lag prob-\nlems. In Advances in Neural Information Processing Systems: Proceedings\nof the 1996 Conference , pp. 473{479. MIT Press, Cambridge, MA.\nHolland, J. H. (1975). Adaptation in Natural and Arti\fcial Systems . Univer-\nsity of Michigan Press, Ann Arbor.\nHolland, J. H. (1976). Adaptation. In R. Rosen and F. M. Snell (eds.),\nProgress in Theoretical Biology , vol. 4, pp. 263{293. Academic Press, New\nYork.\nHolland, J. H. (1986). Escaping brittleness: The possibility of general-purpose\nlearning algorithms applied to rule-based systems. In R. S. Michalski,\nJ. G. Carbonell, and T. M. Mitchell (eds.), Machine Learning: An Arti\fcial\nIntelligence Approach , vol. 2, pp. 593{623. Morgan Kaufmann, San Mateo,\nCA.\nHouk, J. C., Adams, J. L., Barto, A. G. (1995). A model of how the basal\nganglia generates and uses neural signals that predict reinforcement. In\nJ. C. Houk, J. L. Davis, and D. G. Beiser (eds.), Models of Information\nProcessing in the Basal Ganglia , pp. 249{270. MIT Press, Cambridge, MA.\n322 CHAPTER 15. PROSPECTS\nHoward, R. (1960). Dynamic Programming and Markov Processes . MIT\nPress, Cambridge, MA.\nHull, C. L. (1943). Principles of Behavior . Appleton-Century, New York.\nHull, C. L. (1952). A Behavior System . Wiley, New York.\nJaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of\nstochastic iterative dynamic programming algorithms. Neural Computa-\ntion, 6:1185{1201.\nJaakkola, T., Singh, S. P., Jordan, M. I. (1995). Reinforcement learning algo-\nrithm for partially observable Markov decision problems. In G. Tesauro,\nD. S. Touretzky, T. Leen (eds.), Advances in Neural Information Process-\ning Systems: Proceedings of the 1994 Conference , pp. 345{352. MIT Press,\nCambridge, MA.\nKaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Pre-\nliminary results. In Proceedings of the Tenth International Conference on\nMachine Learning , pp. 167{173. Morgan Kaufmann, San Mateo, CA.\nKaelbling, L. P. (1993b). Learning in Embedded Systems . MIT Press, Cam-\nbridge, MA.\nKaelbling, L. P. (ed.). (1996). Special issue of Machine Learning on reinforce-\nment learning, 22.\nKaelbling, L. P., Littman, M. L., Moore, A. W. (1996). Reinforcement learn-\ning: A survey. Journal of Arti\fcial Intelligence Research , 4:237{285.\nKakutani, S. (1945). Markov processes and the Dirichlet problem. Proceedings\nof the Japan Academy , 21:227{233.\nKalos, M. H., Whitlock, P. A. (1986). Monte Carlo Methods . Wiley, New\nYork.\nKanerva, P. (1988). Sparse Distributed Memory . MIT Press, Cambridge, MA.\nKanerva, P. (1993). Sparse distributed memory and related models. In\nM. H. Hassoun (ed.), Associative Neural Memories: Theory and Imple-\nmentation , pp. 50{76. Oxford University Press, New York.\nKashyap, R. L., Blaydon, C. C., Fu, K. S. (1970). Stochastic approximation.\nIn J. M. Mendel and K. S. Fu (eds.), Adaptive, Learning, and Pattern\nRecognition Systems: Theory and Applications , pp. 329{355. Academic\nPress, New York.\nKeerthi, S. S., Ravindran, B. (1997). Reinforcement learning. In E. Fiesler and\nR. Beale (eds.), Handbook of Neural Computation , C3. Oxford University\nPress, New York.\n15.5. OTHER FRONTIER DIMENSIONS 323\nKimble, G. A. (1961). Hilgard and Marquis' Conditioning and Learning .\nAppleton-Century-Crofts, New York.\nKimble, G. A. (1967). Foundations of Conditioning and Learning . Appleton-\nCentury-Crofts, New York.\nKlopf, A. H. (1972). Brain function and adaptive systems|A heterostatic\ntheory. Technical Report AFCRL-72-0164, Air Force Cambridge Research\nLaboratories, Bedford, MA. A summary appears in Proceedings of the In-\nternational Conference on Systems, Man, and Cybernetics . IEEE Systems,\nMan, and Cybernetics Society, Dallas, TX, 1974.\nKlopf, A. H. (1975). A comparison of natural and arti\fcial intelligence.\nSIGART Newsletter , 53:11{13.\nKlopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning,\nand Intelligence . Hemisphere, Washington, DC.\nKlopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiol-\nogy, 16:85{125.\nKohonen, T. (1977). Associative Memory: A System Theoretic Approach .\nSpringer-Verlag, Berlin.\nKoller, D., Friedman, N. (2009). Probabilistic Graphical Models: Principles\nand Techniques . MIT Press, 2009.\nKorf, R. E. (1988). Optimal path \fnding algorithms. In L. N. Kanal and\nV. Kumar (eds.), Search in Arti\fcial Intelligence , pp. 223{267. Springer\nVerlag, Berlin.\nKoza, J. R. (1992). Genetic programming: On the programming of computers\nby means of natural selection (Vol. 1). MIT press.\nKraft, L. G., Campagna, D. P. (1990). A summary comparison of CMAC\nneural network and traditional adaptive control systems. In T. Miller,\nR. S. Sutton, and P. J. Werbos (eds.), Neural Networks for Control , pp. 143{\n169. MIT Press, Cambridge, MA.\nKraft, L. G., Miller, W. T., Dietz, D. (1992). Development and application\nof CMAC neural network-based control. In D. A. White and D. A. Sofge\n(eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Ap-\nproaches , pp. 215{232. Van Nostrand Reinhold, New York.\nKumar, P. R., Varaiya, P. (1986). Stochastic Systems: Estimation, Identi\fca-\ntion, and Adaptive Control . Prentice-Hall, Englewood Cli\u000bs, NJ.\nKumar, P. R. (1985). A survey of some results in stochastic adaptive control.\nSIAM Journal of Control and Optimization , 23:329{380.\n324 CHAPTER 15. PROSPECTS\nKumar, V., Kanal, L. N. (1988). The CDP: A unifying formulation for heuris-\ntic search, dynamic programming, and branch-and-bound. In L. N. Kanal\nand V. Kumar (eds.), Search in Arti\fcial Intelligence , pp. 1{37. Springer-\nVerlag, Berlin.\nKushner, H. J., Dupuis, P. (1992). Numerical Methods for Stochastic Control\nProblems in Continuous Time . Springer-Verlag, New York.\nLai, T. L., Robbins, H. (1985). Asymptotically e\u000ecient adaptive allocation\nrules. Advances in applied mathematics , 6(1):4{22.\nLang, K. J., Waibel, A. H., Hinton, G. E. (1990). A time-delay neural network\narchitecture for isolated word recognition. Neural Networks , 3:33{43.\nLin, C.-S., Kim, H. (1991). CMAC-based adaptive critic self-learning control.\nIEEE Transactions on Neural Networks , 2:530{533.\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learn-\ning, planning and teaching. Machine Learning , 8:293{321.\nLin, L.-J., Mitchell, T. (1992). Reinforcement learning with hidden states.\nInProceedings of the Second International Conference on Simulation of\nAdaptive Behavior: From Animals to Animats , pp. 271{280. MIT Press,\nCambridge, MA.\nLittman, M. L. (1994). Markov games as a framework for multi-agent rein-\nforcement learning. In Proceedings of the Eleventh International Confer-\nence on Machine Learning , pp. 157{163. Morgan Kaufmann, San Fran-\ncisco.\nLittman, M. L., Cassandra, A. R., Kaelbling, L. P. (1995). Learning policies\nfor partially observable environments: Scaling up. In A. Prieditis and\nS. Russell (eds.), Proceedings of the Twelfth International Conference on\nMachine Learning , pp. 362{370. Morgan Kaufmann, San Francisco.\nLittman, M. L., Dean, T. L., Kaelbling, L. P. (1995). On the complexity of\nsolving Markov decision problems. In Proceedings of the Eleventh Annual\nConference on Uncertainty in Arti\fcial Intelligence , pp. 394{402.\nLiu, J. S. (2001). Monte Carlo strategies in scienti\fc computing . Berlin,\nSpringer-Verlag.\nLjung, L., S\u007f oderstrom, T. (1983). Theory and Practice of Recursive Identi\f-\ncation . MIT Press, Cambridge, MA.\nLovejoy, W. S. (1991). A survey of algorithmic methods for partially observed\nMarkov decision processes. Annals of Operations Research , 28:47{66.\nLuce, D. (1959). Individual Choice Behavior . Wiley, New York.\n15.5. OTHER FRONTIER DIMENSIONS 325\nMaclin, R., Shavlik, J. W. (1994). Incorporating advice into agents that learn\nfrom reinforcements. In Proceedings of the Twelfth National Conference\non Arti\fcial Intelligence , pp. 694{699. AAAI Press, Menlo Park, CA.\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations,\nalgorithms, and empirical results. Machine Learning , 22:159{196.\nMarkey, K. L. (1994). E\u000ecient learning of multiple degree-of-freedom control\nproblems with quasi-independent Q-agents. In M. C. Mozer, P. Smolensky,\nD. S. Touretzky, J. L. Elman, and A. S. Weigend (eds.), Proceedings of the\n1990 Connectionist Models Summer School . Erlbaum, Hillsdale, NJ.\nMazur, J. E. (1994). Learning and Behavior , 3rd ed. Prentice-Hall, Englewood\nCli\u000bs, NJ.\nMcCallum, A. K. (1993). Overcoming incomplete perception with utile dis-\ntinction memory. In Proceedings of the Tenth International Conference on\nMachine Learning , pp. 190{196. Morgan Kaufmann, San Mateo, CA.\nMcCallum, A. K. (1995). Reinforcement Learning with Selective Perception\nand Hidden State . Ph.D. thesis, University of Rochester, Rochester, NY.\nMendel, J. M. (1966). A survey of learning control systems. ISA Transactions ,\n5:297{303.\nMendel, J. M., McLaren, R. W. (1970). Reinforcement learning control and\npattern recognition systems. In J. M. Mendel and K. S. Fu (eds.), Adap-\ntive, Learning and Pattern Recognition Systems: Theory and Applications ,\npp. 287{318. Academic Press, New York.\nMichie, D. (1961). Trial and error. In S. A. Barnett and A. McLaren (eds.),\nScience Survey, Part 2 , pp. 129{145. Penguin, Harmondsworth.\nMichie, D. (1963). Experiments on the mechanisation of game learning. 1.\ncharacterization of the model and its parameters. Computer Journal ,\n1:232{263.\nMichie, D. (1974). On Machine Intelligence . Edinburgh University Press,\nEdinburgh.\nMichie, D., Chambers, R. A. (1968). BOXES: An experiment in adaptive\ncontrol. In E. Dale and D. Michie (eds.), Machine Intelligence 2 , pp. 137{\n152. Oliver and Boyd, Edinburgh.\nMiller, S., Williams, R. J. (1992). Learning to control a bioreactor using a neu-\nral net Dyna-Q system. In Proceedings of the Seventh Yale Workshop on\nAdaptive and Learning Systems , pp. 167{172. Center for Systems Science,\nDunham Laboratory, Yale University, New Haven.\nMiller, W. T., Scalera, S. M., Kim, A. (1994). Neural network control of\n326 CHAPTER 15. PROSPECTS\ndynamic balance for a biped walking robot. In Proceedings of the Eighth\nYale Workshop on Adaptive and Learning Systems , pp. 156{161. Center\nfor Systems Science, Dunham Laboratory, Yale University, New Haven.\nMinsky, M. L. (1954). Theory of Neural-Analog Reinforcement Systems and\nIts Application to the Brain-Model Problem . Ph.D. thesis, Princeton Uni-\nversity.\nMinsky, M. L. (1961). Steps toward arti\fcial intelligence. Proceedings of the\nInstitute of Radio Engineers , 49:8{30. Reprinted in E. A. Feigenbaum and\nJ. Feldman (eds.), Computers and Thought , pp. 406{450. McGraw-Hill,\nNew York, 1963.\nMinsky, M. L. (1967). Computation: Finite and In\fnite Machines . Prentice-\nHall, Englewood Cli\u000bs, NJ.\nMontague, P. R., Dayan, P., Sejnowski, T. J. (1996). A framework for mesen-\ncephalic dopamine systems based on predictive Hebbian learning. Journal\nof Neuroscience , 16:1936{1947.\nMoore, A. W. (1990). E\u000ecient Memory-Based Learning for Robot Control .\nPh.D. thesis, University of Cambridge.\nMoore, A. W. (1994). The parti-game algorithm for variable resolution rein-\nforcement learning in multidimensional spaces. In J. D. Cohen, G. Tesauro\nand J. Alspector (eds.), Advances in Neural Information Processing Sys-\ntems: Proceedings of the 1993 Conference , pp. 711{718. Morgan Kauf-\nmann, San Francisco.\nMoore, A. W., Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement\nlearning with less data and less real time. Machine Learning , 13:103{130.\nMoore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., and\nBarto, A. G. (1986). Simulation of the classically conditioned nictitating\nmembrane response by a neuron-like adaptive element: I. Response to-\npography, neuronal \fring, and interstimulus intervals. Behavioural Brain\nResearch , 21:143{154.\nNarendra, K. S., Thathachar, M. A. L. (1974). Learning automata|A survey.\nIEEE Transactions on Systems, Man, and Cybernetics , 4:323{334.\nNarendra, K. S., Thathachar, M. A. L. (1989). Learning Automata: An\nIntroduction . Prentice-Hall, Englewood Cli\u000bs, NJ.\nNarendra, K. S., Wheeler, R. M. (1986). Decentralized learning in \fnite\nMarkov chains. IEEE Transactions on Automatic Control , AC31(6):519{\n526.\nNie, J., Haykin, S. (1996). A dynamic channel assignment policy through\n15.5. OTHER FRONTIER DIMENSIONS 327\nQ-learning. CRL Report 334. Communications Research Laboratory,\nMcMaster University, Hamilton, Ontario.\nNow\u0013 e, A., Vrancx, P., De Hauwere, Y. M. (2012). Game theory and multi-\nagent reinforcement learning. In Reinforcement Learning (pp. 441-470).\nSpringer Berlin Heidelberg.\nPage, C. V. (1977). Heuristics for signature table analysis as a pattern recog-\nnition technique. IEEE Transactions on Systems, Man, and Cybernetics ,\n7:77{86.\nParr, R., Russell, S. (1995). Approximating optimal policies for partially\nobservable stochastic domains. In Proceedings of the Fourteenth Interna-\ntional Joint Conference on Arti\fcial Intelligence , pp. 1088{1094. Morgan\nKaufmann.\nPavlov, P. I. (1927). Conditioned Re\rexes . Oxford University Press, London.\nPearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Prob-\nlem Solving . Addison-Wesley, Reading, MA.\nPearl, J. (1995). Causal diagrams for empirical research. Biometrika , 82(4),\n669-688.\nBalke, A., Pearl, J. (1994). Counterfactual probabilities: Computational\nmethods, bounds and applications. In Proceedings of the Tenth Inter-\nnational Conference on Uncertainty in Arti\fcial Intelligence (pp. 46-54).\nMorgan Kaufmann.\nPeng, J. (1993). E\u000ecient Dynamic Programming-Based Learning for Control .\nPh.D. thesis, Northeastern University, Boston.\nPeng, J., Williams, R. J. (1993). E\u000ecient learning and planning within the\nDyna framework. Adaptive Behavior , 1(4):437{454.\nPeng, J., Williams, R. J. (1994). Incremental multi-step Q-learning. In\nW. W. Cohen and H. Hirsh (eds.), Proceedings of the Eleventh International\nConference on Machine Learning , pp. 226{232. Morgan Kaufmann, San\nFrancisco.\nPeng, J., Williams, R. J. (1996). Incremental multi-step Q-learning. Machine\nLearning , 22:283{290.\nPhansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization\nalgorithms for generalized learning automata. Neural Computation , 7:950{\n973.\nPoggio, T., Girosi, F. (1989). A theory of networks for approximation and\nlearning. A.I. Memo 1140. Arti\fcial Intelligence Laboratory, Massachusetts\nInstitute of Technology, Cambridge, MA.\n328 CHAPTER 15. PROSPECTS\nPoggio, T., Girosi, F. (1990). Regularization algorithms for learning that are\nequivalent to multilayer networks. Science , 247:978{982.\nPowell, M. J. D. (1987). Radial basis functions for multivariate interpola-\ntion: A review. In J. C. Mason and M. G. Cox (eds.), Algorithms for\nApproximation , pp. 143{167. Clarendon Press, Oxford.\nPowell, W. B. (2011). Approximate Dynamic Programming: Solving the Curses\nof Dimensionality , Second edition. John Wiley and Sons.\nPrecup, D., Sutton, R. S., Dasgupta, S. (2001). O\u000b-policy temporal-di\u000berence\nlearning with function approximation. In Proceedings of the 18th Interna-\ntional Conference on Machine Learning .\nPrecup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for o\u000b-policy\npolicy evaluation. In Proceedings of the 17th International Conference on\nMachine Learning , pp. 759{766. Morgan Kaufmann.\nPuterman, M. L. (1994). Markov Decision Problems . Wiley, New York.\nPuterman, M. L., Shin, M. C. (1978). Modi\fed policy iteration algorithms\nfor discounted Markov decision problems. Management Science , 24:1127{\n1137.\nReetz, D. (1977). Approximate solutions of a discounted Markovian decision\nprocess. Bonner Mathematische Schriften , 98:77{92.\nRing, M. B. (1994). Continual Learning in Reinforcement Environments .\nPh.D. thesis, University of Texas, Austin.\nRivest, R. L., Schapire, R. E. (1987). Diversity-based inference of \fnite\nautomata. In Proceedings of the Twenty-Eighth Annual Symposium on\nFoundations of Computer Science , pp. 78{87. Computer Society Press of\nthe IEEE, Washington, DC.\nRobbins, H. (1952). Some aspects of the sequential design of experiments.\nBulletin of the American Mathematical Society , 58:527{535.\nRobertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon.\nInside Backgammon , 2:14{22.\nRosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the\nTheory of Brain Mechanisms . Spartan Books, Washington, DC.\nRoss, S. (1983). Introduction to Stochastic Dynamic Programming . Academic\nPress, New York.\nRubinstein, R. Y. (1981). Simulation and the Monte Carlo Method . Wiley,\nNew York.\nRumelhart, D. E., Hinton, G. E., Williams, R. J. (1986). Learning internal\n15.5. OTHER FRONTIER DIMENSIONS 329\nrepresentations by error propagation. In D. E. Rumelhart and J. L. Mc-\nClelland (eds.), Parallel Distributed Processing: Explorations in the Mi-\ncrostructure of Cognition , vol. I, Foundations . Bradford/MIT Press, Cam-\nbridge, MA.\nRummery, G. A. (1995). Problem Solving with Reinforcement Learning .\nPh.D. thesis, Cambridge University.\nRummery, G. A., Niranjan, M. (1994). On-line Q-learning using connection-\nist systems. Technical Report CUED/F-INFENG/TR 166. Engineering\nDepartment, Cambridge University.\nRussell, S., Norvig, P. (2009). Arti\fcial Intelligence: A Modern Approach .\nPrentice-Hall, Englewood Cli\u000bs, NJ.\nRust, J. (1996). Numerical dynamic programming in economics. In H. Am-\nman, D. Kendrick, and J. Rust (eds.), Handbook of Computational Eco-\nnomics , pp. 614{722. Elsevier, Amsterdam.\nSamuel, A. L. (1959). Some studies in machine learning using the game\nof checkers. IBM Journal on Research and Development , 3:211{229.\nReprinted in E. A. Feigenbaum and J. Feldman (eds.), Computers and\nThought , pp. 71{105. McGraw-Hill, New York, 1963.\nSamuel, A. L. (1967). Some studies in machine learning using the game of\ncheckers. II|Recent progress. IBM Journal on Research and Develop-\nment , 11:601{617.\nSchultz, D. G., Melsa, J. L. (1967). State Functions and Linear Control\nSystems . McGraw-Hill, New York.\nSchultz, W., Dayan, P., Montague, P. R. (1997). A neural substrate of pre-\ndiction and reward. Science , 275:1593{1598.\nSchwartz, A. (1993). A reinforcement learning method for maximizing undis-\ncounted rewards. In Proceedings of the Tenth International Conference on\nMachine Learning , pp. 298{305. Morgan Kaufmann, San Mateo, CA.\nSchweitzer, P. J., Seidmann, A. (1985). Generalized polynomial approxima-\ntions in Markovian decision processes. Journal of Mathematical Analysis\nand Applications , 110:568{582.\nSelfridge, O. J., Sutton, R. S., Barto, A. G. (1985). Training and tracking in\nrobotics. In A. Joshi (ed.), Proceedings of the Ninth International Joint\nConference on Arti\fcial Intelligence , pp. 670{672. Morgan Kaufmann, San\nMateo, CA.\nShannon, C. E. (1950). Programming a computer for playing chess. Philo-\nsophical Magazine , 41:256{275.\n330 CHAPTER 15. PROSPECTS\nShelton, C. R. (2001). Importance Sampling for Reinforcement Learning with\nMultiple Objectives . PhD thesis, Massachusetts Institute of Technology.\nShewchuk, J., Dean, T. (1990). Towards learning time-varying functions with\nhigh input dimensionality. In Proceedings of the Fifth IEEE International\nSymposium on Intelligent Control , pp. 383{388. IEEE Computer Society\nPress, Los Alamitos, CA.\nSi, J., Barto, A., Powell, W., Wunsch, D. (Eds.). (2004). Handbook of learning\nand approximate dynamic programming . John Wiley and Sons.\nSingh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract\nmodels. In Proceedings of the Tenth National Conference on Arti\fcial\nIntelligence , pp. 202{207. AAAI/MIT Press, Menlo Park, CA.\nSingh, S. P. (1992b). Scaling reinforcement learning algorithms by learning\nvariable temporal resolution models. In Proceedings of the Ninth Inter-\nnational Machine Learning Conference , pp. 406{415. Morgan Kaufmann,\nSan Mateo, CA.\nSingh, S. P. (1993). Learning to Solve Markovian Decision Processes . Ph.D. the-\nsis, University of Massachusetts, Amherst. Appeared as CMPSCI Techni-\ncal Report 93-77.\nSingh, S. P., Bertsekas, D. (1997). Reinforcement learning for dynamic channel\nallocation in cellular telephone systems. In Advances in Neural Information\nProcessing Systems: Proceedings of the 1996 Conference , pp. 974{980. MIT\nPress, Cambridge, MA.\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-\nestimation in partially observable Markovian decision problems. In W. W. Co-\nhen and H. Hirsch (eds.), Proceedings of the Eleventh International Con-\nference on Machine Learning , pp. 284{292. Morgan Kaufmann, San Fran-\ncisco.\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1995). Reinforcement learing with\nsoft state aggregation. In G. Tesauro, D. S. Touretzky, T. Leen (eds.),\nAdvances in Neural Information Processing Systems: Proceedings of the\n1994 Conference , pp. 359{368. MIT Press, Cambridge, MA.\nSingh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing\neligibility traces. Machine Learning , 22:123{158.\nSivarajan, K. N., McEliece, R. J., Ketchum, J. W. (1990). Dynamic channel\nassignment in cellular radio. In Proceedings of the 40th Vehicular Technol-\nogy Conference , pp. 631{637.\nSkinner, B. F. (1938). The Behavior of Organisms: An Experimental Analysis .\n15.5. OTHER FRONTIER DIMENSIONS 331\nAppleton-Century, New York.\nSofge, D. A., White, D. A. (1992). Applied learning: Optimal control for\nmanufacturing. In D. A. White and D. A. Sofge (eds.), Handbook of\nIntelligent Control: Neural, Fuzzy, and Adaptive Approaches , pp. 259{281.\nVan Nostrand Reinhold, New York.\nSpong, M. W. (1994). Swing up control of the acrobot. In Proceedings of the\n1994 IEEE Conference on Robotics and Automation , pp. 2356-2361. IEEE\nComputer Society Press, Los Alamitos, CA.\nStaddon, J. E. R. (1983). Adaptive Behavior and Learning . Cambridge\nUniversity Press, Cambridge.\nSutton, R. S. (1978a). Learning theory support for a single channel theory of\nthe brain. Unpublished report.\nSutton, R. S. (1978b). Single channel theory: A neuronal theory of learn-\ning. Brain Theory Newsletter , 4:72{75. Center for Systems Neuroscience,\nUniversity of Massachusetts, Amherst, MA.\nSutton, R. S. (1978c). A uni\fed theory of expectation in classical and instru-\nmental conditioning. Bachelors thesis, Stanford University.\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning .\nPh.D. thesis, University of Massachusetts, Amherst.\nSutton, R. S. (1988). Learning to predict by the method of temporal di\u000ber-\nences. Machine Learning , 3:9{44.\nSutton, R. S. (1990). Integrated architectures for learning, planning, and\nreacting based on approximating dynamic programming. In Proceedings\nof the Seventh International Conference on Machine Learning , pp. 216{\n224. Morgan Kaufmann, San Mateo, CA.\nSutton, R. S. (1991a). Dyna, an integrated architecture for learning, planning,\nand reacting. SIGART Bulletin , 2:160{163. ACM Press.\nSutton, R. S. (1991b). Planning by incremental dynamic programming. In\nL. A. Birnbaum and G. C. Collins (eds.), Proceedings of the Eighth Interna-\ntional Workshop on Machine Learning , pp. 353{357. Morgan Kaufmann,\nSan Mateo, CA.\nSutton, R. S. (1995). TD models: Modeling the world at a mixture of time\nscales. In A. Prieditis and S. Russell (eds.), Proceedings of the Twelfth In-\nternational Conference on Machine Learning , pp. 531{539. Morgan Kauf-\nmann, San Francisco.\nSutton, R. S. (1996). Generalization in reinforcement learning: Successful\nexamples using sparse coarse coding. In D. S. Touretzky, M. C. Mozer\n332 CHAPTER 15. PROSPECTS\nand M. E. Hasselmo (eds.), Advances in Neural Information Processing\nSystems: Proceedings of the 1995 Conference , pp. 1038{1044. MIT Press,\nCambridge, MA.\nSutton, R. S. (ed.). (1992). Special issue of Machine Learning on reinforce-\nment learning, 8. Also published as Reinforcement Learning . Kluwer\nAcademic, Boston, 1992.\nSutton, R. S., Barto, A. G. (1981a). Toward a modern theory of adaptive\nnetworks: Expectation and prediction. Psychological Review , 88:135{170.\nSutton, R. S., Barto, A. G. (1981b). An adaptive network that constructs\nand uses an internal model of its world. Cognition and Brain Theory ,\n3:217{246.\nSutton, R. S., Barto, A. G. (1987). A temporal-di\u000berence model of classi-\ncal conditioning. In Proceedings of the Ninth Annual Conference of the\nCognitive Science Society , pp. 355-378. Erlbaum, Hillsdale, NJ.\nSutton, R. S., Barto, A. G. (1990). Time-derivative models of Pavlovian\nreinforcement. In M. Gabriel and J. Moore (eds.), Learning and Com-\nputational Neuroscience: Foundations of Adaptive Networks , pp. 497{537.\nMIT Press, Cambridge, MA.\nSutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new\nQ(\u0015) with interim forward view and Monte Carlo equivalence. In Proceed-\nings of the 31st International Conference on Machine Learning , Beijing,\nChina.\nSutton, R. S., Pinette, B. (1985). The learning of world models by connec-\ntionist networks. In Proceedings of the Seventh Annual Conference of the\nCognitive Science Society , pp. 54{64.\nSutton, R. S., Singh, S. (1994). On bias and step size in temporal-di\u000berence\nlearning. In Proceedings of the Eighth Yale Workshop on Adaptive and\nLearning Systems , pp. 91{96. Center for Systems Science, Dunham Labo-\nratory, Yale University, New Haven.\nSutton, R. S., Whitehead, D. S. (1993). Online learning with random repre-\nsentations. In Proceedings of the Tenth International Machine Learning\nConference , pp. 314{321. Morgan Kaufmann, San Mateo, CA.\nSzepesvri, C. (2010). Algorithms for reinforcement learning. Synthesis Lec-\ntures on Arti\fcial Intelligence and Machine Learning 4(1), 1{103.\nSzita, I. (2012). Reinforcement learning in games. In Reinforcement Learning\n(pp. 539-577). Springer Berlin Heidelberg.\nTadepalli, P., Ok, D. (1994). H-learning: A reinforcement learning method to\n15.5. OTHER FRONTIER DIMENSIONS 333\noptimize undiscounted average reward. Technical Report 94-30-01. Oregon\nState University, Computer Science Department, Corvallis.\nTan, M. (1991). Learning a cost-sensitive internal representation for reinforce-\nment learning. In L. A. Birnbaum and G. C. Collins (eds.), Proceedings\nof the Eighth International Workshop on Machine Learning , pp. 358{362.\nMorgan Kaufmann, San Mateo, CA.\nTan, M. (1993). Multi-agent reinforcement learning: Independent vs. coop-\nerative agents. In Proceedings of the Tenth International Conference on\nMachine Learning , pp. 330{337. Morgan Kaufmann, San Mateo, CA.\nTesauro, G. J. (1986). Simple neural models of classical conditioning. Biolog-\nical Cybernetics , 55:187{200.\nTesauro, G. J. (1992). Practical issues in temporal di\u000berence learning. Ma-\nchine Learning , 8:257{277.\nTesauro, G. J. (1994). TD-Gammon, a self-teaching backgammon program,\nachieves master-level play. Neural Computation , 6(2):215{219.\nTesauro, G. J. (1995). Temporal di\u000berence learning and TD-Gammon. Com-\nmunications of the ACM , 38:58{68.\nTesauro, G. J., Galperin, G. R. (1997). On-line policy improvement using\nMonte-Carlo search. In Advances in Neural Information Processing Sys-\ntems: Proceedings of the 1996 Conference , pp. 1068{1074. MIT Press,\nCambridge, MA.\nTham, C. K. (1994). Modular On-Line Function Approximation for Scaling\nup Reinforcement Learning . PhD thesis, Cambridge University.\nThathachar, M. A. L. and Sastry, P. S. (1985). A new approach to the design\nof reinforcement schemes for learning automata. IEEE Transactions on\nSystems, Man, and Cybernetics , 15:168{175.\nThompson, W. R. (1933). On the likelihood that one unknown probability\nexceeds another in view of the evidence of two samples. Biometrika ,\n25:285{294.\nThompson, W. R. (1934). On the theory of apportionment. American Journal\nof Mathematics , 57:450{457.\nThorndike, E. L. (1911). Animal Intelligence . Hafner, Darien, CT.\nThorp, E. O. (1966). Beat the Dealer: A Winning Strategy for the Game of\nTwenty-One . Random House, New York.\nTolman, E. C. (1932). Purposive Behavior in Animals and Men . Century,\nNew York.\n334 CHAPTER 15. PROSPECTS\nTsetlin, M. L. (1973). Automaton Theory and Modeling of Biological Systems .\nAcademic Press, New York.\nTsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-\nlearning. Machine Learning , 16:185{202.\nTsitsiklis, J. N. (2002). On the convergence of optimistic policy iteration.\nJournal of Machine Learning Research , 3:59{72.\nTsitsiklis, J. N. and Van Roy, B. (1996). Feature-based methods for large\nscale dynamic programming. Machine Learning , 22:59{94.\nTsitsiklis, J. N., Van Roy, B. (1997). An analysis of temporal-di\u000berence\nlearning with function approximation. IEEE Transactions on Automatic\nControl , 42:674{690.\nTsitsiklis, J. N., Van Roy, B. (1999). Average cost temporal-di\u000berence learn-\ning. Automatica , 35:1799{1808. Also: Technical Report LIDS-P-2390.\nLaboratory for Information and Decision Systems, Massachusetts Institute\nof Technology, Cambridge, MA, 1997.\nTuring, A. M. (1950). Computing machinery and intelligence. Mind 433{460.\nTuring, A. M. (1948). Intelligent Machinery, A Heretical Theory. The Turing\nTest: Verbal Behavior as the Hallmark of Intelligence , 105.\nUngar, L. H. (1990). A bioreactor benchmark for adaptive network-based\nprocess control. In W. T. Miller, R. S. Sutton, and P. J. Werbos (eds.),\nNeural Networks for Control , pp. 387{402. MIT Press, Cambridge, MA.\nUrbanowicz, R. J., Moore, J. H. (2009). Learning classi\fer systems: A com-\nplete introduction, review, and roadmap. Journal of Arti\fcial Evolution\nand Applications .\nWaltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning\ncontrol systems. IEEE Transactions on Automatic Control , 10:390{398.\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards . Ph.D. thesis,\nCambridge University.\nWatkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine Learning ,\n8:279{292.\nWiering, M., Van Otterlo, M. (2012). Reinforcement Learning . Springer Berlin\nHeidelberg.\nWerbos, P. J. (1977). Advanced forecasting methods for global crisis warning\nand models of intelligence. General Systems Yearbook , 22:25{38.\nWerbos, P. J. (1982). Applications of advances in nonlinear sensitivity analy-\nsis. In R. F. Drenick and F. Kozin (eds.), System Modeling and Optimiza-\n15.5. OTHER FRONTIER DIMENSIONS 335\ntion, pp. 762{770. Springer-Verlag, Berlin.\nWerbos, P. J. (1987). Building and understanding adaptive systems: A statis-\ntical/numerical approach to factory automation and brain research. IEEE\nTransactions on Systems, Man, and Cybernetics , 17:7{20.\nWerbos, P. J. (1988). Generalization of back propagation with applications to\na recurrent gas market model. Neural Networks , 1:339{356.\nWerbos, P. J. (1989). Neural networks for control and system identi\fcation. In\nProceedings of the 28th Conference on Decision and Control , pp. 260{265.\nIEEE Control Systems Society.\nWerbos, P. J. (1990). Consistency of HDP applied to a simple reinforcement\nlearning problem. Neural Networks , 3:179{189.\nWerbos, P. J. (1992). Approximate dynamic programming for real-time con-\ntrol and neural modeling. In D. A. White and D. A. Sofge (eds.), Handbook\nof Intelligent Control: Neural, Fuzzy, and Adaptive Approaches , pp. 493{\n525. Van Nostrand Reinhold, New York.\nWhite, D. J. (1969). Dynamic Programming . Holden-Day, San Francisco.\nWhite, D. J. (1985). Real applications of Markov decision processes. Inter-\nfaces , 15:73{83.\nWhite, D. J. (1988). Further real applications of Markov decision processes.\nInterfaces , 18:55{61.\nWhite, D. J. (1993). A survey of applications of Markov decision processes.\nJournal of the Operational Research Society , 44:1073{1096.\nWhitehead, S. D., Ballard, D. H. (1991). Learning to perceive and act by trial\nand error. Machine Learning , 7:45{83.\nWhitt, W. (1978). Approximations of dynamic programs I. Mathematics of\nOperations Research , 3:231{243.\nWhittle, P. (1982). Optimization over Time , vol. 1. Wiley, New York.\nWhittle, P. (1983). Optimization over Time , vol. 2. Wiley, New York.\nWidrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with\na critic in adaptive threshold systems. IEEE Transactions on Systems,\nMan, and Cybernetics , 3:455{465.\nWidrow, B., Ho\u000b, M. E. (1960). Adaptive switching circuits. In 1960\nWESCON Convention Record Part IV , pp. 96{104. Institute of Radio\nEngineers, New York. Reprinted in J. A. Anderson and E. Rosenfeld,\nNeurocomputing: Foundations of Research , pp. 126{134. MIT Press, Cam-\nbridge, MA, 1988.\n336 CHAPTER 15. PROSPECTS\nWidrow, B., Smith, F. W. (1964). Pattern-recognizing control systems. In\nJ. T. Tou and R. H. Wilcox (eds.), Computer and Information Sciences ,\npp. 288{317. Spartan, Washington, DC.\nWidrow, B., Stearns, S. D. (1985). Adaptive Signal Processing . Prentice-Hall,\nEnglewood Cli\u000bs, NJ.\nWilliams, R. J. (1986). Reinforcement learning in connectionist networks: A\nmathematical analysis. Technical Report ICS 8605. Institute for Cognitive\nScience, University of California at San Diego, La Jolla.\nWilliams, R. J. (1987). Reinforcement-learning connectionist systems. Tech-\nnical Report NU-CCS-87-3. College of Computer Science, Northeastern\nUniversity, Boston.\nWilliams, R. J. (1988). On the use of backpropagation in associative rein-\nforcement learning. In Proceedings of the IEEE International Conference\non Neural Networks , pp. I263{I270. IEEE San Diego section and IEEE\nTAB Neural Network Committee.\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning. Machine Learning , 8:229{256.\nWilliams, R. J., Baird, L. C. (1990). A mathematical analysis of actor-critic\narchitectures for learning optimal controls through incremental dynamic\nprogramming. In Proceedings of the Sixth Yale Workshop on Adaptive\nand Learning Systems , pp. 96{101. Center for Systems Science, Dunham\nLaboratory, Yale University, New Haven.\nWilson, S. W. (1994). ZCS: A zeroth order classi\fer system. Evolutionary\nComputation , 2:1{18.\nWitten, I. H. (1976). The apparent con\rict between estimation and control|\nA survey of the two-armed problem. Journal of the Franklin Institute ,\n301:161{189.\nWitten, I. H. (1977). An adaptive optimal controller for discrete-time Markov\nenvironments. Information and Control , 34:286{295.\nWitten, I. H., Corbin, M. J. (1973). Human operators and automatic adaptive\ncontrollers: A comparative study on a particular control task. Interna-\ntional Journal of Man{Machine Studies , 5:75{104.\nWoodworth, R. S., Schlosberg, H. (1938). Experimental psychology . New York:\nHenry Holt and Company.\nYee, R. C., Saxena, S., Utgo\u000b, P. E., Barto, A. G. (1990). Explaining temporal\ndi\u000berences to create useful concepts for evaluating states. In Proceedings\nof the Eighth National Conference on Arti\fcial Intelligence , pp. 882{888.\n15.5. OTHER FRONTIER DIMENSIONS 337\nAAAI Press, Menlo Park, CA.\nYoung, P. (1984). Recursive Estimation and Time-Series Analysis . Springer-\nVerlag, Berlin.\nZhang, M., Yum, T. P. (1989). Comparisons of channel-assignment strategies\nin cellular mobile telephone systems. IEEE Transactions on Vehicular\nTechnology , 38:211-215.\nZhang, W. (1996). Reinforcement Learning for Job-shop Scheduling . Ph.D.\nthesis, Oregon State University. Technical Report CS-96-30-1.\nZhang, W., Dietterich, T. G. (1995). A reinforcement learning approach to\njob-shop scheduling. In Proceedings of the Fourteenth International Joint\nConference on Arti\fcial Intelligence , pp. 1114{1120. Morgan Kaufmann.\nZhang, W., Dietterich, T. G. (1996). High-performance job-shop scheduling\nwith a time{delay TD( \u0015) network. In D. S. Touretzky, M. C. Mozer,\nM. E. Hasselmo (eds.), Advances in Neural Information Processing Sys-\ntems: Proceedings of the 1995 Conference , pp. 1024{1030. MIT Press,\nCambridge, MA.\nZweben, M., Daun, B., Deale, M. (1994). Scheduling and rescheduling with\niterative repair. In M. Zweben and M. S. Fox (eds.), Intelligent Scheduling ,\npp. 241{255. Morgan Kaufmann, San Francisco.\n338 CHAPTER 15. PROSPECTS\nIndex\n"}